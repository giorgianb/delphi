Cover,"This page intentionally left blank SIGNALS & SYSTEMS PRENTICE HALL SIGNAL PROCESSING SERIES Alan V. Oppenheim, Series Editor ANDREWS & HUNT Digital Image Restoration BRACEWELL Two Dimensional Imaging BRIGHAM The Fast Fourier Transform and Its Applications BuRDIC Underwater Acoustic System Analysis 2/E CASTLEMAN Digital Image Processing CoHEN Time-Frequency Analysis CROCHIERE & RABINER Multirate Digital Signal Processing DuDGEON & MERSEREAU Multidimensional Digital Signal Processing HAYKIN Advances in Spectrum Analysis and Array Processing. Vols. I, II & III HAYKIN, Eo. Array Signal Processing JoHNSON & DuDGEON Array Signal Processing KAY Fundamentals of Statistical Signal Processing KAY Modern Spectral Estimation KINO Acoustic Waves: Devices, Imaging, and Analog Signal Processing LIM Two-Dimensional Signal and Image Processing LIM, Eo. Speech Enhancement LIM & OPPENHEIM, Eos. Advanced Topics in Signal Processing MARPLE Digital Spectral Analysis with Applications MccLELLAN & RADER Number Theory in Digital Signal Processing MENDEL Lessons in Estimation Theory for Signal Processing Communications and Control 2/E NIKIAS & PETROPULU Higher Order Spectra Analysis OPPENHEIM & NAWAB Symbolic and Knowledge-Based Signal Processing OPPENHEIM & WILLSKY, WITH NAWAB Signals and Systems, 2/E OPPENHEIM & ScHAFER Digital Signal Processing OPPENHEIM & ScHAFER Discrete-Time Signal Processing 0RFANIDIS Signal Processing PHILLIPS & NAGLE Digital Control Systems Analysis and Design, 3/E PICINBONO Random Signals and Systems RABINER & GoLD Theory and Applications of Digital Signal Processing RABINER & SCHAFER Digital Processing of Speech Signals RABINER & JuANG Fundamentals of Speech Recognition RoBINSON & TREITEL Geophysical Signal Analysis STEARNS & DAVID Signal Processing Algorithms in Fortran and C STEARNS & DAVID Signal Processing Algorithms in MATIAB TEKALP Digital Video Processing THERRIEN Discrete Random Signals and Statistical Signal Processing TRIBOLET Seismic Applications of Homomorphic Signal Processing VETTERLI & KovACEVIC Wavelets and Subband Coding VIADYANATHAN Multirate Systems and Filter Banks WIDROW & STEARNS Adaptive Signal Processing SECOND EDITION SIGNALS & SYSTEMS ALAN V. OPPENHEIM ALAN S. WILLSKY MASSACHUSETTS INSTITUTE OF TECHNOLOGY WITH S. HAMID NAWAB BOSTON UNIVERSITY PRENTICE HALL UPPER SADDLE RIVER, NEW JERSEY 07458 Library of Congress Cataloging-in-Publication Data Oppenheim, Alan V. Signals and systems / Alan V. Oppenheim, Alan S. Willsky, with S. Hamid Nawab. - 2nd ed. p. cm. - Prentice-Hall signal processing series Includes bibliographical references and index. ISBN 0-13-814757-4 l. System analysis. 2. Signal theory (Telecommunication) I. Willsky, Alan S. II. Nawab, Syed Hamid. III. Title. IV. Series QA402.063 1996 621.382'23–dc20 96-19945 CIP Acquisitions editor: Tom Robbins Production service: TKM Productions Editorial/production supervision: Sharyn Vitrano Copy editor: Brian Baker Interior and cover design: Patrice Van Acker Art director: Amy Rosen Managing editor: Bayani Mendoza DeLeon Editor-in-Chief: Marcia Horton Director of production and manufacturing: David W. Riccardi Manufacturing buyer: Donna Sullivan Editorial assistant: Phyllis Morgan © 1997 by Alan V. Oppenheim and Alan S. Willsky © 1983 by Alan V. Oppenheim, Alan S. Willsky, and Ian T. Young Published by Prentice-Hall, Inc. Simon & Schuster / A Viacom Company Upper Saddle River, New Jersey 07458 Printed in the United States of America 10 9 8 7 6 5 4 ISBN 0-13–814757–4 Prentice-Hall International (UK) Limited, London Prentice-Hall of Australia Pty. Limited, Sydney Prentice-Hall Canada Inc., Toronto Prentice-Hall Hispanoamericana, S.A., Mexico Prentice-Hall of India Private Limited, New Delhi Prentice-Hall of Japan, Inc., Tokyo Simon & Schuster Asia Pte. Ltd., Singapore Editora Prentice-Hall do Brasil, Ltda., Rio de Janeiro To Phyllis, Jason, and Justine To Susanna, Lydia, and Kate CONTENTS PREFACE XVII ACKNOWLEDGEMENTS XXV FOREWORD XXVII 1 SIGNALS AND SYSTEMS 1 1.0 Introduction 1 1.1 Continuous-Time and Discrete-Time Signals 1 1.1.1 Examples and Mathematical Representation 1 1.1.2 Signal Energy and Power 5 1.2 Transformations of the Independent Variable 7 1.2.1 Examples of Transformations of the Independent Variable 8 1.2.2 Periodic Signals 11 1.2.3 Even and Odd Signals 13 1.3 Exponential and Sinusoidal Signals 14 1.3.1 Continuous-Time Complex Exponential and Sinusoidal Signals 15 1.3.2 Discrete-Time Complex Exponential and Sinusoidal Signals 21 1.3.3 Periodicity Properties of Discrete-Time Complex Exponentials 25 1.4 The Unit Impulse and Unit Step Functions 30 1.4.1 The Discrete-Time Unit Impulse and Unit Step Sequences 30 1.4.2 The Continuous-Time Unit Step and Unit Impulse Functions 32 1.5 Continuous-Time and Discrete-Time Systems 38 1.5.1 Simple Examples of Systems 39 1.5.2 Interconnections of Systems 41 1.6 Basic System Properties 44 1.6.1 Systems with and without Memory 44 1.6.2 Invertibility and Inverse Systems 45 1.6.3 Causality 46 1.6.4 Stability 48 1.6.5 Time Invariance 50 1.6.6 Linearity 53 1.7 Summary 56 Problems 57 2 LINEAR TIME-INVARIANT SYSTEMS 74 2.0 Introduction 74 2.1 Discrete-Time LTI Systems: The Convolution Sum 75 vii"
Contents,"CONTENTS PREFACE XVII ACKNOWLEDGEMENTS XXV FOREWORD XXVII 1 SIGNALS AND SYSTEMS 1 1.0 Introduction 1 1.1 Continuous-Time and Discrete-Time Signals 1 1.1.1 Examples and Mathematical Representation 1 1.1.2 Signal Energy and Power 5 1.2 Transformations of the Independent Variable 7 1.2.1 Examples of Transformations of the Independent Variable 8 1.2.2 Periodic Signals 11 1.2.3 Even and Odd Signals 13 1.3 Exponential and Sinusoidal Signals 14 1.3.1 Continuous-Time Complex Exponential and Sinusoidal Signals 15 1.3.2 Discrete-Time Complex Exponential and Sinusoidal Signals 21 1.3.3 Periodicity Properties of Discrete-Time Complex Exponentials 25 1.4 The Unit Impulse and Unit Step Functions 30 1.4.1 The Discrete-Time Unit Impulse and Unit Step Sequences 30 1.4.2 The Continuous-Time Unit Step and Unit Impulse Functions 32 1.5 Continuous-Time and Discrete-Time Systems 38 1.5.1 Simple Examples of Systems 39 1.5.2 Interconnections of Systems 41 1.6 Basic System Properties 44 1.6.1 Systems with and without Memory 44 1.6.2 Invertibility and Inverse Systems 45 1.6.3 Causality 46 1.6.4 Stability 48 1.6.5 Time Invariance 50 1.6.6 Linearity 53 1.7 Summary 56 Problems 57 2 LINEAR TIME-INVARIANT SYSTEMS 74 2.0 Introduction 74 2.1 Discrete-Time LTI Systems: The Convolution Sum 75 vii viii Contents 2.1.1 The Representation of Discrete-Time Signals in Terms of Impulses 75 2.1.2 The Discrete-Time Unit Impulse Response and the Convolution-Sum Representation of LTI Systems 77 2.2 Continuous-Time LTI Systems: The Convolution Integral 90 2.2.1 The Representation of Continuous-Time Signals in Terms of Impulses 90 2.2.2 The Continuous-Time Unit Impulse Response and the Convolution Integral Representation of LTI Systems 94 2.3 Properties of Linear Time-Invariant Systems 103 2.3.1 The Commutative Property 104 2.3.2 The Distributive Property 104 2.3.3 The Associative Property 107 2.3.4 LTI Systems with and without Memory 108 2.3.5 Invertibility of LTI Systems 109 2.3.6 Causality for LTI Systems 112 2.3.7 Stability for LTI Systems 113 2.3.8 The Unit Step Response of an LTI System 115 2.4 Causal LTI Systems Described by Differential and Difference Equations 116 2.4.1 Linear Constant -Coefficient Differential Equations 117 2.4.2 Linear Constant-Coefficient Difference Equations 121 2.4.3 Block Diagram Representations of First-Order Systems Described by Differential and Difference Equations 124 2.5 Singularity Functions 127 2.5.1 The Unit Impulse as an Idealized Short Pulse 128 2.5.2 Defining the Unit Impulse through Convolution 131 2.5.3 Unit Doublets and Other Singularity Functions 132 2.6 Summary 137 Problems 137 3 FOURIER SERIES REPRESENTATION OF PERIODIC SIGNALS 177 3.0 Introduction 177 3.1 A Historical Perspective 178 3.2 The Response of LTI Systems to Complex Exponentials 182 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 186 3.3.1 Linear Combinations of Harmonically Related Complex Exponentials 186 3.3.2 Determination of the Fourier Series Representation of a Continuous-Time Periodic Signal 190 3.4 Convergence of the Fourier Series 195 3.5 Properties of Continuous-Time Fourier Series 202 3.5.1 Linearity 202 Contents ix 3.5.2 Time Shifting 202 3.5.3 Time Reversal 203 3.5.4 Time Scaling 204 3.5.5 Multiplication 204 3.5.6 Conjugation and Conjugate Symmetry 204 Parseval's Relation for Continuous-Time Periodic Signals 3.5.7 205 3.5.8 Summary of Properties of the Continuous-Time Fourier Series 205 3.5.9 Examples 205 3.6 Fourier Series Representation of Discrete-Time Periodic Signal 211 3.6.1 Linear Combinations of Harmonically Related Complex Exponentials 211 3.6.2 Determination of the Fourier Series Representation of a Periodic Signal 212 3.7 Properties of Discrete-Time Fourier Series 221 3.7.1 Multiplication 222 3.7.2 First Difference 222 3.7.3 Parseval's Relation for Discrete-Time Periodic Signals 223 3.7.4 Examples 223 3.8 Fourier Series and LTI Systems 226 3.9 Filtering 231 3.9.1 Frequency-Shaping Filters 232 3.9.2 Frequency-Selective Filters 236 3.10 Examples of Continuous-Time Filters Described by Differential Equations 239 3.10.1 A Simple RC Lowpass Filter 239 3.10.2 A Simple RC Highpass Filter 241 3.11 Examples of Discrete-Time Filters Described by Difference Equations 244 3.11.1 First-Order Recursive Discrete-Time Filters 244 3.11.2 Nonrecursive Discrete-Time Filters 245 3.12 Summary 249 Problems 250 4 THE CONTINUOUS-TIME FOURIER TRANSFORM 284 4.0 Introduction 284 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 285 4.1.1 Development of the Fourier Transform Representation of an Aperiodic Signal 285 4.1.2 Convergence of Fourier Transforms 289 4.1.3 Examples of Continuous-Time Fourier Transforms 290 4.2 The Fourier Transform for Periodic Signals 296 4.3 Properties of the Continuous-Time Fourier Transform 300 4.3.1 Linearity 301 x Contents 4.3.2 Time Shifting 301 4.3.3 Conjugation and Conjugate Symmetry 303 4.3.4 Differentiation and Integration 306 4.3.5 Time and Frequency Scaling 308 4.3.6 Duality 309 4.3.7 Parseval's Relation 312 4.4 The Convolution Property 314 4.4.1 Examples 317 4.5 The Multiplication Property 322 4.5.1 Frequency-Selective Filtering with Variable Center Frequency 325 4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs 328 4.7 Systems Characterized by Linear Constant-Coefficient Differential Equations 330 4.8 Summary 333 Problems 334 5 THE DISCRETE-TIME FOURIER TRANSFORM 358 5.0 Introduction 358 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 359 5.1.1 Development of the Discrete-Time Fourier Transform 359 5.1.2 Examples of Discrete-Time Fourier Transforms 362 5.1.3 Convergence Issues Associated with the Discrete-Time Fourier Transform 366 5.2 The Fourier Transform for Periodic Signals 367 5.3 Properties of the Discrete-Time Fourier Transform 372 5.3.1 Periodicity of the Discrete-Time Fourier Transform 373 5.3.2 Linearity of the Fourier Transform 373 5.3.3 Time Shifting and Frequency Shifting 373 5.3.4 Conjugation and Conjugate Symmetry 375 5.3.5 Differencing and Accumulation 375 5.3.6 Time Reversal 376 5.3.7 Time Expansion 377 5.3.8 Differentiation in Frequency 380 5.3.9 Parseval's Relation 380 5.4 The Convolution Property 382 5.4.1 Examples 383 5.5 The Multiplication Property 388 5.6 Tables of Fourier Transform Properties and Basic Fourier Transform Pairs 390 5.7 Duality 390 5.7.1 Duality in the Discrete-Time Fourier Series 391 5.7.2 Duality between the Discrete-Time Fourier Transform and the Continuous-Time Fourier Series 395 Contents xi 5.8 Systems Characterized by Linear Constant-Coefficient Difference Equations 396 5.9 Summary 399 Problems 400 6 TIME AND FREQUENCY CHARACTERIZATION OF SIGNALS AND SYSTEMS 423 6.0 Introduction 423 6.1 The Magnitude-Phase Representation of the Fourier Transform 423 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 427 6.2.1 Linear and Nonlinear Phase 428 6.2.2 Group Delay 430 6.2.3 Log-Magnitude and Bode Plots 436 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 439 6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters 444 6.5 First-Order and Second-Order Continuous-Time Systems 448 6.5.1 First-Order Continuous-Time Systems 448 6.5.2 Second-Order Continuous-Time Systems 451 6.5.3 Bode Plots for Rational Frequency Responses 456 6.6 First-Order and Second-Order Discrete-Time Systems 461 6.6.1 First-Order Discrete-Time Systems 461 6.6.2 Second-Order Discrete-Time Systems 465 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 472 6.7.1 Analysis of an Automobile Suspension System 473 6.7.2 Examples of Discrete-Time Nonrecursive Filter 476 6.8 Summary 482 Problems 483 7 SAMPLING 514 7.0 Introduction 514 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 515 7.1.1 Impulse-Train Sampling 516 7.1.2 Sampling with a Zero-Order Hold 520 7.2 Reconstruction of a Signal from Its Samples Using Interpolation 522 7.3 The Effect of Undersampling: Aliasing 527 7.4 Discrete-Time Processing of Continuous-Time Signals 534 7.4.1 Digital Differentiator 541 7.4.2 Half-Sample Delay 543 xii Contents 7.5 Sampling of Discrete-Time Signals 545 7.5.1 Impulse-Train Sampling 545 7.5.2 Discrete-Time Decimation and Interpolation 549 7.6 Summary 555 Problems 556 8 COMMUNICATION SYSTEMS 582 8.0 Introduction 582 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 583 8.1.1 Amplitude Modulation with a Complex Exponential Carrier 583 8.1.2 Amplitude Modulation with a Sinusoidal Carrier 585 8.2 Demodulation for Sinusoidal AM 587 8.2.1 Synchronous Demodulation 587 8.2.2 Asynchronous Demodulation 590 8.3 Frequency-Division Multiplexing 594 8.4 Single-Sideband Sinusoidal Amplitude Modulation 597 8.5 Amplitude Modulation with a Pulse-Train Carrier 601 8.5.1 Modulation of a Pulse-Train Carrier 601 8.5.2 Time-Division Multiplexing 604 8.6 Pulse-Amplitude Modulation 604 8.6.1 Pulse-Amplitude Modulated Signals 604 8.6.2 Intersymbol Interference in PAM Systems 607 8.6.3 Digital Pulse-Amplitude and Pulse-Code Modulation 610 8.7 Sinusoidal Frequency Modulation 611 8.7.1 Narrowband Frequency Modulation 613 8.7.2 Wideband Frequency Modulation 615 8.7.3 Periodic Square-Wave Modulating Signal 617 8.8 Discrete-Time Modulation 619 8.8.1 Discrete-Time Sinusoidal Amplitude Modulation 619 8.8.2 Discrete-Time Transmodulation 623 8.9 Summary 623 Problems 625 9 THE LAPLACE TRANSFORM 654 9.0 Introduction 654 9.1 The Laplace Transform 655 9.2 The Region of Convergence for Laplace Transforms 662 9.3 The Inverse Laplace Transform 670 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 674 9.4.1 First-Order Systems 676 9.4.2 Second-Order Systems 677 9.4.3 All-Pass Systems 681 9.5 Properties of the Laplace Transform 682 9.5.1 Linearity of the Laplace Transform 683 9.5.2 Time Shifting 684 Contents xiii 9.5.3 Shifting in the s-Domain 685 9.5.4 Time Scaling 685 9.5.5 Conjugation 687 9.5.6 Convolution Property 687 9.5.7 Differentiation in the Time Domain 688 9.5.8 Differentiation in the s-Domain 688 9.5.9 Integration in the Time Domain 690 9.5.10 The Initial- and Final-Value Theorems 690 9.5.11 Table of Properties 691 9.6 Some Laplace Transform Pairs 692 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 693 9.7.1 Causality 693 9.7.2 Stability 695 9.7.3 LTI Systems Characterized by Linear Constant-Coefficient Differential Equations 698 9.7.4 Examples Relating System Behavior to the System Function 701 9.7.5 Butterworth Filters 703 9.8 System Function Algebra and Block Diagram Representations 706 9.8.1 System Functions for Interconnections of LTI Systems 707 9.8.2 Block Diagram Representations for Causal LTI Systems Described by Differential Equations and Rational System Functions 708 9.9 The Unilateral Laplace Transform 714 9.9.1 Examples of Unilateral Laplace Transforms 714 9.9.2 Properties of the Unilateral Laplace Transform 716 9.9.3 Solving Differential Equations Using the Unilateral Laplace Transform 719 9.10 Summary 720 Problems 721 10 THE Z-TRANSFORM 741 10.0 Introduction 741 10.1 The z-Transform 741 10.2 The Region of Convergence for the z-Transform 748 10.3 The Inverse z-Transform 757 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 763 10.4.1 First-Order Systems 763 10.4.2 Second-Order Systems 765 10.5 Properties of the z-Transform 767 10.5.1 Linearity 767 10.5.2 Time Shifting 767 10.5.3 Scaling in the z-Domain 768 10.5.4 Time Reversal 769 10.5.5 Time Expansion 769 xiv Contents 10.5.6 Conjugation 770 10.5.7 The Convolution Property 770 10.5.8 Differentiation in the z-Domain 772 10.5.9 The Initial-Value Theorem 773 10.5.10 Summary of Properties 774 10.6 Some Common z-Transform Pairs 774 10.7 Analysis and Characterization of LTI Systems Using z-Transforms 774 10.7.1 Causality 776 10.7.2 Stability 777 10.7.3 LTI Systems Characterized by Linear Constant-Coefficient Difference Equations 779 10.7.4 Examples Relating System Behavior to the System Function 781 10.8 System Function Algebra and Block Diagram Representations 783 10.8.1 System Functions for Interconnections of LTI Systems 784 10.8.2 Block Diagram Representations for Causal LTI Systems Described by Difference Equations and Rational System Functions 784 10.9 The Unilateral z-Transform 789 10.9 .1 Examples of Unilateral z-Transforms and Inverse Transforms 790 10.9 .2 Properties of the Unilateral z-Transform 792 10.9 .3 Solving Difference Equations Using the Unilateral z-Transform 795 10.10 Summary 796 Problems 797 11 LINEAR FEEDBACK SYSTEMS 816 11.0 Introduction 816 11.1 Linear Feedback Systems 819 11.2 Some Applications and Consequences of Feedback 820 11.2.1 Inverse System Design 820 11.2.2 Compensation for Nonideal Elements 821 11.2.3 Stabilization of Unstable Systems 823 11.2.4 Sampled-Data Feedback Systems 826 11.2.5 Tracking Systems 828 11.2.6 Destabilization Caused by Feedback 830 11.3 Root-Locus Analysis of Linear Feedback Systems 832 11.3.1 An Introductory Example 833 11.3.2 Equation for the Closed-Loop Poles 834 11.3.3 The End Points of the Root Locus: The Closed-Loop Poles for K = 0 and |K| = +∞ 836 11.3.4 The Angle Criterion 836 11.3.5 Properties of the Root Locus 841 11.4 The Nyquist Stability Criterion 846 11.4.1 The Encirclement Property 847 Contents xv 11.4.2 The Nyquist Criterion for Continuous-Time LTI Feedback Systems 850 11.4.3 The Nyquist Criterion for Discrete-Time LTI Feedback Systems 856 11.5 Gain and Phase Margins 858 11.6 Summary 866 Problems 867 APPENDIX PARTIAL-FRACTION EXPANSION 909 BIBLIOGRAPHY 921 ANSWERS 931 INDEX 941 This page intentionally left blank PREFACE This book is the second edition of a text designed for undergraduate courses in signals and systems. While such courses are frequently found in electrical engineering curricula, the concepts and techniques that form the core of the subject are of fundamental importance in all engineering disciplines. In fact, the scope of potential and actual applications of the methods of signal and system analysis continues to expand as engineers are confronted with new challenges involving the synthesis or analysis of complex processes. For these reasons we feel that a course in signals and systems not only is an essential element in an engineer- ing program but also can be one of the most rewarding, exciting, and useful courses that engineering students take during their undergraduate education. Our treatment of the subject of signals and systems in this second edition maintains the same general philosophy as in the first edition but with significant rewriting, restructuring, and additions. These changes are designed to help both the instructor in presenting the sub- ject material and the student in mastering it. In the preface to the first edition we stated that our overall approach to signals and systems had been guided by the continuing develop- ments in technologies for signal and system design and implementation, which made it in- creasingly important for a student to have equal familiarity with techniques suitable for analyzing and synthesizing both continuous-time and discrete-time systems. As we write the preface to this second edition, that observation and guiding principle are even more true than before. Thus, while students studying signals and systems should certainly have a solid foundation in disciplines based on the laws of physics, they must also have a firm grounding in the use of computers for the analysis of phenomena and the implementation of systems and algorithms. As a consequence, engineering curricula now reflect a blend of subjects, some involving continuous-time models and others focusing on the use of computers and discrete representations. For these reasons, signals and systems courses that bring discrete- time and continuous-time concepts together in a unified way play an increasingly important role in the education of engineering students and in their preparation for current and future developments in their chosen fields. It is with these goals in mind that we have structured this book to develop in parallel the methods of analysis for continuous-time and discrete-time signals and systems. This ap- proach also offers a distinct and extremely important pedagogical advantage. Specifically, we are able to draw on the similarities between continuous- and discrete-time methods in order to share insights and intuition developed in each domain. Similarly, we can exploit the differences between them to sharpen an understanding of the distinct properties of each. In organizing the material both originally and now in the second edition, we have also considered it essential to introduce the student to some of the important uses of the basic methods that are developed in the book. Not only does this provide the student with an appreciation for the range of applications of the techniques being learned and for directions for further study, but it also helps to deepen understanding of the subject. To achieve this xvii"
Preface,"PREFACE This book is the second edition of a text designed for undergraduate courses in signals and systems. While such courses are frequently found in electrical engineering curricula, the concepts and techniques that form the core of the subject are of fundamental importance in all engineering disciplines. In fact, the scope of potential and actual applications of the methods of signal and system analysis continues to expand as engineers are confronted with new challenges involving the synthesis or analysis of complex processes. For these reasons we feel that a course in signals and systems not only is an essential element in an engineer- ing program but also can be one of the most rewarding, exciting, and useful courses that engineering students take during their undergraduate education. Our treatment of the subject of signals and systems in this second edition maintains the same general philosophy as in the first edition but with significant rewriting, restructuring, and additions. These changes are designed to help both the instructor in presenting the sub- ject material and the student in mastering it. In the preface to the first edition we stated that our overall approach to signals and systems had been guided by the continuing develop- ments in technologies for signal and system design and implementation, which made it in- creasingly important for a student to have equal familiarity with techniques suitable for analyzing and synthesizing both continuous-time and discrete-time systems. As we write the preface to this second edition, that observation and guiding principle are even more true than before. Thus, while students studying signals and systems should certainly have a solid foundation in disciplines based on the laws of physics, they must also have a firm grounding in the use of computers for the analysis of phenomena and the implementation of systems and algorithms. As a consequence, engineering curricula now reflect a blend of subjects, some involving continuous-time models and others focusing on the use of computers and discrete representations. For these reasons, signals and systems courses that bring discrete- time and continuous-time concepts together in a unified way play an increasingly important role in the education of engineering students and in their preparation for current and future developments in their chosen fields. It is with these goals in mind that we have structured this book to develop in parallel the methods of analysis for continuous-time and discrete-time signals and systems. This ap- proach also offers a distinct and extremely important pedagogical advantage. Specifically, we are able to draw on the similarities between continuous- and discrete-time methods in order to share insights and intuition developed in each domain. Similarly, we can exploit the differences between them to sharpen an understanding of the distinct properties of each. In organizing the material both originally and now in the second edition, we have also considered it essential to introduce the student to some of the important uses of the basic methods that are developed in the book. Not only does this provide the student with an appreciation for the range of applications of the techniques being learned and for directions for further study, but it also helps to deepen understanding of the subject. To achieve this xvii xviii Preface goal we include introductory treatments on the subjects of filtering, communications, sam- pling, discrete-time processing of continuous-time signals, and feedback. In fact, in one of the major changes in this second edition, we have introduced the concept of frequency- domain filtering very early in our treatment of Fourier analysis in order to provide both motivation for and insight into this very important topic. In addition, we have again included an up-to-date bibliography at the end of the book in order to assist the student who is inter- ested in pursuing additional and more advanced studies of the methods and applications of signal and system analysis. The organization of the book reflects our conviction that full mastery of a subject of this nature cannot be accomplished without a significant amount of practice in using and apply- ing the tools that are developed. Consequently, in the second edition we have significantly increased the number of worked examples within each chapter. We have also enhanced one of the key assets of the first edition, namely the end-of-chapter homework problems. As in the first edition, we have included a substantial number of problems, totaling more than 600 in number. A majority of the problems included here are new and thus provide additional flexibility for the instructor in preparing homework assignments. In addition, in order to enhance the utility of the problems for both the student and the instructor we have made a number of other changes to the organization and presentation of the problems. In particular, we have organized the problems in each chapter under several specific headings, each of which spans the material in the entire chapter but with a different objective. The first two sections of problems in each chapter emphasize the mechanics of using the basic concepts and methods presented in the chapter. For the first of these two sections, which has the heading Basic Problems with Answers, we have also provided an- swers (but not solutions) at the end of the book. These answers provide a simple and imme- diate way for the student to check his or her understanding of the material. The problems in this first section are generally appropriate for inclusion in homework sets. Also, in order to give the instructor additional flexibility in assigning homework problems, we have provided a second section of Basic Problems for which answers have not been included. A third section of problems in each chapter, organized under the heading of Advanced Problems, is oriented toward exploring and elaborating upon the foundations and practical implications of the material in the text. These problems often involve mathematical deriva- tions and more sophisticated use of the concepts and methods presented in the chapter. Some chapters also include a section of Extension Problems which involve extensions of material presented in the chapter and/or involve the use of knowledge from applications that are outside the scope of the main text (such as advanced circuits or mechanical systems). The overall variety and quantity of problems in each chapter will hopefully provide students with the means to develop their understanding of the material and instructors with consid- erable flexibility in putting together homework sets that are tailored to the specific needs of their students. A solutions manual is also available to instructors through the publisher. Another significant additional enhancement to this second edition is the availability of the companion book Explorations in Signals and Systems Using MATLAB by Buck, Daniel, and Singer. This book contains MATLAB™-based computer exercises for each topic in the text, and should be of great assistance to both instructor and student. Preface xix Students using this book are assumed to have a basic background in calculus as well as some experience in manipulating complex numbers and some exposure to differential equa- tions. With this background, the book is self-contained. In particular, no prior experience with system analysis, convolution, Fourier analysis, or Laplace and z-transforms is as- sumed. Prior to learning the subject of signals and systems most students will have had a course such as basic circuit theory for electrical engineers or fundamentals of dynamics for mechanical engineers. Such subjects touch on some of the basic ideas that are developed more fully in this text. This background can clearly be of great value to students in providing additional perspective as they proceed through the book. The Foreword, which follows this preface, is written to offer the reader motivation and perspective for the subject of signals and systems in general and our treatment of it in par- ticular. We begin Chapter 1 by introducing some of the elementary ideas related to the mathematical representation of signals and systems. In particular we discuss transfor- mations (such as time shifts and scaling) of the independent variable of a signal. We also introduce some of the most important and basic continuous-time and discrete-time signals, namely real and complex exponentials and the continuous-time and discrete-time unit step and unit impulse. Chapter 1 also introduces block diagram representations of interconnec- tions of systems and discusses several basic system properties such as causality, linearity and time-invariance. In Chapter 2 we build on these last two properties, together with the sifting property of unit impulses to develop the convolution-sum representation for discrete- time linear, time-invariant (LTI) systems and the convolution integral representation for continuous-time LTI systems. In this treatment we use the intuition gained from our devel- opment of the discrete-time case as an aid in deriving and understanding its continuous- time counterpart. We then turn to a discussion of causal, LTI systems characterized by linear constant-coefficient differential and difference equations. In this introductory discussion we review the basic ideas involved in solving linear differential equations (to which most stu- dents will have had some previous exposure) and we also provide a discussion of analogous methods for linear difference equations. However, the primary focus of our development in Chapter 2 is not on methods of solution, since more convenient approaches are developed later using transform methods. Instead, in this first look, our intent is to provide the student with some appreciation for these extremely important classes of systems, which will be encountered often in subsequent chapters. Finally, Chapter 2 concludes with a brief discus- sion of singularity functions—steps, impulses, doublets, and so forth—in the context of their role in the description and analysis of continuous-time LTI systems. In particular, we stress the interpretation of these signals in terms of how they are defined under convolu- tion—that is, in terms of the responses of LTI systems to these idealized signals. Chapters 3 through 6 present a thorough and self-contained development of the methods of Fourier analysis in both continuous and discrete time and together represent the most significant reorganization and revision in the second edition. In particular, as we indicated previously, we have introduced the concept of frequency-domain filtering at a much earlier point in the development in order to provide motivation for and a concrete application of the Fourier methods being developed. As in the first edition, we begin the discussions in Chapter 3 by emphasizing and illustrating the two fundamental reasons for the important xx Preface role Fourier analysis plays in the study of signals and systems in both continuous and dis- crete time: (1) extremely broad classes of signals can be represented as weighted sums or integrals of complex exponentials; and (2) the response of an LTI system to a complex exponential input is the same exponential multiplied by a complex-number characteristic of the system. However, in contrast to the first edition, the focus of attention in Chapter 3 is on Fourier series representations for periodic signals in both continuous time and discrete time. In this way we not only introduce and examine many of the properties of Fourier representations without the additional mathematical generalization required to obtain the Fourier transform for aperiodic signals, but we also can introduce the application to filtering at a very early stage in the development. In particular, taking advantage of the fact that complex exponentials are eigenfunctions of LTI systems, we introduce the frequency re- sponse of an LTI system and use it to discuss the concept of frequency-selective filtering, to introduce ideal filters, and to give several examples of nonideal filters described by dif- ferential and difference equations. In this way, with a minimum of mathematical prelimi- naries, we provide the student with a deeper appreciation for what a Fourier representation means and why it is such a useful construct. Chapters 4 and 5 then build on the foundation provided by Chapter 3 as we develop first the continuous-time Fourier transform in Chapter 4 and, in a parallel fashion, the discrete- time Fourier transform in Chapter 5. In both chapters we derive the Fourier transform rep- resentation of an aperiodic signal as the limit of the Fourier series for a signal whose period becomes arbitrarily large. This perspective emphasizes the close relationship between Fou- rier series and transforms, which we develop further in subsequent sections and which al- lows us to transfer the intuition developed for Fourier series in Chapter 3 to the more general context of Fourier transforms. In both chapters we have included a discussion of the many important properties of Fourier transforms, with special emphasis placed on the convolution and multiplication properties. In particular, the convolution property allows us to take a second look at the topic of frequency-selective filtering, while the multiplication property serves as the starting point for our treatment of sampling and modulation in later chapters. Finally, in the last sections in Chapters 4 and 5 we use transform methods to determine the frequency responses of LTI systems described by differential and difference equations and to provide several examples illustrating how Fourier transforms can be used to compute the responses for such systems. To supplement these discussions (and later treatments of La- place and z-transforms) we have again included an Appendix at the end of the book that contains a description of the method of partial fraction expansion. Our treatment of Fourier analysis in these two chapters is characteristic of the parallel treatment we have developed. Specifically, in our discussion in Chapter 5, we are able to build on much of the insight developed in Chapter 4 for the continuous-time case, and to- ward the end of Chapter 5 we emphasize the complete duality in continuous-time and dis- crete-time Fourier representations. In addition, we bring the special nature of each domain into sharper focus by contrasting the differences between continuous- and discrete-time Fourier analysis. As those familiar with the first edition will note, the lengths and scopes of Chapters 4 and 5 in the second edition are considerably smaller than their first edition counterparts. This is due not only to the fact that Fourier series are now dealt with in a separate chapter but also to our moving several topics into Chapter 6. The result, we believe, has several Preface xxi significant benefits. First, the presentation in three shorter chapters of the basic concepts and results of Fourier analysis, together with the introduction of the concept of frequency- selective filtering, should help the student in organizing his or her understanding of this material and in developing some intuition about the frequency domain and appreciation for its potential applications. Then, with Chapters 3-5 as a foundation, we can engage in a more detailed look at a number of important topics and applications. In Chapter 6 we take a deeper look at both the time- and frequency-domain characteristics of LTI systems. For example, we introduce magnitude-phase and Bode plot representations for frequency responses and discuss the effect of frequency response phase on the time domain characteristics of the output of an LTI system. In addition, we examine the time- and frequency-domain behavior of ideal and nonideal filters and the tradeoffs between these that must be addressed in prac- tice. We also take a careful look at first- and second-order systems and their roles as basic building blocks for more complex system synthesis and analysis in both continuous and discrete time. Finally, we discuss several other more complex examples of filters in both continuous and discrete time. These examples together with the numerous other aspects of filtering explored in the problems at the end of the chapter provide the student with some appreciation for the richness and flavor of this important subject. While each of the topics in Chapter 6 was present in the first edition, we believe that by reorganizing and collecting them in a separate chapter following the basic development of Fourier analysis, we have both simplified the introduction of this important topic in Chapters 3-5 and presented in Chapter 6 a considerably more cohesive picture of time- and frequency-domain issues. In response to suggestions and preferences expressed by many users of the first edition we have modified notation in the discussion of Fourier transforms to be more consistent with notation most typically used for continuous-time and discrete-time Fourier transforms. Specifically, beginning with Chapter 3 we now denote the continuous-time Fourier trans- form as X( jω ) and the discrete-time Fourier transform as X(e jω). As with all options with notation, there is not a unique best choice for the notation for Fourier transforms. However, it is our feeling, and that of many of our colleagues, that the notation used in this edition represents the preferable choice. Our treatment of sampling in Chapter 7 is concerned primarily with the sampling theo- rem and its implications. However, to place this subject in perspective we begin by discuss- ing the general concepts of representing a continuous-time signal in terms of its samples and the reconstruction of signals using interpolation. After using frequency-domain meth- ods to derive the sampling theorem, we consider both the frequency and time domains to provide intuition concerning the phenomenon of aliasing resulting from undersampling. One of the very important uses of sampling is in the discrete-time processing of continuous- time signals, a topic that we explore at some length in this chapter. Following this, we turn to the sampling of discrete-time signals. The basic result underlying discrete-time sampling is developed in a manner that parallels that used in continuous time, and the applications of this result to problems of decimation and interpolation are described. Again a variety of other applications, in both continuous and discrete time, are addressed in the problems. Once again the reader acquainted with our first edition will note a change, in this case involving the reversal in the order of the presentation of sampling and communications. We have chosen to place sampling before communications in the second edition both because xxii Preface we can call on simple intuition to motivate and describe the processes of sampling and reconstruction from samples and also because this order of presentation then allows us in Chapter 8 to talk more easily about forms of communication systems that are closely related to sampling or rely fundamentally on using a sampled version of the signal to be transmitted. Our treatment of communications in Chapter 8 includes an in -depth discussion of con- tinuous-time sinusoidal amplitude modulation (AM), which begins with the straightforward application of the multiplication property to describe the effect of sinusoidal AM in the frequency domain and to suggest how the original modulating signal can be recovered. Fol- lowing this, we develop a number of additional issues and applications related to sinusoidal modulation, including frequency-division multiplexing and single-sideband modulation. Many other examples and applications are described in the problems. Several additional topics are covered in Chapter 8. The first of these is amplitude modulation of a pulse train and time-division multiplexing, which has a close connection to the topic of sampling in Chapter 7. Indeed we make this tie even more explicit and provide a look into the important field of digital communications by introducing and briefly describing the topics of pulse- amplitude modulation (PAM) and intersymbol interference. Finally, our discussion of fre- quency modulation (FM) provides the reader with a look at a nonlinear modulation problem. Although the analysis of FM systems is not as straightforward as for the AM case, our introductory treatment indicates how frequency-domain methods can be used to gain a sig- nificant amount of insight into the characteristics of FM signals and systems. Through these discussions and the many other aspects of modulation and communications explored in the problems in this chapter we believe that the student can gain an appreciation both for the richness of the field of communications and for the central role that the tools of signals and systems analysis play in it. Chapters 9 and 10 treat the Laplace and z-transforms, respectively. For the most part, we focus on the bilateral versions of these transforms, although in the last section of each chapter we discuss unilateral transforms and their use in solving differential and difference equations with nonzero initial conditions. Both chapters include discussions on: the close relationship between these transforms and Fourier transforms; the class of rational trans- forms and their representation in terms of poles and zeros; the region of convergence of a Laplace or z-transform and its relationship to properties of the signal with which it is asso- ciated; inverse transforms using partial fraction expansion; the geometric evaluation of sys- tem functions and frequency responses from pole-zero plots; and basic transform properties. In addition, in each chapter we examine the properties and uses of system functions for LTI systems. Included in these discussions are the determination of system functions for systems characterized by differential and difference equations; the use of system function algebra for interconnections of LTI systems; and the construction of cascade, parallel- and direct- form block-diagram representations for systems with rational system functions. The tools of Laplace and z-transforms form the basis for our examination of linear feed- back systems in Chapter 11. We begin this chapter by describing a number of the important uses and properties of feedback systems, including stabilizing unstable systems, designing tracking systems, and reducing system sensitivity. In subsequent sections we use the tools that we have developed in previous chapters to examine three topics that are of importance for both continuous-time and discrete-time feedback systems. These are root locus analysis, Preface xxiii Nyquist plots and the Nyquist criterion, and log-magnitude/phase plots and the concepts of phase and gain margins for stable feedback systems. The subject of signals and systems is an extraordinarily rich one, and a variety of ap- proaches can be taken in designing an introductory course. It was our intention with the first edition and again with this second edition to provide instructors with a great deal of flexi- bility in structuring their presentations of the subject. To obtain this flexibility and to max- imize the usefulness of this book for instructors, we have chosen to present thorough, in- depth treatments of a cohesive set of topics that forms the core of most introductory courses on signals and systems. In achieving this depth we have of necessity omitted introductions to topics such as descriptions of random signals and state space models that are sometimes included in first courses on signals and systems. Traditionally, at many schools, such topics are not included in introductory courses but rather are developed in more depth in follow- on undergraduate courses or in courses explicitly devoted to their investigation. Although we have not included an introduction to state space in the book, instructors of introductory courses can easily incorporate it into the treatments of differential and difference equations that can be found throughout the book. In particular, the discussions in Chapters 9 and I 0 on block diagram representations for systems with rational system functions and on unilat- eral transforms and their use in solving differential and difference equations with initial conditions form natural points of departure for the discussions of state-space representa- tions. A typical one-semester course at the sophomore-junior level using this book would cover Chapters 1-5 in reasonable depth (although various topics in each chapter are easily omitted at the discretion of the instructor) with selected topics chosen from the remaining chapters. For example, one possibility is to present several of the basic topics in Chapters 6-8 together with a treatment of Laplace and z-transforms and perhaps a brief introduction to the use of system function concepts to analyze feedback systems. A variety of alternate formats are possible, including one that incorporates an introduction to state space or one in which more focus is placed on continuous-time systems by de-emphasizing Chapters 5 and 10 and the discrete-time topics in Chapters 3, 7, 8, and 11. In addition to these course formats this book can be used as the basic text for a thorough, two-semester sequence on linear systems. Alternatively, the portions of the book not used in a first course on signals and systems can, together with other sources, form the basis for a subsequent course. For example, much of the material in this book forms a direct bridge to subjects such as state space analysis, control systems, digital signal processing, commu- nications and statistical signal processing. Consequently, a follow-on course can be con- structed that uses some of the topics in this book together with supplementary material in order to provide an introduction to one or more of these advanced subjects. In fact, a new course following this model has been developed at MIT and has proven not only to be a popular course among our students but also a crucial component of our signals and systems curriculum. As it was with the first edition, in the process of writing this book we have been fortunate to have received assistance, suggestions, and support from numerous colleagues, students and friends. The ideas and perspectives that form the heart of this book have continued to evolve as a result of our own experiences in teaching signals and systems and the influences xxiv Preface of the many colleagues and students with whom we have worked. We would like to thank Professor Ian T. Young for his contributions to the first edition of this book and to thank and welcome Professor Hamid Nawab for the significant role he played in the development and complete restructuring of the examples and problems for this second edition. We also express our appreciation to John Buck, Michael Daniel and Andrew Singer for writing the MATLAB companion to the text. In addition, we would like to thank Jason Oppenheim for the use of one of his original photographs and Vivian Berman for her ideas and help in arriving at a cover design. Also, as indicated on the acknowledgement page that follows, we are deeply grateful to the many students and colleagues who devoted a signifi- cant number of hours to a variety of aspects of the preparation of this second edition. We would also like to express our sincere thanks to Mr. Ray Stata and Analog Devices, Inc. for their generous and continued support of signal processing and this text through funding of the Distinguished Professor Chair in Electrical Engineering. We also thank M.I.T. for providing support and an invigorating environment in which to develop our ideas. The encouragement, patience, technical support, and enthusiasm provided by Prentice- Hall, and in particular by Marcia Horton, Tom Robbins, Don Fowley, and their predecessors and by Ralph Pescatore of TKM Productions and the production staff at Prentice-Hall, have been crucial in making this second edition a reality. Alan V. Oppenheim Alan S. Willsky Cambridge, Massachusetts AcKNOWLEDGMENTS In producing this second edition we were fortunate to receive the assistance of many col- leagues, students, and friends who were extremely generous with their time. We express our deep appreciation to: Jon Maiara and Ashok Popat for their help in generating many of the figures and images. Babak Ayazifar and Austin Frakt for their help in updating and assembling the bibliography. Ramamurthy Mani for preparing the solutions manual for the text and for his help in generating many of t.he figures. Michael Daniel for coordinating and managing the LaTeX files as the various drafts of the second edition were being produced and modified. John Buck for his thorough reading of the entire draft of this second edition. Robert Becker, Sally Bemus, Maggie Beucler, Ben Halpern, Jon Maira, Chirag Patel, and Jerry Weinstein for their efforts in producing the various LaTeX drafts of the book. And to all who helped in careful reviewing of the page proofs: Babak Ayazifar Christina Lamarre Richard Barron Nicholas Laneman Rebecca Bates Li Lee George Bevis Sean Lindsay Sarit Birzon Jeffrey T. Ludwig Nabil Bitar Seth Pappas Nirav Dagli Adrienne Prahler Anne Findlay Ryan Riddolls Austin Frakt Alan Seefeldt Siddhartha Gupta Sekhar Tatikonda Christoforos Hadjicostis Shawn Verbout Terrence Ho Kathleen Wage Mark Ibanez Alex Wang Seema Jaggi Joseph Winograd Patrick Kreidl XXV"
Acknowledgements,"AcKNOWLEDGMENTS In producing this second edition we were fortunate to receive the assistance of many col- leagues, students, and friends who were extremely generous with their time. We express our deep appreciation to: Jon Maiara and Ashok Popat for their help in generating many of the figures and images. Babak Ayazifar and Austin Frakt for their help in updating and assembling the bibliography. Ramamurthy Mani for preparing the solutions manual for the text and for his help in generating many of t.he figures. Michael Daniel for coordinating and managing the LaTeX files as the various drafts of the second edition were being produced and modified. John Buck for his thorough reading of the entire draft of this second edition. Robert Becker, Sally Bemus, Maggie Beucler, Ben Halpern, Jon Maira, Chirag Patel, and Jerry Weinstein for their efforts in producing the various LaTeX drafts of the book. And to all who helped in careful reviewing of the page proofs: Babak Ayazifar Christina Lamarre Richard Barron Nicholas Laneman Rebecca Bates Li Lee George Bevis Sean Lindsay Sarit Birzon Jeffrey T. Ludwig Nabil Bitar Seth Pappas Nirav Dagli Adrienne Prahler Anne Findlay Ryan Riddolls Austin Frakt Alan Seefeldt Siddhartha Gupta Sekhar Tatikonda Christoforos Hadjicostis Shawn Verbout Terrence Ho Kathleen Wage Mark Ibanez Alex Wang Seema Jaggi Joseph Winograd Patrick Kreidl XXV This page intentionally left blank FoREWORD The concepts of signals and systems arise in a wide variety of fields, and the ideas and techniques associated with these concepts play an important role in such diverse areas of science and technology as communications, aeronautics and astronautics, circuit design, acoustics, seismology, biomedical engineering, energy generation and distribution sys- tems, chemical process control, and speech processing. Although the physical nature of the signals and systems that arise in these various disciplines may be drastically different, they all have two very basic features in common. The signals, which are functions of one or more independent variables, contain information about the behavior or nature of some phenomenon, whereas the systems respond to particular signals by producing other sig- nals or some desired behavior. Voltages and currents as a function of time in an electrical circuit are examples of signals, and a circuit is itself an example of a system, which in this case responds to applied voltages and currents. As another example, when an automobile driver depresses the accelerator pedal, the automobile responds by increasing the speed of the vehicle. In this case, the system is the automobile, the pressure on the accelerator pedal the input to the system, and the automobile speed the response. A computer program for the automated diagnosis of electrocardiograms can be viewed as a system which has as its input a digitized electrocardiogram and which produces estimates of parameters such as heart rate as outputs. A camera is a system that receives light from different sources and reflected from objects and produces a photograph. A robot arm is a system whose movements are the response to control inputs. In the many contexts in which signals and systems arise, there are a variety of prob- lems and questions that are of importance. In some cases, we are presented with a specific system and are interested in characterizing it in detail to understand how it will respond to various inputs. Examples include the analysis of a circuit in order to quantify its response to different voltage and current sources and the determination of an aircraft's response characteristics both to pilot commands and to wind gusts. In other problems of signal and system analysis, rather than analyzing existing sys- tems, our interest may be focused on designing systems to process signals in particular ways. One very common context in which such problems arise is in the design of systems to enhance or restore signals that have been degraded in some way. For example, when a pilot is communicating with an air traffic control tower, the communication can be de- graded by the high level of background noise in the cockpit. In this and many similar cases, it is possible to design systems that will retain the desired signal, in this case the pilot's voice, and reject (at least approximately) the unwanted signal, i.e., the noise. A similar set of objectives can also be found in the general area of image restoration and image enhancement. For example, images from deep space probes or earth-observing satellites typically represent degraded versions of the scenes being imaged because of limitations of the imaging equipment, atmospheric effects, and errors in signal transmission in returning the images to earth. Consequently, images returned from space are routinely processed by systems to compensate for some of these degradations. In addition, such images are usu- xxvii"
Foreword,"FoREWORD The concepts of signals and systems arise in a wide variety of fields, and the ideas and techniques associated with these concepts play an important role in such diverse areas of science and technology as communications, aeronautics and astronautics, circuit design, acoustics, seismology, biomedical engineering, energy generation and distribution sys- tems, chemical process control, and speech processing. Although the physical nature of the signals and systems that arise in these various disciplines may be drastically different, they all have two very basic features in common. The signals, which are functions of one or more independent variables, contain information about the behavior or nature of some phenomenon, whereas the systems respond to particular signals by producing other sig- nals or some desired behavior. Voltages and currents as a function of time in an electrical circuit are examples of signals, and a circuit is itself an example of a system, which in this case responds to applied voltages and currents. As another example, when an automobile driver depresses the accelerator pedal, the automobile responds by increasing the speed of the vehicle. In this case, the system is the automobile, the pressure on the accelerator pedal the input to the system, and the automobile speed the response. A computer program for the automated diagnosis of electrocardiograms can be viewed as a system which has as its input a digitized electrocardiogram and which produces estimates of parameters such as heart rate as outputs. A camera is a system that receives light from different sources and reflected from objects and produces a photograph. A robot arm is a system whose movements are the response to control inputs. In the many contexts in which signals and systems arise, there are a variety of prob- lems and questions that are of importance. In some cases, we are presented with a specific system and are interested in characterizing it in detail to understand how it will respond to various inputs. Examples include the analysis of a circuit in order to quantify its response to different voltage and current sources and the determination of an aircraft's response characteristics both to pilot commands and to wind gusts. In other problems of signal and system analysis, rather than analyzing existing sys- tems, our interest may be focused on designing systems to process signals in particular ways. One very common context in which such problems arise is in the design of systems to enhance or restore signals that have been degraded in some way. For example, when a pilot is communicating with an air traffic control tower, the communication can be de- graded by the high level of background noise in the cockpit. In this and many similar cases, it is possible to design systems that will retain the desired signal, in this case the pilot's voice, and reject (at least approximately) the unwanted signal, i.e., the noise. A similar set of objectives can also be found in the general area of image restoration and image enhancement. For example, images from deep space probes or earth-observing satellites typically represent degraded versions of the scenes being imaged because of limitations of the imaging equipment, atmospheric effects, and errors in signal transmission in returning the images to earth. Consequently, images returned from space are routinely processed by systems to compensate for some of these degradations. In addition, such images are usu- xxvii xxviii Foreworc ally processed to enhance certain features, such as lines (corresponding, for example, to river beds or faults) or regional boundaries in which there are sharp contrasts in color or darkness. In addition to enhancement and restoration, in many applications there is a need to design systems to extract specific pieces of information from signals. The estimation of heart rate from an electrocardiogram is one example. Another arises in economic forecast- ing. We may, for example, wish to analyze the history of an economic time series, such as a set of stock market averages, in order to estimate trends and other characteristics such as seasonal variations that may be of use in making predictions about future behavior. In other applications, the focus may be on the design of signals with particular properties. Specifically, in communications applications considerable attention is paid to designing signals to meet the constraints and requirements for successful transmission. For exam- ple, long distance communication through the atmosphere requires the use of signals with frequencies in a particular part of the electromagnetic spectrum. The design of communi- cation signals must also take into account the need for reliable reception in the presence of both distortion due to transmission through the atmosphere and interference from other signals being transmitted simultaneously by other users. Another very important class of applications in which the concepts and techniques of signal and system analysis arise are those in which we wish to modify or control the characteristics of a given system, perhaps through the choice of specific input signals or by combining the system with other systems. Illustrative of this kind of application is the design of control systems to regulate chemical processing plants. Plants of this type are equipped with a variety of sensors that measure physical signals such as temperature, hu- midity, and chemical composition. The control system in such a plant responds to these sensor signals by adjusting quantities such as flow rates and temperature in order to regu- late the ongoing chemical process. The design of aircraft autopilots and computer control systems represents another example. In this case, signals measuring aircraft speed, alti- tude, and heading are used by the aircraft's control system in order to adjust variables such as throttle setting and the position of the rudder and ailerons. These adjustments are made to ensure that the aircraft follows a specified course, to smooth out the aircraft's ride, and to enhance its responsiveness to pilot commands. In both this case and in the previous ex- ample of chemical process control, an important concept, referred to as feedback, plays a major role, as measured signals are fed back and used to adjust the response characteristics of a system. The examples in the preceding paragraphs represent only a few of an extraordinarily wide variety of applications for the concepts of signals and systems. The importance of these concepts stems not only from the diversity of phenomena and processes in which they arise, but also from the collection of ideas, analytical techniques, and methodologies that have been and are being developed and used to solve problems involving signals and systems. The history of this development extends back over many centuries, and although most of this work was motivated by specific applications, many of these ideas have proven to be of central importance to problems in a far larger variety of contexts than those for which they were originally intended. For example, the tools of Fourier analysis, which form the basis for the frequency-domain analysis of signals and systems, and which we will develop in some detail in this book, can be traced from problems of astronomy studied by the ancient Babylonians to the development of mathematical physics in the eighteenth and nineteenth centuries. Foreword xxix In some of the examples that we have mentioned, the signals vary continuously in time, whereas in others, their evolution is described only at discrete points in time. For example, in the analysis of electrical circuits and mechanical systems we are concerned with signals that vary continuously. On the other hand, the daily closing stock market average is by its very nature a signal that evolves at discrete points in time (i.e., at the close of each day). Rather than a curve as a function of a continuous variable, then, the closing stock market average is a sequence of numbers associated with the discrete time instants at which it is specified. This distinction in the basic description of the evolution of signals and of the systems that respond to or process these signals leads naturally to two parallel frameworks for signal and system analysis, one for phenomena and processes that are described in continuous time and one for those that are described in discrete time. The concepts and techniques associated both with continuous-time signals and sys- tems and with discrete-time signals and systems have a rich history and are conceptually closely related. Historically, however, because their applications have in the past been suf- ficiently different, they have for the most part been studied and developed somewhat sepa- rately. Continuous-time signals and systems have very strong roots in problems associated with physics and, in the more recent past, with electrical circuits and communications. The techniques of discrete-time signals and systems have strong roots in numerical analy- sis, statistics, and time-series analysis associated with such applications as the analysis of economic and demographic data. Over the past several decades, however, the disciplines of continuous-time and discrete-time signals and systems have become increasingly en- twined and the applications have become highly interrelated. The major impetus for this has come from the dramatic advances in technology for the implementation of systems and for the generation of signals. Specifically, the continuing development of high-speed digital computers, integrated circuits, and sophisticated high-density device fabrication techniques has made it increasingly advantageous to consider processing continuous-time signals by representing them by time samples (i.e., by converting them to discrete-time signals). As one example, the computer control system for a modem high-performance aircraft digitizes sensor outputs such as vehicle speed in order to produce a sequence of sampled measurements which are then processed by the control system. Because of the growing interrelationship between continuous-time signals and sys- tems and discrete-time signals and systems and because of the close relationship among the concepts and techniques associated with each, we have chosen in this text to develop the concepts of continuous-time and discrete-time signals and systems in parallel. Since many of the concepts are similar (but not identical), by treating them in parallel, insight and intuition can be shared and both the similarities and differences between them become better focused. In addition, as will be evident as we proceed through the material, there are some concepts that are inherently easier to understand in one framework than the other and, once understood, the insight is easily transferable. Furthermore, this parallel treatment greatly facilitates our understanding of the very important practical context in which con- tinuous and discrete time are brought together, namely the sampling of continuous-time signals and the processing of continuous-time signals using discrete-time systems. As we have so far described them, the notions of signals and systems are extremely general concepts. At this level of generality, however, only the most sweeping statements can be made about the nature of signals and systems, and their properties can be discussed only in the most elementary terms. On the other hand, an important and fundamental notion in dealing with signals and systems is that by carefully choosing subclasses of each with XXX Foreword particular properties that can then be exploited, we can analyze and characterize these signals and systems in great depth. The principal focus in this book is on the particular class of linear time-invariant systems. The properties of linearity and time inv ariance that define this class lead to a remarkable set of concepts and techniques which are not only of major practical importance but also analytically tractable and intellectually satisfying. As we have emphasized in this foreword, signal and system analysis has a long his- tory out of which have emerged some basic techniques and fundamental principles which have extremely broad areas of application. Indeed, signal and system analysis is constantly evolving and developing in response to new problems, techniques, and opportunities. We fully expect this development to accelerate in pace as improved technology makes possi- ble the implementation of increasingly complex systems and signal processing techniques. In the future we will see signals and systems tools and concepts applied to an expanding scope of applications. For these reasons, we feel that the topic of signal and system analy- sis represents a body of knowledge that is of essential concern to the scientist and engineer. We have chosen the set of topics presented in this book, the organization of the presen- tation, and the problems in each chapter in a way that we feel will most help the reader to obtain a solid foundation in the fundamentals of signal and system analysis; to gain an understanding of some of the very important and basic applications of these fundamentals to problems in filtering, sampling, communications, and feedback system analysis; and to develop some appreciation for an extremely powerful and broadly applicable approach to formulating and solving complex problems. 1 SIGNALS AND SYSTEMS 1.0 INTRODUCTION As described in the Foreword, the intuitive notions of signals and systems arise in a rich va- riety of contexts. Moreover, as we will see in this book, there is an analytical framework- that is, a language for describing signals and systems and an extremely powerful set of tools for analyzing them-that applies equally well to problems in many fields. In this chapter, we begin our development of the analytical framework for signals and systems by intro- ducing their mathematical description and representations. In the chapters that follow, we build on this foundation in order to develop and describe additional concepts and methods that add considerably both to our understanding of signals and systems and to our ability to analyze and solve problems involving signals and systems that arise in a broad array of applications. 1. 1 CONTINUOUS-TIME AND DISCRETE-TIME SIGNALS 1 . 1 . 1 Examples and Mathematical Representation Signals may describe a wide variety of physical phenomena. Although signals can be rep- resented in many ways, in all cases the information in a signal is contained in a pattern of variations of some form. For example, consider the simple circuit in Figure 1.1. In this case, the patterns of variation over time in the source and capacitor voltages, v, and Vc, are exam- ples of signals. Similarly, as depicted in Figure 1.2, the variations over time of the applied force f and the resulting automobile velocity v are signals. As another example, consider the human vocal mechanism, which produces speech by creating fluctuations in acous- tic pressure. Figure 1.3 is an illustration of a recording of such a speech signal, obtained by 1"
1 Signals and Systems,"1 SIGNALS AND SYSTEMS 1.0 INTRODUCTION As described in the Foreword, the intuitive notions of signals and systems arise in a rich va- riety of contexts. Moreover, as we will see in this book, there is an analytical framework- that is, a language for describing signals and systems and an extremely powerful set of tools for analyzing them-that applies equally well to problems in many fields. In this chapter, we begin our development of the analytical framework for signals and systems by intro- ducing their mathematical description and representations. In the chapters that follow, we build on this foundation in order to develop and describe additional concepts and methods that add considerably both to our understanding of signals and systems and to our ability to analyze and solve problems involving signals and systems that arise in a broad array of applications. 1. 1 CONTINUOUS-TIME AND DISCRETE-TIME SIGNALS 1 . 1 . 1 Examples and Mathematical Representation Signals may describe a wide variety of physical phenomena. Although signals can be rep- resented in many ways, in all cases the information in a signal is contained in a pattern of variations of some form. For example, consider the simple circuit in Figure 1.1. In this case, the patterns of variation over time in the source and capacitor voltages, v, and Vc, are exam- ples of signals. Similarly, as depicted in Figure 1.2, the variations over time of the applied force f and the resulting automobile velocity v are signals. As another example, consider the human vocal mechanism, which produces speech by creating fluctuations in acous- tic pressure. Figure 1.3 is an illustration of a recording of such a speech signal, obtained by 1"
1.0 Introduction,"1 SIGNALS AND SYSTEMS 1.0 INTRODUCTION As described in the Foreword, the intuitive notions of signals and systems arise in a rich va- riety of contexts. Moreover, as we will see in this book, there is an analytical framework- that is, a language for describing signals and systems and an extremely powerful set of tools for analyzing them-that applies equally well to problems in many fields. In this chapter, we begin our development of the analytical framework for signals and systems by intro- ducing their mathematical description and representations. In the chapters that follow, we build on this foundation in order to develop and describe additional concepts and methods that add considerably both to our understanding of signals and systems and to our ability to analyze and solve problems involving signals and systems that arise in a broad array of applications. 1. 1 CONTINUOUS-TIME AND DISCRETE-TIME SIGNALS 1 . 1 . 1 Examples and Mathematical Representation Signals may describe a wide variety of physical phenomena. Although signals can be rep- resented in many ways, in all cases the information in a signal is contained in a pattern of variations of some form. For example, consider the simple circuit in Figure 1.1. In this case, the patterns of variation over time in the source and capacitor voltages, v, and Vc, are exam- ples of signals. Similarly, as depicted in Figure 1.2, the variations over time of the applied force f and the resulting automobile velocity v are signals. As another example, consider the human vocal mechanism, which produces speech by creating fluctuations in acous- tic pressure. Figure 1.3 is an illustration of a recording of such a speech signal, obtained by 1"
1.1 Continuous-Time and Discrete-Time Signals,"1 SIGNALS AND SYSTEMS 1.0 INTRODUCTION As described in the Foreword, the intuitive notions of signals and systems arise in a rich va- riety of contexts. Moreover, as we will see in this book, there is an analytical framework- that is, a language for describing signals and systems and an extremely powerful set of tools for analyzing them-that applies equally well to problems in many fields. In this chapter, we begin our development of the analytical framework for signals and systems by intro- ducing their mathematical description and representations. In the chapters that follow, we build on this foundation in order to develop and describe additional concepts and methods that add considerably both to our understanding of signals and systems and to our ability to analyze and solve problems involving signals and systems that arise in a broad array of applications. 1. 1 CONTINUOUS-TIME AND DISCRETE-TIME SIGNALS 1 . 1 . 1 Examples and Mathematical Representation Signals may describe a wide variety of physical phenomena. Although signals can be rep- resented in many ways, in all cases the information in a signal is contained in a pattern of variations of some form. For example, consider the simple circuit in Figure 1.1. In this case, the patterns of variation over time in the source and capacitor voltages, v, and Vc, are exam- ples of signals. Similarly, as depicted in Figure 1.2, the variations over time of the applied force f and the resulting automobile velocity v are signals. As another example, consider the human vocal mechanism, which produces speech by creating fluctuations in acous- tic pressure. Figure 1.3 is an illustration of a recording of such a speech signal, obtained by 1 2 Signals and Systems Chap. 1 R c ~pv Figure 1. 1 A simple RC circuit with source Figure 1 .2 An automobile responding to an voltage Vs and capacitor voltage Vc. applied force t from the engine and to a re- tarding frictional force pv proportional to the automobile's velocity v. ~-------------------200msec--------------------~ I I I I 1_ ____ .!_ _____ 1 _____ !._ _____1 __________ ~ _____ I _____ J j sh oul d r - - - - -~- - - - - I - - - - I - - - - - ~- - - - - I - - - - -~- - - - - I - - - - -~ I I I I I I I I I ~------------------------------------------- w e Figure 1.3 Example of a record- r - - - - -~- - - - - I - - - - I - - - - - - - - - I - - - - - - - - I - - - - ing of speech. [Adapted from Ap- ~- -~- -~ I I I plications of Digital Signal Process- ing, A.V. Oppenheim, ed. (Englewood Cliffs, N.J.: Prentice-Hall, Inc., 1978), I ~ _____1 _____ ~ ____ ~ _____ 1_ ____ .!_ _____ I _____ ~ _____ I p. 121.] The signal represents acous- ch a tic pressure variations as a function of time for the spoken words ""should we chase."" The top line of the figure corresponds to the word ""should,"" the second line to the word ""we,"" and the last two lines to the word ""chase."" (We have indicated the ap- I I I I 1_ ____ ~ _____ 1 _____ ~ _____1 _____ I_ ____ _____ 1_ ____ J proximate beginnings and endings ~ a se of each successive sound in each I word.) using a microphone to sense variations in acoustic pressure, which are then converted into an electrical signal. As can be seen in the figure, different sounds correspond to different patterns in the variations of acoustic pressure, and the human vocal system produces intel- ligible speech by generating particular sequences of these patterns. Alternatively, for the monochromatic picture, shown in Figure 1.4, it is the pattern of variations in brightness across the image, that is important. Sec. 1.1 Continuous-Time and Discrete-Time Signals 3 Figure 1 .4 A monochromatic picture. Signals are represented mathematically as functions of one or more independent variables. For example, a speech signal can be represented mathematically by acoustic pressure as a function of time, and a picture can be represented by brightness as a func- tion of two spatial variables. In this book, we focus our attention on signals involving a single independent variable. For convenience, we will generally refer to the independent variable as time, although it may not in fact represent time in specific applications. For example, in geophysics, signals representing variations with depth of physical quantities such as density, porosity, and electrical resistivity are used to study the structure of the earth. Also, knowledge of the variations of air pressure, temperature, and wind speed with altitude are extremely important in meteorological investigations. Figure 1.5 depicts a typ- ical example of annual average vertical wind profile as a function of height. The measured variations of wind speed with height are used in examining weather patterns, as well as wind conditions that may affect an aircraft during final approach and landing. Throughout this book we will be considering two basic types of signals: continuous- time signals and discrete-time signals. In the case of continuous-time signals the inde- pendent variable is continuous, and thus these signals are defined for a continuum of values 26 24 22 20 18 :§'16 0 ~ 14 ~ 12 ~ 10 (f) 8 6 4 Figure 1 .s Typical annual vertical 2 wind profile. (Adapted from Crawford and Hudson, National Severe Storms 0 200 400 600 800 1,000 1,200 1,400 1,600 Laboratory Report, ESSA ERLTM-NSSL Height (feet) 48, August 1970.) 4 Signals and Systems Chap. 1 400- 350 - 300 250 200 150 100 50 ot Jan. 5,1929 Jan. 4,1930 Figure 1 .6 An example of a discrete-time signal: The weekly Dow-Jones stock market index from January 5, 1929, to January 4, 1930. of the independent variable. On the other hand, discrete-time signals are defined only at discrete times, and consequently, for these signals, the independent variable takes on only a discrete set of values. A speech signal as a function of time and atmospheric pressure as a function of altitude are examples of continuous-time signals. The weekly Dow-Jones stock market index, as illustrated in Figure 1.6, is an example of a discrete-time signal. Other examples of discrete-time signals can be found in demographic studies in which various attributes, such as average budget, crime rate, or pounds of fish caught, are tab- ulated against such discrete variables as family size, total population, or type of fishing vessel, respectively. To distinguish between continuous-time and discrete-time signals, we will use the symbol t to denote the continuous-time independent variable and n to denote the discrete- time independent variable. In addition, for continuous-time signals we will enclose the independent variable in parentheses ( · ), whereas for discrete-time signals we will use brackets [ · ] to enclose the independent variable. We will also have frequent occasions when it will be useful to represent signals graphically. Illustrations of a continuous-time signal x(t) and a discrete-time signal x[n] are shown in Figure 1.7. It is important to note that the discrete-time signal x[n] is defined only for integer values of the independent variable. Our choice of graphical representation for x[ n] emphasizes this fact, and for further emphasis we will on occasion refer to x[n] as a discrete-time sequence. A discrete-time signal x[n] may represent a phenomenon for which the independent variable is inherently discrete. Signals such as demographic data are examples of this. On the other hand, a very important class of discrete-time signals arises from the sampling of continuous-time signals. In this case, the discrete-time signal x[n] represents successive samples of an underlying phenomenon for which the independent variable is continuous. Because of their speed, computational power, and flexibility, modem digital processors are used to implement many practical systems, ranging from digital autopilots to digital audio systems. Such systems require the use of discrete-time sequences representing sampled versions of continuous-time signals--e.g., aircraft position, velocity, and heading for an Sec. 1.1 Continuous-Time and Discrete-Time Signals 5 x(t) 0 (a) x[n] x[O] n Figure 1. 7 Graphical representations of (a) continuous-time and (b) discrete- time signals. autopilot or speech and music for an audio system. Also, pictures in newspapers-or in this book, for that matter-actually consist of a very fine grid of points, and each of these points represents a sample of the brightness of the corresponding point in the original image. No matter what the source of the data, however, the signal x[n] is defined only for integer values of n. It makes no more sense to refer to the 3 ~ th sample of a digital speech signal than it does to refer to the average budget for a family with 2~ family members. Throughout most of this book we will treat discrete-time signals and continuous-time signals separately but in parallel, so that we can draw on insights developed in one setting to aid our understanding of another. In Chapter 7 we will return to the question of sampling, and in that context we will bring continuous-time and discrete-time concepts together in order to examine the relationship between a continuous-time signal and a discrete-time signal obtained from it by sampling. 1. 1 .2 Signal Energy and Power From the range of examples provided so far, we see that signals may represent a broad variety of phenomena. In many, but not all, applications, the signals we consider are di- rectly related to physical quantities capturing power and energy in a physical system. For example, if v(t) and i(t) are, respectively, the voltage and current across a resistor with resistance R, then the instantaneous power is p(t) = v(t)i(t) = ~v2 (t). (1.1) 6 Signals and Systems Chap. 1 The total energy expended over the time interval t 1 :s t :s t2 is 12 12 p(t) dt = { ~v2 { (t) dt, (1.2) Jt ] Jf] and the average power over this time interval is 1 -1- J 2 p(t)dt = -1- Jt2 -1R v2(t)dt. (1.3) t2 - t! f] t2 - tl f] Similarly, for the automobile depicted in Figure 1.2, the instantaneous power dissipated through friction is p(t) = bv2(t), and we can then define the total energy and average power over a time interval in the same way as in eqs. (1.2) and (1.3). With simple physical examples such as these as motivation, it is a common and worthwhile convention to use similar terminology for power and energy for any continuous- time signal x(t) or any discrete-time signal x[n]. Moreover, as we will see shortly, we will frequently find it convenient to consider signals that take on complex values. In this case, the total energy over the time interval t 1 :s t :s t2 in a continuous-time signal x(t) is defined as (1.4) where lxl denotes the magnitude of the (possibly complex) number x. The time-averaged power is obtained by dividing eq. (1.4) by the length, t2 - t 1, of the time interval. Simi- larly, the total energy in a discrete-time signal x[n] over the time interval n1 :s n :s n2 is defined as (1.5) and dividing by the number of points in the interval, n2 - n 1 + 1, yields the average power over the interval. It is important to remember that the terms ""power"" and ""energy"" are used here independently of whether the quantities in eqs. (1.4) and (1.5) actually are related to physical energy. 1 Nevertheless, we will find it convenient to use these terms in a general fashion. Furthermore, in many systems we will be interested in examining power and energy in signals over an infinite time interval, i.e., for -oo < t < +oo or for -oo < n < +oo. In these cases, we define the total energy as limits of eqs. (1.4) and (1.5) as the time interval increases without bound. That is, in continuous time, ~ }~ IT I+ oc Eoo -T lx(t)l2 dt = -oc lx(t)l2 dt, (1.6) and in discrete time, +N +oo Eoo ~ lim L lx[nJI2 = L lx[nJI2 . (1.7) N~oo n=-N n=-oc 1E ven if such a relationship does exist, eqs. ( 1.4) and ( 1.5) may have the wrong dimensions and scalings. For example, comparing eqs. (1.2) and (1.4 ), we see that if x(t) represents the voltage across a resistor, then eq. (1.4) must be divided by the resistance (measured, for example, in ohms) to obtain units of physical energy. Sec. 1.2 Transformations of the Independent Variable 7 Note that for some signals the integral in eq. (1.6) or sum in eq. (1.7) might not converge- e.g., if x(t) or x[n] equals a nonzero constant value for all time. Such signals have infinite energy, while signals with E~ < co have finite energy. In an analogous fashion, we can define the time-averaged power over an infinite interval as PeN =(', lim - 1 IT /x(t)/ 2 dt (1.8) T---+~ 2 T -T and 1 +N PeN ~ lim L /x[n]/ 2 (1.9) N---+~ 2N + 1 n=-N in continuous time and discrete time, respectively. With these definitions, we can identify three important classes of signals. The first of these is the class of signals with finite total energy, i.e., those signals for which Eoo <co. Such a signal must have zero average power, since in the continuous time case, for example, we see from eq. (1.8) that . E~ P~ = 1 m-= 0. (1.10) T---+~2T An example of a finite-energy signal is a signal that takes on the value 1 for 0 :::::: t :::::: 1 and 0 otherwise. In this case, E~ = 1 and Px = 0. A second class of signals are those with finite average power P oo. From what we have just seen, if Px > 0, then, of necessity, Ex = co. This, of course, makes sense, since if there is a nonzero average energy per unit time (i.e., nonzero power), then integrating or summing this over an infinite time interval yields an infinite amount of energy. For example, the constant signal x[n] = 4 has infinite energy, but average power Px = 16. There are also signals for which neither P x nor Ex are finite. A simple example is the signal x(t) = t. We will encounter other examples of signals in each of these classes in the remainder of this and the following chapters. 1.2 TRANSFORMATIONS OF THE INDEPENDENT VARIABlE A central concept in signal and system analysis is that of the transformation of a signal. For example, in an aircraft control system, signals corresponding to the actions of the pilot are transformed by electrical and mechanical systems into changes in aircraft thrust or the positions of aircraft control surfaces such as the rudder or ailerons, which in tum are transformed through the dynamics and kinematics of the vehicle into changes in aircraft velocity and heading. Also, in a high-fidelity audio system, an input signal representing music as recorded on a cassette or compact disc is modified in order to enhance desirable characteristics, to remove recording noise, or to balance the several components of the signal (e.g., treble and bass). In this section, we focus on a very limited but important class of elementary signal transformations that involve simple modification of the independent variable, i.e., the time axis. As we will see in this and subsequent sections of this chapter, these elementary transformations allow us to introduce several basic properties of signals and systems. In later chapters, we will find that they also play an important role in defining and characterizing far richer and important classes of systems."
1.2 Transformations of the Independent Variable,"Sec. 1.2 Transformations of the Independent Variable 7 Note that for some signals the integral in eq. (1.6) or sum in eq. (1.7) might not converge- e.g., if x(t) or x[n] equals a nonzero constant value for all time. Such signals have infinite energy, while signals with E~ < co have finite energy. In an analogous fashion, we can define the time-averaged power over an infinite interval as PeN =(', lim - 1 IT /x(t)/ 2 dt (1.8) T---+~ 2 T -T and 1 +N PeN ~ lim L /x[n]/ 2 (1.9) N---+~ 2N + 1 n=-N in continuous time and discrete time, respectively. With these definitions, we can identify three important classes of signals. The first of these is the class of signals with finite total energy, i.e., those signals for which Eoo <co. Such a signal must have zero average power, since in the continuous time case, for example, we see from eq. (1.8) that . E~ P~ = 1 m-= 0. (1.10) T---+~2T An example of a finite-energy signal is a signal that takes on the value 1 for 0 :::::: t :::::: 1 and 0 otherwise. In this case, E~ = 1 and Px = 0. A second class of signals are those with finite average power P oo. From what we have just seen, if Px > 0, then, of necessity, Ex = co. This, of course, makes sense, since if there is a nonzero average energy per unit time (i.e., nonzero power), then integrating or summing this over an infinite time interval yields an infinite amount of energy. For example, the constant signal x[n] = 4 has infinite energy, but average power Px = 16. There are also signals for which neither P x nor Ex are finite. A simple example is the signal x(t) = t. We will encounter other examples of signals in each of these classes in the remainder of this and the following chapters. 1.2 TRANSFORMATIONS OF THE INDEPENDENT VARIABlE A central concept in signal and system analysis is that of the transformation of a signal. For example, in an aircraft control system, signals corresponding to the actions of the pilot are transformed by electrical and mechanical systems into changes in aircraft thrust or the positions of aircraft control surfaces such as the rudder or ailerons, which in tum are transformed through the dynamics and kinematics of the vehicle into changes in aircraft velocity and heading. Also, in a high-fidelity audio system, an input signal representing music as recorded on a cassette or compact disc is modified in order to enhance desirable characteristics, to remove recording noise, or to balance the several components of the signal (e.g., treble and bass). In this section, we focus on a very limited but important class of elementary signal transformations that involve simple modification of the independent variable, i.e., the time axis. As we will see in this and subsequent sections of this chapter, these elementary transformations allow us to introduce several basic properties of signals and systems. In later chapters, we will find that they also play an important role in defining and characterizing far richer and important classes of systems. 8 Signals and Systems Chap. 1 1 .2. 1 Examples of Transformations of the Independent Variable A simple and very important example of transforming the independent variable of a signal is a time shift. A time shift in discrete time is illustrated in Figure 1.8, in which we have two signals x[n] and x[n- n0 ] that are identical in shape, but that are displaced or shifted relative to each other. We will also encounter time shifts in continuous time, as illustrated in Figure 1.9, in which x(t - t0 ) represents a delayed (if to is positive) or advanced (if to is negative) version of x(t). Signals that are related in this fashion arise in applications such as radar, sonar, and seismic signal processing, in which several receivers at different locations observe a signal being transmitted through a medium (water, rock, air, etc.). In this case, the difference in propagation time from the point of origin of the transmitted signal to any two receivers results in a time shift between the signals at the two receivers. A second basic transformation of the time axis is that of time reversal. For example, as illustrated in Figure 1.1 0, the signal x[- n] is obtained from the signal x[ n] by a reflec- tion about n = 0 (i.e., by reversing the signal). Similarly, as depicted in Figure 1.11, the signal x(- t) is obtained from the signal x(t) by a reflection about t = 0. Thus, if x(t) rep- resents an audio tape recording, then x( -t) is the same tape recording played backward. Another transformation is that of time scaling. In Figure 1.12 we have illustrated three signals, x(t), x(2t), and x(t/2), that are related by linear scale changes in the independent variable. If we again think of the example of x(t) as a tape recording, then x(2t) is that recording played at twice the speed, and x(t/2) is the recording played at half-speed. It is often of interest to determine the effect of transforming the independent variable of a given signal x(t) to obtain a signal of the form x(at + {3), where a and {3 are given numbers. Such a transformation of the independent variable preserves the shape of x(t), except that the resulting signal may be linearly stretched if Ia I < 1, linearly compressed if Ia I > 1, reversed in time if a < 0, and shifted in time if {3 is nonzero. This is illustrated in the following set of examples. x[n] n x[n-n0] Figure 1 .8 Discrete-time signals related by a time shift. In this figure n0 > 0, so that x[n- n0 ] is a delayed 0 n verson of x[n] (i.e., each point in x[n] occurs later in x[n- n0]). Sec. 1.2 Transformations of the Independent Variable 9 x[n) n (a) x[-n) n Figure 1.9 Continuous-time signals related by a time shift. In this figure t0 < 0, so that (b) x(t - to) is an advanced version of x(t) (i.e., each point in x(t) occurs at an earlier time in Figure 1 .1 O (a) A discrete-time signal x[n]; (b) its reflec- x(t - to)). tion x[-n] about n = 0. x(t) x(t) d\ x(2t) (a) x(-t) & x(t/2) ~ (b) Figure 1.11 (a) A continuous-time signal x(t); (b) its Figure 1. 12 Continuous-time signals reflection x( - t) about t = 0. related by time scaling. 10 Signals and Systems Chap. 1 Example 1.1 Given the signal x(t) shown in Figure l.13(a), the signal x(t + 1) corresponds to an advance (shift to the left) by one unit along the taxis as illustrated in Figure l.13(b). Specifically, we note that the value of x(t) at t = to occurs in x(t + 1) at t = to - 1. For 11 'l'I 0 1 2 (a) 1~ -1 0 1 2 (b) -1 0 1 (c) 1I ' 1i11 -~ 0 2/3 4/3 (d) -2/3 0 2/3 (e) Figure 1. 13 (a) The continuous-time signal x(t) used in Examples 1.1-1.3 to illustrate transformations of the independent variable; (b) the time-shifted signal x(t + 1) ; (c) the signal x(-t + 1) obtained by a time shift and a time reversal; (d) the time-scaled signal xa t); and (e) the signal xa t + 1) obtained by time-shifting and scaling. Sec. 1.2 Transformations of the Independent Variable 11 example, the value of x(t) at t = 1 is found in x(t + 1) at t = 1 - 1 = 0. Also, since x(t) is zero fort < 0, we have x(t + 1) zero fort < -1. Similarly, since x(t) is zero for t > 2, x(t + 1) is zero for t > 1. Let us also consider the signal x( - t + 1) , which may be obtained by replacing t with -t in x(t + 1). That is, x(-t + 1) is the time reversed version of x(t + 1) . Thus, x( - t + 1) may be obtained graphically by reflecting x( t + 1) about the t axis as shown in Figure 1.13(c). Example 1.2 Given the signal x(t), shown in Figure l.13(a), the signal x(~t) corresponds to a linear compression of x(t) by a factor of~ as illustrated in Figure l.13(d). Specifically we note that the value of x(t) at t = to occurs in x(~t) at t = ~t0 . For example, the value of x(t) at t = 1 is found in x(~t) at t = ~ (1) = ~-Also, since x(t) is zero fort< 0, we have x(~t) zero fort< 0. Similarly, since x(t) is zero fort> 2, x(~t) is zero fort> ~- Example 1.3 Suppose that we would like to determine the effect of transforming the independent vari- able of a given signal, x(t), to obtain a signal of the form x(at + /3), where a and f3 are given numbers. A systematic approach to doing this is to first delay or advance x(t) in accordance with the value of f3, and then to perform time scaling and/or time reversal on the resulting signal in accordance with the value of a. The delayed or advanced signal is linearly stretched if fa[ < 1, linearly compressed if fa[ > 1, and reversed in time if a < 0. To illustrate this approach, let us show how x( ~ t + 1) may be determined for the signal x(t) shown in Figure 1.13(a). Since f3 = 1, we first advance (shift to the left) x(t) by 1 as shown· in Figure 1.l 3(b ). Since fa [ = ~, we may linearly compress the shifted signal of Figure 1.13(b) by a factor of~ to obtain the signal shown in Figure 1.13(e). In addition to their use in representing physical phenomena such as the time shift in a sonar signal and the speeding up or reversal of an audiotape, transformations of the independent variable are extremely useful in signal and system analysis. In Section 1.6 and in Chapter 2, we will use transformations of the independent variable to introduce and analyze the properties of systems. These transformations are also important in defining and examining some important properties of signals. 1.2.2 Periodic Signals An important class of signals that we will encounter frequently throughout this book is the class of periodic signals. A periodic continuous-time signal x(t) has the property that there is a positive value of T for which x(t) = x(t + T) (l.11) for all values oft. In other words, a periodic signal has the property that it is unchanged by a time shift of T. In this case, we say that x(t) is periodic with period T. Periodic continuous- time signals arise in a variety of contexts. For example, as illustrated in Problem 2.61, the natural response of systems in which energy is conserved, such as ideal LC circuits without resistive energy dissipation and ideal mechanical systems without frictional losses, are periodic and, in fact, are composed of some of the basic periodic signals that we will introduce in Section 1.3. 12 Signals and Systems Chap. 1 x(t) ···!\ [\ & [\ !\··· Figure 1. 14 A continuous-time -2T -T 0 T 2T periodic signal. An example of a periodic continuous-time signal is given in Figure 1.14. From the figure or from eq. ( 1.11 ), we can readily deduce that if x(t) is periodic with period T, then x(t) = x(t + mT) for all t and for any integer m. Thus, x(t) is also periodic with period 2T, 3T, 4T, .... The fundamental period To of x(t) is the smallest positive value ofT for which eq. ( 1.11) holds. This definition of the fundamental period works, except if x(t) is a constant. In this case the fundamental period is undefined, since x(t) is periodic for any choice ofT (so there is no smallest positive value). A signal x(t) that is not periodic will be referred to as an aperiodic signal. Periodic signals are defined analogously in discrete time. Specifically, a discrete- time signal x[n] is periodic with period N, where N is a positive integer, if it is unchanged by a time shift of N, i.e., if x[n] = x[n + N] (1.12) for all values of n. If eq. (1.12) holds, then x[n] is also periodic with period 2N, 3N, .... The fundamental period N0 is the smallest positive value of N for which eq. ( 1.12) holds. An example of a discrete-time periodic signal with fundamental period No = 3 is shown in Figure 1.15. x[n] Figure 1 . 1 5 A discrete-time pe- n riodic signal with fundamental period No= 3. Example 1.4 Let us illustrate the type of problem solving that may be required in determining whether or not a given signal is periodic. The signal whose periodicity we wish to check is given by X (t ) = { c.o s(t) i. f t < 0 . (1.13) sm(t) If t ~ 0 From trigonometry, we know that cos(t + 27T) = cos(t) and sin(t + 27T) = sin(t). Thus, considering t > 0 and t < 0 separately, we see that x(t) does repeat itself over every interval oflength 27T. However, as illustrated in Figure 1.16, x(t) also has a discontinuity at the time origin that does not recur at any other time. Since every feature in the shape of a periodic signal must recur periodically, we conclude that the signal x(t) is not periodic. Sec. 1.2 Transformations of the Independent Variable 13 x(t) Figure 1. 16 The signal x{t) considered in Example 1.4. 1 .2.3 Even and Odd Signals Another set of useful properties of signals relates to their symmetry under time reversal. A signal x(t) or x[n] is referred to as an even signal if it is identical to its time-reversed counterpart, i.e., with its reflection about the origin. In continuous time a signal is even if x(- t) = x(t), (1.14) while a discrete-time signal is even if x[- n] = x[n]. ( 1.15) A signal is referred to as odd if x( -t) = - x(t), ( 1.16) x[-n] = -x[n]. (1.17) An odd signal must necessarily be 0 at t = 0 or n = 0, since eqs. ( 1.16) and ( 1.17) require that x(O) = - x(O) and x[O] = - x[O]. Examples of even and odd continuous-time signals are shown in Figure 1.17. x(t) 0 (a) x(t) Figure 1. 1 7 (a) An even con- tinuous-time signal; (b) an odd continuous-time signal. 14 Signals and Systems Chap. 1 x[n] = { 1, n;::::: 0 0, n < 0 -3-2-1 0 1 2 3 n Sv{x[nl} = { ~: ~: ~ 2, n > 0 1 t t 1I~~ t t ... -3-2-1 0 1 2 3 n - ~· n < 0 ea{x[nl}= ?·n=O { 2, n > 0 1 2 -3-2-1 r r r ···1110123 n Figure 1. 18 Example of the even- 1 odd decomposition of a discrete-time -2 signal. An important fact is that any signal can be broken into a sum of two signals, one of which is even and one of which is odd. To see this, consider the signal 1 8v { x(t)} = 2 [x (t) + x(- t)], ( 1.18) which is referred to as the even part of x(t). Similarly, the odd part of x(t) is given by 1 0d{x(t)} = 2[x(t)- x( -t)]. (1.19) It is a simple exercise to check that the even part is in fact even, that the odd part is odd, and that x(t) is the sum of the two. Exactly analogous definitions hold in the discrete- time case. An example of the even -odd decomposition of a discrete-time signal is given in Figure 1.18. 1 .3 EXPONENTIAL AND SINUSOIDAL SIGNALS In this section and the next, we introduce several basic continuous-time and discrete-time signals. Not only do these signals occur frequently, but they also serve as basic building blocks from which we can construct many other signals."
1.3 Exponential and Sinusoidal Signals,"14 Signals and Systems Chap. 1 x[n] = { 1, n;::::: 0 0, n < 0 -3-2-1 0 1 2 3 n Sv{x[nl} = { ~: ~: ~ 2, n > 0 1 t t 1I~~ t t ... -3-2-1 0 1 2 3 n - ~· n < 0 ea{x[nl}= ?·n=O { 2, n > 0 1 2 -3-2-1 r r r ···1110123 n Figure 1. 18 Example of the even- 1 odd decomposition of a discrete-time -2 signal. An important fact is that any signal can be broken into a sum of two signals, one of which is even and one of which is odd. To see this, consider the signal 1 8v { x(t)} = 2 [x (t) + x(- t)], ( 1.18) which is referred to as the even part of x(t). Similarly, the odd part of x(t) is given by 1 0d{x(t)} = 2[x(t)- x( -t)]. (1.19) It is a simple exercise to check that the even part is in fact even, that the odd part is odd, and that x(t) is the sum of the two. Exactly analogous definitions hold in the discrete- time case. An example of the even -odd decomposition of a discrete-time signal is given in Figure 1.18. 1 .3 EXPONENTIAL AND SINUSOIDAL SIGNALS In this section and the next, we introduce several basic continuous-time and discrete-time signals. Not only do these signals occur frequently, but they also serve as basic building blocks from which we can construct many other signals. Sec. 1.3 Exponential and Sinusoidal Signals 15 1 .3. 1 Continuous-Time Complex Exponential and Sinusoidal Signals The continuous-time complex exponential signal is of the form x(t) = Ce01 , (1.20) where C and a are, in general, complex numbers. Depending upon the values of these parameters, the complex exponential can exhibit several different characteristics. Real Exponential Signals As illustrated in Figure 1.19, if C and a are real [in which case x(t) is called a real exponential], there are basically two types of behavior. If a is positive, then as t in- creases x(t) is a growing exponential, a form that is used in describing many different physical processes, including chain reactions in atomic explosions and complex chemical reactions. If a is negative, then x(t) is a decaying exponential, a signal that is also used to describe a wide variety of phenomena, including the process of radioactive decay and the responses of RC circuits and damped mechanical systems. In particular, as shown in Problems 2.61 and 2.62, the natural responses of the circuit in Figure 1.1 and the automobile in Figure 1.2 are decaying exponentials. Also, we note that for a = 0, x(t) is constant. x(t) (a) x(t) Figure 1.19 Continuous-time real exponential x(t) = Ce31 : (a) a > 0; (b) (b) a< 0. 16 Signals and Systems Chap. 1 Periodic Complex Exponential and Sinusoidal Signals A second important class of complex exponentials is obtained by constraining a to be purely imaginary. Specifically, consider (1.21) An important property of this signal is that it is periodic. To verify this, we recall from eq. (1.11) that x(t) will be periodic with period T if (1.22) Or, since it follows that for periodicity, we must have (1.23) If w 0 = 0, then x(t) = 1, which is periodic for any value ofT. If w 0 =/:- 0, then the fun- damental period To of x(t)-that is, the smallest positive value ofT for which eq. (1.23) holds-is 21T = lwol' (1.24) To Thus, the signals eiwot and e- Jwot have the same fundamental period. A signal closely related to the periodic complex exponential is the sinusoidal signal x(t) = A cos(wot + cf>), (1.25) as illustrated in Figure 1.20. With seconds as the units oft, the units of cf> and w 0 are radians and radians per second, respectively. It is also common to write w 0 = 21T fo, where fo has the units of cycles per second, or hertz (Hz). Like the complex exponential signal, the si- nusoidal signal is periodic with fundamental period T0 given by eq. (1.24). Sinusoidal and x(t) = A cos (w0t + <!>) Figure 1.20 Continuous-time sinu- soidal signal. Sec. 1.3 Exponential and Sinusoidal Signals 17 complex exponential signals are also used to describe the characteristics of many physical processes-in particular, physical systems in which energy is conserved. For example, as shown in Problem 2.61, the natural response of an LC circuit is sinusoidal, as is the simple harmonic motion of a mechanical system consisting of a mass connected by a spring to a stationary support. The acoustic pressure variations corresponding to a single musical tone are also sinusoidal. By using Euler's relation? the complex exponential in eq. (1.21) can be written in terms of sinusoidal signals with the same fundamental period: e.iwot = cos Wot + j sin wot. (1.26) Similarly, the sinusoidal signal of eq. (1.25) can be written in terms of periodic complex exponentials, again with the same fundamental period: (1.27) Note that the two exponentials in eq. (1.27) have complex amplitudes. Alternatively, we can express a sinusoid in terms of a complex exponential signal as (1.28) where, if cis a complex number, CRe{ c} denotes its real part. We will also use the notation 9m{c} for the imaginary part of c, so that, for example, A sin(wot + ¢) = A9m{ej(wut+¢l}. (1.29) From eq. (1.24), we see that the fundamental period T0 of a continuous-time sinu- soidal signal or a periodic complex exponential is inversely proportional to lw0 j, which we will refer to as the fundamental frequency. From Figure 1.21, we see graphically what this means. If we decrease the magnitude of w 0 , we slow down the rate of oscillation and therefore increase the period. Exactly the opposite effects occur if we increase the mag- nitude of w 0 . Consider now the case w0 = 0. In this case, as we mentioned earlier, x(t) is constant and therefore is periodic with period T for any positive value of T. Thus, the fundamental period of a constant signal is undefined. On the other hand, there is no am- biguity in defining the fundamental frequency of a constant signal to be zero. That is, a constant signal has a zero rate of oscillation. Periodic signals-and in particular, the complex periodic exponential signal in eq. (1.21) and the sinusoidal signal in eq. (1.25)-provide important examples of signals with infinite total energy but finite average power. For example, consider the periodic ex- ponential signal of eq. (1.21), and suppose that we calculate the total energy and average power in this signal over one period: E period = f.oTo je.iwoti2 dt ( 1.30) = foT"" I · d t = To. 2Euler's relation and other basic ideas related to the manipulation of complex numbers and exponentials are considered in the mathematical review section of the problems at the end of the chapter. 18 Signals and Systems Chap. 1 (a) (b) Figure 1 .21 Relationship between the fundamental frequency and period for continuous-time sinusoidal signals; here, w1 > £1>2 > w 3, which implies (c) that T1 < T2 < r3. 1 P period = T Eperiod = 1. (1.31) 0 Since there are an infinite number of periods as t ranges from -'X! to +oo, the total energy integrated over all time is infinite. However, each period of the signal looks exactly the same. Since the average power of the signal equals 1 over each period, averaging over multiple periods always yields an average power of 1. That is, the complex periodic ex- Sec. 1.3 Exponential and Sinusoidal Signals 19 ponential signal has finite average power equal to Px = _lim _1 f T leiwotl2 dt = 1. (1.32) r~x 2T -T Problem 1.3 provides additional examples of energy and power calculations for periodic and aperiodic signals. Periodic complex exponentials will play a central role in much of our treatment of signals and systems, in part because they serve as extremely useful building blocks for many other signals. We will often find it useful to consider sets of harmonically related complex exponentials- that is, sets of periodic exponentials, all of which are periodic with a common period T0 . Specifically, a necessary condition for a complex exponential ejwr to be periodic with period T0 is that (1.33) which implies that wT0 is a multiple of 27T, i.e., wTo = 21Tk, k = 0,::!::: 1, ±:2, 0. 0 0 ( 1.34) Thus, if we define wo 27T = ( 1.35) To' we see that, to satisfy eq. ( 1.34), w must be an integer multiple of w0 . That is, a harmoni- cally related set of complex exponentials is a set of periodic exponentials with fundamental frequencies that are all multiples of a single positive frequency w0 : k = 0, ::!::: 1, ±:2,. 0. 0 ( 1.36) Fork = 0, ¢k(t) is a constant, while for any other value of k, ¢k(t) is periodic with fun- damental frequency Ik lwo and fundamental period 27T _ To lklwo - m· ( 1.37) The kth harmonic ¢k(t) is still periodic with period T0 as well, as it goes through exactly lkl of its fundamental periods during any time interval of length T0 . Our use of the term ""harmonic"" is consistent with its use in music, where it refers to tones resulting from variations in acoustic pressure at frequencies that are integer mul- tiples of a fundamental frequency. For example, the pattern of vibrations of a string on an instrument such as a violin can be described as a superposition-i.e., a weighted sum-of harmonically related periodic exponentials. In Chapter 3, we will see that we can build a very rich class of periodic signals using the harmonically related signals of eq. ( 1.36) as the building blocks. Example 1.5 It is sometimes desirable to express the sum of two complex exponentials as the product of a single complex exponential and a single sinusoid. For example, suppose we wish to 20 Signals and Systems Chap. 1 plot the magnitude of the signal (1.38) To do this, we first factor out a complex exponential from the right side of eq. (1.38), where the frequency of this exponential factor is taken as the average of the frequencies of the two exponentials in the sum. Doing this, we obtain (1.39) which, because of Euler's relation, can be rewritten as x(t) = 2ej2.51 cos(0.5t). (1.40) From this, we can directly obtain an expression for the magnitude of x(t): lx(t)l = 21 cos(0.5t)l. (1.41) Here, we have used the fact that the magnitude of the complex exponential ej2·51 is always unity. Thus, lx(t)l is what is commonly referred to as a full-wave rectified sinusoid, as shown in Figure 1.22. lx(t)l 2 Figure 1 .22 The full-wave rectified sinusoid of Example 1.5. General Complex Exponential Signals The most general case of a complex exponential can be expressed and interpreted in terms of the two cases we have examined so far: the real exponential and the periodic complex exponential. Specifically, consider a complex exponential C eat, where C is expressed in polar form and a in rectangular form. That is, and a= r + Jwo. Then (1.42) Using Euler's relation, we can expand this further as C eat = ICiert cos(wot + 0) + JICiert sin(wot + 0). (1.43) Sec. 1.3 Exponential and Sinusoidal Signals 21 Thus, for r = 0, the real and imaginary parts of a complex exponential are sinusoidal. For r > 0 they correspond to sinusoidal signals multiplied by a growing exponential, and for r < 0 they correspond to sinusoidal signals multiplied by a decaying exponential. These two cases are shown in Figure 1.23. The dashed lines in the figure correspond to the func- tions ± ICiert. From eq. ( 1.42), we see that ICiert is the magnitude of the complex expo- nential. Thus, the dashed curves act as an envelope for the oscillatory curve in the figure in that the peaks of the oscillations just reach these curves, and in this way the envelope provides us with a convenient way to visualize the general trend in the amplitude of the oscillations. x(t) (a) x(t) Figure 1 .23 (a) Growing sinusoidal signal x(t) = Cert cos (w0 t + 8), r > 0; (b) decaying sinusoid x{t) = (b) Cert cos (w0t + 8), r < 0. Sinusoidal signals multiplied by decaying exponentials are commonly referred to as damped sinusoids. Examples of damped sinusoids arise in the response of RLC circuits and in mechanical systems containing both damping and restoring forces, such as automo- tive suspension systems. These kinds of systems have mechanisms that dissipate energy (resistors, damping forces such as friction) with oscillations that decay in time. Examples illustrating such systems and their damped sinusoidal natural responses can be found in Problems 2.61 and 2.62. 1.3.2 Discrete-Time Complex Exponential and Sinusoidal Signals As in continuous time, an important signal in discrete time is the complex exponential signal or sequence, defined by ( 1.44) 22 Signals and Systems Chap. 1 where C and a are, in general, complex numbers. This could alternatively be expressed in the form x[n] = Cef3 11 , (1.45) where Although the form of the discrete-time complex exponential sequence given in eq. (1.45) is more analogous to the form of the continuous-time exponential, it is often more convenient to express the discrete-time complex exponential sequence in the form of eq. (1.44). Real Exponential Signals If C and a are real, we can have one of several types of behavior, as illustrated in Fig- ure 1.24. If Ia I > 1 the magnitude of the signal grows exponentially with n, while if Ia I < 1 we have a decaying exponential. Furthermore, if a is positive, all the values of Ca 11 are of the same sign, but if a is negative then the sign of x[n] alternates. Note also that if a = 1 then x[n] is a constant, whereas if a = -1, x[n] alternates in value between +C and -C. Real-valued discrete-time exponentials are often used to describe population growth as a function of generation and total return on investment as a function of day, month, or quarter. Sinusoidal Signals Another important complex exponential is obtained by using the form given in eq. (1.45) and by constraining {3 to be purely imaginary (so that Ia I 1). Specifically, consider (1.46) As in the continuous-time case, this signal is closely related to the sinusoidal signal x[n] = A cos(w0n + ¢). (1.47) If we taken to be dimensionless, then both wo and cp have units of radians. Three examples of sinusoidal sequences are shown in Figure 1.25. As before, Euler's relation allows us to relate complex exponentials and sinusoids: ejwon = cos won + j sin w0n (1.48) and (1.49) The signals in eqs. (1.46) and ( 1.47) are examples of discrete-time signals with infinite total energy but finite average power. For example, since lejwonl 2 = 1, every sample of the signal in eq. (1.46) contributes 1 to the signal's energy. Thus, the total energy for -'X! < n < 'X! is infinite, while the average power per time point is obviously equal to 1. Other examples of energy and power calculations for discrete-time signals are given in Problem 1.3. Sec. 1.3 Exponential and Sinusoidal Signals 23 n (a) n (b) n (c) n Figure 1 .24 The real exponential signal x[n] = can: (a) a > 1; (b) 0 < a < 1; (d) (c) -1 <a< O; (d) a< -1. 24 Signals and Systems Chap. 1 x[n] = cos (2Tin/12 ) ' I n (a) x[n] = cos (8Tin/31) n (b) x[n] = cos (n/6) n (c) Figure 1 .25 Discrete-time sinusoidal signals. General Complex Exponential Signals The general discrete-time complex exponential can be written and interpreted in terms of real exponentials and sinusoidal signals. Specifically, if we write C and a in polar form, Sec. 1.3 Exponential and Sinusoidal Signals 25 viz., C = ICieiH and then ( 1.50) Thus, for Ia I = 1, the real and imaginary parts of a complex exponential sequence are sinusoidal. For Ia I < 1 they conespond to sinusoidal sequences multiplied by a decaying exponential, while for lal > 1 they conespond to sinusoidal sequences multiplied by a growing exponential. Examples of these signals are depicted in Figure 1.26. n (a) ' \ n (b) Figure 1.26 (a) Growing discrete-time sinusoidal signals; (b) decaying discrete-time sinusoid. 1.3.3 Periodicity Properties of Discrete-Time Complex Exponentials While there are many similarities between continuous-time and discrete-time signals, there are also a number of important differences. One of these concerns the discrete-time exponential signal e.iluon. In Section 1.3.1, we identified the following two properties of its 26 Signals and Systems Chap. 1 continuous-time counterpart eiwor: ( 1) the larger the magnitude of w0, the higher is the rate of oscillation in the signal; and (2) eiwor is periodic for any value of w0• In this section we describe the discrete-time versions of both of these properties, and as we will see, there are definite differences between each of these and its continuous-time counterpart. The fact that the first of these properties is different in discrete time is a direct conse- quence of another extremely important distinction between discrete-time and continuous- time complex exponentials. Specifically, consider the discrete-time complex exponential with frequency w 0 + 2-rr: (1.51) From eq. ( 1.51 ), we see that the exponential at frequency w0 + 2-rr is the same as that at frequency w 0 . Thus, we have a very different situation from the continuous-time case, in which the signals eiwor are all distinct for distinct values of w 0 . In discrete time, these signals are not distinct, as the signal with frequency w 0 is identical to the signals with frequencies w0 ± 2-rr, w 0 ± 4-rr, and so on. Therefore, in considering discrete-time com- plex exponentials, we need only consider a frequency interval of length 2-rr in which to choose w 0 . Although, according to eq. (1.51), any interval of length 2-rr will do, on most occasions we will use the interval 0 ::::; w0 < 2-rr or the interval --rr ::::; w 0 < 1r. Because of the periodicity implied by eq. ( 1.51), the signal eiwon does not have a continually increasing rate of oscillation as w 0 is increased in magnitude. Rather, as il- lustrated in Figure 1.27, as we increase w 0 from 0, we obtain signals that oscillate more and more rapidly until we reach w 0 = 1r. As we continue to increase W(), we decrease the rate of oscillation until we reach w 0 = 2-rr, which produces the same constant sequence as w 0 = 0. Therefore, the low-frequency (that is, slowly varying) discrete-time exponentials have values of w 0 near 0, 2-rr, and any other even multiple of 1r, while the high frequen- cies (corresponding to rapid variations) are located near w 0 = ± 1r and other odd multiples of 1r. Note in particular that for w 0 = 1r or any other odd multiple of 1r, (1.52) so that this signal oscillates rapidly, changing sign at each point in time [as illustrated in Figure 1.27(e)]. The second property we wish to consider concerns the periodicity of the discrete- time complex exponential. In order for the signal eiwon to be periodic with period N > 0, we must have (1.53) or equivalently, (1.54) For eq. ( 1.54) to hold, w 0N must be a multiple of 2-rr. That is, there must be an integer m such that woN = 2-rrm, (1.55) or equivalently, m (1.56) N (b) (c) x[n] = cos (Tin/2) x[n] = cos Tin x[n] = cos (37Tn/2) - - - - - - - - - n n ------- .... n .. . .. . (d) (e) (f) x[n] = cos (7Tin/4) x[n] = cos (15Tin/8) x[n] = cos 2Tin ~~ ~~··· ···llliiiiiiiiiiiiliiiiiiiiiiiiiii··· n n (i) (g) (h) Figure 1 .27 Discrete-time sinusoidal sequences for several different frequencies. 28 Signals and Systems Chap. 1 According to eq. (1.56), the signal ejwon is periodic if w 0!27r is a rational number and is not periodic otherwise. These same observations also hold for discrete-time sinusoids. For example, the signals depicted in Figure 1.25( a) and (b) are periodic, while the signal in Figure 1.25( c) is not. Using the calculations that we have just made, we can also determine the funda- mental period and frequency of discrete-time complex exponentials, where we define the fundamental frequency of a discrete-time periodic signal as we did in continuous time. That is, if x[ n] is periodic with fundamental period N, its fundamental frequency is 27r/ N. Consider, then, a periodic complex exponential x[n] = ejwon with w 0 =I= 0. As we have just seen, w 0 must satisfy eq. (1.56) for some pair of integers m and N, with N > 0. In Problem 1.35, it is shown that if w 0 =I= 0 and if N and m have no factors in common, then the fundamental period of x[n] is N. Using this fact together with eq. (1.56), we find that the fundamental frequency of the periodic signal ejwon is 27r wo (1.57) N m Note that the fundamental period can also be written as (1.58) These last two expressions again differ from their continuous-time counterparts. In Table 1.1, we have summarized some of the differences between the continuous-time sig- nal ejwot and the discrete-time signal ejwon. Note that, as in the continuous-time case, the constant discrete-time signal resulting from setting w 0 = 0 has a fundamental frequency of zero, and its fundamental period is undefined. TABLE 1.1 Comparison of the signals ejwot and ejwon. Distinct signals for distinct values of w0 Identical signals for values of w0 separated by multiples of 27T Periodic for any choice of w 0 Periodic only if w 0 = 27Tm/N for some integers N > 0 and m. Fundamental frequency w0 Fundamental frequency* w0/m Fundamental period Fundamental period* w0 = 0: undefined w0 = 0: undefined wo ¥-0: ~ wo Wo ¥- 0: m(~) wo *Assumes that m and N do not have any factors in common. To gain some additional insight into these properties, let us examine again the signals depicted in Figure 1.25. First, consider the sequence x[n] = cos(27rn/12), depicted in Figure 1.25(a), which we can think of as the set of samples ofthe continuous-time sinusoid x(t) = cos(27rt/12) at integer time points. In this case, x(t) is periodic with fundamental period 12 and x[n] is also periodic with fundamental period 12. That is, the values of x[n] repeat every 12 points, exactly in step with the fundamental period of x(t). Sec. 1.3 Exponential and Sinusoidal Signals 29 In contrast, consider the signal x[ n] = cos(8 7Tn/31 ), depicted in Figure 1.25 (b), which we can view as the set of samples of x(t) = cos (87Tt/31) at integer points in time. In this case, x(t) is periodic with fundamental period 31/4. On the other hand, x[n] is pe- riodic with fundamental period 31. The reason for this difference is that the discrete-time signal is defined only for integer values of the independent variable. Thus, there is no sample at timet = 3114, when x(t) completes one period (starting from t = 0). Similarly, there is no sample at t = 2 · 31/4 or t = 3 · 31/4, when x(t) has completed two or three periods, but there is a sample at t = 4 · 3114 = 31, when x(t) has completed four periods. This can be seen in Figure 1.25(b), where the pattern of x[n] values does not repeat with each single cycle of positive and negative values. Rather, the pattern repeats after four such cycles, namely, every 31 points. Similarly, the signal x[n] = cos(n/6) can be viewed as the set of samples of the signal x(t) = cos(t/6) at integer time points. In this case, the values of x(t) at integer sample points never repeat, as these sample points never span an interval that is an exact multiple of the period, 127T, of x(t). Thus, x[n] is not periodic, although the eye visually interpolates between the sample points, suggesting the envelope x(t), which is periodic. The use of the concept of sampling to gain insight into the periodicity of discrete-time sinusoidal sequences is explored further in Problem 1.36. Example 1.6 Suppose that we wish to determine the fundamental period of the discrete-time signal x[n] = ei<27ri3Jn + ei<hl4ln. (1.59) The first exponential on the right-hand side of eq. (1.59) has a fundamental period of 3. While this can be verified from eq. ( 1.58), there is a simpler way to obtain that answer. In particular, note that the angle (27T/3)n of the first term must be incremented by a multiple of 27T for the values of this exponential to begin repeating. We then immediately see that if n is incremented by 3, the angle will be incremented by a single multiple of 27T. With regard to the second term, we see that incrementing the angle (37T/4)n by 27T would require n to be incremented by 8/3, which is impossible, since n is restricted to being an integer. Similarly, incrementing the angle by 47T would require a noninteger increment of 16/3 to n. However, incrementing the angle by 61r requires an increment of 8 to n, and thus the fundamental period of the second term is 8. Now, for the entire signal x[n] to repeat, each of the terms in eq. (1.59) must go through an integer number of its own fundamental period. The smallest increment of n that accomplishes this is 24. That is, over an interval of 24 points, the first term on the right-hand side of eq. ( 1.59) will have gone through eight of its fundamental periods, the second term through three of its fundamental periods, and the overall signal x[n] through exactly one of its fundamental periods. As in continuous time, it is also of considerable value in discrete-time signal and system analysis to consider sets of harmonically related periodic exponentials-that is, periodic exponentials with a common period N. From eq. (1.56), we know that these are precisely the signals which are at frequencies which are multiples of 27T/N. That is, k = 0, ±1, .... (1.60) 30 Signals and Systems Chap. 1 In the continuous-time case, all of the harmonically related complex exponentials ei k(27TIT)t, k = 0, ± 1, ± 2, ... , are distinct. However, because of eq. ( 1.51 ), this is not the case in discrete time. Specifically, <f>k+N[n] = ei<k+N)(2TT!N)n (1.61) = eik(2TT!N)n e)27Tn = <f>k[n]. This implies that there are only N distinct periodic exponentials in the set given in eq. ( 1.60). For example, ( 1.62) are all distinct, and any other cf>k[n] is identical to one of these (e.g., cf>N[n] = <f>0[n] and <f>-I[n] = cf>N-I[n]). 1 .4 THE UNIT IMPULSE AND UNIT STEP FUNCTIONS In this section, we introduce several other basic signals-specifically, the unit impulse and step functions in continuous and discrete time-that are also of considerable importance in signal and system analysis. In Chapter 2, we will see how we can use unit impulse signals as basic building blocks for the construction and representation of other signals. We begin with the discrete-time case. 1 .4. 1 The Discrete-Time Unit Impulse and Unit Step Sequences One of the simplest discrete-time signals is the unit impulse (or unit sample), which is defined as n=rfO ll[nj = { ~: (1.63) n = 0 and which is shown in Figure 1.28. Throughout the book, we will refer too [n ] interchange- ably as the unit impulse or unit sample. 8[n] • • • • • • • • • • • • • • • Figure 1.28 Discrete-time unit im- n pulse (sample). A second basic discrete-time signal is the discrete-time unit step, denoted by u[n] and defined by u[n] = { ~: n<O o· ( 1.64) n ~ The unit step sequence is shown in Figure 1.29."
1.4 The Unit Impulse and Unit Step Functions,"30 Signals and Systems Chap. 1 In the continuous-time case, all of the harmonically related complex exponentials ei k(27TIT)t, k = 0, ± 1, ± 2, ... , are distinct. However, because of eq. ( 1.51 ), this is not the case in discrete time. Specifically, <f>k+N[n] = ei<k+N)(2TT!N)n (1.61) = eik(2TT!N)n e)27Tn = <f>k[n]. This implies that there are only N distinct periodic exponentials in the set given in eq. ( 1.60). For example, ( 1.62) are all distinct, and any other cf>k[n] is identical to one of these (e.g., cf>N[n] = <f>0[n] and <f>-I[n] = cf>N-I[n]). 1 .4 THE UNIT IMPULSE AND UNIT STEP FUNCTIONS In this section, we introduce several other basic signals-specifically, the unit impulse and step functions in continuous and discrete time-that are also of considerable importance in signal and system analysis. In Chapter 2, we will see how we can use unit impulse signals as basic building blocks for the construction and representation of other signals. We begin with the discrete-time case. 1 .4. 1 The Discrete-Time Unit Impulse and Unit Step Sequences One of the simplest discrete-time signals is the unit impulse (or unit sample), which is defined as n=rfO ll[nj = { ~: (1.63) n = 0 and which is shown in Figure 1.28. Throughout the book, we will refer too [n ] interchange- ably as the unit impulse or unit sample. 8[n] • • • • • • • • • • • • • • • Figure 1.28 Discrete-time unit im- n pulse (sample). A second basic discrete-time signal is the discrete-time unit step, denoted by u[n] and defined by u[n] = { ~: n<O o· ( 1.64) n ~ The unit step sequence is shown in Figure 1.29. Sec. 1.4 The Unit Impulse and Unit Step Functions 31 u[n] • • • • • • • • • • • JIIIIII Figure 1.29 Discrete-time unit step 0 n sequence. Interval of summation o[m] • • • • • • •I • • • . I . . . . . . . . n 0 m (a) Interval of summation o[m] • • • • • • • • • .. I . . . . . . . . . 0 n m Figure 1.30 Running sum of (b) eq. (1.66): (a) n < 0; (b) n > 0. There is a close relationship between the discrete-time unit impulse and unit step. In particular, the discrete-time unit impulse is the first difference of the discrete-time step B [n] = u[n] - u[n - 1]. (1.65) Conversely, the discrete-time unit step is the running sum of the unit sample. That is, n u[n] = ~ B[m]. (1.66) m= -oo Equation (1.66) is illustrated graphically in Figure 1.30. Since the only nonzero value of the unit sample is at the point at which its argument is zero, we see from the figure that the running sum in eq. (1.66) is 0 for n < 0 and 1 for n 2: 0. Furthermore, by changing the variable of summation from m to k = n - min eq. (1.66), we find that the discrete-time unit step can also be written in terms of the unit sample as 0 u[n] = ~ B[n- k], k='X or equivalently, u[n] = ~ B[n - k]. (1.67) k=O 32 Signals and Systems Chap. 1 Interval of summation ~----------- o[n-k] I • • • .I. n • • • 0• • • • • • • • • k (a) Interval of summation '----- ""3[n~k]---- . . . . . . . . . . . . . . I . . . . 0 n k Figure 1 .31 Relationship given in (b) eq. (1.67): (a) n < 0; (b) n > 0. Equation (1.67) is illustrated in Figure 1.31. In this case the nonzero value of 8[n- k] is at the value of k equal ton, so that again we see that the summation in eq. (1.67) is 0 for n < 0 and 1 for n 2:: 0. An interpretation of eq. ( 1.67) is as a superposition of delayed impulses; i.e., we can view the equation as the sum of a unit impulse 8[n] at n = 0, a unit impulse 8[n- 1] at n = 1, another, 8[n- 2], at n = 2, etc. We will make explicit use of this interpretation in Chapter 2. The unit impulse sequence can be used to sample the value of a signal at n = 0. In particular, since 8[n] is nonzero (and equal to 1) only for n = 0, it follows that x[n]o[n] = x[O]o[n]. (1.68) More generally, if we consider a unit impulse 8[n - n0 ] at n = n0 , then x[n]8[n - no] = x[no]8[n- no]. (1.69) This sampling property of the unit impulse will play an important role in Chapters 2 and 7. 1 .4.2 The Continuous-Time Unit Step and Unit Impulse Functions The continuous-time unit step function u(t) is defined in a manner similar to its discrete- time counterpart. Specifically, u(t) ~ { ~: t<O (1.70) t > 0' as is shown in Figure 1.32. Note that the unit step is discontinuous at t 0. The continuous-time unit impulse function 8(t) is related to the unit step in a manner analogous Sec. 1.4 The Unit Impulse and Unit Step Functions 33 u(t) Figure 1 .32 Continuous-time unit 0 step function. to the relationship between the discrete-time unit impulse and step functions. In particular, the continuous-time unit step is the running integral of the unit impulse (1.71) This also suggests a relationship between 8(t) and u(t) analogous to the expression for 8[n] in eq. (1.65). In particular, it follows from eq. (1.71) that the continuous-time unit impulse can be thought of as the first derivative of the continuous-time unit step: 8(t) = d~;t). (1.72) In contrast to the discrete-time case, there is some formal difficulty with this equa- tion as a representation of the unit impulse function, since u(t) is discontinuous at t = 0 and consequently is formally not differentiable. We can, however, interpret eq. (1.72) by considering an approximation to the unit step u11 (t), as illustrated in Figure 1.33, which rises from the value 0 to the value 1 in a short time interval of length Ll. The unit step, of course, changes values instantaneously and thus can be thought of as an idealization of u11 (t) for Ll so short that its duration doesn't matter for any practical purpose. Formally, u(t) is the limit of Uf1 (t) as Ll ~ 0. Let us now consider the derivative ~ ( ) _ du11(t) u 11 t - -----;[(' (1.73) as shown in Figure 1.34. u~(t) 0 11 Figure 1 .33 Continuous approximation to Figure 1.34 Derivative of the unit step, u~(t). u~(t). 34 Signals and Systems Chap. 1 0 0 Figure 1.35 Continuous- Figure 1.36 Scaled im- time unit impulse. pulse. Note that o~(t) is a short pulse, of duration~ and with unit area for any value of~. As~ ~ 0, o~(t) becomes narrower and higher, maintaining its unit area. Its limiting form, o(t) = lim o~(t), (1.74) ~---->0 can then be thought of as an idealization of the short pulse o~(t) as the duration~ becomes insignificant. Since o(t) has, in effect, no duration but unit area, we adopt the graphical notation for it shown in Figure 1.35, where the arrow at t = 0 indicates that the area of the pulse is concentrated at t = 0 and the height of the arrow and the ""1"" next to the arrow are used to represent the area of the impulse. More generally, a scaled impulse ko(t) will have an area k, and thus, L% kD(r)dr = ku(t). A scaled impulse with area k is shown in Figure 1.36, where the height of the arrow used to depict the scaled impulse is chosen to be proportional to the area of the impulse. As with discrete time, we can provide a simple graphical interpretation of the running integral of eq. (1.71); this is shown in Figure 1.37. Since the area of the continuous-time unit impulse o(T ) is concentrated at T = 0, we see that the running integral is 0 fort < 0 and 1 for t > 0. Also, we note that the relationship in eq. ( 1. 71) between the continuous- time unit step and impulse can be rewritten in a different form, analogous to the discrete- time form in eq. (1.67), by changing the variable of integration from T to u = t- T: u(t) = L% (j( r) dr = rD (t - <r)( -da""), or equivalently, u(t) = rD (t- a) d<r. (1.75) The graphical interpretation of this form of the relationship between u(t) and o(t) is given in Figure 1.38. Since in this case the area of o(t - u) is concentrated at the point u = t, we again see that the integral in eq. (1.75) is 0 fort < 0 and 1 fort> 0. This type of graphical interpretation of the behavior of the unit impulse under integration will be extremely useful in Chapter 2. Sec. 1.4 The Unit Impulse and Unit Step Functions 35 Interval of integration Interval of integration 8(t-u) 0 'T 0 (}' (a) (a) Interval of integration Interval of integration --------------~-----, 8(-r) : 8(t-u) I I I I 0 'T 0 (}' (b) (b) Figure 1.37 Running integral given in eq. (1.71 ): Figure 1.38 Relationship given in eq. (1.75): (a) t < 0; (b) t > 0. (a) t < 0; (b) t > 0. As with the discrete-time impulse, the continuous-time impulse has a very important sampling property. In particular, for a number of reasons it will be important to consider the product of an impulse and more well-behaved continuous-time functions x(t). The in- terpretation of this quantity is most readily developed using the definition of o(t) according to eq. (1.74). Specifically, consider Xt (t) = x(t)Dtl.(t). In Figure 1.39(a) we have depicted the two time functions x(t) and Ofl.(t), and in Fig- ure 1.39(b) we see an enlarged view of the nonzero portion of their product. By construc- tion, x1( t) is zero outside the interval 0 ::::; t ::::; ~. For~ sufficiently small so that x(t) is approximately constant over this interval, Since o(t) is the limit as~ ~ 0 of Dfl.(t), it follows that x(t)B(t) = x(O)o(t). ( 1. 76) By the same argument, we have an analogous expression for an impulse concentrated at an arbitrary point, say, t0 . That is, x(t)o (t - to) = x(to)D(t - to). 36 Signals and Systems Chap. 1 o~(t) 0 ~ (a) x(O)~ Figure 1.39 The product x(t)o~(t): (a) graphs of both functions; (b) en- 0 larged view of the nonzero portion of (b) their product. Although our discussion of the unit impulse in this section has been somewhat in- formal, it does provide us with some important intuition about this signal that will be useful throughout the book. As we have stated, the unit impulse should be viewed as an idealization. As we illustrate and discuss in more detail in Section 2.5, any real physi- cal system has some inertia associated with it and thus does not respond instantaneously to inputs. Consequently, if a pulse of sufficiently short duration is applied to such a sys- tem, the system response will not be noticeably influenced by the pulse's duration or by the details of the shape of the pulse, for that matter. Instead, the primary characteristic of the pulse that will matter is the net, integrated effect of the pulse-i.e., its area. For systems that respond much more quickly than others, the pulse will have to be of much shorter duration before the details of the pulse shape or its duration no longer matter. Nev- ertheless, for any physical system, we can always find a pulse that is ""short enough."" The unit impulse then is an idealization of this concept-the pulse that is short enough for any system. As we will see in Chapter 2, the response of a system to this idealized pulse plays a crucial role in signal and system analysis, and in the process of devel- oping and understanding this role, we will develop additional insight into the idealized signal.3 3The unit impulse and other related functions (which are often collectively referred to as singularity functions) have been thoroughly studied in the field of mathematics under the alternative names of general- ized functions and the theory of distributions. For more detailed discussions of this subject, see Distribution Theory and Transfonn Analysis, by A. H. Zemanian (New York: McGraw-Hill Book Company, 1965), Gen- eralised Functions, by R.F. Hoskins (New York: Halsted Press, 1979), or the more advanced text, Fourier Analysis and Generalized Functions, by M. J. Lighthill (New York: Cambridge University Press, 1958). Our discussion of singularity functions in Section 2.5 is closely related in spirit to the mathematical theory described in these texts and thus provides an informal introduction to concepts that underlie this topic in mathematics. Sec. 1.4 The Unit Impulse and Unit Step Functions 37 Example 1.7 Consider the discontinuous signal x(t) depicted in Figure 1.40(a). Because of the rela- tionship between the continuous-time unit impulse and unit step, we can readily calculate and graph the derivative of this signal. Specifically, the derivative of x(t) is clearly 0, except at the discontinuities. In the case of the unit step, we have seen [eq. (1.72)] that differentiation gives rise to a unit impulse located at the point of discontinuity. Further- more, by multiplying both sides of eq. (1.72) by any number k, we see that the derivative of a unit step with a discontinuity of size k gives rise to an impulse of area k at the point of discontinuity. This rule also holds for any other signal with a jump discontinuity, such as x(t) in Figure 1.40(a). Consequently, we can sketch its derivative x(t), as in Fig- ure 1.40(b), where an impulse is placed at each discontinuity of x(t), with area equal to the size of the discontinuity. Note, for example, that the discontinuity in x(t) at t = 2 has a value of- 3, so that an impulse scaled by -3 is located at t = 2 in the signal x(t). x(t) 2- - 1 - 2 3 (a) 4 -1 ~ x(t) 2f- 1 r- 2 3 (b) 4 -1 r- -2 t- -3 f- - - ~Interval of integration 1 -: 2 3 (c) 4 -1 - -2- -3- Figure 1.40 (a) The discontinuous signal x(t) analyzed in Example 1.7; (b) its derivative x(t); (c) depiction of the recovery of x(t) as the running inte- gral of x(t), illustrated for a value of t between 0 and 1. 38 Signals and Systems Chap. 1 As a check of our result, we can verify that we can recover x(t) from x(t). Specif- ically, since x(t) and x(t) are both zero fort :::::: 0, we need only check that fort > 0, 1 X(t) = { X( 7) dT. (1.77) Jo As illustrated in Figure 1.40(c), fort< 1, the integral on the right-hand side of eq. (1.77) is zero, since none of the impulses that constitute x(t) are within the interval of integra- tion. For 1 < t < 2, the first impulse (located at t = 1) is the only one within the inte- gration interval, and thus the integral in eq. (1.77) equals 2, the area of this impulse. For 2 < t < 4, the first two impulses are within the interval of integration, and the integral accumulates the sum of both of their areas, namely, 2 - 3 = - 1. Finally, for t > 4, all three impulses are within the integration interval, so that the integral equals the sum of all three areas-that is, 2 - 3 + 2 = + 1. The result is exactly the signal x(t) depicted in Figure 1.40(a). 1 .5 CONTINUOUS-TIME AND DISCRETE-TIME SYSTEMS Physical systems in the broadest sense are an interconnection of components, devices, or subsystems. In contexts ranging from signal processing and communications to elec- tromechanical motors, automotive vehicles, and chemical-processing plants, a system can be viewed as a process in which input signals are transformed by the system or cause the system to respond in some way, resulting in other signals as outputs. For example, a high- fidelity system takes a recorded audio signal and generates a reproduction of that signal. If the hi-fi system has tone controls, we can change the tonal quality of the reproduced sig- nal. Similarly, the circuit in Figure 1.1 can be viewed as a system with input voltage Vs(t) and output voltage vc(t), while the automobile in Figure 1.2 can be thought of as a system with input equal to the force f(t) and output equal to the velocity v(t) of the vehicle. An image-enhancement system transforms an input image into an output image that has some desired properties, such as improved contrast. A continuous-time system is a system in which continuous-time input signals are applied and result in continuous-time output signals. Such a system will be represented pictorially as in Figure 1.41(a), where x(t) is the input and y(t) is the output. Alterna- tively, we will often represent the input-output relation of a continuous-time system by the notation x(t) ~ y(t). (1.78) x(t) --~• Continuous-time ....... _...,. y(t) system (a) Discrete-time x[n]--~ ......- ...... y[n] system Figure 1 .41 (a) Continuous-time (b) system; (b) discrete-time system."
1.5 Continuous-Time and Discrete-Time Systems,"38 Signals and Systems Chap. 1 As a check of our result, we can verify that we can recover x(t) from x(t). Specif- ically, since x(t) and x(t) are both zero fort :::::: 0, we need only check that fort > 0, 1 X(t) = { X( 7) dT. (1.77) Jo As illustrated in Figure 1.40(c), fort< 1, the integral on the right-hand side of eq. (1.77) is zero, since none of the impulses that constitute x(t) are within the interval of integra- tion. For 1 < t < 2, the first impulse (located at t = 1) is the only one within the inte- gration interval, and thus the integral in eq. (1.77) equals 2, the area of this impulse. For 2 < t < 4, the first two impulses are within the interval of integration, and the integral accumulates the sum of both of their areas, namely, 2 - 3 = - 1. Finally, for t > 4, all three impulses are within the integration interval, so that the integral equals the sum of all three areas-that is, 2 - 3 + 2 = + 1. The result is exactly the signal x(t) depicted in Figure 1.40(a). 1 .5 CONTINUOUS-TIME AND DISCRETE-TIME SYSTEMS Physical systems in the broadest sense are an interconnection of components, devices, or subsystems. In contexts ranging from signal processing and communications to elec- tromechanical motors, automotive vehicles, and chemical-processing plants, a system can be viewed as a process in which input signals are transformed by the system or cause the system to respond in some way, resulting in other signals as outputs. For example, a high- fidelity system takes a recorded audio signal and generates a reproduction of that signal. If the hi-fi system has tone controls, we can change the tonal quality of the reproduced sig- nal. Similarly, the circuit in Figure 1.1 can be viewed as a system with input voltage Vs(t) and output voltage vc(t), while the automobile in Figure 1.2 can be thought of as a system with input equal to the force f(t) and output equal to the velocity v(t) of the vehicle. An image-enhancement system transforms an input image into an output image that has some desired properties, such as improved contrast. A continuous-time system is a system in which continuous-time input signals are applied and result in continuous-time output signals. Such a system will be represented pictorially as in Figure 1.41(a), where x(t) is the input and y(t) is the output. Alterna- tively, we will often represent the input-output relation of a continuous-time system by the notation x(t) ~ y(t). (1.78) x(t) --~• Continuous-time ....... _...,. y(t) system (a) Discrete-time x[n]--~ ......- ...... y[n] system Figure 1 .41 (a) Continuous-time (b) system; (b) discrete-time system. Sec. 1.5 Continuous-Time and Discrete-Time Systems 39 Similarly, a discrete-time system-that is, a system that transforms discrete-time inputs into discrete-time outputs-will be depicted as in Figure 1.41 (b) and will sometimes be represented symbolically as x[n] ~ y[n]. (1.79) In most of this book, we will treat discrete-time systems and continuous-time systems separately but in parallel. In Chapter 7, we will bring continuous-time and discrete-time systems together through the concept of sampling, and we will develop some insights into the use of discrete-time systems to process continuous-time signals that have been sampled. 1.5.1 Simple Examples of Systems One of the most important motivations for the development of general tools for analyzing and designing systems is that systems from many different applications have very similar mathematical descriptions. To illustrate this, we begin with a few simple examples. Example 1.8 Consider the RC circuit depicted in Figure 1.1. If we regard vs(t) as the input signal and vc(t) as the output signal, then we can use simple circuit analysis to derive an equation describing the relationship between the input and output. Specifically, from Ohm's law, the current i(t) through the resistor is proportional (with proportionality constant 11 R) to the voltage drop across the resistor; i.e., '( ) Vs(t) - Vc(t) 1 t = R . (1.80) Similarly, using the defining constitutive relation for a capacitor, we can relate i(t) to the rate of change with time of the voltage across the capacitor: '( ) - cddvc( lt- [t)' (1.81) Equating the right-hand sides of eqs. (1.80) and (1.81), we obtain a differential equation describing the relationship between the input vs(t) and the output vc(t): dvc(t) 1 ( ) _ 1 ( ) dt + RC Vc t - RC Vs t . (1.82) Example 1.9 Consider Figure 1.2, in which we regard the force f(t) as the input and the velocity v(t) as the output. If we let m denote the mass of the automobile and mpv the resistance due to friction, then equating acceleration-i.e., the time derivative of velocity-with net force divided by mass, we obtain dv(t) 1 - [f(t)- pv(t) J, (1.83) dt m 40 Signals and Systems Chap. 1 i.e., dv(t) + p_v(t) = _!_ j(t). (1.84) dt m m Examining and comparing eqs. (1.82) and (1.84) in the above examples, we see that the input-output relationships captured in these two equations for these two very different physical systems are basically the same. In particular, they are both examples of first-order linear differential equations of the form dy(t) -----;[( + ay(t) = bx(t), (1.85) where x(t) is the input, y(t) is the output, and a and bare constants. This is one very simple example of the fact that, by developing methods for analyzing general classes of systems such as that represented by eq. (1.85), we will be able to use them in a wide variety of applications. Example 1. 1 0 As a simple example of a discrete-time system, consider a simple model for the balance in a bank account from month to month. Specifically, let y[n] denote the balance at the end of the nth month, and suppose that y[n] evolves from month to month according to the equation y[n] = l.Oly[n- 1] + x[n], (1.86) or equivalently, y[n] - l.Oly[n- 1] = x[n], (1.87) where x[n] represents the net deposit (i.e., deposits minus withdrawals) during the nth month and the term 1.01y[n- 1] models the fact that we accrue 1% interest each month. Example 1. 11 As a second example, consider a simple digital simulation of the differential equation in eq. (1.84) in which we resolve time into discrete intervals oflength Ll and approximate dv(t)ldt at t = nLl by the first backward difference, i.e., v(nLl) - v((n- 1)Ll) Ll In this case, if we let v[n] = v(nLl) and f[n] = j(nLl), we obtain the following discrete- time model relating the sampled signals f[n] and v[n]: m Ll v[n] - (m + pLl) v[n - 1] = (m + pLl) f[n]. (1.88) Comparing eqs. (1.87) and (1.88), we see that they are both examples of the same general first-order linear difference equation, namely, y[n] + ay[n - 1] = bx [n]. (1.89) Sec. 1.5 Continuous-Time and Discrete-Time Systems 41 As the preceding examples suggest, the mathematical descriptions of systems from a wide variety of applications frequently have a great deal in common, and it is this fact that provides considerable motivation for the development of broadly applicable tools for signal and system analysis. The key to doing this successfully is identifying classes of systems that have two important characteristics: ( 1) The systems in this class have prop- erties and structures that we can exploit to gain insight into their behavior and to develop effective tools for their analysis~ and (2) many systems of practical importance can be accurately modeled using systems in this class. It is on the first of these characteristics that most of this book focuses, as we develop tools for a particular class of systems re- ferred to as linear, time-invariant systems. In the next section, we will introduce the prop- erties that characterize this class, as well as a number of other very important basic system properties. The second characteristic mentioned in the preceding paragraph is of obvious impor- tance for any system analysis technique to be of value in practice. It is a well-established fact that a wide range of physical systems (including those in Examples 1.8-1.10) can be well modeled within the class of systems on which we focus in this book. However, a critical point is that any model used in describing or analyzing a physical system rep- resents an idealization of that system, and thus, any resulting analysis is only as good as the model itself. For example, the simple linear model of a resistor in eq. (1.80) and that of a capacitor in eq. ( 1.81) are idealizations. However, these idealizations are quite accurate for real resistors and capacitors in many applications, and thus, analy- ses employing such idealizations provide useful results and conclusions, as long as the voltages and currents remain within the operating conditions under which these simple linear models are valid. Similarly, the use of a linear retarding force to represent fric- tional effects in eq. (1.83) is an approximation with a range of validity. Consequently, although we will not address this issue in the book, it is important to remember that an essential component of engineering practice in using the methods we develop here consists of identifying the range of validity of the assumptions that have gone into a model and ensuring that any analysis or design based on that model does not violate those assumptions. 1.5.2 Interconnections of Systems An important idea that we will use throughout this book is the concept of the interconnec- tion of systems. Many real systems are built as interconnections of several subsystems. One example is an audio system, which involves the interconnection of a radio receiver, compact disc player, or tape deck with an amplifier and one or more speakers. Another is a digitally controlled aircraft, which is an interconnection of the aircraft, described by its equations of motion and the aerodynamic forces affecting it~ the sensors, which measure various aircraft variables such as accelerations, rotation rates, and heading~ a digital au- topilot, which responds to the measured variables and to command inputs from the pilot (e.g., the desired course, altitude, and speed)~ and the aircraft's actuators, which respond to inputs provided by the autopilot in order to use the aircraft control surfaces (rudder, tail, ailerons) to change the aerodynamic forces on the aircraft. By viewing such a system as an interconnection of its components, we can use our understanding of the component 42 Signals and Systems Chap. 1 Input System 1 System 2 Output (a) Output (b) System 1 Input~ System 4 Output System 3 (c) Figure 1.42 Interconnection of two systems: (a) series (cascade) intercon- nection; (b) parallel interconnection; (c) series-parallel interconnection. systems and of how they are interconnected in order to analyze the operation and behavior of the overall system. In addition, by describing a system in terms of an interconnection of simpler subsystems, we may in fact be able to define useful ways in which to synthesize complex systems out of simpler, basic building blocks. While one can construct a variety of system interconnections, there are several basic ones that are frequently encountered. A series or cascade interconnection of two systems is illustrated in Figure 1.42(a). Diagrams such as this are referred to as block diagrams. Here, the output of System 1 is the input to System 2, and the overall system transforms an input by processing it first by System 1 and then by System 2. An example of a series interconnection is a radio receiver followed by an amplifier. Similarly, one can define a series interconnection of three or more systems. A parallel interconnection of two systems is illustrated in Figure 1.42(b ). Here, the same input signal is applied to Systems 1 and 2. The symbol ""EB"" in the figure denotes addition, so that the output of the parallel interconnection is the sum of the outputs of Systems 1 and 2. An example of a parallel interconnection is a simple audio system with several microphones feeding into a single amplifier and speaker system. In addition to the simple parallel interconnection in Figure 1.42(b ), we can define parallel interconnections of more than two systems, and we can combine both cascade and parallel interconnections Sec. 1.5 Continuous-Time and Discrete-Time Systems 43 Figure 1 .43 Feedback interconnec- tion. to obtain more complicated interconnections. An example of such an interconnection is given in Figure 1.42(c).4 Another important type of system interconnection is a feedback interconnection, an example of which is illustrated in Figure 1.43. Here, the output of System 1 is the input to System 2, while the output of System 2 is fed back and added to the external input to pro- duce the actual input to System 1. Feedback systems arise in a wide variety of applications. For example, a cruise control system on an automobile senses the vehicle's velocity and adjusts the fuel flow in order to keep the speed at the desired level. Similarly, a digitally controlled aircraft is most naturally thought of as a feedback system in which differences between actual and desired speed, heading, or altitude are fed back through the autopilot in order to correct these discrepancies. Also, electrical circuits are often usefully viewed as containing feedback interconnections. As an example, consider the circuit depicted in Figure 1.44(a). As indicated in Figure 1.44(b), this system can be viewed as the feedback interconnection of the two circuit elements. + i1 (t) t i2 (t) t ti (t) C R v(t) + Capacitor i(t) i1 (t) ~ + v(t) v(t) = ~ ~-~i 1 (T)dT - Resistor Figure 1 .44 (a) Simple electrical i2 (t) . (t) _ v(t) circuit; (b) block diagram in which the 12 -R circuit is depicted as the feedback inter- connection of two circuit elements. 40n occasion, we will also use the symbol 0 in our pictorial representation of systems to denote the operation of multiplying two signals (see, for example, Figure 4.26). 44 Signals and Systems Chap. 1 1 .6 BASIC SYSTEM PROPERTIES In this section we introduce and discuss a number of basic properties of continuous-time and discrete-time systems. These properties have important physical interpretations and relatively simple mathematical descriptions using the signals and systems language that we have begun to develop. 1 . 6. 1 Systems with and without Memory A system is said to be memoryl ess if its output for each value of the independent variable at a given time is dependent only on the input at that same time. For example, the system specified by the relationship (1.90) is memory less, as the value of y[n] at any particular time n0 depends only on the value of x[n] at that time. Similarly, a resistor is a memory less system; with the input x(t) taken as the current and with the voltage taken as the output y(t), the input-output relationship of a resistor is y(t) = Rx(t), (1.91) where R is the resistance. One particularly simple memoryless system is the identity sys- tem, whose output is identical to its input. That is, the input-output relationship for the continuous-time identity system is y(t) = x(t), and the corresponding relationship in discrete time is y[n] = x[n]. An example of a discrete-time system with memory is an accumulator or summer II y[n] = L x[k], (1.92) k=-CXJ and a second example is a delay y[n] = x[n - 1]. (1.93) A capacitor is an example of a continuous-time system with memory, since if the input is taken to be the current and the output is the voltage, then y(t) = C1 f'- x x(r)dr, (1.94) where C is the capacitance. Roughly speaking, the concept of memory in a system corresponds to the presence of a mechanism in the system that retains or stores information about input values at times"
1.6 Basic System Properties,"44 Signals and Systems Chap. 1 1 .6 BASIC SYSTEM PROPERTIES In this section we introduce and discuss a number of basic properties of continuous-time and discrete-time systems. These properties have important physical interpretations and relatively simple mathematical descriptions using the signals and systems language that we have begun to develop. 1 . 6. 1 Systems with and without Memory A system is said to be memoryl ess if its output for each value of the independent variable at a given time is dependent only on the input at that same time. For example, the system specified by the relationship (1.90) is memory less, as the value of y[n] at any particular time n0 depends only on the value of x[n] at that time. Similarly, a resistor is a memory less system; with the input x(t) taken as the current and with the voltage taken as the output y(t), the input-output relationship of a resistor is y(t) = Rx(t), (1.91) where R is the resistance. One particularly simple memoryless system is the identity sys- tem, whose output is identical to its input. That is, the input-output relationship for the continuous-time identity system is y(t) = x(t), and the corresponding relationship in discrete time is y[n] = x[n]. An example of a discrete-time system with memory is an accumulator or summer II y[n] = L x[k], (1.92) k=-CXJ and a second example is a delay y[n] = x[n - 1]. (1.93) A capacitor is an example of a continuous-time system with memory, since if the input is taken to be the current and the output is the voltage, then y(t) = C1 f'- x x(r)dr, (1.94) where C is the capacitance. Roughly speaking, the concept of memory in a system corresponds to the presence of a mechanism in the system that retains or stores information about input values at times Sec. 1.6 Basic System Properties 45 other than the current time. For example, the delay in eq. (1.93) must retain or store the preceding value of the input. Similarly, the accumulator in eq. (1.92) must ""remember"" or store information about past inputs. In particular, the accumulator computes the running sum of all inputs up to the current time, and thus, at each instant of time, the accumulator must add the current input value to the preceding value of the running sum. In other words, the relationship between the input and output of an accumulator can be described as n-1 y[n] = L x[k] + x[n], (1.95) k=-X or equivalently, y[n] = y[n - 1] + x[n]. (1.96) Represented in the latter way, to obtain the output at the current time n, the accumulator must remember the running sum of previous input values, which is exactly the preceding value of the accumulator output. In many physical systems, memory is directly associated with the storage of energy. For example, the capacitor in eq. (1.94) stores energy by accumulating electrical charge, represented as the integral of the current. Thus, the simple RC circuit in Example 1.8 and Figure 1.1 has memory physically stored in the capacitor. Similarly, the automobile in Figure 1.2 has memory stored in its kinetic energy. In discrete-time systems implemented with computers or digital microprocessors, memory is typically directly associated with storage registers that retain values between clock pulses. While the concept of memory in a system would typically suggest storing past input and output values, our formal definition also leads to our referring to a system as having memory if the current output is dependent on future values of the input and output. While systems having this dependence on future values might at first seem unnatural, they in fact form an important class of systems, as we discuss further in Section 1.6.3. 1.6.2 lnvertibility and Inverse Systems A system is said to be invertible if distinct inputs lead to distinct outputs. As illustrated in Figure 1.45(a ) for the discrete-time case, if a system is invertible, then an inverse system exists that, when cascaded with the original system, yields an output w[n] equal to the input x[n] to the first system. Thus, the series interconnection in Figure 1.45(a) has an overall input-output relationship which is the same as that for the identity system. An example of an invertible continuous-time system is y(t) = 2x(t), (1.97) for which the inverse system is 1 w(t) = 2: y(t). (1.98) This example is illustrated in Figure 1.45(b) . Another example of an invertible system is the accumulator of eq. (1.92). For this system, the difference between two successive 46 Signals and Systems Chap. 1 y[n] x[n] System ......- -...~~ w[n] = x[n] (a) x(t) (b) y(n] •I w(n] ~ y[n] - y(n -1] ~ w(n] x[n] x[n[ --I......__Y_[n_J_=_k _}_ _x_ _x[-kJ_ _. (c) Figure 1 .45 Concept of an inverse system for: (a) a general invertible sys- tem; (b) the invertible system described by eq. (1.97); (c) the invertible system defined in eq. (1.92). values of the output is precisely the last input value. Therefore, in this case, the inverse system is w[n] = y[n] - y[n - 1], (1.99) as illustrated in Figure 1.45( c). Examples of noninvertible systems are y[n] = 0, (1.100) that is, the system that produces the zero output sequence for any input sequence, and (1.101) in which case we cannot determine the sign of the input from knowledge of the output. The concept of invertibility is important in many contexts. One example arises in systems for encoding used in a wide variety of communications applications. In such a system, a signal that we wish to transmit is first applied as the input to a system known as an encoder. There are many reasons for doing this, ranging from the desire to encrypt the original message for secure or private communication to the objective of providing some redundancy in the signal (for example, by adding what are known as parity bits) so that any errors that occur in transmission can be detected and, possibly, corrected. For lossless coding, the input to the encoder must be exactly recoverable from the output; i.e., the encoder must be invertible. 1 .6.3 Causality A system is causal if the output at any time depends only on values of the input at the present time and in the past. Such a system is often referred to as being nonanticipative, as Sec. 1.6 Basic System Properties 47 the system output does not anticipate future values of the input. Consequently, if two inputs to a causal system are identical up to some point in time to or n0 , the corresponding outputs must also be equal up to this same time. The RC circuit of Figure 1.1 is causal, since the capacitor voltage responds only to the present and past values of the source voltage. Similarly, the motion of an automobile is causal, since it does not anticipate future actions of the driver. The systems described in eqs. (1.92)- (1.94) are also causal, but the systems defined by y[n] = x[n] - x[n + 1] (1.102) and y(t) = x(t + 1) (1.103) are not. All memory less systems are causal, since the output responds only to the current value of the input. Although causal systems are of great importance, they do not by any means constitute the only systems that are of practical significance. For example, causality is not often an essential constraint in applications in which the independent variable is not time, such as in image processing. Furthermore, in processing data that have been recorded previously, as often happens with speech, geophysical, or meteorological signals, to name a few, we are by no means constrained to causal processing. As another example, in many applications, including historical stock market analysis and demographic studies, we may be interested in determining a slowly varying trend in data that also contain high-frequency fluctuations about that trend. In this case, a commonly used approach is to average data over an interval in order to smooth out the fluctuations and keep only the trend. An example of a noncausal averaging system is 1 +M y[n] 2M + 1 k~ x[ n - k]. (1.104) M Example 1. 1 2 When checking the causality of a system, it is important to look carefully at the input- output relation. To illustrate some of the issues involved in doing this, we will check the causality of two particular systems. The first system is defined by y[n] = x[ -n]. (1.105) Note that the output y[n0 ] at a positive time no depends only on the value of the input signal x[- no] at time (- n0 ), which is negative and therefore in the past of no. We may be tempted to conclude at this point that the given system is causal. However, we should always be careful to check the input-output relation for all times. In particular, for n < 0, e.g. n = -4, we see that y[ -4] = x[4 ], so that the output at this time depends on a future value of the input. Hence, the system is not causal. It is also important to distinguish carefully the effects of the input from those of any other functions used in the definition of the system. For example, consider the system y(t) = x(t) cos(t + 1). (1.106) 48 Signals and Systems Chap. 1 In this system, the output at any time t equals the input at that same time multiplied by a number that varies with time. Specifically, we can rewrite eq. ( 1.1 06) as y(t) = x(t)g(t), where g(t) is a time-varying function, namely g(t) = cos(t + 1). Thus, only the current value of the input x(t) influences the current value of the output y(t), and we conclude that this system is causal (and, in fact, memoryless). 1 .6.4 Stability Stability is another important system property. Informally, a stable system is one in which small inputs lead to responses that do not diverge. For example, consider the pendulum in Figure 1.46(a), in which the input is the applied force x(t) and the output is the angular deviation y(t) from the vertical. In this case, gravity applies a restoring force that tends to return the pendulum to the vertical position, and frictional losses due to drag tend to slow it down. Consequently, if a small force x(t) is applied, the resulting deflection from vertical will also be small. In contrast, for the inverted pendulum in Figure 1.46(b ), the effect of gravity is to apply a force that tends to increase the deviation from vertical. Thus, a small applied force leads to a large vertical deflection causing the pendulum to topple over, despite any retarding forces due to friction. The system in Figure 1.46(a) is an example of a stable system, while that in Fig- ure 1.46(b) is unstable. Models for chain reactions or for population growth with unlim- ited food supplies and no predators are examples of unstable systems, since the system response grows without bound in response to small inputs. Another example of an unsta- ble system is the model for a bank account balance in eq. (1.86), since if an initial deposit is made (i.e., x[O] = a positive amount) and there are no subsequent withdrawals, then that deposit will grow each month without bound, because of the compounding effect of interest payments. x(t) (a) x(t) Figure 1 .46 (a) A stable pendulum; (b) (b) an unstable inverted pendulum. Sec. 1.6 Basic System Properties 49 There are also numerous examples of stable systems. Stability of physical systems generally results from the presence of mechanisms that dissipate energy. For example, assuming positive component values in the simple RC circuit of Example 1.8, the resistor dissipates energy and this circuit is a stable system. The system in Example 1.9 is also stable because of the dissipation of energy through friction. The preceding examples provide us with an intuitive understanding of the concept of stability. More formally, if the input to a stable system is bounded (i.e., if its magnitude does not grow without bound), then the output must also be bounded and therefore cannot diverge. This is the definition of stability that we will use throughout this book. For exam- ple, consider applying a constant force j(t) = F to the automobile in Figure 1.2, with the vehicle initially at rest. In this case the velocity of the car will increase, but not without bound, since the retarding frictional force also increases with velocity. In fact, the velocity will continue to increase until the frictional force exactly balances the applied force; so, from eq. (1.84), we see that this terminal velocity value V must satisfy _eV=_!__F (1.107) m m' i.e., v = !'__, (1.108) p As another example, consider the discrete-time system defined by eq. (1.1 04) , and suppose that the input x[n] is bounded in magnit~de by some number, say, B, for all values of n. Then the largest possible magnitude for y[n] is also B, because y[n] is the average of a finite set of values of the input. Therefore, y[n] is bounded and the system is stable. On the other hand, consider the accumulator described by eq. (1.92). Unlike the system in eq. ( 1.104 ), this system sums all of the past values of the input rather than just a finite set of values, and the system is unstable, since the sum can grow continually even if x[n] is bounded. For example, if the input to the accumulator is a unit step u[n], the output will be 11 y[n] :Z u[k] = (n + 1)u[n]. k =-'X That is, y[O] = 1, y[l] = 2, y[2] = 3, and so on, and y[n] grows without bound. Example 1 . 1 3 If we suspect that a system is unstable, then a useful strategy to verify this is to look for a specific bounded input that leads to an unbounded output. Finding one such example enables us to conclude that the given system is unstable. If such an example does not exist or is difficult to find, we must check for stability by using a method that does not utilize specific examples of input signals. To illustrate this approach, let us check the stability of two systems, sl: y(t) = tx(t) (1.109) 50 Signals and Systems Chap. 1 and (1.110) In seeking a specific counterexample in order to disprove stability, we might try simple bounded inputs such as a constant or a unit step. For system S1 in eq. (1.109), a constant input x(t) = 1 yields y(t) = t, which is unbounded, since no matter what finite con- stant we pick, iy(t)l will exceed that constant for some t. We conclude that system 5 1 is unstable. For system S2 , which happens to be stable, we would be unable to find a bounded input that results in an unbounded output. So we proceed to verify that all bounded inputs result in bounded outputs. Specifically, let B be an arbitrary positive number, and let x(t) be an arbitrary signal bounded by B; that is, we are making no assumption about x(t), except that ix(t)i < B, (1.111) or -B < x(t) < B, (1.112) for all t. Using the definition of S2 in eq. (1.110), we then see that if x(t) satisfies eq. (1.111), then y(t) must satisfy (1.113) We conclude that if any input to S2 is bounded by an arbitrary positive number B, the corresponding output is guaranteed to be bounded by e8 . Thus, S2 is stable. The system properties and concepts that we have introduced so far in this section are of great importance, and we will examine some of these in more detail later in the book. There remain, however, two additional properties-time invariance and linearity- that play a particularly central role in the subsequent chapters of the book, and in the remainder of this section we introduce and provide initial discussions of these two very important concepts. 1.6.5 Time lnvariance Conceptually, a system is time invariant if the behavior and characteristics of the system are fixed over time. For example, the RC circuit of Figure 1.1 is time invariant if the resistance and capacitance values R and C are constant over time: We would expect to get the same results from an experiment with this circuit today as we would if we ran the identical experiment tomorrow. On the other hand, if the values of R and C are changed or fluctuate over time, then we would expect the results of our experiment to depend on the time at which we run it. Similarly, if the frictional coefficient b and mass m of the automobile in Figure 1.2 are constant, we would expect the vehicle to respond identically independently of when we drive it. On the other hand, if we load the auto's trunk with heavy suitcases one day, thus increasing m, we would expect the car to behave differently than at other times when it is not so heavily loaded. The property of time in variance can be described very simply in terms of the signals and systems language that we have introduced. Specifically, a system is time invariant if Sec. 1.6 Basic System Properties 51 a time shift in the input signal results in an identical time shift in the output signal. That is, if y[n] is the output of a discrete-time, time-invariant system when x[n] is the input, then y [n- n0 ] is the output when x [n- n0 ] is applied. In continuous time with y(t) the output corresponding to the input x(t), a time-invariant system will have y (t- to) as the output when x (t - to) is the input. To see how to determine whether a system is time invariant or not, and to gain some insight into this property, consider the following examples: Example 1 . 1 4 Consider the continuous-time system defined by y(t) = sin [ x(t)]. (1.114) To check that this system is time invariant, we must determine whether the time- invariance property holds for any input and any time shift t0 • Thus, let x 1( t) be an arbitrary input to this system, and let (1.115) be the corresponding output. Then consider a second input obtained by shifting x 1 (t) in time: x2(t) = x, (t - to). (1.116) The output corresponding to this input is Y2(t) = sin [ x2(t)] = sin [ x, (t - to)]. (1.117) Similarly, from eq. (1.115), y,(t- to) = sin[ Xt (t- to)]. (1.118) Comparing eqs. (1.117) and (1.118), we see that y2(t) = y 1 (t- to), and therefore, this system is time invariant. Example 1 . 1 5 As a second example, consider the discrete-time system y[n] = nx[n]. (1.119) This is a time-varying system, a fact that can be verified using the same formal procedure as that used in the preceding example (see Problem 1.28). However, when a system is suspected of being time varying, an approach to showing this that is often very useful is to seek a counterexample-i.e., to use our intuition to find an input signal for which the condition of time invariance is violated. In particular, the system in this example represents a system with a time-varying gain. For example, if we know that the current input value is 1, we cannot determine the current output value without knowing the current time. Consequently, consider the input signal x 1 [n] = 8[n], which yields an output y 1 [n] that is identically 0 (since n8[n] = 0). However, the input x 2[n] = 8[n -1] yields the output y2[n] = n8[n- 1] = 8[n- 1]. Thus, while x 2[n] is a shifted version of x 1 [n], Y2[n] is not a shifted version of y 1 [n]. 52 Signals and Systems Chap. 1 While the system in the preceding example has a time-varying gain and as a result is a time-varying system, the system in eq. (1.97) has a constant gain and, in fact, is time invariant. Other examples of time-invariant systems are given by eqs. (1.91)-(1.104). The following example illustrates a time-varying system. Example 1 . 1 6 Consider the system y(t) = x(2t). (1.120) This system represents a time scaling. That is, y(t) is a time-compressed (by a factor of 2) version of x(t). Intuitively, then, any time shift in the input will also be compressed by a factor of 2, and it is for this reason that the system is not time invariant. To demon- strate this by counterexample, consider the input x 1( t) shown in Figure 1.47(a) and the resulting output y 1 (t) depicted in Figure 1.47(b). If we then shift the input by 2-i.e., consider x 2(t) = x 1 (t- 2), as shown in Figure 1.47(c)-we obtain the resulting output x1(t) ~(t) -2 2 -1 (a) (b) x2(t) = x1(t-2) Y2(t) 0 4 0 2 (c) (d) (e) Figure 1.47 (a) The input x1(t) to the system in Example 1.16; (b) the output Y1 (t) corresponding to x1 (t); (c) the shifted input x2(t) = x1 (t- 2); (d) the output Y2(t) corresponding to x2(t); (e) the shifted signal Y1 (t - 2). Note that Y2 ( t) =1= Y1 ( t - 2), showing that the system is not time invariant. Sec. 1.6 Basic System Properties 53 y2(t) = x 2(2t) shown in Figure 1.47(d). Comparing Figures 1.47(d) and (e), we see that yz(t) =I= y 1 (t- 2), so that the system is not time invariant. (In fact, y2(t) = y 1 (t- 1) , so that the output time shift is only half as big as it should be for time in variance, due to the time compression imparted by the system.) 1 .6.6 Linearity A linear system, in continuous time or discrete time, is a system that possesses the impor- tant property of superposition: If an input consists of the weighted sum of several signals, then the output is the superposition-that is, the weighted sum-of the responses of the system to each of those signals. More precisely, let y 1 (t) be the response of a continuous- time system to an input x 1 (t), and let y2(t) be the output corresponding to the input x 2(t). Then the system is linear if: 1. The response to x 1 (t) + x2(t) is Yl (t) + Y2(t). 2. The response to ax1( t) is ay1( t), where a is any complex constant. The first of these two properties is known as the additivity property; the second is known as the scaling or homogeneity property. Although we have written this description using continuous-time signals, the same definition holds in discrete time. The systems specified by eqs. (1.91)-(1.100), (1.102)-(1.104), and (1.119) are linear, while those defined by eqs. (1.101) and (1.114) are nonlinear. Note that a system can be linear without being time invariant, as in eq. (1.119), and it can be time invariant without being linear, as in eqs. (1.1 0 1) and (1.114). The two properties defining a linear system can be combined into a single statement: continuous time: ax1( t) + bx2(t) ~ ay1( t) + by2(t), (1.121) discrete time: ax1[ n] + bx2[n] ~ ay1 [n] + by2[n]. ( 1.122) Here, a and b are any complex constants. Furthermore, it is straightforward to show from the definition of linearity that if xdn], k = 1, 2, 3, ... , are a set of inputs to a discrete- time linear system with corresponding outputs ydn], k = 1, 2, 3, ... , then the response to a linear combination of these inputs given by x[n] = ~ akxdn] = a1 XJ [n] + a2x2[n] + a3x3[n] + . . . (1.123) k is y[n] = ~ akyk[n] = G(VJ [n] + a2y2[n] + a3y3[n] + .... ( 1.124) k This very important fact is known as the superposition property, which holds for linear systems in both continuous and discrete time. A direct consequence of the superposition property is that, for linear systems, an input which is zero for all time results in an output which is zero for all time. For example, if x[n] ~ y[n], then the homogeneity property tells us that 0 = 0 · x[n] ~ 0 · y[n] = 0. (1.125) 54 Signals and Systems Chap. 1 In the following examples we illustrate how the linearity of a given system can be checked by directly applying the definition of linearity. Example 1 . 1 7 Consider a systemS whose input x(t) and output y(t) are related by y(t) = tx(t) To determine whether or not Sis linear, we consider two arbitrary inputs x 1( t) and x2(t). Xt (t) ~ Yt (t) = tXt (t) x2(t) ~ Yz(t) = tx2(t) Let x3(t) be a linear combination of x 1( t) and x2(t). That is, X3(t) = ax1 (t) + bx2(t) where a and b are arbitrary scalars. If x3(t) is the input to S, then the corresponding output may be expressed as y3(t) = tx:,(t) = t(ax1 (t) + bx2(t)) = atx1 (t) + btx2(t) = ay1 (t) + byz(t) We conclude that the system S is linear. Example 1 . 1 8 Let us apply the linearity-checking procedure of the previous example to another system S whose input x(t) and output y(t) are related by y(t) = x 2(t) Defining x 1( t), xz(t), and x3(t) as in the previous example, we have Xt (t) ~ Yt (t) = xi(t) Xz(t) ~ Yz(t) = x~(t) and X3(t) ~ y3(t) = x~(t) = (ax1 (t) + bx 2 2(t)) = a2 xi(t) + b2 x~(t) + 2abxt (t)xz(t) = a2y 2 1( t) + b yz(t) + 2abxt (t)xz(t) Clearly, we can specify x 1( t), xz(t), a, and b such that y3(t) is not the same as ay1( t) + byz(t).Forexample,ifx1(t) = l,x2(t) = O,a = 2,andb = O,theny3(t) = (2x 1(t))2 = 4, but 2y 1( t) = 2(x1 (t))2 = 2. We conclude that the systemS is not linear. Example 1 . 1 9 In checking the linearity of a system, it is important to remember that the system must satisfy both the additivity and homogeneity properties and that the signals, as well as any scaling constants, are allowed to be complex. To emphasize the importance of these Sec. 1.6 Basic System Properties 55 points, consider the system specified by y[n] = (Re{x[n]}. (1.126) As shown in Problem 1.29, this system is additive; however, it does not satisfy the ho- mogeneity property, as we now demonstrate. Let x 1 [n] = r[n] + js[n] (1.127) be an arbitrary complex input with real and imaginary parts r[n] and s[n], respectively, so that the corresponding output is y, [n] = r[n]. (1.128) Now, consider scaling x 1 [n] by a complex number, for example, a = j; i.e., consider the input x2[n] = jx 1 [n] = j(r[n] + js[n]) (1.129) = -s[n] + jr[n]. The output corresponding to x2 [n] is Y2[n] = CRe{x2[n]} = -s[n], (1.130) which is not equal to the scaled version of y 1 [n], ay 1 [n] = jr[n]. (1.131) We conclude that the system violates the homogeneity property and hence is not linear. Example 1 .20 Consider the system y[n] = 2x[n] + 3. (1.132) This system is not linear, as can be verified in several ways. For example, the system violates the additivity property: If x 1 [n] = 2 and x2 [n] = 3, then x, [n] ~ y, [n] = 2x, [n] + 3 = 7, (1.133) x2[n] ~ Y2[n] = 2x2[n] + 3 = 9. (1.134) However, the response to x3[n] = x 1[n] + x2[n] is y3[n] = 2[x,[n] + x2[n]] + 3 = 13, (1.135) which does not equal y 1 [n] + y2[n] = 16. Alternatively, since y[n] = 3 if x[n] = 0, we see that the system violates the ""zero-in/zero-out"" property of linear systems given in eq. (1.125). It may seem surprising that the system in the above example is nonlinear, since eq. (1.132) is a linear equation. On the other hand, as depicted in Figure 1.48, the output of this system can be represented as the sum of the output of a linear system and another signal equal to the zero-input response of the system. For the system in eq. (1.132), the linear system is x[n] ~ 2x[n], and the zero-input response is Yo[n] = 3. 56 Signals and Systems Chap. 1 Yo (t) Linear x(t)--~1 system 1---......~ + ~-----~~ y(t) Figure 1.48 Structure of an incrementally linear system. Here, y0[n] is the zero-input response of the system. There are, in fact, large classes of systems in both continuous and discrete time that can be represented as in Figure 1.48-i.e., for which the overall system output consists of the superposition of the response of a linear system with a zero-input response. As shown in Problem 1.47, such systems correspond to the class of incrementally linear systems-i.e., systems in continuous or discrete time that respond linearly to changes in the input. In other words, the difference between the responses to any two inputs to an incrementally linear system is a linear (i.e., additive and homogeneous) function of the difference between the two inputs. For example, if x 1 [n] and x2[n] are two inputs to the system specified by eq. (1.132), and if y 1 [n] and y2 [n] are the corresponding outputs, then y, [n] - Y2[n] = 2x, [n] + 3 - {2x2[n] + 3} = 2 { x, [n] - x2[n]} . (1.136) 1.7 SUMMARY In this chapter, we have developed a number of basic concepts related to continuous-time and discrete-time signals and systems. We have presented both an intuitive picture of what signals and systems are through several examples and a mathematical representation for signals and systems that we will use throughout the book. Specifically, we introduced graphical and mathematical representations of signals and used these representations in performing transformations of the independent variable. We also defined and examined several basic signals, both in continuous time and in discrete time. These included com- plex exponential signals, sinusoidal signals, and unit impulse and step functions. In ad- dition, we investigated the concept of periodicity for continuous-time and discrete-time signals. In developing some of the elementary ideas related to systems, we introduced block diagrams to facilitate our discussions concerning the interconnection of systems, and we defined a number of important properties of systems, including causality, stability, time invariance, and linearity. The primary focus in most of this book will be on the class of linear, time-invariant (LTI) systems, both in continuous time and in discrete time. These systems play a par- ticularly important role in system analysis and design, in part due to the fact that many systems encountered in nature can be successfully modeled as linear and time invariant. Furthermore, as we shall see in the following chapters, the properties of linearity and time in variance allow us to analyze in detail the behavior of LTI systems."
1.7 Summary,"56 Signals and Systems Chap. 1 Yo (t) Linear x(t)--~1 system 1---......~ + ~-----~~ y(t) Figure 1.48 Structure of an incrementally linear system. Here, y0[n] is the zero-input response of the system. There are, in fact, large classes of systems in both continuous and discrete time that can be represented as in Figure 1.48-i.e., for which the overall system output consists of the superposition of the response of a linear system with a zero-input response. As shown in Problem 1.47, such systems correspond to the class of incrementally linear systems-i.e., systems in continuous or discrete time that respond linearly to changes in the input. In other words, the difference between the responses to any two inputs to an incrementally linear system is a linear (i.e., additive and homogeneous) function of the difference between the two inputs. For example, if x 1 [n] and x2[n] are two inputs to the system specified by eq. (1.132), and if y 1 [n] and y2 [n] are the corresponding outputs, then y, [n] - Y2[n] = 2x, [n] + 3 - {2x2[n] + 3} = 2 { x, [n] - x2[n]} . (1.136) 1.7 SUMMARY In this chapter, we have developed a number of basic concepts related to continuous-time and discrete-time signals and systems. We have presented both an intuitive picture of what signals and systems are through several examples and a mathematical representation for signals and systems that we will use throughout the book. Specifically, we introduced graphical and mathematical representations of signals and used these representations in performing transformations of the independent variable. We also defined and examined several basic signals, both in continuous time and in discrete time. These included com- plex exponential signals, sinusoidal signals, and unit impulse and step functions. In ad- dition, we investigated the concept of periodicity for continuous-time and discrete-time signals. In developing some of the elementary ideas related to systems, we introduced block diagrams to facilitate our discussions concerning the interconnection of systems, and we defined a number of important properties of systems, including causality, stability, time invariance, and linearity. The primary focus in most of this book will be on the class of linear, time-invariant (LTI) systems, both in continuous time and in discrete time. These systems play a par- ticularly important role in system analysis and design, in part due to the fact that many systems encountered in nature can be successfully modeled as linear and time invariant. Furthermore, as we shall see in the following chapters, the properties of linearity and time in variance allow us to analyze in detail the behavior of LTI systems. Chap. 1 Problems 57 Basic problems emphasize the mechanics of using concepts and methods in a man- ner similar to that illustrated in the examples that are solved in the text. Advanced problems explore and elaborate upon the foundations and practical im- plications of the textual material. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The next two sections contain problems belonging to the basic and advanced categories, respectively. A final section, Mathematical Review, pro- vides practice problems on the fundamental ideas of complex arithmetic and algebra. BASIC PROBLEMS WITH ANSWERS 1.1. Express each of the following complex numbers in Cartesian form (x + jy): ~ei1T, l2e - }1r , e.i1T12 , e- }1r!2 , e.i51T!2 ,y- fL2e .i1T14 ,-JfL2e .i91TI4 ,yfL2 e- j91TI4 ,yfL2 e- )1TI4 . ~ 1.2. Express each of the following complex numbers in polar form (re.i8, with - 'TT < () ~ 'TT): 5, -2, -3j, ~ - j J!' 1 + j, (1- j)2, j(l- j), (1 + j)/(1- j), ( h + Jfi)l (l+j}3). - - 1.3. Determine the values of P x and Ex for each of the following signals: (a) x (t) = e- 21 1 u(t) (b) x2(t) = e.i(:21+1T/4J (c) x3(t) = cos(t) (d) x1[n] = (~)''u[n] (e) x2[n] = e.i(1TI2n+1TI'd) (f) x3[n] = cos(*n) 1.4. Let x[n] be a signal with x[n] = 0 for n < -2 and n > 4. For each signal given below, determine the values of n for which it is guaranteed to be zero. (a) x[n - 3] (b) x[n + 4] (c) x[- n] (d) x[ -n + 2] (e) x[ -n- 2] 1.5. Let x(t) be a signal with x(t) = 0 fort < 3. For each signal given below, determine the values oft for which it is guaranteed to be zero. (a) x(l - t) (b) x(l - t) + x(2- t) (c) x(l - t)x(2 - t) (d) x(3t) (e) x(t/3) 1.6. Determine whether or not each of the following signals is periodic: (a) x 1(t) = 2e.i(1+1TI4lu(t) (b) x2[n] = u[n] + u[ -n] (c) x3[n] = ~7=-Y~{8[n- 4k]- 8[n- 1- 4k]} 1.7. For each signal given below, determine all the values of the independent variable at which the even part of the signal is guaranteed to be zero. (a) x 1 [n] = u[n] - u[n - 4] (b) x2(t) = sin( ~t) (c) x3[n] = (~)''u[n- 3] (d) x4(t) = e- 51~(t + 2) 1.8. Express the real part of each of the following signals in the form Ae-ar cos(wt + cp), where A, a, w, and cp are real numbers with A> 0 and -7r < cp ~ 'TT: (a) x 1( t) = -2 (b) x2(t) = heJ1TI4 cos(3t + 27T) (c) x3(t) = e- 1 sin(3t + 'TT) (d) x4 (t) = je(-2-r JIOO)t 1.9. Determine whether or not each of the following signals is periodic. If a signal is periodic, specify its fundamental period. (a) x 1(t) = jej\Ot (b) x2(t) = e(-l+iJr (c) x3(n) = ej71rn (d) x4[n] = 3ej31T!n+l/2)/5 (e) x5[n] = 3ei3/5(n+l/2)"
Problems,"Chap. 1 Problems 57 Basic problems emphasize the mechanics of using concepts and methods in a man- ner similar to that illustrated in the examples that are solved in the text. Advanced problems explore and elaborate upon the foundations and practical im- plications of the textual material. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The next two sections contain problems belonging to the basic and advanced categories, respectively. A final section, Mathematical Review, pro- vides practice problems on the fundamental ideas of complex arithmetic and algebra. BASIC PROBLEMS WITH ANSWERS 1.1. Express each of the following complex numbers in Cartesian form (x + jy): ~ei1T, l2e - }1r , e.i1T12 , e- }1r!2 , e.i51T!2 ,y- fL2e .i1T14 ,-JfL2e .i91TI4 ,yfL2 e- j91TI4 ,yfL2 e- )1TI4 . ~ 1.2. Express each of the following complex numbers in polar form (re.i8, with - 'TT < () ~ 'TT): 5, -2, -3j, ~ - j J!' 1 + j, (1- j)2, j(l- j), (1 + j)/(1- j), ( h + Jfi)l (l+j}3). - - 1.3. Determine the values of P x and Ex for each of the following signals: (a) x (t) = e- 21 1 u(t) (b) x2(t) = e.i(:21+1T/4J (c) x3(t) = cos(t) (d) x1[n] = (~)''u[n] (e) x2[n] = e.i(1TI2n+1TI'd) (f) x3[n] = cos(*n) 1.4. Let x[n] be a signal with x[n] = 0 for n < -2 and n > 4. For each signal given below, determine the values of n for which it is guaranteed to be zero. (a) x[n - 3] (b) x[n + 4] (c) x[- n] (d) x[ -n + 2] (e) x[ -n- 2] 1.5. Let x(t) be a signal with x(t) = 0 fort < 3. For each signal given below, determine the values oft for which it is guaranteed to be zero. (a) x(l - t) (b) x(l - t) + x(2- t) (c) x(l - t)x(2 - t) (d) x(3t) (e) x(t/3) 1.6. Determine whether or not each of the following signals is periodic: (a) x 1(t) = 2e.i(1+1TI4lu(t) (b) x2[n] = u[n] + u[ -n] (c) x3[n] = ~7=-Y~{8[n- 4k]- 8[n- 1- 4k]} 1.7. For each signal given below, determine all the values of the independent variable at which the even part of the signal is guaranteed to be zero. (a) x 1 [n] = u[n] - u[n - 4] (b) x2(t) = sin( ~t) (c) x3[n] = (~)''u[n- 3] (d) x4(t) = e- 51~(t + 2) 1.8. Express the real part of each of the following signals in the form Ae-ar cos(wt + cp), where A, a, w, and cp are real numbers with A> 0 and -7r < cp ~ 'TT: (a) x 1( t) = -2 (b) x2(t) = heJ1TI4 cos(3t + 27T) (c) x3(t) = e- 1 sin(3t + 'TT) (d) x4 (t) = je(-2-r JIOO)t 1.9. Determine whether or not each of the following signals is periodic. If a signal is periodic, specify its fundamental period. (a) x 1(t) = jej\Ot (b) x2(t) = e(-l+iJr (c) x3(n) = ej71rn (d) x4[n] = 3ej31T!n+l/2)/5 (e) x5[n] = 3ei3/5(n+l/2) 58 Signals and Systems Chap. 1 1.10. Determine the fundamental period of the signal x(t) = 2cos(10t + 1)- sin(4t -1). 1.11. Determine the fundamental period of the signal x[n] = 1 + ej4mzn - ej2mz!S. 1.12. Consider the discrete-time signal x[n] = 1 - .L o[n - 1 - k]. k=3 Determine the values of the integers M and n0 so that x[n] may be expressed as x[n] = u[Mn- no]. 1.13. Consider the continuous-time signal x(t) = o(t + 2) - o(t - 2). Calculate the value of Eoo for the signal y(t) = tx X(T)dT. 1.14. Consider a periodic signal 0 ~ t ~ 1 l<t<2 with period T = 2. The derivative of this signal is related to the ""impulse train"" g(t) = .L o(t- 2k) k=-x with period T = 2. It can be shown that dx(t) ----;[( = A I g(t - tJ) + A2g(t - t2)· Determine the values of A 1, t1, A 2, and t2. 1.15. Consider a systemS with input x[ n] and output y[ n]. This system is obtained through a series interconnection of a system S1 followed by a system S2. The input-output relationships for S 1 and S2 are YI [n] = 2xi [n] + 4xi [n - 1], 1 Y2[n] = x2[n- 2] + 2x2[n- 3], where x 1 [n] and x2 [n] denote input signals. (a) Determine the input-output relationship for systemS. (b) Does the input-output relationship of systemS change if the order in which S1 and S2 are connected in series is reversed (i.e., if S2 follows SJ)? 1.16. Consider a discrete-time system with input x[n] and output y[n]. The input-output relationship for this system is y[n] = x[n]x[n - 2]. Chap. 1 Problems 59 (a) Is the system memoryless? (b) Determine the output of the system when the input is A8[n], where A is any real or complex number. (c) Is the system invertible? 1.17. Consider a continuous-time system with input x(t) and output y(t) related by y(t) = x(sin(t)). (a) Is this system causal? (b) Is this system linear? 1.18. Consider a discrete-time system with input x[n] and output y[n] related by n+n0 y[n] = L x[k], k= n-no where n0 is a finite positive integer. (a) Is this system linear? (a) Is this system time-invariant? (c) If x[n] is known to be bounded by a finite integer B (i.e., jx[n]j < B for all n), it can be shown that y[n] is bounded by a finite number C. We conclude that the given system is stable. Express C in terms of Band n0 . 1.19. For each of the following input-output relationships, determine whether the corre- sponding system is linear, time invariant or both. (a) y(t) = t2 x(t- 1) (b) y[n] = x 2 [n- 2] (c) y[n] = x[n + 1] - x[n- 1] (d) y[n] = Od{x(t)} 1.20. A continuous-time linear systemS with input x(t) and output y(t) yields the follow- ing input-output pairs: x(t) = ei2t ~ y(t) = ei3t, x(t) = e- i 2t ~ y(t) = e-131 . (a) If x 1( t) = cos(2t), determine the corresponding output y1 (t) for systemS. (b) If x2(t) = cos(2(t - ~)), determine the corresponding output y2(t) for sys- temS. BASIC PROBLEMS 1.21. A continuous-time signal x(t) is shown in Figure P1.21. Sketch and label carefully each of the following signals: (a) x(t- 1) (b) x(2- t) (c) x(2t + 1) (d) x(4- ~) (e) [x(t) + x(-t)]u(t) (f) x(t)[8(t + ~) - 8(t- ~)] 1.22. A discrete-time signal is shown in Figure P1.22. Sketch and label carefully each of the following signals: (a) x[n- 4] (b) x[3 - n] (c) x[3n] (d) x[3n + 1] (e) x[n]u[3 - n] (f) x[n - 2]8[n - 2] (g) ~x[n] + ~( -1) 11 x[n] (h) x[(n - 1)2] 60 Signals and Systems Chap. 1 2,....__ __ -2 0 2 -1 .l 1 .l 2 2 -1 0 1 2 3 4 5 n -1 Figure P1.21 Figure P1.22 1.23. Determine and sketch the even and odd parts of the signals depicted in Figure Pl.23. Label your sketches carefully. 1 2 (a) x(t) ~ -2 -1 1 (b) The line --..__The line x(t) = - 2t for t < 0 x(t) = t for t > 0 -1 (c) Figure P1.23 1.24. Determine and sketch the even and odd parts of the signals depicted in Figure P 1.24. Label your sketches carefully. Chap. 1 Problems 61 ···llllllt1 23 n (a) 3 2 n (b) 2 n Figure P 1 .24 1.25. Determine whether or not each of the following continuous-time signals is periodic. If the signal is periodic, determine its fundamental period. (a) x(t) = 3cos(4t+ }) (b) x(t) = ei(7Tf-l) (c) x(t) = [cos(2t- })f (d) x(t) = 8v{cos(47Tt)u(t)} (e) x(t) = 8v{sin(47Tt)u(t)} (f) x(t) = L e-<2t-n) u(2t - n) n= -oo 1.26. Determine whether or not each of the following discrete-time signals is periodic. If the signal is periodic, determine its fundamental period. (a) x[n] = sin( 6 ; n + 1) (b) x[n] = cos(i- 7T) (c) x[n] = cos(in2 ) (d) x[n] = cos(~n)cos(*n) (e) x[n] = 2cos(*n) +sin( in)- 2cos(~n + ~) 1.27. In this chapter, we introduced a number of general properties of systems. In partic- ular, a system may or may not be (1) Memoryless (2) Time invariant (3) Linear (4) Causal (S) Stable Determine which of these properties hold and which do not hold for each of the following continuous-time systems. Justify your answers. In each example, y(t) de- notes the system output and x(t) is the system input. 62 Signals and Systems Chap. 1 (a) y(t) = x(t - 2) + x(2 - t) (b) y(t) = [cos(3t)]x(t) t<O (c) y(t) = J! : x( T)dT (d) y(t) = { ~(t) + x(t - 2), t 2:: 0 0 x(t) < 0 (e) y(t) = { x'(t) + x(t - 2), x(t) 2:: 0 (f) y(t) = x(t/3) (g) y(t) = d~~~t) 1.28. Determine which of the properties listed in Problem 1.27 hold and which do not hold for each of the following discrete-time systems. Justify your answers. In each example, y[n] denotes the system output and x[n] is the system input. (a) y[n] = x[- n] (b) y[n] = x[n - 2] - 2x[n - 8] (c) y[n] = nx[n] (d) y[n] = Sv{x[n - 1]} x[n], n 2:: 1 { x[n], n 2:: 1 (e) y[n] = 0, n = 0 (f) y[n] = 0, n = 0 { x[n + 1], n ::::: -1 x[n], n ::::: -1 (g) y[n] = x[4n + 1] 1.29. (a) Show that the discrete-time system whose input x[n] and output y[n] are related by y[n] = ffi-e{x[n]} is additive. Does this system remain additive if its input- output relationship is changed to y[n] = ffi-e{ei7Tnl4 x[n]}? (Do not assume that x[n] is real in this problem.) (b) In the text, we discussed the fact that the property of linearity for a system is equivalent to the system possessing both the additivity property and homogene- ity property. Determine whether each of the systems defined below is additive and/or homogeneous. Justify your answers by providing a proof for each prop- erty if it holds or a counterexample if it does not. x[n]x[n-2] [ 1] .....1- 0 (i) y(t) = _l__[dx(t)]2 (ii) y[n] = x[n-1] ' X n- ' x(t) dt { 0, x[n - 1] = 0 1.30. Determine if each of the following systems is invertible. If it is, construct the inverse system. If it is not, find two input signals to the system that have the same output. (a) y(t) = x(t- 4) (b) y(t) = cos[x(t)] (C) y[n] = nx[n] (d) y(t) = cry: X( T)dT x[n - 1], n 2:: 1 (e) y[n] = 0, n = 0 (f) y[n] = x[n]x[n - 1] { x[n], n ::::: -1 (g) y[n] = x[l - n] (h) y(t) = cr£ e-(t-T) X( T)dT (i) y[n] = L~= -r£(4yz-k x[k] (j) y(t) = d~;t) (k) [n] = { x[n + 1], n 2:: 0 (I) y(t) = x(2t) y x[n], n ::::: -1 (m) y[n] = x[2n] (n) y[n] = { x[n/2], n even 0, nodd 1.31. In this problem, we illustrate one of the most important consequences of the prop- erties of linearity and time invariance. Specifically, once we know the response of a linear system or a linear time-invariant (LTI) system to a single input or the responses to several inputs, we can directly compute the responses to many other Chap. 1 Problems 63 input signals. Much of the remainder of this book deals with a thorough exploitation of this fact in order to develop results and techniques for analyzing and synthesizing LTI systems. (a) Consider an LTI system whose response to the signal x1( t) in Figure P1.31(a) is the signal y 1 ( t) illustrated in Figure P 1.31 (b). Determine and sketch carefully the response of the system to the input x2(t) depicted in Figure P1.31(c). (b) Determine and sketch the response of the system considered in part (a) to the input x 3(t) shown in Figure P1.31(d). 0 2 0 1 2 (a) (b) I I 2 3 4) t -1 0 1 2 -1 - (c) (d) Figure P 1. 31 ADVANCED PROBLEMS 1.32. Let x(t) be a continuous-time signal, and let Yt (t) = x(2t) and Y2(t) = x(t/2). The signal y 1 (t) represents a speeded up version of x(t) in the sense that the duration of the signal is cut in half. Similarly, y2(t) represents a slowed down version of x(t) in the sense that the duration of the signal is doubled. Consider the following statements: (1) If x(t) is periodic, then y1 (t) is periodic. (2) If y 1( t) is periodic, then x(t) is periodic. (3) If x(t) is periodic, then y2(t) is periodic. (4) If y2(t) is periodic, then x(t) is periodic. For each of these statements, determine whether it is true, and if so, determine the relationship between the fundamental periods of the two signals considered in the statement. If the statement is not true, produce a counterexample to it. 1.33. Let x[ n] be a discrete-time signal, and let x[n/2] n even y 1 [n] = x[2n] and Y2[n] = O, ' { n odd · 64 Signals and Systems Chap. 1 The signals y 1 [n] and y2 [n] respectively represent in some sense the speeded up and slowed down versions of x[n]. However, it should be noted that the discrete-time notions of speeded up and slowed down have subtle differences with respect to their continuous-time counterparts. Consider the following statements: (1) If x[n] is periodic, then y 1 [n] is periodic. (2) If y 1 [n] is periodic, then x[n] is periodic. (3) If x[n] is periodic, then y2[n] is periodic. (4) If y2[n] is periodic, then x[n] is periodic. For each of these statements, determine whether it is true, and if so, determine the relationship between the fundamental periods of the two signals considered in the statement. If the statement is not true, produce a counterexample to it. 1.34. In this problem, we explore several of the properties of even and odd signals. (a) Show that if x[n] is an odd signal, then +x ~ x[n] = 0. n=-x (b) Show that if x 1 [n] is an odd signal and x2[n] is an even signal, then x 1 [n]x2[n] is an odd signal. (c) Let x[n] be an arbitrary signal with even and odd parts denoted by Xe[n] = 8v{x[n]} and X0 [n] = 0d{x[n]}. Show that +x +oo +oo ~ x2[n] = ~ x;[n] + ~ x~[n]. n=-x n=-x n= -oo (d) Although parts (a)-(c) have been stated in terms of discrete-time signals, the analogous properties are also valid in continuous time. To demonstrate this, show that where Xe(t) and X0 (t) are, respectively, the even and odd parts of x(t). 1.35. Consider the periodic discrete-time exponential time signal x[n] = eim(27TIN)n. Show that the fundamental period of this signal is No = Nlgcd(m, N), where gcd(m, N) is the greatest common divisor of m and N-that is, the largest integer that divides both m and Nan integral number of times. For example, gcd(2, 3) = 1, gcd(2, 4) = 2, gcd(8, 12) = 4. Note that No = N if m and N have no factors in common. Chap. 1 Problems 65 1.36. Let x(t) be the continuous-time complex exponential signal x(t) = e.iwot with fundamental frequency w0 and fundamental period To = 27T!w0 . Consider the discrete-time signal obtained by taking equally spaced samples of x(t)-that is, x[n] = x(nT) = e.iwonT. (a) Show that x[n] is periodic if and only if TIT0 is a rational number-that is, if and only if some multiple of the sampling interval exactly equals a multiple of the period of x(t). (b) Suppose that x[n] is periodic-that is, that T p (P1.36-1) To q' where p and q are integers. What are the fundamental period and fundamental frequency of x[n]? Express the fundamental frequency as a fraction of w 0T. (c) Again assuming that T!T0 satisfies eq. (Pl.36-1), determine precisely how many periods of x(t) are needed to obtain the samples that form a single period of x[n]. 1.37. An important concept in many communications applications is the correlation be- tween two signals. In the problems at the end of Chapter 2, we will have more to say about this topic and will provide some indication of how it is used in practice. For now, we content ourselves with a brief introduction to correlation functions and some of their properties. Let x(t) and y(t) be two signals; then the correlation function is defined as ""'""""(!) = r~ x(t + T)y( T)dT. The function <f>xxCt) is usually referred to as the autocorrelationf unction of the signal x(t), while </>xy(t) is often called a cross-correlation function. (a) What is the relationship between </>xy(t) and </>yx(t)? (b) Compute the odd part of <f>xxU). (c) Suppose that y(t) = x(t + T). Express </>xy(t) and </>yy(t) in terms of <f>xx(t). 1.38. In this problem, we examine a few of the properties of the unit impulse function. (a) Show that 1 8(2t) = 28(t). Hint: Examine Dtl(t). (See Figure 1.34.) (b) In Section 1.4, we defined the continuous-time unit impulse as the limit of the signal Dtl(t). More precisely, we defined several of the properties of 8(t) by examining the corresponding properties of Dtl(t). For example, since the signal ULI(t) = t% ih(T)dT 66 Signals and Systems Chap. 1 converges to the unit step u(t) = lim Ut,.(f), (Pl.38-1) t. ---->0 we could interpret B(t) through the equation u(t) ~ Loc li(T)dT or by viewing B(t) as the formal derivative of u(t). This type of discussion is important, as we are in effect trying to define B(t) through its properties rather than by specifying its value for each t, which is not possible. In Chapter 2, we provide a very simple characterization of the behavior of the unit impulse that is extremely useful in the study of linear time- invariant systems. For the present, however, we concentrate on demonstrating that the important concept in using the unit impulse is to understand how it behaves. To do this, consider the six signals depicted in Figure P1.38. Show rl (t) r~(t) ~m 1! D t1 t1 ~ 2~ 2 2 (a) (b) r~ (t) ri (t) (c) (d) r; (t) 2 ~ -~ (e) (f) Figure P1.38 Chap. 1 Problems 67 that each ""behaves like an impulse"" as Ll ~ 0 in that, if we let u~(t) = L~ ,-it,(r)dr, then lim u~ (t) = u(t). Ll-->0 In each case, sketch and label carefully the signal u~ (t). Note that ri (0) = ri (0) = 0 for all Ll. Therefore, it is not enough to define or to think of o(t) as being zero fort =/= 0 and infinite fort = 0. Rather, it is properties such as eq. (P1.38-1) that define the impulse. In Section 2.5 we will define a whole class of signals known as singularity functions, which are related to the unit impulse and which are also defined in terms of their properties rather than their values. 1.39. The role played by u(t), o(t), and other singularity functions in the study of linear time-invariant systems is that of an idealization of a physical phenomenon, and, as we will see, the use of these idealizations allow us to obtain an exceedingly impor- tant and very simple representation of such systems. In using singularity functions, we need, however, to be careful. In particular, we must remember that they are ideal- izations, and thus, whenever we perform a calculation using them, we are implicitly assuming that this calculation represents an accurate description of the behavior of the signals that they are intended to idealize. To illustrate, consider the equation x(t)o(t) = x(O)o(t). (P1.39-l) This equation is based on the observation that x(t)o11 (t) = x(O)o11 (t). (P1.39-2) Taking the limit of this relationship then yields the idealized one given by eq. (P1.39-1). However, a more careful examination of our derivation of eq. (P1.39-2) shows that that equation really makes sense only if x(t) is continuous at t = 0. If it is not, then we will not have x(t) = x(O) for t small. To make this point clearer, consider the unit step signal u(t). Recall from eq. (1.70) that u(t) = 0 fort < 0 and u(t) = 1 fort > 0, but that its value at t = 0 is not defined. [Note, for example, that u11(0) = 0 for all Ll, while ui(O) = 4 (from Problem 1.38(b)).] The fact that u(O) is not defined is not particularly bothersome, as long as the calculations we perform using u(t) do not rely on a specific choice for u(O). For example, if f(t) is a signal that is continuous at t = 0, then the value of roooo f(u)u(u)da does not depend upon a choice for u(O). On the other hand, the fact that u(O) is undefined is significant in that it means that certain calculations involving singular- ity functions are undefined. Consider trying to define a value for the product u(t)o(t). 68 Signals and Systems Chap. 1 To see that this cannot be defined, show that lim [u 11 (t)o(t)] = 0, 11~o but In general, we can define the product of two signals without any difficulty, as long as the signals do not contain singularities (discontinuities, impulses, or the other singularities introduced in Section 2.5) whose locations coincide. When the locations do coincide, the product is undefined. As an example, show that the signal +oc g(t) = J- oc U(T)O(t- T)dT is identical to u(t); that is, it is 0 fort < 0, it equals 1 fort > 0, and it is undefined fort = 0. 1.40. (a) Show that if a system is either additive or homogeneous, it has the property that if the input is identically zero, then the output is also identically zero. (b) Determine a system (either in continuous or discrete time) that is neither ad- ditive nor homogeneous but which has a zero output if the input is identically zero. (c) From part (a), can you conclude that if the input to a linear system is zero be- tween times t1 and t2 in continuous time or between times n1 and n2 in discrete time, then its output must also be zero between these same times? Explain your answer. 1.41. Consider a systemS with input x[n] and output y[n] related by y[n] = x[n]{g[n] + g[n- 1]}. (a) If g[n] = 1 for all n, show that Sis time invariant. (b) If g[n] = n, show that Sis not time invariant. (c) If g[n] = 1 + ( -l)n, show that Sis time invariant. 1.42. (a) Is the following statement true or false? The series interconnection of two linear time-invariant systems is itself a linear, time-invariant system. Justify your answer. (b) Is the following statement true or false? The series interconnection of two nonlinear systems is itself nonlinear. Justify your answer. (c) Consider three systems with the following input-output relationships: System 1: y[n] = { x[n/2], n even 0, n odd ' Chap. 1 Problems 69 + 1 + 1 System 2: y[n] = x[n] 2x[n- 1] 4x[n- 2], System 3: y[n] = x[2n]. Suppose that these systems are connected in series as depicted in Figure P1.42. Find the input-output relationship for the overall interconnected system. Is this system linear? Is it time invariant? y[n] Figure P1.42 1.43. (a) Consider a time-invariant system with input x(t) and output y(t). Show that if x(t) is periodic with period T, then so is y(t). Show that the analogous result also holds in discrete time. (b) Give an example of a time-invariant system and a nonperiodic input signal x(t) such that the corresponding output y(t) is periodic. 1.44. (a) Show that causality for a continuous-time linear system is equivalent to the following statement: For any time to and any input x(t) such that x(t) = 0 fort < t0, the correspond- ing output y(t) must also be zero fort < t0 . The analogous statement can be made for a discrete-time linear system. (b) Find a nonlinear system that satisfies the foregoing condition but is not causal. (c) Find a nonlinear system that is causal but does not satisfy the condition. (d) Show that invertibility for a discrete-time linear system is equivalent to the following statement: The only input that produces y[n] = 0 for all n is x[n] = 0 for all n. The analogous statement is also true for a continuous-time linear system. (e) Find a nonlinear system that satisfies the condition of part (d) but is not invert- ible. 1.45. In Problem 1.37, we introduced the concept of correlation functions. It is often im- portant in practice to compute the correlation function cf>hx(t), where h(t) is a fixed given signal, but where x(t) may be any of a wide variety of signals. In this case, what is done is to design a systemS with input x(t) and output cf>hx(t). (a) IsS linear? IsS time invariant? IsS causal? Explain your answers. (b) Do any of your answers to part (a) change if we take as the output cf>xh(t) rather than cf>hx(t)? 1.46. Consider the feedback system of Figure P1.46. Assume that y[n] = 0 for n < 0. + 'i2 e [n] I x[n] ----t•~..~. ~ ----_-_.._ ...· ~-· -y-[n_]_=_e-[n---1]-~~~~~.~.. ... ---!•~ y[n] Figure P1.46 70 Signals and Systems Chap. 1 (a) Sketch the output when x[n] = 8[n]. (b) Sketch the output when x[n] = u[n]. 1.47. (a) LetS denote an incrementally linear system, and let x 1 [n] be an arbitrary input signal to S with corresponding output y 1 [n]. Consider the system illustrated in Figure Pl.47(a). Show that this system is linear and that, in fact, the overall input-output relationship between x[n] and y[n] does not depend on the partic- ular choice of x 1 [ n]. (b) Use the result of part (a) to show that Scan be represented in the form shown in Figure 1.48. (c) Which ofthe following systems are incrementally linear? Justify your answers, and if a system is incrementally linear, identify the linear system Land the zero- input response y0 [n] or y0(t) for the representation of the system as shown in Figure 1.48. (i) y[n] = n + x[n] + 2x[n + 4] n/2, n even (ii) [ ] (n-1 )/2 Y n = { (n - 1)/2 + k~-n x[k], n odd x[n] ·+~ ·I s :cp )t y[n] (a) x1[n] Y1[n] t X (t) .~ w (t) ·I y(t) ~ d~?) )t y (t) (b) cos (1rn) v [n] z [n] z [n] v2 = [n] + x [n] ~y[n] w [n] w [n] 2 = x [n] (c) Figure P1.47 Chap. 1 Problems 71 (. "") [ ] { x[n] - x[n - 1] + 3, if x[O] 2: 0 111 Y n = x[n] - x[n- 1] - 3, if x[O] < 0 (iv) The system depicted in Figure P1.47(b). (v) The system depicted in Figure P1.47(c). (d) Suppose that a particular incrementally linear system has a representation as in Figure 1.48, with L denoting the linear system and y0 [n] the zero-input re- sponse. Show that Sis time invariant if and only if Lis a time-invariant system and y0 [ n] is constant. MATHEMATICAL REVIEW The complex number z can be expressed in several ways. The Cartesian or rectangular form for z is Z =X+ jy, where j = J=1 and x andy are real numbers referred to respectively as the real part and the imaginary part of z. As we indicated earlier, we will often use the notation x = CRe{z}, y = 9m{z}. The complex number z can also be represented in polar form as z = rej8 , where r > 0 is the magnitude of z and (} is the angle or phase of z. These quantities will often be written as r = lzi, 8 = <t:z. The relationship between these two representations of complex numbers can be de- termined either from Euler s relation, eje = cos(} + j sin 8, or by plotting z in the complex plane, as shown in Figure P1.48, in which the coordinate axes are CRe{z} along the horizontal axis and 9m{z} along the vertical axis. With respect to this graphical representation, x andy are the Cartesian coordinates of z, and rand (} are its polar coordinates. !1m y Figure P1.48 72 Signals and Systems Chap. 1 1.48. Let zo be a complex number with polar coordinates (r0, 00 ) and Cartesian coordi- nates (x0, y0 ). Determine expressions for the Cartesian coordinates of the following complex numbers in terms of x0 and YO· Plot the points zo, Zt, Z2, Z3, Z4, and zs in the complex plane when r0 = 2 and 00 = TTI4 and when r0 = 2 and 00 = TTI2. Indicate on your plots the real and imaginary parts of each point. (a) Zt = roe- J&o (b) Z2 = ro (c) Z3 = roef(&o+1T) (d) Z4 = roei(-&o+1T) (e) zs = roe.i(&o+21T) 1.49. Express each of the following complex numbers in polar form, and plot them in the complex plane, indicating the magnitude and angle of each number: (a) 1 + jj3 (b) -5 (c) -5- 5j (d) 3 + 4j (e) (1 - j}3)3 (f) (1 + j)s ( ) ("" ':l3J + ·3)(1 _ "") (h) 2- j(6/jj) (i) I+ Jfi g 1 1 2+ j(6Jjj) J3+ .i G) j(l + j)e.i7TI6 (k) ( j3 + j)2 j2e- j7T/4 (I) eirrl~-1 I+Jfi 1.50. (a) Using Euler's relationship or Figure Pl.48, determine expressions for x andy in terms of r and (J. (b) Determine expressions for r and (J in terms of x and y. (c) If we are given only rand tan 0, can we uniquely determine x andy? Explain your answer. 1.51. Using Euler's relation, derive the following relationships: (a) cos (J = ~(e.i8 + e- .i8) (b) sin (J = -d-J(e.i8 - e- .i8 ) (c) cos2 (J = ~(1 + cos 20) (d) (sinO)(sin<f>) = ~ cos((J- </>)- ~ cos((J + </>) (e) sin( (J + </>) = sin (J cos </> + cos (J sin </> 1.52. Let z denote a complex variable; that is, z = x + jy = re.i8 . The complex conjugate of z is z* = x- jy = re- J&_ Derive each of the following relations, where z, z1, and z2 are arbitrary complex numbers: (a) zz* = r 2 (b) ~ = e.i2& (c) z + z* = 2CRe{z} (d) z - z* = 2jdm{z} (e) (zt + z2)* = z~ + z; (f) (l!Zt z2)* ~~ az~ z;, where a is any real number (g) (:-'- )* = ~ .... 2 ...... 2 (h) Cfl~{~} = _!_[z1z~+~:)'z2] .c2 2 ~2""-2 1.53. Derive the following relations, where z, z1, and z2 are arbitrary complex numbers: (a) (e2)* = e2* (b) ztz; + z~z2 = 2Cfl~{ztz;} = 2eR~{z~z2} Chap. 1 Problems 73 (c) lzl = lz*l (d) lz1 z2l = lz1llz2l (e) CRe{z} :::; lzl, dm{z} :::; lzl (f) lz1z; + ziz2l :::; 21z1z2l (g) (lzii - lz2l)2 :::; lz1 + z2l2 :::; (lzii + lz2l? 1.54. The relations considered in this problem are used on many occasions throughout the book. (a) Prove the validity of the following expression: NL-1a n { N a= 1 = l~aN, n=O for any complex number a =I= 1 · 1-a This is often referred to as the finite sum formula. (b) Show that if Ia I < 1, then 00 1 Lan = 1-a· n=O This is often referred to as the infinite sum formula. (c) Show also if lal < 1, then 20.0: nan = __a _-=- n=O (1 - a)2. (d) Evaluate assuming that Ia I < 1. 1.55. Using the results from Problem 1.54, evaluate each of the following sums and ex- press your answer in Cartesian (rectangular) form: (a) L~=Oej1Tnl2 (b) L~=-2ej1Tnl2 (c) ""L:=o(~)nej1Tn!2 (d) ""L:=2(~)nej1Tn/2 (e) L~=O cos(~n) (f) ""L:=o(~)n cos(~n) 1.56. Evaluate each of the following integrals, and express your answer in Cartesian (rect- angular) form: (a) fo4ejml2dt (b) f 6 12 0 ejm dt (c) f 8 2 ejm12dt (d) fooc e-(1 + j)t dt (e) fooc e-t cos(t)dt (f) f oc e-2t sin(3t)dt 0 2 LINEAR TIME-INVARIANT SYSTEMS 2.0 INTRODUCTION In Section 1.6 we introduced and discussed a number of basic system properties. Two of these, linearity and time invariance, play a fundamental role in signal and system analysis for two major reasons. First, many physical processes possess these properties and thus can be modeled as linear time-invariant (LTI) systems. In addition, LTI systems can be analyzed in considerable detail, providing both insight into their properties and a set of powerful tools that form the core of signal and system analysis. A principal objective of this book is to develop an understanding of these proper- ties and tools and to provide an introduction to several of the very important applications in which the tools are used. In this chapter, we begin the development by deriving and examining a fundamental and extremely useful representation for LTI systems and by in- traducing an important class of these systems. One of the primary reasons LTI systems are amenable to analysis is that any such system possesses the superposition property described in Section 1.6.6. As a consequence, if we can represent the input to an LTI system in terms of a linear combination of a set of basic signals, we can then use superposition to compute the output of the system in terms of its responses to these basic signals. As we will see in the following sections, one of the important characteristics of the unit impulse, both in discrete time and in continuous time, is that very general signals can be represented as linear combinations of delayed impulses. This fact, together with the properties of superposition and time invariance, will allow us to develop a complete characterization of any LTI system in terms of its response to a unit impulse. Such a representation, referred to as the convolution sum in the discrete-time case and the convo- lution integral in continuous time, provides considerable analytical convenience in dealing 74"
2 Linear Time-Invariant Systems,"2 LINEAR TIME-INVARIANT SYSTEMS 2.0 INTRODUCTION In Section 1.6 we introduced and discussed a number of basic system properties. Two of these, linearity and time invariance, play a fundamental role in signal and system analysis for two major reasons. First, many physical processes possess these properties and thus can be modeled as linear time-invariant (LTI) systems. In addition, LTI systems can be analyzed in considerable detail, providing both insight into their properties and a set of powerful tools that form the core of signal and system analysis. A principal objective of this book is to develop an understanding of these proper- ties and tools and to provide an introduction to several of the very important applications in which the tools are used. In this chapter, we begin the development by deriving and examining a fundamental and extremely useful representation for LTI systems and by in- traducing an important class of these systems. One of the primary reasons LTI systems are amenable to analysis is that any such system possesses the superposition property described in Section 1.6.6. As a consequence, if we can represent the input to an LTI system in terms of a linear combination of a set of basic signals, we can then use superposition to compute the output of the system in terms of its responses to these basic signals. As we will see in the following sections, one of the important characteristics of the unit impulse, both in discrete time and in continuous time, is that very general signals can be represented as linear combinations of delayed impulses. This fact, together with the properties of superposition and time invariance, will allow us to develop a complete characterization of any LTI system in terms of its response to a unit impulse. Such a representation, referred to as the convolution sum in the discrete-time case and the convo- lution integral in continuous time, provides considerable analytical convenience in dealing 74"
2.0 Introduction,"2 LINEAR TIME-INVARIANT SYSTEMS 2.0 INTRODUCTION In Section 1.6 we introduced and discussed a number of basic system properties. Two of these, linearity and time invariance, play a fundamental role in signal and system analysis for two major reasons. First, many physical processes possess these properties and thus can be modeled as linear time-invariant (LTI) systems. In addition, LTI systems can be analyzed in considerable detail, providing both insight into their properties and a set of powerful tools that form the core of signal and system analysis. A principal objective of this book is to develop an understanding of these proper- ties and tools and to provide an introduction to several of the very important applications in which the tools are used. In this chapter, we begin the development by deriving and examining a fundamental and extremely useful representation for LTI systems and by in- traducing an important class of these systems. One of the primary reasons LTI systems are amenable to analysis is that any such system possesses the superposition property described in Section 1.6.6. As a consequence, if we can represent the input to an LTI system in terms of a linear combination of a set of basic signals, we can then use superposition to compute the output of the system in terms of its responses to these basic signals. As we will see in the following sections, one of the important characteristics of the unit impulse, both in discrete time and in continuous time, is that very general signals can be represented as linear combinations of delayed impulses. This fact, together with the properties of superposition and time invariance, will allow us to develop a complete characterization of any LTI system in terms of its response to a unit impulse. Such a representation, referred to as the convolution sum in the discrete-time case and the convo- lution integral in continuous time, provides considerable analytical convenience in dealing 74 Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 75 with LTI systems. Following our development of the convolution sum and the convolution integral we use these characterizations to examine some of the other properties of LTI sys- tems. We then consider the class of continuous-time systems described by linear constant- coefficient differential equations and its discrete-time counterpart, the class of systems described by linear constant-coefficient difference equations. We will return to examine these two very important classes of systems on a number of occasions in subsequent chap- ters. Finally, we will take another look at the continuous-time unit impulse function and a number of other signals that are closely related to it in order to provide some additional insight into these idealized signals and, in particular, to their use and interpretation in the context of analyzing LTI systems. 2.1 DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM 2. 1 . 1 The Representation of Discrete-Time Signals in Terms of Impulses The key idea in visualizing how the discrete-time unit impulse can be used to construct any discrete-time signal is to think of a discrete-time signal as a sequence of individual im- pulses. To see how this intuitive picture can be turned into a mathematical representation, consider the signal x[n] depicted in Figure 2.1(a). In the remaining parts of this figure, we have depicted five time-shifted, scaled unit impulse sequences, where the scaling on each impulse equals the value of x[n] at the particular instant the unit sample occurs. For example, x[-1]8[n + 1] = { x[- 1], n = -1 0, n -::/= -1 ' x[O]o[n] = { x[O], n = 0 0, n-::/= 0' x[l]o[n- 1] = { x[ 1], n = 1 0, n¥=1"" Therefore, the sum of the five sequences in the figure equals x[n] for -2 ::; n ::; 2. More generally, by including additional shifted, scaled impulses, we can write x[n] = ... + x[- 3]8[n + 3] + x[ -2]8[n + 2] + x[ -1]8[n + 1] + x[O]o[n] (2.1) + x[1]8[n- 1] + x[2]8[n - 2] + x[3]8[n- 3] + .... For any value of n, only one of the terms on the right-hand side of eq. (2.1) is nonzero, and the scaling associated with that term is precisely x[n]. Writing this summation in a more compact form, we have +x x[n] = ~ x[k]o[n- k]. (2.2) k=-X This corresponds to the representation of an arbitrary sequence as a linear combination of shifted unit impulses o[n- k], where the weights in this linear combination are x[k]. As an example, consider x[n] = u[n], the unit step. In this case, since u[k] = 0 for k < 0"
2.1 Discrete-Time LTI Systems: The Convolution Sum,"Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 75 with LTI systems. Following our development of the convolution sum and the convolution integral we use these characterizations to examine some of the other properties of LTI sys- tems. We then consider the class of continuous-time systems described by linear constant- coefficient differential equations and its discrete-time counterpart, the class of systems described by linear constant-coefficient difference equations. We will return to examine these two very important classes of systems on a number of occasions in subsequent chap- ters. Finally, we will take another look at the continuous-time unit impulse function and a number of other signals that are closely related to it in order to provide some additional insight into these idealized signals and, in particular, to their use and interpretation in the context of analyzing LTI systems. 2.1 DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM 2. 1 . 1 The Representation of Discrete-Time Signals in Terms of Impulses The key idea in visualizing how the discrete-time unit impulse can be used to construct any discrete-time signal is to think of a discrete-time signal as a sequence of individual im- pulses. To see how this intuitive picture can be turned into a mathematical representation, consider the signal x[n] depicted in Figure 2.1(a). In the remaining parts of this figure, we have depicted five time-shifted, scaled unit impulse sequences, where the scaling on each impulse equals the value of x[n] at the particular instant the unit sample occurs. For example, x[-1]8[n + 1] = { x[- 1], n = -1 0, n -::/= -1 ' x[O]o[n] = { x[O], n = 0 0, n-::/= 0' x[l]o[n- 1] = { x[ 1], n = 1 0, n¥=1"" Therefore, the sum of the five sequences in the figure equals x[n] for -2 ::; n ::; 2. More generally, by including additional shifted, scaled impulses, we can write x[n] = ... + x[- 3]8[n + 3] + x[ -2]8[n + 2] + x[ -1]8[n + 1] + x[O]o[n] (2.1) + x[1]8[n- 1] + x[2]8[n - 2] + x[3]8[n- 3] + .... For any value of n, only one of the terms on the right-hand side of eq. (2.1) is nonzero, and the scaling associated with that term is precisely x[n]. Writing this summation in a more compact form, we have +x x[n] = ~ x[k]o[n- k]. (2.2) k=-X This corresponds to the representation of an arbitrary sequence as a linear combination of shifted unit impulses o[n- k], where the weights in this linear combination are x[k]. As an example, consider x[n] = u[n], the unit step. In this case, since u[k] = 0 for k < 0 76 Linear Time-Invariant Systems Chap. 2 x[n] n (a) x[ -2] o[n + 2] -4•- 3•- I 2-1• •0 1• 2• 3• 4• n (b) x[-1] o[n + 1] -1 -4•- 3• -2• I • • • 0 1• 2 3 4• n (c) I x[O] o[n] -4•- 3•- 2•- 1• 0 •1 2• 3• •4 n (d) x[1] o[n-1] I -4•- 3•- 2•- 1• •0 •2 •3 4• n (e) x[2] o[n-2] 2 -4•- 3•- 2•- 1• •0 •1 3•I 4• n Figure 2.1 Decomposition of a discrete-time signal into a weighted (f) sum of shifted impulses. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 77 and u[k] = 1 for k 2:: 0, eq. (2.2) becomes +x u[n] = L B[n - k], k=O which is identical to the expression we derived in Section 1.4. [See eq. (1.67).] Equation (2.2) is called the sifting property of the discrete-time unit impulse. Be- cause the sequence B[n - k] is nonzero only when k = n, the summation on the right- hand side of eq. (2.2) ""sifts"" through the sequence of values x[k] and preserves only the value corresponding to k = n. In the next subsection, we will exploit this representa- tion of discrete-time signals in order to develop the convolution -sum representation for a discrete-time LTI system. 2.1.2 The Discrete-Time Unit Impulse Response and the Convolution- Sum Representation of LTI Systems The importance of the sifting property of eqs. (2.1) and (2.2) lies in the fact that it repre- sents x[n] as a superposition of scaled versions of a very simple set of elementary functions, namely, shifted unit impulses B[n- k], each of which is nonzero (with value 1) at a single point in time specified by the corresponding value of k. The response of a linear system to x[n] will be the superposition of the scaled responses of the system to each of these shifted impulses. Moreover, the property of time in variance tells us that the responses of a time-invariant system to the time-shifted unit impulses are simply time-shifted versions of one another. The convolution -sum representation for discrete-time systems that are both linear and time invariant results from putting these two basic facts together. More specifically, consider the response of a linear (but possibly time-varying) sys- tem to an arbitrary input x[n]. We can represent the input through eq. (2.2) as a linear combination of shifted unit impulses. Let hk[n] denote the response of the linear system to the shifted unit impulse B[n - k]. Then, from the superposition property for a linear system [eqs. (1.123) and (1.124)], the response y[n] of the linear system to the input x[n] in eq. (2.2) is simply the weighted linear combination of these basic responses. That is, with the input x[n] to a linear system expressed in the form of eq. (2.2), the output y[n] can be expressed as +oo y[n] = L x[k]hk[n]. (2.3) k= -00 Thus, according to eq. (2.3), if we know the response of a linear system to the set of shifted unit impulses, we can construct the response to an arbitrary input. An interpreta- tion of eq. (2.3) is illustrated in Figure 2.2. The signal x[n] is applied as the input to a linear system whose responses h_ 1 [n], h0 [n], and h1 [n] to the signals B[n + 1], B[n], and B[n- 1], respectively, are depicted in Figure 2.2(b). Since x[n] can be written as a linear combination of B[n + 1], B[n], and B[n- 1], superposition allows us to write the response to x[n] as a linear combination of the responses to the individual shifted impulses. The individual shifted and scaled impulses that constitute x[n] are illustrated on the left-hand side of Figure 2.2( c), while the responses to these component signals are pictured on the right-hand side. In Figure 2.2(d) we have depicted the actual input x[n], which is the sum of the components on the left side of Figure 2.2(c) and the actual output y[n], which, by 78 Linear Time-Invariant Systems Chap. 2 x[n] -1 n (a) h0 [n] n n n (b) Figure 2.2 Graphical interpretation of the response of a discrete-time linear system as expressed in eq. (2.3). superposition, is the sum of the components on the right side of Figure 2.2(c). Thus, the response at time n of a linear system is simply the superposition of the responses due to the input value at each point in time. In general, of course, the responses hdn] need not be related to each other for differ- ent values of k. However, if the linear system is also time invariant, then these responses to time-shifted unit impulses are all time-shifted versions of each other. Specifically, since B[n- k] is a time-shifted version of B[n], the response hk[n] is a time-shifted version of h0 [n]; i.e., (2.4) For notational convenience, we will drop the subscript on h0 [n] and define the unit impulse (sample) response h[n] = h0 [n]. (2.5) That is, h[n] is the output ofthe LTI system when B[n] is the input. Then for an LTI system. eq. (2.3) becomes +x y[n] ~ x[k]h[n - k]. (2.6) k=-X This result is referred to as the convolution sum or superposition sum, and the oper- ation on the right-hand side of eq. (2.6) is known as the convolution of the sequences x[n] and h[n]. We will represent the operation of convolution symbolically as y[n] = x[n] * h[n]. (2.7) Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 79 x[-1]o[n +1] ... n n x[O]o[n] x[O] h0[n] ... t. ... .. tt.t .. 0 n lo n x[1] o[n-1] ... ll ... 0 n n (c) x[n] y(n] T n 0 n (d) Figure 2.2 Continued Note that eq. (2.6) expresses the response of an LTI system to an arbitrary input in terms of the system's response to the unit impulse. From this, we see that an LTI system is completely characterized by its response to a single signal, namely, its response to the unit impulse. The interpretation of eq. (2.6) is similar to the one we gave for eq. (2.3), where, in the case of an LTI system, the response due to the input x[k] applied at time k is x[k]h[n- k]; i.e., it is a shifted and scaled version (an ""echo"") of h[n]. As before, the actual output is the superposition of all these responses. Linear Time-Invariant Systems Chap.2 Example 2.1 Consider an LTI system with impulse response h[n] and input x[n], as illustrated in Figure 2.3(a). For this case, since only x[O] and x[1] are nonzero, eq. (2.6) simplifies to the expression y[n] = x[O]h[n- 0] + x[1]h[n - 1] = 0.5h[n] + 2h[n- 1]. (2.8) The sequences 0.5h[n] and 2h[n- 1] are the two echoes of the impulse response needed for the superposition involved in generating y[n]. These echoes are displayed in Fig- ure 2.3(b). By summing the two echoes for each value of n, we obtain y[n], which is shown in Figure 2.3(c). h[n] 11 • • I I 0 2 • • n 21 x[n] 0.5 • • T 0 • • • n (a) 0.5h[n] 0.5 • • T T T 0 2 • • n 2h[n-1] 21 • • • 1 1 • 0 2 3 n (b) 251 y[n] o.5T • • Ir 0 2 3 • n (c) Figure 2.3 (a) The impulse response h[n] of an LTI system and an input x[n] to the system; (b) the responses or ""echoes,"" 0.5h[n] and 2h[n - 1], to the nonzero values of the input, namely, x[O] = 0.5 and x[1] = 2; (c) the overall response y[n], which is the sum of the echos in (b). Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 81 By considering the effect of the superposition sum on each individual output sample, we obtain another very useful way to visualize the calculation of y[ n] using the convolution sum. In particular, consider the evaluation of the output value at some specific time n. A particularly convenient way of displaying this calculation graphically begins with the two signals x[k] and h[n - k] viewed as functions of k. Multiplying these two functions, we obtain a sequence g[k] = x[k]h[n - k], which, at each time k, is seen to represent the contribution of x[ k] to the output at time n. We conclude that summing all the samples in the sequence of g[k] yields the output value at the selected time n. Thus, to calculate y[n] for all values of n requires repeating this procedure for each value of n. Fortunately, changing the value of n has a very simple graphical interpretation for the two signals x[k] and h[n- k], viewed as functions of k. The following examples illustrate this and the use of the aforementioned viewpoint in evaluating convolution sums. Example 2.2 Let us consider again the convolution problem encountered in Example 2.1. The se- quence x[k] is shown in Figure 2.4(a), while the sequence h[n - k], for n fixed and viewed as a function of k, is shown in Figure 2.4(b) for several different values of n. In sketching these sequences, we have used the fact that h[n- k] (viewed as a function of k with n fixed) is a time-reversed and shifted version of the impulse response h[k]. In particular, as k increases, the argument n - k decreases, explaining the need to perform a time reversal of h[k]. Knowing this, then in order to sketch the signal h[n- k], we need only determine its value for some particular value of k. For example, the argument n - k will equal 0 at the value k = n. Thus, if we sketch the signal h[- k], we can obtain the signal h[n - k] simply by shifting to the right (by n) if n is positive or to the left if n is negative. The result for our example for values of n < 0, n = 0, 1, 2, 3, and n > 3 are shown in Figure 2.4(b). Having sketched x[k] and h[n - k] for any particular value of n, we multiply these two signals and sum over all values of k. For our example, for n < 0, we see from Figure 2.4 that x[k]h[n- k] = 0 for all k, since the nonzero values of x[k] and h[n- k] do not overlap. Consequently, y[n] = 0 for n < 0. For n = 0, since the product of the sequence x[k] with the sequence h[O- k] has only one nonzero sample with the value 0.5, we conclude that X y[O] = L x[k]h[O- k] = 0.5. (2.9) k=-X The product of the sequence x[k] with the sequence h[1 - k] has two nonzero samples, which may be summed to obtain y[1] = L x[k]h[1 - k] = 0.5 + 2.0 = 2.5. (2.10) k= -X Similarly, y[2] = L x[k]h[2- k] = 0.5 + 2.0 = 2.5, (2.11) k=-YO and 82 Linear Time-Invariant Systems Chap.2 x[k] . 21 0 .5, • - 0 • • • k (a) f h[n-k], n<O J J n-2 n-1 n •0 • • • k h[O-k] -2 -1 0 • • • k h[1-k] -1 0 • • k h[2-k] • 0 2 • k h[3-k] • • J J 0 2 3 k h[n-k], n>3 • • • • J J J 0 n-2 n-1 n k (b) Figure 2.4 Interpretation of eq. (2.6) for the signals h[n] and x[n] in Fig- ure 2.3; (a) the signal x[k] and (b) the signal h[n - k] (as a function of k with n fixed) for several values of n (n < 0; n = 0, 1, 2, 3; n > 3). Each of these signals is obtained by reflection and shifting of the unit impulse re- sponse h[k]. The response y[n] for each value of n is obtained by multiplying the signals x[k] and h[n- k] in (a) and (b) and then summing the products over all values of k. The calculation for this example is carried out in detail in Example 2.2. y[3] = L x[k]h[3 - k] = 2.0. (2.12) k =-X Finally, for n > 3, the product x[k]h[n - k] is zero for all k, from which we conclude that y[n] = 0 for n > 3. The resulting output values agree with those obtained in Exam- ple 2.1. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 83 Example 2.3 Consider an input x[n] and a unit impulse response h[n] given by x[n] = anu[n], h[n] = u[n], with 0 < a < 1. These signals are illustrated in Figure 2.5. Also, to help us in visualizing and calculating the convolution of the signals, in Figure 2.6 we have depicted the signal x[k] followed by h[- k], h[ -1- k], and h[1- k] (that is, h[n- k] for n = 0, -1, and+ 1) and, finally, h[n- k] for an arbitrary positive value of nand an arbitrary negative value of n. From this figure, we note that for n < 0, there is no overlap between the nonzero points in x[k] and h[n - k]. Thus, for n < 0, x[k]h[n- k] = 0 for all values of k, and hence, from eq. (2.6), we see that y[n] = 0, n < 0. For n 2:: 0, x[k]h[n - k] = { ak, 0 s;. k. s;. n . 0, otherwise x[n] = anu[n] 1 ........... llliiiiilttttttttt 0 n (a) 0 n (b) Figure 2.5 The signals x[n] and h[n] in Example 2.3. Thus, for n 2:: 0, n y[n] = Lak, k=O and using the result of Problem 1.54 we can write this as n 1 -an+! y[n] = Lak = --- for n 2:: 0. (2.13) k=O 1 -a Thus, for all n, 1 an+!) y[n] = ( _ a u[n]. 1 The signal y[n] is sketched in Figure 2.7. 84 I Linear Time-Invariant Systems Chap. 2 x[k] ~ a'u[k] .............. llliiiiiJitttttttT ... 0 k (a) h[-k] ... JiliiJiliil I. ................... . 0 k (b) ... Illllll Hill. :~.:.k: ... .. .. .. . .. ... -1 0 k (c) h[1-k] ... Jliillliiil II. .................. . 01 k (d) ... II I I I I J J I I I I I II I J I : [:-.k~ .......n :O ••. 0 n k (e) h[n-k] ••• IIIIII ..... ju u uuuuuuen~O··· n 0 k (f) Figure 2.6 Graphical interpretation of the calculation of the convolution sum for Example 2.3. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 85 y[n] 1 -an +1) = u[n] ( 1-a 1-a 0 n Figure 2.7 Output for Example 2.3. The operation of convolution is sometimes described in terms of ""sliding"" the se- quence h[n- k] past x[k]. For example, suppose we have evaluated y[n] for some partic- ular value of n, say, n = n0 . That is, we have sketched the signal h[n0 - k], multiplied it by the signal x[k], and summed the result over all values of k. To evaluate y[n] at the next value of n-i.e., n = n0 + 1-we need to sketch the signal h[(n0 + 1)- k]. However, we can do this simply by taking the signal h[n0 - k] and shifting it to the right by one point. For each successive value of n, we continue this process of shifting h[n- k] to the right by one point, multiplying by x[k], and summing the result over k. Example 2.4 As a further example, consider the two sequences 1, 0 :::; n :::; 4 x [ n ] = { 0, otherwise and 11 h[n] = { a , 0 :::; n .:::; 6. 0, otherwise These signals are depicted in Figure 2.8 for a positive value of a > 1. In order to calculate the convolution of the two signals, it is convenient to consider five separate intervals for n. This is illustrated in Figure 2.9. Intervall. For n < 0, there is no overlap between the nonzero portions of x[k] and h[n - k], and consequently, y[n] = 0. Interval 2. For 0 :::; n :::; 4, n-k x[k]h[n - k] = a ' 0:::; k:::; n { 0, otherwise 86 Linear Time-Invariant Systems Chap.2 x[n] -2-1 0 1 2 3 4 5 n (a) h[n] 01234567 n (b) Figure 2.8 The signals to be convolved in Example 2.4. Thus, in this interval, n y[n] = Lan-k. (2.14) k=O We can evaluate this sum using the finite sum formula, eq. (2.13). Specifically, changing the variable of summation in eq. (2.14) from k tor = n- k, we obtain n 1-an+l y[n] = L,ar = --- r=O 1 -a Interval 3. For n > 4 but n - 6 :s 0 (i.e., 4 < n :::::; 6), n-k x[k]h[n- k] 0:::::; k:::::; 4 = a ' { 0, otherwise Thus, in this interval, 4 y[n] = Lan-k. (2.15) k=O Once again, we can use the geometric sum formula in eq. (2.13) to evaluate eq. (2.15). Specifically, factoring out the constant factor of an from the summation in eq. (2.15) yields (2.16) Interval4. For n > 6 but n- 6 :s 4 (i.e., for 6 < n :s 10), x[k]h[n - k] = { an-k, (n - 6) :::::; k :::::; 4 0, otherwise Ix [k] .......... 1111 ............. . 0 4 k (a) h[n-k] n < 0 •l l llttr •• _j• ••••••••••••••••• n 0 k n-6 I (b) h[n -k] ...... Jitt_rr ............... . 0 n k n-6 (c) h[n-k] 4<n~6 •••..•••• lI l lttr ••••••••.•••• / 0 n k n-6 (d) h[n-k] j 6<n~10 ··········-·l t I Itrrrn 0 ......... . k n-6 (e) h[n-k] l n>1 0 j ··········-······· lllttr •••• 0 n-6 n k (f) Figure 2. 9 Graphical interpretation of the convolution performed in Example 2.4. 87 88 Linear Time-Invariant Systems Chap.2 so that 4 y[n] = ~ a""- k k=n-6 We can again use eq. (2.13) to evaluate this summation. Letting r = k- n + 6, we obtain 10-n 10-n 1 -an-I I an-4- a 7 y[n] = ~ a6-r = a6 ~(a-IY = a6 -1 = ---- r=O r=O 1 -a 1 -a Interval 5. For n - 6 > 4, or equivalently, n > 10, there is no overlap between the nonzero portions of x[k] and h[n- k], and hence, y[n] = 0. Summarizing, then, we obtain 0, n<O 1- an+l 1- a ' y[n] = 6<n~10 1-a 0, 10 < n which is pictured in Figure 2.10. y[n] 0 4 6 10 n Figure 2.1 o Result of performing the convolution in Example 2.4. Example 2.5 Consider an LTI system with input x[n] and unit impulse response h[n] specified as follows: x[n] = 2""u[ -n], (2.17) h[n] = u[n]. (2.18) Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 89 1 1 2 x[k] = 2ku[ -k] 1 1 1 16 8 4 • ' t I 1 -2 -1 0 • • • • k 1 1 r h[n-k] 1 1 1 n • • k • (a) 2 y[n] 1 2 -3 -2 -1 0 2 3 n (b) Figure 2.11 (a) The sequences x[k] and h[n- k] for the convolution prob- lem considered in Example 2.5; (b) the resulting output signal y[n]. The sequences x[k] and h[n- k] are plotted as functions of kin Figure 2.11(a). Note that x[k] is zero fork> 0 and h[n- k] is zero fork> n. We also observe that, regardless of the value of n, the sequence x[k]h[n- k] always has nonzero samples along the k-axis. When n :2: 0, x[k]h[n - k] has nonzero samples in the interval k :::; 0. It follows that, for n :2: 0, 0 0 y[n] = L x[k]h[n- k] = L 2k. (2.19) k=-X k=-X To evaluate the infinite sum in eq. (2.19), we may use the infinite sum formula, LX ak 1 = --, 0 < lal < 1. (2.20) k=O 1 -a Changing the variable of summation in eq. (2.19) from k tor = - k, we obtain 1 = 2. (2.21) 1 - (112) Thus, y[n] takes on a constant value of 2 for n ;::: 0. 90 Linear Time-Invariant Systems Chap. 2 When n < 0, x[k]h[n - k] has nonzero samples for k :::::: n. It follows that, for n < 0, II II y[n] = L x[k]lz[n- kl = L 2k. (2.22) k=-X k=-CC By performing a change of variable I = - k and then m = I + n, we can again make use of the infinite sum formula, eq. (2.20), to evaluate the sum in eq. (2.22). The result is the following for n < 0: 11 111 .vrnl = .L:c : (12 ) ' = .L-""' (12 ) m--Il = (12 ) - .L:;: (12 ) = 211 . 2 = 211+ '· (2.23) I= -11 m=O m=O The complete sequence of y[n] is sketched in Figure 2.11 (b). These examples illustrate the usefulness of visualizing the calculation of the con- volution sum graphically. Moreover, in addition to providing a useful way in which to calculate the response of an LTI system, the convolution sum also provides an extremely useful representation for LTI systems that allows us to examine their properties in great detail. In particular, in Section 2.3 we will describe some of the properties of convolution and will also examine some of the system properties introduced in the previous chapter in order to see how these properties can be characterized for LTI systems. 2.2 CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL In analogy with the results derived and discussed in the preceding section, the goal of this section is to obtain a complete characterization of a continuous-time LTI system in terms of its unit impulse response. In discrete time, the key to our developing the convolution sum was the sifting property of the discrete-time unit impulse-that is, the mathematical representation of a signal as the superposition of scaled and shifted unit impulse functions. Intuitively, then, we can think of the discrete-time system as responding to a sequence of individual impulses. In continuous time, of course, we do not have a discrete sequence of input values. Nevertheless, as we discussed in Section 1.4.2, if we think of the unit im- pulse as the idealization of a pulse which is so short that its duration is inconsequential for any real, physical system, we can develop a representation for arbitrary continuous-time signals in terms of these idealized pulses with vanishingly small duration, or equivalently, impulses. This representation is developed in the next subsection, and, following that, we will proceed very much as in Section 2.1 to develop the convolution integral representation for continuous-time LTI systems. 2.2.1 The Representation of Continuous-Time Signals in Terms of Impulses To develop the continuous-time counterpart of the discrete-time sifting property in eq. (2.2), we begin by considering a pulse or ""staircase"" approximation, x(t), to a continuous-time signal x(t), as illustrated in Figure 2.12( a). In a manner similar to that"
2.2 Continuous-Time LTI Systems: The Convolution Integral,"90 Linear Time-Invariant Systems Chap. 2 When n < 0, x[k]h[n - k] has nonzero samples for k :::::: n. It follows that, for n < 0, II II y[n] = L x[k]lz[n- kl = L 2k. (2.22) k=-X k=-CC By performing a change of variable I = - k and then m = I + n, we can again make use of the infinite sum formula, eq. (2.20), to evaluate the sum in eq. (2.22). The result is the following for n < 0: 11 111 .vrnl = .L:c : (12 ) ' = .L-""' (12 ) m--Il = (12 ) - .L:;: (12 ) = 211 . 2 = 211+ '· (2.23) I= -11 m=O m=O The complete sequence of y[n] is sketched in Figure 2.11 (b). These examples illustrate the usefulness of visualizing the calculation of the con- volution sum graphically. Moreover, in addition to providing a useful way in which to calculate the response of an LTI system, the convolution sum also provides an extremely useful representation for LTI systems that allows us to examine their properties in great detail. In particular, in Section 2.3 we will describe some of the properties of convolution and will also examine some of the system properties introduced in the previous chapter in order to see how these properties can be characterized for LTI systems. 2.2 CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL In analogy with the results derived and discussed in the preceding section, the goal of this section is to obtain a complete characterization of a continuous-time LTI system in terms of its unit impulse response. In discrete time, the key to our developing the convolution sum was the sifting property of the discrete-time unit impulse-that is, the mathematical representation of a signal as the superposition of scaled and shifted unit impulse functions. Intuitively, then, we can think of the discrete-time system as responding to a sequence of individual impulses. In continuous time, of course, we do not have a discrete sequence of input values. Nevertheless, as we discussed in Section 1.4.2, if we think of the unit im- pulse as the idealization of a pulse which is so short that its duration is inconsequential for any real, physical system, we can develop a representation for arbitrary continuous-time signals in terms of these idealized pulses with vanishingly small duration, or equivalently, impulses. This representation is developed in the next subsection, and, following that, we will proceed very much as in Section 2.1 to develop the convolution integral representation for continuous-time LTI systems. 2.2.1 The Representation of Continuous-Time Signals in Terms of Impulses To develop the continuous-time counterpart of the discrete-time sifting property in eq. (2.2), we begin by considering a pulse or ""staircase"" approximation, x(t), to a continuous-time signal x(t), as illustrated in Figure 2.12( a). In a manner similar to that Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 91 x(t) -~ 0 ~ 2~ k~ (a) x(-2~)8/l(t + 2~)~ x( 2A)D I -2.:1 -.:1 (b) x(-~)8/l(t + ~)~ x(-A)O -~ 0 (c) x(0)8!l(t)~ nx(O) 0 ~ (d) x(~)8 ~ (t- ~)~ x(~) Figure 2.12 Staircase approxima- (e) tion to a continuous-time signal. 92 Linear Time-Invariant Systems Chap.2 employed in the discrete-time case, this approximation can be expressed as a linear com- bination of delayed pulses, as illustrated in Figure 2.12(a)-(e). If we define ±. O:=:::t<~ ot!(t) = { (2.24) 0, otherwise then, since ~o11 (t) has unit amplitude, we have the expression x(t) = ~ x(k~)ot!(t- k~)~. (2.25) k=~x From Figure 2.12, we see that, as in the discrete-time case [eq. (2.2)], for any value oft, only one term in the summation on the right-hand side of eq. (2.25) is nonzero. As we let~ approach 0, the approximation x(t) becomes better and better, and in the limit equals x(t). Therefore, +x x(t) = lim ~ x(k~)o.1(t- k~)~. (2.26) 11~0 k= ~x Also, as~ ~ 0, the summation in eq. (2.26) approaches an integral. This can be seen by considering the graphical interpretation of the equation, illustrated in Figure 2.13. Here, we have illustrated the signals x(T), o11(t- T), and their product. We have also indicated a shaded region whose area approaches the area under x(T)ot!(t- T) as~ ~ 0. Note that the shaded region has an area equal to x(m~) where t- ~ < m~ < t. Furthermore, for this value oft, only the term with k = m is nonzero in the summation in eq. (2.26), and thus, the right-hand side of this equation also equals x(m~). Consequently, it follows from eq. (2.26) and from the preceding argument that x(t) equals the limit as~ ~ 0 of the area under x(T)o11(t- T). Moreover, from eq. (1.74), we know that the limit as~ ~ 0 of o11(t) is the unit impulse function o(t). Consequently, +x X(t) = f~x X(T)o(t- T)dT. (2.27) As in discrete time, we refer to eq. (2.27) as the sifting property of the continuous-time impulse. We note that, for the specific example of x(t) = u(t), eq. (2.27) becomes +C/0 ~X u(t) = f~ oc U( T)o(t - T)dT = Jo o(t - T)dT, (2.28) since u(T) = 0 forT< 0 and u(T) = 1 forT> 0. Equation (2.28) is identicalto eq. (1.75), derived in Section 1.4.2. Once again, eq. (2.27) should be viewed as an idealization in the sense that, for ~ ""small enough,"" the approximation of x(t) in eq. (2.25) is essentially exact for any practical purpose. Equation (2.27) then simply represents an idealization of eq. (2.25) by taking~ to be vanishingly small. Note also that we could have derived eq. (2.27) directly by using several of the basic properties of the unit impulse that we derived in Section 1.4.2. Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 93 (a) oll(t- ,.) 1 X m~ t- ~ 'T (b) Figure 2.13 Graphical interpreta- (c) tion of eq. (2.26). Specifically, as illustrated in Figure 2.14(b), the signal B(t- r) (viewed as a function of T with t fixed) is a unit impulse located at T = t. Thus, as shown in Figure 2.14(c), the signal x( r)B(t - r) (once again viewed as a function of r) equals x(t)B(t - r) [i.e., it is a scaled impulse at T = t with an area equal to the value of x(t)]. Consequently, the integral of this signal from T = -oo to T = +oo equals x(t); that is, +oo I +oo I +oo I- oo x( r)B(t - r)dr = -oo x(t)B(t- r)dr = x(t) -oo B(t - r)dr = x(t). Although this derivation follows directly from Section 1.4.2, we have included the deriva- tion given in eqs. (2.24 )-(2.27) to stress the similarities with the discrete-time case and, in particular, to emphasize the interpretation of eq. (2.27) as representing the signal x(t) as a ""sum"" (more precisely, an integral) of weighted, shifted impulses. 94 Linear Time-Invariant Systems Chap.2 x(•) T (a) T (b) x(t) Figure 2.14 (a) Arbitrary signal x( T); (b) impulse B(t- T) as a function , of T with t fixed; (c) product of these (c) two signals. 2.2.2 The Continuous-Time Unit Impulse Response and the Convolution Integral Representation of LTI Systems As in the discrete-time case, the representation developed in the preceding section provides us with a way in which to view an arbitrary continuous-time signal as the superposition of scaled and shifted pulses. In particular, the approximate representation in eq. (2.25) repre- sents the signal x(t) as a sum of scaled and shifted versions of the basic pulse signal8j_(t). Consequently, the response y(t) of a linear system to this signal will be the superposition of the responses to the scaled and shifted versions of 811 (t). Specifically, let us define hk!l (t) as the response of an LTI system to the input 8:1(t- k~). Then, from eq. (2.25) and the superposition property, for continuous-time linear systems, we see that +x y(t) = ~ x(k~)hk.'1(t)~. (2.29) k= -'X The interpretation of eq. (2.29) is similar to that for eq. (2.3) in discrete time. In particular, consider Figure 2.15, which is the continuous-time counterpart of Figure 2.2. In Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 95 x(t) I I I I I I I I I I I 011 (a) 1\ x(O)h0(t)Ll x(O) OLl (b) x(Ll) (c) 1\ x(kLl)hkll (t)Ll x(kLl) rv- kLl (d) ~(t) y(t) 0 0 (e) x(t) y(t) Figure 2.15 Graphical interpreta- 0 0 tion of the response of a continuous- time linear system as expressed in (f) eqs. (2.29) and (2.30). 96 Linear Time-Invariant Systems Chap.2 Figure 2.15(a) we have depicted the input x(t) and its approximation x(t), while in Figure 2.15(b )-(d), we have shown the responses of the system to three of the weighted pulses in the expression for x(t). Then the output Y(t) corresponding to x(t) is the superposition of all of these responses, as indicated in Figure 2.15(e). What remains, then, is to consider what happens as Ll becomes vanishingly small- i.e., as Ll ~ 0. In particular, with x(t) as expressed in eq. (2.26), x(t) becomes an increas- ingly good approximation to x(t), and in fact, the two coincide as Ll ~ 0. Consequently, the response to x(t), namely, y(t) in eq. (2.29), must converge to y(t), the response to the actual input x(t), as illustrated in Figure 2.15(f). Furthermore, as we have said, for Ll ""small enough,"" the duration of the pulse Ot,(t- kil) is of no significance, in that, as far as the system is concerned, the response to this pulse is essentially the same as the response to a unit impulse at the same point in time. That is, since the pulse Ot,(t- kil) corresponds to a shifted unit impulse as Ll ~ 0, the response hkt,(t) to this input pulse becomes the response to an impulse in the limit. Therefore, if we let h7 (t) denote the response at timet tO a unit impulse O(t - T) located at timeT, then +x y(t) = lim L x(kil)hkt/t)Ll. (2.30) 6,--.() k= -Y: As Ll ~ 0, the summation on the right-hand side becomes an integral, as can be seen graphically in Figure 2.16. Specifically, in Figure 2.16 the shaded rectangle represents one term in the summation on the right-hand side of eq. (2.30) and as Ll ~ 0 the summation approaches the area under x(T)h7 (t) viewed as a function ofT. Therefore, +x y(t) = f-x X(T)h 7 (f)dT. (2.31) The interpretation of eq. (2.31) is analogous to the one for eq. (2.29). As we showed in Section 2.2.1, any input x(t) can be represented as x(t) = rxx X(T)/l(t- T)JT. Shaded area = x(k6.)hu(t)A Figure 2.16 Graphical illustration k..l (k+1)..l of eqs. (2.30) and (2.31 ). Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 97 That is, we can intuitively think of x(t) as a ""sum"" of weighted shifted impulses, where the weight on the impulse o(t- T) is x( T)dT. With this interpretation, eq. (2.31) represents the superposition of the responses to each of these inputs, and by linearity, the weight on the response h7 (t) to the shifted impulse o(t- T) is also x(T)dT. Equation (2.31) represents the general form of the response of a linear system in continuous time. If, in addition to being linear, the system is also time invariant, then h7 (t) = h0(t - T); i.e., the response of an LTI system to the unit impulse o(t - T), which is shifted by T seconds from the origin, is a similarly shifted version of the response to the unit impulse function o(t). Again, for notational convenience, we will drop the subscript and define the unit impulse response h(t) as h(t) = ho(t); (2.32) i.e., h(t) is the response to o(t). In this case, eq. (2.31) becomes +x y(t) = -x X(T)h(t- T)dT. (2.33) J Equation (2.33), referred to as the convolution integral or the superposition integral, is the continuous-time counterpart of the convolution sum of eq. (2.6) and corresponds to the representation of a continuous-time LTI system in terms of its response to a unit impulse. The convolution of two signals x(t) and h(t) will be represented symbolically as y(t) = x(t) * h(t). (2.34) While we have chosen to use the same symbol * to denote both discrete-time and continuous-time convolution, the context will generally be sufficient to distinguish the two cases. As in discrete time, we see that a continuous-time LTI system is completely char- acterized by its impulse response-i.e., by its response to a single elementary signal, the unit impulse o(t). In the next section, we explore the implications of this as we examine a number of the properties of convolution and of LTI systems in both continuous time and discrete time. The procedure for evaluating the convolution integral is quite similar to that for its discrete-time counterpart, the convolution sum. Specifically, in eq. (2.33) we see that, for any value oft, the output y(t) is a weighted integral of the input, where the weight on x(T) is h(t- T). To evaluate this integral for a specific value oft, we first obtain the signal h(t- T) (regarded as a function ofT with t fixed) from h( T) by a reflection about the origin and a shift to the right by t if t > 0 or a shift to the left by ltl for t < 0. We next multiply together the signals x( T) and h(t - T), and y(t) is obtained by integrating the resulting product from T = -'X toT = +'X. To illustrate the evaluation of the convolution integral, let us consider several examples. 98 Linear Time-Invariant Systems Chap.2 Example 2.6 Let x(t) be the input to an LTI system with unit impulse response h(t), where x(t) = e-at u(t), a > 0 and h(t) = u(t). In Figure 2.17, we have depicted the functions h(r), x(r), and h(t- r) for a negative value oft and for a positive value oft. From this figure, we see that fort < 0, the product of x(r) and h(t- r) is zero, and consequently, y(t) is zero. Fort> 0, e-aT, 0 < T < t x(r)h(t- r) = { h . . 0, ot erwtse h('r) 0 T X(T) 11'---- 0 T h(t-T) I 11 t<O 0 h(t-T) t>O I 0 T Figure 2.17 Calculation of the convolution integral for Example 2.6. Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 99 From this expression, we can compute y(t) for t > 0: y(t) = J: e-ar dT = --1e -ar II a o !o - e-ar). a Thus, for all t, y(t) is 1 y(t) = -(1 - e-at)u(t), a which is shown in Figure 2.18. y(t) = 1 (1- e-at )u(t) a 1a --------------------- 0 Figure 2.18 Response of the system in Example 2.6 with impulse re- sponse h(t) = u(t) to the input x(t) = e-at u(t). Example 2.7 Consider the convolution of the following two signals: ~: 0 < t < T x(t) = { otherwise ' 6, 0 < t < 2T h(t) = { otherwise · As in Example 2.4 for discrete-time convolution, it is convenient to consider the evalu- ation of y(t) in separate intervals. In Figure 2.19, we have sketched x(r) and have illus- tratedh(t-r)ineachoftheintervalsofinterest.Fort < Oandfort > 3T, x(r)h(t-r) = 0 for all values ofT, and consequently, y(t) = 0. For the other intervals, the product x( r)h(t- r) is as indicated in Figure 2.20. Thus, for these three intervals, the integration can be carried out graphically, with the result that 0, t < 0 lt2 0 < t < T 2 ' y(t) = !Tt- ~T2 , T < t < 2T , - l t2 + T t + '}_ T 2 2T < t < 3 T 2 2 ' 0, 3T < t which is depicted in Figure 2.21. X(T) 1h 0 T T h(t-T) ~tT t < 0 I t 0 T t- 2T h(t-T) N: 0< t < T I 0 t T t- 2T h(t-T) rK T < t < 2T I o t- 2T h(t-T) 2T!~ 2T < t < 3T 0 \ T t- 2T h(t-T) 2Tt ~ t > 3T 0 I t- 2T Figure 2.19 Signals x( T) and h( t - T) for different values of t for Example 2.7. 100 Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 101 0 < t < T 0 t 'T (a) X('T)h{t-'T) t-T1~ T < t < 2T 0 T 'T (b) X('T)h(t-T) 2T~lD 2T < t < 3T t-T t T 'T t-2T (c) Figure 2.20 Product x( T)h(t- T) for Example 2.7 for the three ranges of values of t for which this product is not identically zero. (See Figure 2.19.) y(t) 0 T 2T 3T Figure 2.21 Signal y(t) = x(t) * h(t) for Example 2.7. Example 2.8 Let y(t) denote the convolution of the following two signals: x(t) = e21 u(- t), (2.35) h(t) = u(t - 3). (2.36) The signals x( T) and h(t - T) are plotted as functions ofT in Figure 2.22(a). We first observe that these two signals have regions of nonzero overlap, regardless of the value 102 Linear Time-Invariant Systems Chap. 2 0 t-3 0 T (a) y(t) 0 3 (b) Figure 2.22 The convolution problem considered in Example 2.8. oft. When t- 3 :::; 0, the product of x(T) and h(t- T) is nonzero for -x < T < t- 3, and the convolution integral becomes (2.37) Fort-3 2: O,theproductx(T)h(t-T)isnonzerofor-x < T < O,sothattheconvolution integral is () 1 _v(t) = I-x e'2T dT = 2. (2.38) The resulting signal y(t) is plotted in Figure 2.22(b ). As these examples and those presented in Section 2.1 illustrate, the graphical in- terpretation of continuous-time and discrete-time convolution is of considerable value in visualizing the evaluation of convolution integrals and sums. Sec. 2.3 Properties of Linear Time-Invariant Systems 103 2.3 PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS In the preceding two sections, we developed the extremely important representations of continuous-time and discrete-time LTI systems in terms of their unit impulse re- sponses. In discrete time the representation takes the form of the convolution sum, while its continuous-time counterpart is the convolution integral, both of which we repeat here for convenience: +oo y[n] = L x[k]h[n - k] = x[n] * h[n] (2.39) k= -ex +oo y(t) = I-e x x( T)h(t- T)d'T = x(t) * h(t) (2.40) As we have pointed out, one consequence of these representations is that the charac- teristics of an LTI system are completely determined by its impulse response. It is impor- tant to emphasize that this property holds in general only for LTI systems. In particular, as illustrated in the following example, the unit impulse response of a nonlinear system does not completely characterize the behavior of the system. Example 2.9 Consider a discrete-time system with unit impulse response h[ ] = { 1, n = 0, 1 (2.41) n 0, otherwise · If the system is LTI, then eq. (2.41) completely determines its input-output behavior. In particular, by substituting eq. (2.41) into the convolution sum, eq. (2.39), we find the following explicit equation describing how the input and output of this LTI system are related: y[n] = x[n] + x[n- 1]. (2.42) On the other hand, there are many nonlinear systems with the same response-i.e., that given in eq. (2.41)-to the input o[n]. For example, both of the following systems have this property: y[n] = (x[n] + x[n - 1])2 , y[n] = max(x[n], x[n- 1]). Consequently, if the system is nonlinear it is not completely characterized by the impulse response in eq. (2.41). The preceding example illustrates the fact that LTI systems have a number of prop- erties not possessed by other systems, beginning with the very special representations that they have in terms of convolution sums and integrals. In the remainder of this section, we explore some of the most basic and important of these properties."
2.3 Properties of Linear Time-Invariant Systems,"Sec. 2.3 Properties of Linear Time-Invariant Systems 103 2.3 PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS In the preceding two sections, we developed the extremely important representations of continuous-time and discrete-time LTI systems in terms of their unit impulse re- sponses. In discrete time the representation takes the form of the convolution sum, while its continuous-time counterpart is the convolution integral, both of which we repeat here for convenience: +oo y[n] = L x[k]h[n - k] = x[n] * h[n] (2.39) k= -ex +oo y(t) = I-e x x( T)h(t- T)d'T = x(t) * h(t) (2.40) As we have pointed out, one consequence of these representations is that the charac- teristics of an LTI system are completely determined by its impulse response. It is impor- tant to emphasize that this property holds in general only for LTI systems. In particular, as illustrated in the following example, the unit impulse response of a nonlinear system does not completely characterize the behavior of the system. Example 2.9 Consider a discrete-time system with unit impulse response h[ ] = { 1, n = 0, 1 (2.41) n 0, otherwise · If the system is LTI, then eq. (2.41) completely determines its input-output behavior. In particular, by substituting eq. (2.41) into the convolution sum, eq. (2.39), we find the following explicit equation describing how the input and output of this LTI system are related: y[n] = x[n] + x[n- 1]. (2.42) On the other hand, there are many nonlinear systems with the same response-i.e., that given in eq. (2.41)-to the input o[n]. For example, both of the following systems have this property: y[n] = (x[n] + x[n - 1])2 , y[n] = max(x[n], x[n- 1]). Consequently, if the system is nonlinear it is not completely characterized by the impulse response in eq. (2.41). The preceding example illustrates the fact that LTI systems have a number of prop- erties not possessed by other systems, beginning with the very special representations that they have in terms of convolution sums and integrals. In the remainder of this section, we explore some of the most basic and important of these properties. 104 Linear Time-Invariant Systems Chap. 2 2.3. 1 The Commutative Property A basic property of convolution in both continuous and discrete time is that it is a commu- tative operation. That is, in discrete time +:r: x[n] * h[n] = h[n] * x[n] L h[k]x[n - k], (2.43) k= -'l"" and in continuous time x(t) * h(t) ~ h(t) * x(t) ~ r: h(T)X(t- T)dT. (2.44) These expressions can be verified in a straightforward manner by means of a substitution of variables in eqs. (2.39) and (2.40). For example, in the discrete-time case, if we let r = n - k or, equivalently, k = n - r, eq. (2.39) becomes x[n] * h[n] = L x[k]h[n- k] = L x[n- r]h[r] = h[n] * x[n]. (2.45) k= -X r=-x With this substitution of variables, the roles of x[n] and h[n] are interchanged. According to eq. (2.45), the output of an LTI system with input x[n] and unit impulse response h[n] is identical to the output of an LTI system with input h[n] and unit impulse response x[n]. For example, we could have calculated the convolution in Example 2.4 by first reflecting and shifting x[k], then multiplying the signals x[n- k] and h[k], and finally summing the products for all values of k. Similarly, eq. (2.44) can be verified by a change of variables, and the implications of this result in continuous time are the same: The output of an LTI system with input x(t) and unit impulse response h(t) is identical to the output of an LTI system with input h(t) and unit impulse response x(t). Thus, we could have calculated the convolution in Example 2.7 by reflecting and shifting x(t), multiplying the signals x(t - T) and h( T), and integrating over -x < T < +x. In specific cases, one of the two forms for computing convolutions [i.e., eq. (2.39) or (2.43) in discrete time and eq. (2.40) or (2.44) in continuous time] may be easier to visualize, but both forms always result in the same answer. 2.3.2 The Distributive Property Another basic property of convolution is the distributive property. Specifically, convolution distributes over addition, so that in discrete time x[n] * (h1 [n] + h2[n]) = x[n] * h1 [n] + x[n] * h2[n], (2.46) and in continuous time (2.47) This property can be verified in a straightforward manner. Sec. 2.3 Properties of Linear Time-Invariant Systems 105 Y1(t) h1(t) x(t) ~y(t) h2(t) Y2(t) (a) x(t)-----1~ 1------1·~ y(t) Figure 2.23 Interpretation of the distributive property of convolution for a parallel interconnection of LTI (b) systems. The distributive property has a useful interpretation in terms of system interconnec- tions. Consider two continuous-time LTI systems in parallel, as indicated in Figure 2.23(a). The systems shown in the block diagram are LTI systems with the indicated unit impulse responses. This pictorial representation is a particularly convenient way in which to denote LTI systems in block diagrams, and it also reemphasizes the fact that the impulse response of an LTI system completely characterizes its behavior. The two systems, with impulse responses h1( t) and h2(t), have identical inputs, and their outputs are added. Since YI (t) = x(t) * ht (t) and the system of Figure 2.23(a) has output y(t) = x(t) * ht (t) + x(t) * h2(t), (2.48) corresponding to the right-hand side of eq. (2.47). The system of Figure 2.23(b) has output y(t) = x(t) * [ht (t) + h2(t)], (2.49) corresponding to the left-hand side of eq. (2.47). Applying eq. (2.47) to eq. (2.49) and comparing the result with eq. (2.48), we see that the systems in Figures 2.23(a) and (b) are identical. There is an identical interpretation in discrete time, in which each of the signals in Figure 2.23 is replaced by a discrete-time counterpart (i.e., x(t), h1( t), h2(t), y 1( t), y2(t), and y(t) are replaced by x[n], ht [n], h2[n], y 1 [n], y2[n], and y[n], respectively). In summary, then, by virtue of the distributive property of convolution, a parallel combina- tion of LTI systems can be replaced by a single LTI system whose unit impulse response is the sum of the individual unit impulse responses in the parallel combination. 106 Linear Time-Invariant Systems Chap. 2 Also, as a consequence of both the commutative and distributive properties, we have [Xt [n] + x2[n]] * h[n] = Xt [n] * h[n] + x2[n] * h[n] (2.50) and [Xt (t) + X2(t)] * h(t) = Xt (t) * h(t) + X2(t) * h(t), (2.51) which simply state that the response of an LTI system to the sum of two inputs must equal the sum of the responses to these signals individually. As illustrated in the next example, the distributive property of convolution can also be exploited to break a complicated convolution into several simpler ones. Example 2.10 Let y[n] denote the convolution of the following two sequences: x[n] = (~ )"" u[n] + 2""u[ -n], (2.52) h[n] = u[n]. (2.53) Note that the sequence x[n] is nonzero along the entire time axis. Direct evaluation of such a convolution is somewhat tedious. Instead, we may use the distributive property to express y[n] as the sum of the results of two simpler convolution problems. In particular, ifweletx1[n] = (112Yu[n]andx2 [n] = 2nu[-n],itfollowsthat y[n] = (x1 [n] + x2[n]) * h[n]. (2.54) Using the distributive property of convolution, we may rewrite eq. (2.54) as y[n] = Y1 [n] + y2[n], (2.55) where Yl [n] = Xi [n] * h[n] (2.56) and Y2[n] = x2[n] * h[n]. (2.57) The convolution in eq. (2.56) for y 1 [n] can be obtained from Example 2.3 (with a = 112), while y2[n] was evaluated in Example 2.5. Their sum is y[n], which is shown in Figure 2.24. y[n] 4 -------------- --- 3 1 1 2 .... ~ T -3-2 -1 0 1 2 3 4 5 6 7 n Figure 2.24 The signal y[n] = x[n] * h[n] for Example 2.10. Sec. 2.3 Properties of Linear Time-Invariant Systems 107 2. 3. 3 The Associative Property Another important and useful property of convolution is that it is associative. That is, in discrete time (2.58) and in continuous time x(t) * [ht (t) * h2(t)] = [x(t) * h1 (t)] * h2(t). (2.59) This property is proven by straightforward manipulations of the summations and integrals involved. Examples verifying it are given in Problem 2.43. As a consequence of the associative property, the expressions y[n] = x[n] * ht [n] * h2[n] (2.60) and y(t) = x(t) * ht (t) * h2(t) (2.61) are unambiguous. That is, according to eqs. (2.58) and (2.59), it does not matter in which order we convolve these signals. An interpretation of the associative property is illustrated for discrete-time systems in Figures 2.25(a) and (b). In Figure 2.25(a), y[n] = w[n] * h2[n] = (x[n] * ht [n]) * h2[n]. In Figure 2.25(b ), y[n] = x[n] * h[n] = x[n] * (ht [n] * h2[n]). According to the associative property, the series interconnection of the two systems in Figure 2.25(a) is equivalent to the single system in Figure 2.25(b). This can be generalized to an arbitrary number of LTI systems in cascade, and the analogous interpretation and conclusion also hold in continuous time. By using the commutative property together with the associative property, we find another very important property of LTI systems. Specifically, from Figures 2.25(a) and (b), we can conclude that the impulse response of the cascade of two LTI systems is the convolution of their individual impulse responses. Since convolution is commutative, we can compute this convolution of h1 [n] and h2 [n] in either order. Thus, Figures 2.25(b) and (c) are equivalent, and from the associative property, these are in tum equivalent to the system of Figure 2.25(d), which we note is a cascade combination of two systems as in Figure 2.25(a), but with the order of the cascade reversed. Consequently, the unit impulse response of a cascade of two LTI systems does not depend on the order in which they are cascaded. In fact, this holds for an arbitrary number of LTI systems in cascade: The order in which they are cascaded does not matter as far as the overall system impulse response is concerned. The same conclusions hold in continuous time as well. x[n]--~L....-h2-[n_J_ :--....,.•~1 h1[n] 1----....,•~ y[n] Figure 2.25 Associative property of convolution and the implication of this and the commutative property for the (d) series interconnection of LTI systems. It is important to emphasize that the behavior of LTI systems in cascade-and, in particular, the fact that the overall system response does not depend upon the order of the systems in the cascade-is very special to such systems. In contrast, the order in which nonlinear systems are cascaded cannot be changed, in general, without changing the over- all response. For instance, if we have two memory less systems, one being multiplication by 2 and the other squaring the input, then if we multiply first and square second, we obtain y[n] = 4x2 [n]. However, if we multiply by 2 after squaring, we have y[n] = 2x2 [n]. Thus, being able to interchange the order of systems in a cascade is a characteristic par- ticular to LTI systems. In fact, as shown in Problem 2.51, we need both linearity and time invariance in order for this property to be true in general. 2.3.4 LTI Systems with and without Memory As specified in Section 1.6.1, a system is memory less if its output at any time depends only on the value of the input at that same time. From eq. (2.39), we see that the only way that this can be true for a discrete-time LTI system is if h[n] = 0 for n =i' 0. In this case Sec. 2.3 Properties of Linear Time-Invariant Systems 109 the impulse response has the form h[n] = Ko[n], (2.62) where K = h[O] is a constant, and the convolution sum reduces to the relation y[n] = Kx[n]. (2.63) If a discrete-time LTI system has an impulse response h[n] that is not identically zero for n =1:- 0, then the system has memory. An example of an LTI system with memory is the system given by eq. (2.42). The impulse response for this system, given in eq. (2.41), is nonzero for n = 1. From eq. (2.40), we can deduce similar properties for continuous-time LTI systems with and without memory. In particular, a continuous-time LTI system is memoryless if h(t) = 0 for t =1:- 0, and such a memoryless LTI system has the form y(t) = Kx(t) (2.64) for some constant K and has the impulse response h(t) = Ko(t). (2.65) Note that if K = 1 in eqs. (2.62) and (2.65), then these systems become identity systems, with output equal to the input and with unit impulse response equal to the unit impulse. In this case, the convolution sum and integral formulas imply that x[n] = x[n] * o[n] and x(t) = x(t) * o(t), which reduce to the sifting properties of the discrete-time and continuous-time unit im- pulses: +::o x[n] = ~ x[k]o[n- k] k= -oc +oo x(t) = I- x x( T)o(t - T)dT. 2.3.5 lnvertibility of LTI Systems Consider a continuous-time LTI system with impulse response h(t). Based on the discus- sion in Section 1.6.2, this system is invertible only if an inverse system exists that, when connected in series with the original system, produces an output equal to the input to the first system. Furthermore, if an LTI system is invertible, then it has an LTI inverse. (See Problem 2.50.) Therefore, we have the picture shown in Figure 2.26. We are given a sys- tem with impulse response h(t). The inverse system, with impulse response h1 (t), results in w(t) = x(t)-such that the series interconnection in Figure 2.26(a) is identical to the 110 Linear Time-Invariant Systems Chap. 2 ~Q___ x(t)~~~ w(t)=x(t) (a) Figure 2.26 Concept of an inverse x(t) ---~1 1dentity system t----~ x(t) system for continuous-time LTI sys- o(t) tems. The system with impulse re- sponse h1 {t) is the inverse of the system with impulse response h(t) if (b) h(t) * h1 (t) = o(t). identity system in Figure 2.26(b). Since the overall impulse response in Figure 2.26(a) is h(t) * h 1( t), we have the condition that h 1( t) must satisfy for it to be the impulse response of the inverse system, namely, h(t) * h1 (t) = o(t). (2.66) Similarly, in discrete time, the impulse response h 1 [n] of the inverse system for an LTI system with impulse response h[n] must satisfy h[n] *hi [n] = o[n]. (2.67) The following two examples illustrate invertibility and the construction of an inverse system. Example 2. 1 1 Consider the LTI system consisting of a pure time shift y(t) = x(t - to). (2.68) Such a system is a delay if to > 0 and an advance if to < 0. For example, if t0 > 0, then the output at time t equals the value of the input at the earlier time t - t0 . If to = 0. the system in eq. (2.68) is the identity system and thus is memoryless. For any other value of t0 , this system has memory, as it responds to the value of the input at a time other than the current time. The impulse response for the system can be obtained from eq. (2.68) by taking the input equal to o(t), i.e., h(t) = o(t- to). (2.69) Therefore, x(t- to) = x(t) * o(t- to). (2.70) That is, the convolution of a signal with a shifted impulse simply shifts the signal. To recover the input from the output, i.e., to invert the system, all that is required is to shift the output back. The system with this compensating time shift is then the inverse Sec. 2.3 Properties of Linear Time-Invariant Systems 111 system. That is, if we take h1 (t) = 5(t + to), then h(t) * h1 (t) = 5(t - to)* 5(t + to) = 5(t). Similarly, a pure time shift in discrete time has the unit impulse response 5[n- n0 ], so that convolving a signal with a shifted impulse is the same as shifting the signal. Furthermore, the inverse of the LTI system with impulse response 5[n - n0 ] is the LTI system that shifts the signal in the opposite direction by the same amount-i.e., the LTI system with impulse response 5[n + n0 ]. Example 2. 1 2 Consider an LTI system with impulse response h[n] = u[n]. (2.71) Using the convolution sum, we can calculate the response of this system to an arbitrary input: +oo y[n] = 2..:, x[k]u[n- k]. (2.72) k=-oo Since u[n- k] is 0 for n- k < 0 and 1 for n- k 2: 0, eq. (2.72) becomes n y[n] = 2..:, x[k]. (2.73) k=-X That is, this system, which we first encountered in Section 1.6.1 [see eq. (1.92)], is a summer or accumulator that computes the running sum of all the values of the input up to the present time. As we saw in Section 1.6.2, such a system is invertible, and its inverse, as given by eq. (1.99), is y[n] = x[n] - x[n- 1], (2.74) which is simply a first difference operation. Choosing x[n] = 5[n], we find that the impulse response of the inverse system is h1 [n] = 5[n] - 5[n- 1]. (2.75) As a check that h[n] in eq. (2.71) and h1 [n] in eq. (2.75) are indeed the impulse re- sponses of LTI systems that are inverses of each other, we can verify eq. (2.67) by direct calculation: h[n] * h1 [n] = u[n] * {5[n] - 5[n - 1]} = u[n] * 5[n] - u[n] * 5[n- 1] (2.76) = u[n] - u[n- 1] = 5[n]. 112 Linear Time-Invariant Systems Chap. 2 2.3.6 Causality for LTI Systems In Section 1.6.3, we introduced the property of causality: The output of a causal system depends only on the present and past values of the input to the system. By using the con- volution sum and integral, we can relate this property to a corresponding property of the impulse response of an LTI system. Specifically, in order for a discrete-time LTI system to be causal, y[n] must not depend on x[k] for k > n. From eq. (2.39), we see that for this to be true, all of the coefficients h[n- k] that multiply values of x[k] fork > n must be zero. This then requires that the impulse response of a causal discrete-time LTI system satisfy the condition h [ n] = 0 for n < 0. (2.77) According to eq. (2.77), the impulse response of a causal LTI system must be zero before the impulse occurs, which is consistent with the intuitive concept of causality. More gener- ally, as shown in Problem 1.44, causality for a linear system is equivalent to the condition of initial rest; i.e., if the input to a causal system is 0 up to some point in time, then the output must also be 0 up to that time. It is important to emphasize that the equivalence of causality and the condition of initial rest applies only to linear systems. For example, as discussed in Section 1.6.6, the system y[n] = 2x[n] + 3 is not linear. However, it is causal and, in fact, memoryless. On the other hand, if x[n] = 0, y[n] = 3 # 0, so it does not satisfy the condition of initial rest. For a causal discrete-time LTI system, the condition in eq. (2.77) implies that the convolution sum representation in eq. (2.39) becomes 11 y[n] = L x[k]h[n - k], (2.78) f.:.= -X and the alternative equivalent form, eq. (2.43), becomes y[n] = L h[k]x[n - k]. (2.79) f.:.=O Similarly, a continuous-time LTI system is causal if h(t) = 0 for t < 0, (2.80) and in this case the convolution integral is given by y(t) = Jt x( T)h(t - T)dT = ( x h( T)x(t - T)dT. (2.81) -x Jo Both the accumulator (h[n] = u[n]) and its inverse (h[n] = o[n]- o[n- 1]), de- scribed in Example 2.12, satisfy eq. (2.77) and therefore are causal. The pure time shift with impulse response h(t) = o(t- t0 ) is causal for t0 2::: 0 (when the time shift is a delay), but is noncausal for to < 0 (in which case the time shift is an advance, so that the output anticipates future values of the input). Sec. 2.3 Properties of Linear Time-Invariant Systems 113 Finally, while causality is a property of systems, it is common terminology to refer to a signal as being causal if it is zero for n < 0 or t < 0. The motivation for this terminology comes from eqs. (2. 77) and (2.80): Causality of an LTI system is equivalent to its impulse response being a causal signal. 2.3.7 Stability for LTI Systems Recall from Section 1.6.4 that a system is stable if every bounded input produces a bounded output. In order to determine conditions under which LTI systems are stable, consider an input x[n] that is bounded in magnitude: lx[n]l < B for all n. (2.82) Suppose that we apply this input to an LTI system with unit impulse response h[n]. Then, using the convolution sum, we obtain an expression for the magnitude of the output: (2.83) Since the magnitude of the sum of a set of numbers is no larger than the sum of the mag- nitudes of the numbers, it follows from eq. (2.83) that +oo ly[nJI ::5 L lh[kJIIx[n- k]l. (2.84) k= -00 From eq. (2.82), lx[n - k]l < B for all values of k and n. Together with eq. (2.84), this implies that +oo ly[nJI ::5 B L lh[kJI for all n. (2.85) k= -00 From eq. (2.85), we can conclude that if the impulse response is absolutely summable, that is, if +oo L lh[kJI < 00, (2.86) k= -00 then y[n] is bounded in magnitude, and hence, the system is stable. Therefore, eq. (2.86) is a sufficient condition to guarantee the stability of a discrete-time LTI system. In fact, this condition is also a necessary condition, since, as shown in Problem 2.49, if eq. (2.86) is not satisfied, there are bounded inputs that result in unbounded outputs. Thus, the~tability of a discrete-time LTI system is completely equivalent to eq. (2.86). In continuous time, we obtain an analogous characterization of stability in terms of the impulse response of an LTI system. Specifically, if lx(t)l < B for all t, then, in analogy with eqs. (2.83)-(2.85), it follows that 114 Linear Time-Invariant Systems Chap.2 ly(t)l ICx h(T)x(t- T)dTI ,-; rxx lh(T)IIx(t- T)ldT +:x> ~ B J-X l lh(T)!dT. Therefore, the system is stable if the impulse response is absolutely integrable, i.e., if (2.87) As in discrete time, if eq. (2.87) is not satisfied, there are bounded inputs that produce unbounded outputs; therefore, the stability of a continuous-time LTI system is equivalent to eq. (2.87). The use of eqs (2.86) and (2.87) to test for stability is illustrated in the next two examples. Example 2. 1 3 Consider a system that is a pure time shift in either continuous time or discrete time. Then, in discrete time +oo +oo L lh[n]l = L l8[n- noll = 1, (2.88) n=-'Xl n=-oc while in continuous time I+x f+x -e n ih(T)idT = -x 18(7- fo)idT = 1, (2.89) and we conclude that both of these systems are stable. This should not be surprising, since if a signal is bounded in magnitude, so is any time-shifted version of that signal. Now consider the accumulator described in Example 2.12. As we discussed in Section 1.6.4, this is an unstable system, since, if we apply a constant input to an accu- mulator, the output grows without bound. That this system is unstable can also be seen from the fact that its impulse response u[n] is not absolutely summable: X X L lu[n]l = L u[n] = oo. n=-xo n=O Similarly, consider the integrator, the continuous-time counterpart of the accumu- lator: y(t) = fx X(T)dT. (2.90) This is an unstable system for precisely the same reason as that given for the accumula- tor; i.e., a constant input gives rise to an output that grows without bound. The impulse Sec. 2.3 Properties of Linear Time-Invariant Systems 115 response for the integrator can be found by letting x(t) = 8(t), in which case h(t) = r""' 8( T)dT = u(t) and I+ x f +x lu(r)ldr = dr = oo. ~x () Since the impulse response is not absolutely integrable, the system is not stable. 2.3.8 The Unit Step Response of an LTI System Up to now, we have seen that the representation of an LTI system in terms of its unit impulse response allows us to obtain very explicit characterizations of system properties. Specifically, since h[n] or h(t) completely determines the behavior of an LTI system, we have been able to relate system properties such as stability and causality to properties of the impulse response. There is another signal that is also used quite often in describing the behavior of LTI systems: the unit step response, s[n] or s(t), corresponding to the output when x[n] = u[n] or x(t) = u(t). We will find it useful on occasion to refer to the step response, and therefore, it is worthwhile relating it to the impulse response. From the convolution -sum representation, the step response of a discrete-time LTI system is the convolution of the unit step with the impulse response; that is, s[n] = u[n] * h[n]. However, by the commutative property of convolution, s[n] = h[n] * u[n], and therefore, s[n] can be viewed as the response to the input h[n] of a discrete-time LTI system with unit impulse response u[n]. As we have seen in Example 2.12, u[n] is the unit impulse response of the accumulator. Therefore, 11 s[n] = L h[k]. (2.91) k=-00 From this equation and from Example 2.12, it is clear that h[ n] can be recovered from s[ n] using the relation h[n] = s[n] - s[n - 1]. (2.92) That is, the step response of a discrete-time LTI system is the running sum of its impulse response [eq. (2.91)]. Conversely, the impulse response of a discrete-time LTI system is the first difference of its step response [eq. (2.92)]. Similarly, in continuous time, the step response of an LTI system with impulse re- sponse h(t) is given by s(t) = u(t) * h(t), which also equals the response of an integra- tor [with impulse response u(t)] to the input h(t). That is, the unit step response of a continuous-time LTI system is the running integral of its impulse response, or s(t) = too h(T)dT, (2.93) 116 Linear Time-Invariant Systems Chap.2 and from eq. (2.93), the unit impulse response is the first derivative of the unit step re- sponse, 1 or h(t ) -_ dds(tt) -_ ' s ( ) t. (2.94) Therefore, in both continuous and discrete time, the unit step response can also be used to characterize an LTI system, since we can calculate the unit impulse response from it. In Problem 2.45, expressions analogous to the convolution sum and convolution integral are derived for the representations of an LTI system in terms of its unit step response. 2.4 CAUSAL LTI SYSTEMS DESCRIBED BY DIFFERENTIAL AND DIFFERENCE EQUATIONS An extremely important class of continuous-time systems is that for which the input and output are related through a linear constant-coefficient differential equation. Equations of this type arise in the description of a wide variety of systems and physical phenomena. For example, as we illustrated in Chapter 1, the response of the RC circuit in Figure 1.1 and the motion of a vehicle subject to acceleration inputs and frictional forces, as depicted in Figure 1.2, can both be described through linear constant-coefficient differential equations. Similar differential equations arise in the description of mechanical systems containing restoring and damping forces, in the kinetics of chemical reactions, and in many other contexts as well. Correspondingly, an important class of discrete-time systems is that for which the in- put and output are related through a linear constant-coefficient difference equation. Equa- tions of this type are used to describe the sequential behavior of many different processes. For instance, in Example 1.10 we saw how difference equations arise in describing the accumulation of savings in a bank account, and in Example 1.11 we saw how they can be used to describe a digital simulation of a continuous-time system described by a dif- ferential equation. Difference equations also arise quite frequently in the specification of discrete-time systems designed to perform particular operations on the input signal. For example, the system that calculates the difference between successive input values, as in eq. (1.99), and the system described by eq. (1.104) that computes the average value of the input over an interval are described by difference equations. Throughout this book, there will be many occasions in which we will consider and examine systems described by linear constant -coefficient differential and difference equa- tions. In this section we take a first look at these systems to introduce some of the basic ideas involved in solving differential and difference equations and to uncover and explore some of the properties of systems described by such equations. In subsequent chapters, we develop additional tools for the analysis of signals and systems that will add considerably both to our ability to analyze systems described by such equations and to our understanding of their characteristics and behavior. 1Throughout this book, we will use both the notations indicated in eq. (2.94) to denote first derivatives. Analogous notation will also be used for higher derivatives."
2.4 Causal LTI Systems Described by Differential and Difference Equations,"116 Linear Time-Invariant Systems Chap.2 and from eq. (2.93), the unit impulse response is the first derivative of the unit step re- sponse, 1 or h(t ) -_ dds(tt) -_ ' s ( ) t. (2.94) Therefore, in both continuous and discrete time, the unit step response can also be used to characterize an LTI system, since we can calculate the unit impulse response from it. In Problem 2.45, expressions analogous to the convolution sum and convolution integral are derived for the representations of an LTI system in terms of its unit step response. 2.4 CAUSAL LTI SYSTEMS DESCRIBED BY DIFFERENTIAL AND DIFFERENCE EQUATIONS An extremely important class of continuous-time systems is that for which the input and output are related through a linear constant-coefficient differential equation. Equations of this type arise in the description of a wide variety of systems and physical phenomena. For example, as we illustrated in Chapter 1, the response of the RC circuit in Figure 1.1 and the motion of a vehicle subject to acceleration inputs and frictional forces, as depicted in Figure 1.2, can both be described through linear constant-coefficient differential equations. Similar differential equations arise in the description of mechanical systems containing restoring and damping forces, in the kinetics of chemical reactions, and in many other contexts as well. Correspondingly, an important class of discrete-time systems is that for which the in- put and output are related through a linear constant-coefficient difference equation. Equa- tions of this type are used to describe the sequential behavior of many different processes. For instance, in Example 1.10 we saw how difference equations arise in describing the accumulation of savings in a bank account, and in Example 1.11 we saw how they can be used to describe a digital simulation of a continuous-time system described by a dif- ferential equation. Difference equations also arise quite frequently in the specification of discrete-time systems designed to perform particular operations on the input signal. For example, the system that calculates the difference between successive input values, as in eq. (1.99), and the system described by eq. (1.104) that computes the average value of the input over an interval are described by difference equations. Throughout this book, there will be many occasions in which we will consider and examine systems described by linear constant -coefficient differential and difference equa- tions. In this section we take a first look at these systems to introduce some of the basic ideas involved in solving differential and difference equations and to uncover and explore some of the properties of systems described by such equations. In subsequent chapters, we develop additional tools for the analysis of signals and systems that will add considerably both to our ability to analyze systems described by such equations and to our understanding of their characteristics and behavior. 1Throughout this book, we will use both the notations indicated in eq. (2.94) to denote first derivatives. Analogous notation will also be used for higher derivatives. Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 117 2 .4. 1 Linear Constant-Coefficient Differential Equations To introduce some of the important ideas concerning systems specified by linear constant- coefficient differential equations, let us consider a first-order differential equation as in eq. (1.85), viz., dy(t) ----;[! + 2y(t) = x(t), (2.95) where y(t) denotes the output of the system and x(t) is the input. For example, comparing eq. (2.95) to the differential equation (1.84) for the velocity of a vehicle subject to applied and frictional forces, we see that eq. (2.95) would correspond exactly to this system if y(t) were identified with the vehicle's velocity v(t), if x(t) were taken as the applied force f(t), and if the parameters in eq. (1.84) were normalized in units such that b/m = 2 and 1/m = 1. A very important point about differential equations such as eq. (2.95) is that they provide an implicit specification of the system. That is, they describe a relationship be- tween the input and the output, rather than an explicit expression for the system output as a function of the input. In order to obtain an explicit expression, we must solve the differential equation. To find a solution, we need more information than that provided by the differential equation alone. For example, to determine the speed of an automobile at the end of a 10 -second interval when it has been subjected to a constant acceleration of 1 m/sec2 for 10 seconds, we would also need to know how fast the vehicle was moving at the start of the interval. Similarly, if we are told that a constant source voltage of 1 volt is applied to the RC circuit in Figure 1.1 for 10 seconds, we cannot determine what theca- pacitor voltage is at the end of that interval without also knowing what the initial capacitor voltage is. More generally, to solve a differential equation, we must specify one or more auxil- iary conditions, and once these are specified, we can then, in principle, obtain an explicit expression for the output in terms of the input. In other words, a differential equation such as eq. (2.95) describes a constraint between the input and the output of a system, but to characterize the system completely, we must also specify auxiliary conditions. Different choices for these auxiliary conditions then lead to different relationships between the in- put and the output. For the most part, in this book we will focus on the use of differential equations to describe causal LTI systems, and for such systems the auxiliary conditions take a particular, simple form. To illustrate this and to uncover some of the basic properties of the solutions to differential equations, let us take a look at the solution of eq. (2.95) for a specific input signal x(t).2 20ur discussion of the solution of linear constant-coefficient differential equations is brief, since we as- sume that the reader has some familiarity with this material. For review, we recommend a text on the solution of ordinary differential equations, such as Ordinary Differential Equations (3rd ed.), by G. Birkhoff and G.-C. Rota (New York: John Wiley and Sons, 1978), or Elementary Differential Equations (3rd ed.), by W.E. Boyce and R.C. DiPrima (New York: John Wiley and Sons, 1977). There are also numerous texts that discuss differential equations in the context of circuit theory. See, for example, Basic Circuit Theory, by L.O. Chua, C.A. Desoer, and E.S. Kuh (New York: McGraw-Hill Book Company, 1987). As mentioned in the text, in the following chapters we present other very useful methods for solving linear differential equations that will be sufficient for our purposes. In addition, a number of exercises involving the solution of differential equations are included in the problems at the end of the chapter. 118 Linear Time-Invariant Systems Chap.2 Example 2. 1 4 Consider the solution of eq. (2.95) when the input signal is x(t) = K e""' u(t), (2.96) where K is a real number. The complete solution to eq. (2.96) consists of the sum of a particular solution, y""(t), and a homogeneous solution, yh(t), i.e., (2.97) where the particular solution satisfies eq. (2.95) and y11 (t) is a solution of the homoge- neous differential equation dv(t) ~ + 2y(t) = 0. (2.98) A common method for finding the particular solution for an exponential input signal as in eq. (2.96) is to look for a so-called forced response-i.e., a signal of the same form as the input. With regard to eq. (2.95), since x(t) = Ke""' fort> 0, we hypothesize a solution for t > 0 of the form (2.99) where Y is a number that we must determine. Substituting eqs. (2.96) and (2.99) into eq. (2.95) fort > 0 yields (2.100) Canceling the factor e""' from both sides of eq. (2.100), we obtain 3Y + 2Y = K, (2.101) or K y =- 5' (2.102) so that K \' (t) = - 3 5 e ' ' t > 0. (2.103) . p In order to determine y11 (t), we hypothesize a solution of the form Yh(t) = Ae 11 • (2.104) Substituting this into eq. (2.98) gives Ase 11 + 2Aesr = Aes'(s + 2) = 0. (2.105) From this equation, we see that we must takes = -2 and that Ae- 2' is a solution to eq. (2.98) for any choice of A. Utilizing this fact and eq. (2.103) in eq. (2.97), we find that the solution of the differential equation for t > 0 is .v (t) = Ae-~''"") + -K5e ·' ' ' t > 0. (2.106) Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 119 As noted earlier, the differential equation (2.95) by itself does not specify uniquely the response y(t) to the input x(t) in eq. (2.96). In particular, the constant A in eq. (2.106) has not yet been determined. In order for the value of A to be determined, we need to specify an auxiliary condition in addition to the differential equation (2.95). As explored in Problem 2.34, different choices for this auxiliary condition lead to different solutions y(t) and, consequently, to different relationships between the input and the output. As we have indicated, for the most part in this book we focus on differential and difference equations used to describe systems that are LTI and causal, and in this case the auxiliary condition takes the form of the condition of initial rest. That is, as shown in Problem 1.44, for a causal LTI system, if x(t) = 0 fort < t0 , then y(t) must also equal 0 fort < t0 . From eq. (2.96), we see that for our example x(t) = 0 fort < 0, and thus, the condition of initial rest implies that y(t) = 0 fort < 0. Evaluating eq. (2.1 06) at t = 0 and setting y(O) = 0 yields K O=A+5-' or Thus, for t > 0, (2.107) while for t < 0, y(t) = 0, because of the condition of initial rest. Combining these two cases, we obtain the full solution (2.108) Example 2.14 illustrates several very important points concerning linear constant- coefficient differential equations and the systems they represent. First, the response to an input x(t) will generally consist of the sum of a particular solution to the differential equation and a homogeneous solution-i.e., a solution to the differential equation with the input set to zero. The homogeneous solution is often referred to as the natural response of the system. The natural responses of simple electrical circuits and mechanical systems are explored in Problems 2.61 and 2.62. In Example 2.14 we also saw that, in order to determine completely the relation- ship between the input and the output of a system described by a differential equation such as eq. (2.95), we must specify auxiliary conditions. An implication of this fact, which is illustrated in Problem 2.34, is that different choices of auxiliary conditions lead to different relationships between the input and the output. As we illustrated in the ex- ample, for the most part we will use the condition of initial rest for systems described by differential equations. In the example, since the input was 0 for t < 0, the condition of initial rest implied the initial condition y(O) = 0. As we have stated, and as illustrated in 120 Linear Time-Invariant Systems Chap.2 Problem 2.33, under the condition of initial rest the system described by eq. (2.95) is LTI and causal.3 For example, if we multiply the input in eq. (2.96) by 2, the resulting output would be twice the output in eq. (2.1 08). It is important to emphasize that the condition of initial rest does not specify a zero initial condition at a fixed point in time, but rather adjusts this point in time so that the response is zero until the input becomes nonzero. Thus, if x(t) = 0 for t ~ to for the causal LTI system described by eq. (2.95), then y(t) = 0 for t ~ t0, and we would use the initial condition y(t0 ) = 0 to solve for the output for t > t0 . As a physical example, consider again the circuit in Figure 1.1, also discussed in Example 1.8. Initial rest for this example corresponds to the statement that, until we connect a nonzero voltage source to the circuit, the capacitor voltage is zero. Thus, if we begin to use the circuit at noon today, the initial capacitor voltage as we connect the voltage source at noon today is zero. Similarly, if we begin to use the circuit at noon tomorrow instead, the initial capacitor voltage as we connect the voltage source at noon tomorrow is zero. This example also provides us with some intuition as to why the condition of initial rest makes a system described by a linear constant-coefficient differential equation time invariant. For example, if we perform an experiment on the circuit, starting from initial rest, then, assuming that the coefficients Rand C don't change over time, we would expect to get the same results whether we ran the experiment today or tomorrow. That is, if we perform identical experiments on the two days, where the circuit starts from initial rest at noon on each day, then we would expect to see identical responses-i.e., responses that are simply time-shifted by one day with respect to each other. While we have used the first-order differential equation (2.95) as the vehicle for the discussion of these issues, the same ideas extend directly to systems described by higher order differential equations. A general Nth-order linear constant-coefficient differential equation is given by ~ dky(t) _ ~ b dk x(t) L ak-kd - L k-dk · (2.109) k=O t k=O t The order refers to the highest derivative of the output y(t) appearing in the equation. In the case when N = 0, eq. (2.1 09) reduces to _ 1 ~ b d k x( t) y ( t ) - - L k--k-. (2.110) ao k=O dt In this case, y(t) is an explicit function of the input x(t) and its derivatives. For N 2: 1, eq. (2.1 09) specifies the output implicitly in terms of the input. In this case, the analysis of the equation proceeds just as in our discussion of the first-order differential equation in Example 2.14. The solution y(t) consists of two parts-a particular solution to eq. (2.109) 3ln fact, as is also shown in Problem 2.34, if the initial condition for eq. (2.95) is nonzero, the resulting system is incrementally linear. That is, the overall response can be viewed, much as in Figure 1.48, as the superposition of the response to the initial conditions alone (with input set to 0) and the response to the input with an initial condition of 0 (i.e., the response of the causal LTI system described by eq. (2.95) ). Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 121 plus a solution to the homogeneous differential equation (2.111) The solutions to this equation are referred to as the natural responses of the system. As in the first-order case, the differential equation (2.1 09) does not completely spec- ify the output in terms of the input, and we need to identify auxiliary conditions to deter- mine completely the input-output relationship for the system. Once again, different choices for these auxiliary conditions result in different input-output relationships, but for the most part, in this book we will use the condition of initial rest when dealing with systems de- scribed by differential equations. That is, if x(t) = 0 fort::; t0 , we assume that y(t) = 0 for t::; t0 , and therefore, the response for t > to can be calculated from the differential equation (2.1 09) with the initial conditions _ dy(to) _ _ dN-iy(to) _ Y ( to ) - dt - ... - dtN-i - 0 . (2.112) Under the condition of initial rest, the system described by eq. (2.109) is causal and LTI. Given the initial conditions in eq. (2.112), the output y(t) can, in principle, be determined by solving the differential equation in the manner used in Example 2.14 and further illus- trated in several problems at the end of the chapter. However, in Chapters 4 and 9 we will develop some tools for the analysis of continuous-time LTI systems that greatly facilitate the solution of differential equations and, in particular, provide us with powerful methods for analyzing and characterizing the properties of systems described by such equations. 2.4.2 linear Constant-Coefficient Difference Equations The discrete-time counterpart of eq. (2.1 09) is the Nth-order linear constant -coefficient difference equation N M L aky[n- k] = L bkx[n - k]. (2.113) k=O k=O An equation of this type can be solved in a manner exactly analogous to that for differential equations. (See Problem 2.32.)4 Specifically, the solution y[n] can be written as the sum of a particular solution to eq. (2.113) and a solution to the homogeneous equation N Laky[n- k] = 0. (2.114) k=O 4For a detailed treatment of the methods for solving linear constant-coefficient difference equations, see Finite Difference Equations, by H. Levy and F. Lessman (New York: Macmillan, Inc., 1961), or Finite Difference Equations and Simulations (Englewood Cliffs, NJ: Prentice-Hall, 1968) by F. B. Hildebrand. In Chapter 6, we present another method for solving difference equations that greatly facilitates the analysis of linear time-invariant systems that are so described. In addition, we refer the reader to the problems at the end of this chapter that deal with the solution of difference equations. 122 Linear Time-Invariant Systems Chap.2 The solutions to this homogeneous equation are often referred to as the natural responses of the system described by eq. (2.113). As in the continuous-time case, eq. (2.113) does not completely specify the output in terms of the input. To do this, we must also specify some auxiliary conditions. While there are many possible choices for auxiliary conditions, leading to different input-output relationships, we will focus for the most part on the condition of initial rest-i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 as well. With initial rest, the system described by eq. (2.113) is LTI and causal. Although all of these properties can be developed following an approach that di- rectly parallels our discussion for differential equations, the discrete-time case offers an alternative path. This stems from the observation that eq. (2.113) can be rearranged in the form y[n] = -1 { LMb kx[n- k]- LNa ky[n- k] } . (2.115) ao k=O k=I Equation (2.115) directly expresses the output at time n in terms of previous values of the input and output. From this, we can immediately see the need for auxiliary conditions. In order to calculate y[n], we need to know y[n- 1], ... , y[n- N]. Therefore, if we are given the input for all n and a set of auxiliary conditions such as y[-N], y[-N + 1] , ... , y[ -1 ], eq. (2.115) can be solved for successive values of y[n]. An equation of the form of eq. (2.113) or eq. (2.115) is called a recursive equation, since it specifies a recursive procedure for determining the output in terms of the input and previous outputs. In the special case when N = 0, eq. (2.115) reduces to y[n] = ±(b k )x[n - k]. (2.116) k=O ao This is the discrete-time counterpart of the continuous-time system given in eq. (2.110). Here, y[n] is an explicit function of the present and previous values of the input. For this reason, eq. (2.116) is often called a nonrecursive equation, since we do not recursively use previously computed values of the output to compute the present value of the output. Therefore, just as in the case of the system given in eq. (2.110), we do not need auxiliary conditions in order to determine y[n]. Furthermore, eq. (2.116) describes an LTI system, and by direct computation, the impulse response of this system is found to be bn h[n] = G;;• 0 ~ n ~ M (2.117) { 0, otherwise That is, eq. (2.116) is nothing more than the convolution sum. Note that the impulse re- sponse for it has finite duration; that is, it is nonzero only over a finite time interval. Because of this property, the system specified by eq. (2.116) is often called a .finite impulse response (FIR) system. Although we do not require auxiliary conditions for the case of N = 0, such condi- tions are needed for the recursive case when N ;;:::: 1. To illustrate the solution of such an equation, and to gain some insight into the behavior and properties of recursive difference equations, let us examine the following simple example: Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 123 Example 2. 1 5 Consider the difference equation 1 y[n] - 2 y[n- 1] = x[n]. (2.118) Eq. (2.118) can also be expressed in the form 1 y[n] = x[n] + 2y [n- 1], (2.119) highlighting the fact that we need the previous value of the output, y[n- 1], to calculate the current value. Thus, to begin the recursion, we need an initial condition. For example, suppose that we impose the condition of initial rest and consider the input x[n] = Ko[n]. (2.120) In this case, since x[ n] = 0 for n ::::; - 1, the condition of initial rest implies that y[ n] = 0 for n ::::; - 1, so that we have as an initial condition y[ -1] = 0. Starting from this initial condition, we can solve for successive values of y[n] for n 2 0 as follows: 1 y[O] = x[O] + 2 y[ -1] = K, (2.121) 1 1 y[1] = x[l] + 2 y[O] = 2 K, (2.122) 2 y[2] = x[2] + 1 (1 ) 2 y[1] = 2 K, (2.123) y[n] = x[n] + 12y [n- 1] = (1 )II 2 K. (2.124) Since the system specified by eq. (2.118) and the condition of initial rest is LTI, its input- output behavior is completely characterized by its impulse response. Setting K = 1, we see that the impulse response for the system considered in this example is h[n] 1 )II = (2 u[n]. (2.125) Note that the causal LTI system in Example 2.15 has an impulse response of infinite duration. In fact, if N ~ 1 in eq. (2.113), so that the difference equation is recursive, it is usually the case that the LTI system corresponding to this equation together with the condition of initial rest will have an impulse response of infinite duration. Such systems are commonly referred to as infinite impulse response ( IIR) systems. As we have indicated, for the most part we will use recursive difference equations in the context of describing and analyzing systems that are linear, time-invariant, and causal, and consequently, we will usually make the assumption of initial rest. In Chapters 5 and 10 we will develop tools for the analysis of discrete-time systems that will provide us 124 Linear Time-Invariant Systems Chap.2 with very useful and efficient methods for solving linear constant-coefficient difference equations and for analyzing the properties of the systems that they describe. 2.4.3 Block Diagram Representations of First-Order Systems Described by Differential and Difference Equations An important property of systems described by linear constant -coefficient difference and differential equations is that they can be represented in very simple and natural ways in terms of block diagram interconnections of elementary operations. This is significant for a number of reasons. One is that it provides a pictorial representation which can add to our understanding of the behavior and properties of these systems. In addition, such representations can be of considerable value for the simulation or implementation of the systems. For example, the block diagram representation to be introduced in this section for continuous-time systems is the basis for early analog computer simulations of systems described by differential equations, and it can also be directly translated into a program for the simulation of such a system on a digital computer. In addition, the corresponding representation for discrete-time difference equations suggests simple and efficient ways in which the systems that the equations describe can be implemented in digital hardware. In this section, we illustrate the basic ideas behind these block diagram representations by constructing them for the causal first-order systems introduced in Examples 1.8-1.11. In Problems 2.57-2.60 and Chapters 9 and 10, we consider block diagrams for systems described by other, more complex differential and difference equations. We begin with the discrete-time case and, in particular, the causal system described by the first-order difference equation y[n] + ay[n- 1] = bx[n]. (2.126) To develop a block diagram representation of this system, note that the evaluation of eq. (2.126) requires three basic operations: addition, multiplication by a coefficient, and delay (to capture the relationship between y[n] and y[n - 1] ). Thus, let us define three basic network elements, as indicated in Figure 2.27. To see how these basic elements can be used to represent the causal system described by eq. (2.126), we rewrite this equation in the form that directly suggests a recursive algorithm for computing successive values of the output y[n]: _v[n] = -ay[n - 1] + bx[n]. (2.127) This algorithm is represented pictorially in Figure 2.28, which is an example of a feedback system, since the output is fed back through a delay and a multiplication by a coefficient and is then added to bx[n]. The presence of feedback is a direct consequence of the recur- sive nature of eq. (2.127). The block diagram in Figure 2.28 makes clear the required memory in this system and the consequent need for initial conditions. In particular, a delay corresponds to a mem- ory element, as the element must retain the previous value of its input. Thus, the initial value of this memory element serves as a necessary initial condition for the recursive cal- culation specified pictorially in Figure 2.28 and mathematically in eq. (2.127). Of course, if the system described by eq. (2.126) is initially at rest, the initial value stored in the memory element is zero. Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 125 x2[n] x [n] ___, ..,~~. .....- -~•~x1 [n] + x [n] 1 2 (a) a x[n] ____. ...,. _____ ax[n] (b) Figure 2.27 Basic elements for the block diagram representation x[n] ·I D ....,__----!•~ x[n-1] of the causal system described by eq. (2.126): (a) an adder; (b) multi- plication by a coefficient; (c) a unit (c) delay. b x[n] D Figure 2.28 Block diagram repre- -a sentation for the causal discrete-time ....__ __,.. ......... _.... y[n -1] system described by eq. (2.126). Consider next the causal continuous-time system described by a first-order differen- tial equation: dy(t) ~ + ay(t) = bx(t). (2.128) As a first attempt at defining a block diagram representation for this system, let us rewrite it as y(t) = -! dy(t) + ~x(t). (2.129) a dt a The right-hand side of this equation involves three basic operations: addition, multiplica- tion by a coefficient, and differentiation. Therefore, if we define the three basic network elements indicated in Figure 2.29, we can consider representing eq. (2.129) as an inter- connection of these basic elements in a manner analogous to that used for the discrete-time system described previously, resulting in the block diagram of Figure 2.30. While the latter figure is a valid representation of the causal system described by eq. (2.128), it is not the representation that is most frequently used or the representation that leads directly to practical implementations, since differentiators are both difficult to implement and extremely sensitive to errors and noise. An alternative implementation that 126 Linear Time-Invariant Systems Chap. 2 (a) a x(t) ---...-ot---- ax(t) (b) Figure 2.29 One possible set of x(t)~d~~t) basic elements for the block diagram representation of the continuous-time system described by eq. (2.128): (a) an adder; (b) multiplication by a (c) coe11icient; (c) a differentiator. b/a x(t)....,.--t~ y(t) D Figure 2.30 Block diagram representation for the system in eqs. (2.128) and (2.129), using adders, -1/a dy(t) multiplications by coefficients, and dt di fferentiators. is much more widely used can be obtained by first rewriting eq. (2.128) as dJv(tt) = bx(t) - ay(t) (2.130) and then integrating from -x tot. Specifically, if we assume that in the system described by eq. (2.130) the value of y( -oo) is zero, then the integral of dy(t)ldt from -oo tot is precisely y(t). Consequently, we obtain the equation y(l) = Lz [h x(T) - ay( T) 1 dT. (2.131) In this form, our system can be implemented using the adder and coefficient multiplier indicated in Figure 2.29, together with an integrator, as defined in Figure 2.31. Figure 2.32 is a block diagram representation for this system using these elements. Sec. 2.5 Singularity Functions 127 Figure 2.31 Pictorial representation of an integrator. b x(t) ---t--( 1-----.......o~~ y(t) Figure 2.32 Block diagram rep- resentation for the system in eqs. (2.128) and (2.131 ), using adders, -a multiplications by coefficients, and in- tegrators. Since integrators can be readily implemented using operational amplifiers, repre- sentations such as that in Figure 2.32 lead directly to analog implementations, and indeed, this is the basis for both early analog computers and modem analog computation systems. Note that in the continuous-time case it is the integrator that represents the memory stor- age element of the system. This is perhaps more readily seen if we consider integrating eq. (2.130) from a finite point in time t0 , resulting in the expression y(t) = y(to) + J.r [bx(T)- ay(T)] dT. (2.132) to Equation (2.132) makes clear the fact that the specification of y(t) requires an initial con- dition, namely, the value of y (t0). It is precisely this value that the integrator stores at time t0 . While we have illustrated block diagram constructions only for the simplest first- order differential and difference equations, such block diagrams can also be developed for higher order systems, providing both valuable intuition for and possible implementations of these systems. Examples of block diagrams for higher order systems can be found in Problems 2.58 and 2.60. 2.5 SINGULARITY FUNCTIONS In this section, we take another look at the continuous-time unit impulse function in order to gain additional intuitions about this important idealized signal and to introduce a set of related signals known collectively as singularity functions. In particular, in Section 1.4.2 we suggested that a continuous-time unit impulse could be viewed as the idealization of a pulse that is ""short enough"" so that its shape and duration is of no practical consequence- i.e., so that as far as the response of any particular LTI system is concerned, all of the area under the pulse can be thought of as having been applied instantaneously. In this section, we would first like to provide a concrete example of what this means and then use the interpretation embodied within the example to show that the key to the use of unit impulses and other singularity functions is in the specification of how LTI systems respond to these idealized signals; i.e., the signals are in essence defined in terms of how they behave under convolution with other signals."
2.5 Singularity Functions,"Sec. 2.5 Singularity Functions 127 Figure 2.31 Pictorial representation of an integrator. b x(t) ---t--( 1-----.......o~~ y(t) Figure 2.32 Block diagram rep- resentation for the system in eqs. (2.128) and (2.131 ), using adders, -a multiplications by coefficients, and in- tegrators. Since integrators can be readily implemented using operational amplifiers, repre- sentations such as that in Figure 2.32 lead directly to analog implementations, and indeed, this is the basis for both early analog computers and modem analog computation systems. Note that in the continuous-time case it is the integrator that represents the memory stor- age element of the system. This is perhaps more readily seen if we consider integrating eq. (2.130) from a finite point in time t0 , resulting in the expression y(t) = y(to) + J.r [bx(T)- ay(T)] dT. (2.132) to Equation (2.132) makes clear the fact that the specification of y(t) requires an initial con- dition, namely, the value of y (t0). It is precisely this value that the integrator stores at time t0 . While we have illustrated block diagram constructions only for the simplest first- order differential and difference equations, such block diagrams can also be developed for higher order systems, providing both valuable intuition for and possible implementations of these systems. Examples of block diagrams for higher order systems can be found in Problems 2.58 and 2.60. 2.5 SINGULARITY FUNCTIONS In this section, we take another look at the continuous-time unit impulse function in order to gain additional intuitions about this important idealized signal and to introduce a set of related signals known collectively as singularity functions. In particular, in Section 1.4.2 we suggested that a continuous-time unit impulse could be viewed as the idealization of a pulse that is ""short enough"" so that its shape and duration is of no practical consequence- i.e., so that as far as the response of any particular LTI system is concerned, all of the area under the pulse can be thought of as having been applied instantaneously. In this section, we would first like to provide a concrete example of what this means and then use the interpretation embodied within the example to show that the key to the use of unit impulses and other singularity functions is in the specification of how LTI systems respond to these idealized signals; i.e., the signals are in essence defined in terms of how they behave under convolution with other signals. 128 Linear Time-Invariant Systems Chap.2 2.5.1 The Unit Impulse as an Idealized Short Pulse From the sifting property, eq. (2.27), the unit impulse o(t) is the impulse response of the identity system. That is, x(f) = x(t) * D(f) (2.133) for any signal x(t). Therefore, if we take x(t) = o(t), we have o(t) = o(t) * o(t). (2.134) Equation (2.134) is a basic property of the unit impulse, and it also has a significant im- plication for our interpretation of the unit impulse as an idealized pulse. For example, as in Section 1.4.2, suppose that we think of o(t) as the limiting form of a rectangular pulse. Specifically, let D.::,.(t) correspond to the rectangular pulse defined in Figure 1.34, and let (2.135) Then r.::,.(f) is as sketched in Figure 2.33. If we wish to interpret o(t) as the limit as~ ~ 0 of D.::,.(t), then, by virtue of eq. (2.134), the limit as~ ~ 0 for r.::,.(t) must also be a unit impulse. In a similar manner, we can argue that the limits as~ ~ 0 of r.::,.(t) * r.::,.(t) or r!l(t) * D.::,.(t) must be unit impulses, and so on. Thus, we see that for consistency, if we define the unit impulse as the limiting form of some signal, then in fact, there is an unlimited number of very dissimilar-looking signals, all of which behave like an impulse in the limit. The key words in the preceding paragraph are ""behave like an impulse,"" where, as we have indicated, what we mean by this is that the response of an LTI system to all of these signals is essentially identical, as long as the pulse is ""short enough,"" i.e.,~ is ""small enough."" The following example illustrates this idea: r ... (t) 1 ..1 Figure 2.33 The signal r.. . (t) 0 2..1 defined in eq. (2.135). Example 2. 1 6 Consider the LTI system described by the first-order differential equation d v(t) -dt · + 2 .v (t) = x(t), (2.136) together with the condition of initial rest. Figure 2.34 depicts the response of this system to 8::.(t), r::.(t), r::.(t) * 8::.(t), and r::.(t) * r::.(t) for several values of d. Ford large enough, the responses to these input signals differ noticeably. However, ford sufficiently small, the responses are essentially indistinguishable, so that all of the input signals ""behave"" in the same way. Furthermore, as suggested by the figure, the limiting form of all of these responses is precisely e- 21 u(t). Since the limit of each of these signals as d ~ 0 is the unit impulse, we conclude that e- 21 u(t) is the impulse response for this system.5 5 In Chapters 4 and 9, we will describe much simpler ways to determine the impulse response of causal LTI systems described by linear constant-coefficient differential equations. Sec. 2.5 Singularity Functions 129 0.5 1 2 1 2 Responses to x(t) = o:,(t) Responses to x(t) = r:,(t) (a) (b) 2 Responses to x(t) = o:,(t)•r:,(t) Responses to x(t) = r:,(t)•r:,(t) (c) (d) h(t) = e - 21u (t) 0.5 (e) Figure 2.34 Interpretation of a unit impulse as the idealization of a pulse whose duration is ""short enough"" so that, as far as the response of an LTI system to this pulse is concerned, the pulse can be thought of as having been applied instantaneously: (a) responses of the causal LTI system de- scribed by eq. (2.136) to the input all (t) for ~ = 0.25, 0.1, and 0.0025; (b) responses of the same system to rfl (t) for the same values of ~; (c) re- sponses to otl(t)*rtl(t); (d) responses to rfl(t)*rtl(t); (e) the impulse response h(t) = e-2t u(t) for the system. Note that, for ~ = 0.25, there are noticeable differences among the responses to these different signals; however, as ~ becomes smaller, the differences diminish, and all of the responses converge to the impulse response shown in (e). 130 Linear Time-Invariant Systems Chap.2 One important point to be emphasized is that what we mean by ""~ small enough"" depends on the particular LTI system to which the preceding pulses are applied. For example, in Figure 2.35, we have illustrated the responses to these pulses for different 0.5 0.1 0.2 0.1 0.2 Responses to x(t) = 3~(t) Responses to x(t) = r~(t) (a) (b) 0.1 0.2 Responses to x(t) = 3~(t)• r ~(t) Responses to x(t) = r ~ (t)• r ~ (t) (c) (d) h(t) = e - 201u (t) 0.5 (e) Figure 2.35 Finding a value of~ that is ""small enough"" depends upon the system to which we are applying inputs: (a) responses of the causal LTI system described by eq. (2.137) to the input 8D.(t) for~= 0.025, 0.01, and 0.00025; (b) responses to rfl(t); (c) responses to 8fl(t)*rfl(t); (d) responses to rfl(t) * rfl(t); (e) the impulse response h(t) = e-201 u(t) for the system. Com- paring these responses to those in Figure 2.34, we see that we need to use a smaller value of ~ in this case before the duration and shape of the pulse are of no consequence. Sec. 2.5 Singularity Functions 131 values of Ll for the causal LTI system described by the first-order differential equation dy(t) ---;[{ + 20y(t) = x(t). (2.137) As seen in the figure, we need a smaller value of Ll in this case in order for the responses to be indistinguishable from each other and from the impulse response h(t) = e- 201 u(t) for the system. Thus, while what we mean by ""Ll small enough"" is different for these two systems, we can find values of Ll small enough for both. The unit impulse is then the idealization of a short pulse whose duration is short enough for all systems. 2.5.2 Defining the Unit Impulse through Convolution As the preceding example illustrates, for Ll small enough, the signals Ba(t), ra(t), ra(t) * Ba(t), and ra(t) * ra(t) all act like impulses when applied to an LTI system. In fact, there are many other signals for which this is true as well. What it suggests is that we should think of a unit impulse in terms of how an LTI system responds to it. While usually a function or signal is defined by what it is at each value of the independent variable, the primary importance of the unit impulse is not what it is at each value oft, but rather what it does under convolution. Thus, from the point of view of linear systems analysis, we may alternatively define the unit impulse as that signal which, when applied to an LTI system, yields the impulse response. That is, we define o(t) as the signal for which x(t) = x(t) * o(t) (2.138) for any x(t). In this sense, signals, such as Ba(t), ra(t), etc., which correspond to short pulses with vanishingly small duration as Ll ~ 0, all behave like a unit impulse in the limit because, if we replace o(t) by any of these signals, then eq. (2.138) is satisfied in the limit. All the properties of the unit impulse that we need can be obtained from the opera- tional definition given by eq. (2.138). For example, if we let x(t) = 1 for all t, then +oc 1 = X(t) = x(t) * D(t) = D(t) *X( f) = J-0 £ D( T)X(t - T) dT +oo = J- oc D( T) dT, so that the unit impulse has unit area. It is sometimes useful to use another completely equivalent operational definition of o(t). To obtain this alternative form, consider taking an arbitrary signal g(t), reversing it in time to obtain g( -t), and then convolving this with o(t). Using eq. (2.138), we obtain +oo g(-t) = g(-t) * D(t) = J-o c g(T- t)D(T)dT, which, for t = 0, yields +oo g(O) = J- oc g(T)D(T)dT. (2.139) 132 Linear Time-Invariant Systems Chap.2 Therefore, the operational definition of o(t) given by eq. (2.138) implies eq. (2.139). On the other hand, eq. (2.139) implies eq. (2.138). To see this, let x(t) be a given signal, fix a time t, and define g(T) = X(t- T). Then, using eq. (2.139), we have +x f +x X(t) = g(0) = f- x g( T) 0( T) dT = -x X(t - T) 0( T) dT, which is precisely eq. (2.138). Therefore, eq. (2.139) is an equivalent operational definition of the unit impulse. That is, the unit impulse is the signal which, when multiplied by a signal g(t) and then integrated from -oo to +oo, produces the value g(O). Since we will be concerned principally with LTI systems, and thus with convolution, the characterization of o(t) given in eq. (2.138) will be the one to which we will refer most often. However, eq. (2.139) is useful in determining some of the other properties of the unit impulse. For example, consider the signal f(t) o(t), where f(t) is another signal. Then, from eq. (2.139), r~~ g(r)f(r)O(r)dT = g(O)f(O). (2.140) On the other hand, if we consider the signal f(O) o(t), we see that r~x g(r)f(O)O(r)dr = g(O)f(O). (2.141) Comparing eqs. (2.140) and (2.141), we find thatthe two signals f(t) o(t) and f(O) o(t) be- have identically when they are multiplied by any signal g(t) and then integrated from -oo to +oo. Consequently, using this form of the operational definition of signals, we conclude that J(t) o(t) = J(O) o(t), (2.142) which is a property that we derived by alternative means in Section 1.4.2. [See eq. ( 1.76).] 2.5.3 Unit Doublets and Other Singularity Functions The unit impulse is one of a class of signals known as singularity functions, each of which can be defined operationally in terms of its behavior under convolution. Consider the LTI system for which the output is the derivative of the input, i.e., dx(t) y ( t) = -- (2.143) dt The unit impulse response of this system is the derivative of the unit impulse, which is called the unit doublet u 1( t). From the convolution representation for LTI systems, we have Sec. 2.5 Singularity Functions 133 dx(t) -----;[( = x(t) * u1( t) (2.144) for any signal x(t). Just as eq. (2.138) serves as the operational definition of l>(t), we will take eq. (2.144) as the operational definition of u1( t). Similarly, we can define u2(t), the second derivative of l>(t), as the impulse response of an LTI system that takes the second derivative of the input, i.e., (2.145) From eq. (2.144 ), we see that ddt (d---x--(;t[{) ) = X(t) * UJ (t) * UJ (t), (2.146) and therefore, U2(t) = UJ (t) * UJ (t). (2.147) In general, uk(t), k > 0, is the kth derivative of l>(t) and thus is the impulse response of a system that takes the kth derivative of the input. Since this system can be obtained as the cascade of k differentiators, we have Uk(t) = UJ (t) * · · · * UJ (t). (2.148) k times As with the unit impulse, each of these singularity functions has properties that can be derived from its operational definition. For example, if we consider the constant signal x(t) = 1, we find that dx(t) I+ oo 0 = -d- = X(t) * u1(t) = UJ(T)X(t- T)dT f -oc +oo = I-o c UJ(T)dT, so that the unit doublet has zero area. Moreover, if we convolve the signal g( -t) with u1( t), we obtain +oo dg( -t) I g(T- t)UJ(T)dT = g(-t) * UJ(t) = -d- -g'(-t), -oc f which, for t = 0, yields +oo -g'(O) = I-o o g(T)UJ(T)dT. (2.149) 134 Linear Time-Invariant Systems Chap.2 In an analogous manner, we can derive related properties of u 1( f) and higher order singu- larity functions, and several of these properties are considered in Problem 2.69. As with the unit impulse, each of these singularity functions can be informally re- lated to short pulses. For example, since the unit doublet is formally the derivative of the unit impulse, we can think of the doublet as the idealization of the derivative of a short pulse with unit area. For instance, consider the short pulse o~(t) in Figure 1.34. This pulse behaves like an impulse as Ll ~ 0. Consequently, we would expect its derivative to be- have like a doublet as Ll ~ 0. As verified in Problem 2.72, do~(t)/dt is as depicted in Figure 2.36: It consists of a unit impulse at t = 0 with area + 1/Ll, followed by a unit impulse of area -1/Ll at t = Ll, i.e., do:?> = ~ [o(t) - ou - .:l)]. (2.150) Consequently, using the fact that x(t) * o(t- t0 ) = x(t- t0 ) [see eq. (2.70)], we find that do~(t) x(t) - x(t- Ll) dx(t) x(t) * ----;]{ = Ll = df' (2.151) where the approximation becomes increasingly accurate as Ll ~ 0. Comparing eq. (2.151) with eq. (2.144), we see that d81.(t)/dt does indeed behave like a unit doublet as Ll ~ 0. In addition to singularity functions that are derivatives of different orders of the unit impulse, we can also define signals that represent successive integrals of the unit im- pulse function. As we saw in Example 2.13, the unit step is the impulse response of an integrator: Therefore, u(t) = L, 8(T)dT, (2.152) d8..l(t) dt _1.. ..1 Figure 2.36 The derivative do-'(t)ldt of the short rectangular pulse o-' (t) of Figure 1.34. Sec. 2.5 Singularity Functions 135 and we also have the following operational definition of u(t): x(t) * u(t) = r x('r) dT. (2.153) 00 Similarly, we can define the system that consists of a cascade of two integrators. Its impulse response is denoted by u_2(t), which is simply the convolution of u(t), the impulse response of one integrator, with itself: U-2(1) = u(t) * u(t) = roo u(T ) dT. (2.154) Since u(t) equals 0 fort < 0 and equals 1 fort > 0, it follows that U-2(t) = tu(t). (2.155) This signal, which is referred to as the unit ramp function, is shown in Figure 2.37. Also, we can obtain an operational definition for the behavior of u_2(t) under convolution from eqs. (2.153) and (2.154): x(t) * U-2(t) = x(t) * u(t) * u(t) = a~oo x(<T)d<T )· u(t) (2.156) = LU~oo x(u)du)dT In an analogous fashion, we can define higher order integrals of 8(t) as the impulse responses of cascades of integrators: U-k(t) = U(t) * ... * U(t) = Jt U-(k-1)( T) dT. (2.157) ~-Cfe k times The convolution of x(t) with u_ 3(t), u_4 (t), ... generate correspondingly higher order integrals of x(t). Also, note that the integrals in eq. (2.157) can be evaluated directly (see Figure 2.37 Unit ramp function. 136 Linear Time-Invariant Systems Chap.2 Problem 2.73), as was done in eq. (2.155), to obtain tk-1 u_k(t) = (k _ )! u(t). (2.158) 1 Thus, unlike the derivatives of o(t), the successive integrals of the unit impulse are func- tions that can be defined for each value oft [eq. (2.158)], as well as by their behavior under convolution. At times it will be worthwhile to use an alternative notation for o(t) and u(t), namely, o(t) = uo(t), (2.159) u(t) = U-1 (t). (2.160) With this notation, uk(t) for k > 0 denotes the impulse response of a cascade of k differ- entiators, u0(t) is the impulse response of the identity system, and, for k < 0, uk(t) is the impulse response of a cascade of Ik l integrators. Furthermore, since a differentiator is the inverse system of an integrator, u(t) * U1 (t) = O(t), or, in our alternative notation, u_ 1( t) * u1 (t) = uo(t). (2.161) More generally, from eqs. (2.148), (2.157), and (2.161 ), we see that for any integers k and r, (2.162) If k and rare both positive, eq. (2.162) states that a cascade of k differentiators followed by r more differentiators yields an output that is the (k + r)th derivative of the input. Similarly, if k is negative and r is negative, we have a cascade of lkl integrators followed by another lrl integrators. Also, if k is negative and r is positive, we have a cascade of lkl integrators followed by r differentiators, and the overall system is equivalent to a cascade of lk + rl integrators if k + r < 0, a cascade of k + r differentiators if k + r > 0, or the identity system if k + r = 0. Therefore, by defining singularity functions in terms of their behavior under convolution, we obtain a characterization that allows us to manipulate them with relative ease and to interpret them directly in terms of their significance for LTI systems. Since this is our primary concern in the book, the operational definition for singularity functions that we have given in this section will suffice for our purposes.6 6 As mentioned in Chapter 1, singularity functions have been heavily studied in the field of mathematics under the alternative names of generalized functions and distribution theory. The approach we have taken in this section is actually closely allied in spirit with the rigorous approach taken in the references given in footnote 3 of Section 1.4. Chap.2 Problems 137 2.6 SUMMARY In this chapter, we have developed important representations for LTI systems, both in dis- crete time and in continuous time. In discrete time we derived a representation of signals as weighted sums of shifted unit impulses, and we then used this to derive the convolution- sum representation for the response of a discrete-time LTI system. In continuous time we derived an analogous representation of continuous-time signals as weighted integrals of shifted unit impulses, and we used this to derive the convolution integral representation for continuous-time LTI systems. These representations are extremely important, as they allow us to compute the response of an LTI system to an arbitrary input in terms of the sys- tem's response to a unit impulse. Moreover, in Section 2.3 the convolution sum and integral provided us with a means of analyzing the properties of LTI systems and, in particular, of relating LTI system properties, including causality and stability, to corresponding proper- ties of the unit impulse response. Also, in Section 2.5 we developed an interpretation of the continuous-time unit impulse and other related singularity functions in terms of their behavior under convolution. This interpretation is particularly useful in the analysis of LTI systems. An important class of continuous-time systems consists of those described by linear constant-coefficient differential equations. Similarly, in discrete time, linear constant- coefficient difference equations play an equally important role. In Section 2.4, we exam- ined simple examples of differential and difference equations and discussed some of the properties of systems described by these types of equations. In particular, systems de- scribed by linear constant-coefficient differential and difference equations together with the condition of initial rest are causal and LTI. In subsequent chapters, we will develop additional tools that greatly facilitate our ability to analyze such systems. Chapte1 2 P1Dblem1 The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. Extension problems introduce applications, concepts, or methods beyond those pre- sented in the text. BASIC PROBLEMS WITH ANSWERS 2.1. Let x[n] = 8[n] + 28[n- 1] - 8[n- 3] and h[n] = 28[n + 1] + 28[n- 1]. Compute and plot each of the following convolutions: (a) YI [n] = x[n] * h[n] (b) Y2[n] = x[n + 2] * h[n] (c) Y3 [n] = x[n] * h[n + 2]"
2.6 Summary,"Chap.2 Problems 137 2.6 SUMMARY In this chapter, we have developed important representations for LTI systems, both in dis- crete time and in continuous time. In discrete time we derived a representation of signals as weighted sums of shifted unit impulses, and we then used this to derive the convolution- sum representation for the response of a discrete-time LTI system. In continuous time we derived an analogous representation of continuous-time signals as weighted integrals of shifted unit impulses, and we used this to derive the convolution integral representation for continuous-time LTI systems. These representations are extremely important, as they allow us to compute the response of an LTI system to an arbitrary input in terms of the sys- tem's response to a unit impulse. Moreover, in Section 2.3 the convolution sum and integral provided us with a means of analyzing the properties of LTI systems and, in particular, of relating LTI system properties, including causality and stability, to corresponding proper- ties of the unit impulse response. Also, in Section 2.5 we developed an interpretation of the continuous-time unit impulse and other related singularity functions in terms of their behavior under convolution. This interpretation is particularly useful in the analysis of LTI systems. An important class of continuous-time systems consists of those described by linear constant-coefficient differential equations. Similarly, in discrete time, linear constant- coefficient difference equations play an equally important role. In Section 2.4, we exam- ined simple examples of differential and difference equations and discussed some of the properties of systems described by these types of equations. In particular, systems de- scribed by linear constant-coefficient differential and difference equations together with the condition of initial rest are causal and LTI. In subsequent chapters, we will develop additional tools that greatly facilitate our ability to analyze such systems. Chapte1 2 P1Dblem1 The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. Extension problems introduce applications, concepts, or methods beyond those pre- sented in the text. BASIC PROBLEMS WITH ANSWERS 2.1. Let x[n] = 8[n] + 28[n- 1] - 8[n- 3] and h[n] = 28[n + 1] + 28[n- 1]. Compute and plot each of the following convolutions: (a) YI [n] = x[n] * h[n] (b) Y2[n] = x[n + 2] * h[n] (c) Y3 [n] = x[n] * h[n + 2]"
Problems,"Chap.2 Problems 137 2.6 SUMMARY In this chapter, we have developed important representations for LTI systems, both in dis- crete time and in continuous time. In discrete time we derived a representation of signals as weighted sums of shifted unit impulses, and we then used this to derive the convolution- sum representation for the response of a discrete-time LTI system. In continuous time we derived an analogous representation of continuous-time signals as weighted integrals of shifted unit impulses, and we used this to derive the convolution integral representation for continuous-time LTI systems. These representations are extremely important, as they allow us to compute the response of an LTI system to an arbitrary input in terms of the sys- tem's response to a unit impulse. Moreover, in Section 2.3 the convolution sum and integral provided us with a means of analyzing the properties of LTI systems and, in particular, of relating LTI system properties, including causality and stability, to corresponding proper- ties of the unit impulse response. Also, in Section 2.5 we developed an interpretation of the continuous-time unit impulse and other related singularity functions in terms of their behavior under convolution. This interpretation is particularly useful in the analysis of LTI systems. An important class of continuous-time systems consists of those described by linear constant-coefficient differential equations. Similarly, in discrete time, linear constant- coefficient difference equations play an equally important role. In Section 2.4, we exam- ined simple examples of differential and difference equations and discussed some of the properties of systems described by these types of equations. In particular, systems de- scribed by linear constant-coefficient differential and difference equations together with the condition of initial rest are causal and LTI. In subsequent chapters, we will develop additional tools that greatly facilitate our ability to analyze such systems. Chapte1 2 P1Dblem1 The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. Extension problems introduce applications, concepts, or methods beyond those pre- sented in the text. BASIC PROBLEMS WITH ANSWERS 2.1. Let x[n] = 8[n] + 28[n- 1] - 8[n- 3] and h[n] = 28[n + 1] + 28[n- 1]. Compute and plot each of the following convolutions: (a) YI [n] = x[n] * h[n] (b) Y2[n] = x[n + 2] * h[n] (c) Y3 [n] = x[n] * h[n + 2] 138 Linear Time-Invariant Systems Chap. 2 2.2. Consider the signal n-1 h[n] = ~ {u[n + 3] - u[n - 10]}. ( ) Express A and B in terms of n so that the following equation holds: h[n - k] = ( _!_ )n- k- I A < k < B 2 ' - - . { 0, elsewhere 2.3. Consider an input x[n] and a unit impulse response h[n] given by x[n] = (U-2 u[n- 2], h[n] = u[n + 2]. Determine and plot the output y[n] = x[n] * h[n]. 2.4. Compute and plot y[n] = x[n] * h[n], where x[n] = { ~ 3:Sn:S8 otherwise ' ~: 4 ::; n ::; 15 h[n] = { otherwise 2.5. Let x[n ] = 1, 0 ::; n ::; 9 and h[ n ] = { 1, 0 ::; n ::; N { 0, elsewhere 0, elsewhere ' where N ::; 9 is an integer. Determine the value of N, given that y[n] = x[n] * h[n] and y[4] = 5, y[l4] = 0. 2.6. Compute and plot the convolution y[n] = x[n] * h[n], where x[n] = (~ r· u[-n- I] and h[n] = u[n- 1]. 2.7. A linear systemS has the relationship CIJ y[n] = L x[k]g[n - 2k] k=-c;IO between its input x[n] and its output y[n], where g[n] = u[n] - u[n- 4]. Chap.2 Problems 139 (a) Determine y[n] when x[n] = o[n- 1]. (b) Determine y[n] when x[n] = o[n- 2]. (c) IsS LTI? (d) Determine y[n] when x[n] = u[n]. 2.8. Determine and sketch the convolution of the following two signals: t + 1, 0 ::; t ::; 1 x(t) = 2 - t, 1 < t ::; 2 , { 0, elsewhere h(t) = o(t + 2) + 2o(t + 1). 2.9. Let h(t) = e21u(-t+4)+e-21 u(t-5). Determine A and B such that -2(t-T) e ' r<A h(t- T) = 0, A<r<B. { e2(t-T), B<T 2.10. Suppose that ~ 0:St:S1 x(t) = { elsewhere and h(t) = x(tla), where 0 <a ::; 1. (a) Determine and sketch y(t) = x(t) * h(t). (b) If dy(t)ldt contains only three discontinuities, what is the value of a? 2.11. Let x(t) = u(t- 3)- u(t- 5) and h(t) = e-3t u(t). (a) Compute y(t) = x(t) * h(t). (b) Compute g(t) = (dx(t)ldt) * h(t). (c) How is g(t) related to y(t)? 2.12. Let 00 y(t) = e-tu(t) * .:L set- 3k). k= -00 Show that y(t) = Ae-t for 0 ::; t < 3, and determine the value of A. 140 Linear Time-Invariant Systems Chap. 2 2.13. Consider a discrete-time system 5 1 with impulse response 1 )fl h[n] = (S u[n]. (a) Find the integer A such that h[n] - Ah[n- 1] = o[n]. (b) Using the result from part (a), determine the impulse response g[n] of an LTI system 52 which is the inverse system of 5 1. 2.14. Which of the following impulse responses correspond(s) to stable LTI systems? (a) h1(t) = e-(1- 2j)ru(t) (b) h2(t) = e-r cos(2t)u(t) 2.15. Which of the following impulse responses correspond(s) to stable LTI systems? (a) h 1[n] = ncos(*n)u[n] (b) h2[n] = 311 u[ -n + 10] 2.16. For each of the following statements, determine whether it is true or false: (a) If x[n] = 0 for n < N 1 and h[n] = 0 for n < N2, then x[n] * h[n] = 0 for n < N1 + N2. (b) If y[n] = x[n] * h[n], then y[n- 1] = x[n- 1] * h[n- 1]. (c) If y(t) = x(t) * h(t), then y( -t) = x( -t) * h( -t). (d) If x(t) = 0 for t > T1 and h(t) = 0 for t > T2, then x(t) * h(t) = 0 for t > T1 + T2. 2.17. Consider an LTI system whose input x(t) and output y(t) are related by the differ- ential equation d dt y(t) + 4 y(t) = x(t). (P2.17-1) The system also satisfies the condition of initial rest. (a) If x(t) = e<-l+3j)tu(t), what is y(t)? (b) Note that CRe{x(t)} will satisfy eq. (P2.17-1) with CRe{y(t)}. Determine the out- put y(t) of the LTI system if x(t) = e -r cos(3t)u(t). 2.18. Consider a causal LTI system whose input x[n] and output y[n] are related by the difference equation 1 y[n] = 4 y[n - 1] + x[n]. Determine y[n] if x[n] = o[n - 1]. 2.19. Consider the cascade of the following two systems 5 1 and 52, as depicted in Figure P2.19: x[n]-j s, w[n]•l s, 1---~•..,.._ y[n] Figure P2. 1 9 Chap.2 Problems 141 S 1 : causal LTI, 1 w[n] = 2w[n- 1] + x[n]; S2 : causal LTI, y[n] = ay[n- 1] + f3w[n]. The difference equation relating x[n] and y[n] is: 1 3 y[n] = - Sy [n - 2] + 4 y[n - 1] + x[n]. (a) Determine a and f3. (b) Show the impulse response of the cascade connection of S1 and S2• 2.20. Evaluate the following integrals: (a) f _xx uo(t) cos(t) dt (b) ffi"" sin(27Tt) B(t + 3) dt (c) f ~5 u1 (1 - T) cos(27TT) dT BASIC PROBLEMS 2.21. Compute the convolution y[n] = x[n] * h[n] of the following pairs of signals: 11 (a) x[n] = a u[n], } a ¥= f3 h[n] = {3 11 u[n], (b) x[n] = h[n] = anu[n] (c) x[n] = (- ~ )"" u[n - 4] h[n] = 4""uC2 - n] (d) x[n] and h[n] are as in Figure P2.21. x[n] h[n] 1 ... JIIII ..... • ••• I I I I I I ... I I I I I I .... -1 0 1 2 3 4 5 n 0 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 n Figure P2.21 2.22. For each of the following pairs of waveforms, use the convolution integral to find the response y(t) of the LTI system with impulse response h(t) to the input x(t). Sketch your results. (a) x(t) = e-ar u(t)} (Do this both when a ¥= f3 and when a = f3 .) h(t) = e-f3t u(t) 142 Linear Time-Invariant Systems Chap.2 (b) x(t) = u(t) - 2u(t - 2) + u(t - 5) h(t) = e2 t u(l - t) (c) x(t) and h(t) are as in Figure P2.22(a). (d) x(t) and h(t) are as in Figure P2.22(b). (e) x(t) and h(t) are as in Figure P2.22(c). x(t) h(t) 2 One period of sin Tit 2 (a) x(t) h(t) 2 _1 3 (b) x(t) h(t) -1 (c) Figure P2.22 2.23. Let h(t) be the triangular pulse shown in Figure P2.23(a), and let x(t) be the impulse train depicted in Figure P2.23(b ). That is, +x x(t) = L, 8(t - kT). k=-X Determine and sketch y(t) = x(t) * h(t) for the following values ofT: (a) T = 4 (b) T = 2 (c) T = 3/2 (d) T = I Chap. 2 Problems 143 h(t) -1 (a) ···t t t t { t t t t t t··· - 2T - T 0 T 2T 3T (b) Figure P2.23 2.24. Consider the cascade interconnection of three causal LTI systems, illustrated in Fig- ure P2.24(a). The impulse response h2[n] is h2[n] = u[n] - u[n - 2], and the overall impulse response is as shown in Figure P2.24(b ). x[n] y[n] (a) • • -1 0 1 2 3 4 5 6 7 n (b) Figure P2.24 (a) Find the impulse response h1 [n]. (b) Find the response of the overall system to the input x[n] = o[n] - o[n- 1]. 144 Linear Time-Invariant Systems Chap.2 2.25. Let the signal y[n] = x[n] * h[n], where x[n] = 3""u[ -n - I] + (~ )"" u[n] and h[n] = (~ )"" u[n + 3]. (a) Determine y[n] without utilizing the distributive property of convolution. (b) Determine y[n] utilizing the distributive property of convolution. 2.26. Consider the evaluation of y[n] = XJ [n] * x2[n] * x3[n], where x 1 [n] = (0.5Yu[n], x2[n] = u[n + 3], and x3[n] = B[n]- B[n- 1]. (a) Evaluate the convolution x1 [n] * x2 [n]. (b) Convolve the result of part (a) with x3[n] in order to evaluate y[n]. (c) Evaluate the convolution x2 [n] * x3 [n]. (d) Convolve the result of part (c) with x 1 [n] in order to evaluate y[n]. 2.27. We define the area under a continuous-time signal v(t) as +oo A\. = I-o o v(t) dt. Show that if y(t) = x(t) * h(t), then 2.28. The following are the impulse responses of discrete-time LTI systems. Determine whether each system is causal and/or stable. Justify your answers. (a) h[n] = (~)nu[n] (b) h[n] = (0.8)nu[n + 2] (c) h[n] = (~)nu[-n] (d) h[n] = (5)nu[3 - n] (e) h[n] = (- ~)nu[n] + (1.01) 11u[n- 1] (f) h[n] = (- ~)nu[n] + (l.Ol)n u[l - n] (g) h[n] = n(~)'1 u[n- 1] 2.29. The following are the impulse responses of continuous-time LTI systems. Determine whether each system is causal and/or stable. Justify your answers. (a) h(t) = e-4r u(t- 2) (b) h(t) = e-6t u(3 - t) (c) h(t) = e-2ru (t + 50) (d) h(t) = e21 u( -1 - t) Chap.2 Problems 145 (e) h(t) = e-61tl (0 h(t) = te-t u(t) (g) h(t) = (2e-t - e(t-IOO)!IOO)u(t) 2.30. Consider the first -order difference equation y[n] + 2y[n - 1] = x[n]. Assuming the condition of initial rest (i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 ), find the impulse response of a system whose input and output are related by this difference equation. You may solve the problem by rearranging the difference equation so as to express y[n] in terms of y[n -1] and x[n] and generating the values of y[O], y[ + 1] , y[ + 2], ... in that order. 2.31. Consider the LTI system initially at rest and described by the difference equation y[n] + 2y[n - 1] = x[n] + 2x[n - 2]. Find the response of this system to the input depicted in Figure P2.31 by solving the difference equation recursively. x[n] 2 1 ••••• T(J J rr\ •••• -2-1 o 12 34 n FigureP2.31 2.32. Consider the difference equation 1 y[n] - y[n- 1] = x[n], (P2.32-1) 2 and suppose that x[n] = (~ )"" u[n]. (P2.32-2) Assume that the solution y[n] consists of the sum of a particular solution Yp[n] to eq. (P2.32-l) and a homogeneous solution Yh[n] satisfying the equation 1 Yh[n]- 2Yh[n- 1] = 0. (a) Verify that the homogeneous solution is given by Yh[n] = AG)"" (b) Let us consider obtaining a particular solution Yp[n] such that Yp[n]- 1Y p[n- 1] = (1 )n 2 3 u[n]. 146 Linear Time-Invariant Systems Chap.2 By assuming that Yp[n] is of the form B( *) '1 for n 2: 0, and substituting this in the above difference equation, determine. the value of B. (c) Suppose that the LTI system described by eq. (P2.32-1) and initially at rest has as its input the signal specified by eq. (P2.32-2). Since x[n] = 0 for n < 0, we have that y[n] = 0 for n < 0. Also, from parts (a) and (b) we have that y[n] has the form y[n] =A (""12 )11 11 + B (1""3 ) for n :::::: 0. In order to solve for the unknown constant A, we must specify a value for y[n] for some n 2: 0. Use the condition of initial rest and eqs. (P2.32-1) and (P2.32-2) to determine y[O]. From this value determine the constant A. The result of this calculation yields the solution to the difference equation (P2.32-1) under the condition of initial rest, when the input is given by eq. (P2.32-2). 2.33. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation dy(t) -----;[( + 2y(t) = x(t). (P2.33-1) The system also satisfies the condition of initial rest. (a) (i) Determine the system output y 1( t) when the input is x 1( t) = e3 t u(t). (ii) Determine the system output y2(t) when the input is x2(t) = e2t u(t). (iii) Determine the system output y3(t) when the input is x3(t) = ae3t u(t) + {3e2tu(t), where a and {3 are real numbers. Show that y3(t) = ay1( t) + {3 Y2(t). (iv) Now let x 1 (t) and x2(t) be arbitrary signals such that x 1(t) = 0, fort< t1, x2(t) = 0, fort < t2. Letting Y1 (t) be the system output for input x 1 (t), y2(t) be the system output for input x2(t), and y3(t) be the system output for x3(t) = ax1( t) + {3x2(t), show that y3(t) = ay1 (t) + {3 Y2(t). We may therefore conclude that the system under consideration is linear. (b) (i) Determine the system output y 1 (t) when the input is x 1 (t) = K e2t u(t). (ii) Determine the system output y2(t) when the input is x2(t) = K e2(t-T) u(t - T). Show that Y2(t) = Y1 (t - T). (iii) Now let x1 (t) be an arbitrary signal such that x 1( t) = 0 fort < t0 . Letting Y! (t) be the system output for input x1 (t) and y2(t) be the system output for x2(t) = x 1( t - T), show that Y2(t) = Y! (t - T). Chap. 2 Problems 147 We may therefore conclude that the system under consideration is time invariant. In conjunction with the result derived in part (a), we conclude that the given system is LTI. Since this system satisfies the condition of initial rest, it is causal as well. 2.34. The initial rest assumption corresponds to a zero-valued auxiliary condition being imposed at a time determined in accordance with the input signal. In this problem we show that if the auxiliary condition used is nonzero or if it is always applied at a fixed time (regardless of the input signal) the corresponding system cannot be LTI. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation (P2.33-1). (a) Given the auxiliary condition y(1) = 1, use a counterexample to show that the system is not linear. (b) Given the auxiliary condition y(l) = 1, use a counterexample to show that the system is not time invariant. (c) Given the auxiliary condition y(1) = 1, show that the system is incrementally linear. (d) Given the auxiliary condition y(l) = 0, show that the system is linear but not time invariant. (e) Given the auxiliary condition y(O) + y(4) = 0, show that the system is linear but not time invariant. 2.35. In the previous problem we saw that application of an auxiliary condition at a fixed time (regardless of the input signal) leads to the corresponding system being not time-invariant. In this problem, we explore the effect of fixed auxiliary conditions on the causality of a system. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation (P2.33-1 ). Assume that the auxiliary condition associated with the differential equation is y(O) = 0. Determine the output of the system for each of the following two inputs: (a) x 1( t) = 0, for all t 0 t<-1 (b) X 2 ( t) = { }: t > _ 1 Observe that if y 1( t) is the output for input x 1( t) and y2(t) is the output for input x2(t), then y1 (t) and y2(t) are not identical fort < -1, even though x 1 (t) and x2(t) are identical fort < - 1. Use this observation as the basis of an argument to conclude that the given system is not causal. 2.36. Consider a discrete-time system whose input x[n] and output y[n] are related by y(n] = (~ )y[n- I] + x[n]. (a) Show that if this system satisfies the condition of initial rest (i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 ), then it is linear and time invariant. (b) Show that if this system does not satisfy the condition of initial rest, but instead uses the auxiliary condition y[O] = 0, it is not causal. [Hint: Use an approach similar to that used in Problem 2.35.] 148 Linear Time-Invariant Systems Chap. 2 2.37. Consider a system whose input and output are related by the first-order differential equation (P2.33-l ). Assume that the system satisfies the condition of final rest [i. e., if x(t) = 0 fort > t0 , then y(t) = 0 fort > t0]. Show that this system is not causal. [Hint: Consider two inputs to the system, x 1 (t) = 0 and x2(t) = er(u(t)- u(t- 1)), which result in outputs y 1( t) and y2(t), respectively. Then show that y 1( t) =I= y2(t) fort< 0.] 2.38. Draw block diagram representations for causal LTI systems described by the fol- lowing difference equations: (a) y[n] = *y[n- 1] + ~x[n] (b) y[n] = *y[n- 1] + x[n- 1] 2.39. Draw block diagram representations for causal LTI systems described by the fol- lowing differential equations: (a) y(t) = -( ~) dy(t)ldt + 4x(t) (b) dy(t)ldt + -3y(t) = x(t) ADVANCED PROBLEMS 2.40. (a) Consider an LTI system with input and output related through the equation y(t) = f~ e (t-T) x(T- 2)dr. What is the impulse response h(t) for this system? (b) Determine the response of the system when the input x(t) is as shown in Figure P2.40. -1 2 Figure P2.40 2.41. Consider the signal x[n] = a 11 u[n]. (a) Sketch the signal g[n] = x[n] - ax[n - 1]. (b) Use the result of part (a) in conjunction with properties of convolution in order to determine a sequence h[n] such that 11 x[n] * h[n] = 1 ) (2 {u[n + 2] - u[n - 2]}. 2.42. Suppose that the signal x(t) = u(t + 0.5) - u(t - 0.5) Chap.2 Problems 149 is convolved with the signal (a) Determine a value of w 0 which ensures that y(O) = 0, where y(t) = x(t) * h(t). (b) Is your answer to the previous part unique? 2.43. One of the important properties of convolution, in both continuous and discrete time, is the associativity property. In this problem, we will check and illustrate this prop- erty. (a) Prove the equality [x(t) * h(t)] * g(t) = x(t) * [h(t) * g(t)] (P2.43-l) by showing that both sides of eq. (P2.43-l) equal I+ oo I+ oo -oo -oo x( r)h(u)g(t - T - u) dr du. (b) Consider two LTI systems with the unit sample responses hdn] and h2 [n] shown in Figure P2.43(a). These two systems are cascaded as shown in Figure P2.43(b). Let x[n] = u[n]. h1[n] = (- ~ )nu[n] 3 • 0000 ri • n h2[n] = u[n] +~ u[n-1] ~ 2 - - - - : x[n] 0 1 2 3 4 n (a) (b) Figure P2.43 150 Linear Time-Invariant Systems Chap.2 (i) Compute y[n] by first computing w[n] = x[n] * h 1 [n] and then computing y[n] = w[n] * h2[n]; that is, y[n] = [x[n] * h1 [n]] * h2[n]. (ii) Now find y[n] by first convolving h1 [n] and h2[n] to obtain g[n] h1 [n] * h2[n] and then convolving x[n] with g[n] to obtain y[n] x[n] * [hi [n] * h2[n]]. The answers to (i) and (ii) should be identical, illustrating the associativity prop- erty of discrete-time convolution. (c) Consider the cascade of two LTI systems as in Figure P2.43(b), where in this case h1 [n] = sin 8n and and where the input is x[n] = B[n] - aB[n- 1]. Determine the output y[n]. (Hint: The use of the associative and commutative properties of convolution should greatly facilitate the solution.) 2.44. (a) If x(t) = 0, Jtl > TJ, and h(t) = 0, JtJ > T2, then x(t) * h(t) = 0, ltl > T3 for some positive number T3 . Express T3 in terms of T1 and T2 . (b) A discrete-time LTI system has input x[n], impulse response h[n], and output y[n]. If h[n] is known to be zero everywhere outside the interval No :s; n :s; N1 and x[n] is known to be zero everywhere outside the interval N2 :s; n :s; N3 , then the output y[n] is constrained to be zero everywhere, except on some interval N4 :s; n :s; Ns. (i) Determine N4 and N 5 in terms of N 0 , N1, N 2 , and N 3 . (ii) If the interval N 0 :s; n :s; N1 is of length Mh, N 2 :s; n :s; N 3 is of length Mx, and N4 :s; n :s; N5 is of length My, express My in terms of M 11 and Mx. (c) Consider a discrete-time LTI system with the property that if the input x[n] = 0 for all n 2: 10, then the output y[n] = 0 for all n 2: 15. What condition must h[n], the impulse response of the system, satisfy for this to be true? (d) Consider an LTI system with impulse response in Figure P2.44. Over what in- terval must we know x(t) in order to determine y(O)? Chap.2 Problems 151 h(t) -2 -1 6 Figure P2.44 2.45. (a) Show that if the response of an LTI system to x(t) is the output y(t), then the response of the system to x'(t) = dx(t) dt is y'(t). Do this problem in three different ways: (i) Directly from the properties of linearity and time invariance and the fact that '( ) _ . x(t) - x(t - h) X t - 1Ill h . h---'>0 (ii) By differentiating the convolution integral. (iii) By examining the system in Figure P2.45. x(t) y(t) Figure P2.45 (b) Demonstrate the validity of the following relationships: (i) y'(t) = x(t) * h'(t) (ii) y(t) = cCxx(r)dr) * h'(t) = Cx[x'(r) * h(r)Jdr = x'(t) * cf~xh(r)dr) [Hint: These are easily done using block diagrams as in (iii) of part (a) and the fact that UJ (t) * U-] (t) = O(t).] (c) An LTI system has the response y(t) = sin w 0t to input x(t) = e-St u(t). Use the result of part (a) to aid in determining the impulse response of this system. (d) Let s(t) be the unit step response of a continuous-time LTI system. Use part (b) to deduce that the response y(t) to the input x(t) is (P2.45-1) Show also that (P2.45-2) 152 Linear Time-Invariant Systems Chap.2 (e) Use eq. (P2.45-1) to determine the response of an LTI system with step response s(t) = (e- 3'- 2e- 2' + l)u(t) to the input x(t) = e' u(t). (0 Let s[n] be the unit step response of a discrete-time LTI system. What are the discrete-time counterparts of eqs. (P2.45-l) and (P2.45-2)? 2.46. Consider an LTI systemS and a signal x(t) = 2e- 3t u(t - 1). If x(t) ~ y(t) and ddx(tt) ~ -3y(t) + e-2tu(t), determine the impulse response h(t) of S. 2.47. We are given a certain linear time-invariant system with impulse response h0(t). We are told that when the input is x0(t) the output is y0(t), which is sketched in Figure P2.47. We are then given the following set of inputs to linear time-invariant systems with the indicated impulse responses: Input x(t) Impulse response h(t) (a) x(t) = 2xo(t) h(t) = h0(t) (b) x(t) = x0(t)- xo(t- 2) h(t) = ho(t) (c) x(t) = xo(t - 2) h(t) = h0(t + 1) (d) x(t) = x0( -t) h(t) = ho(t) (e) x(t) = x0( -t) h(t) = h0(- t) (0 x(t) = xb(t) h(t) = hb(t) [Here xb(t) and hb(t) denote the first derivatives of x0(t) and h0(t), respectively.] 0 2 Figure P2.47 In each of these cases, determine whether or not we have enough information to determine the output y(t) when the input is x(t) and the system has impulse re- sponse h(t). If it is possible to determine y(t), provide an accurate sketch of it with numerical values clearly indicated on the graph. Chap. 2 Problems 153 2.48. Determine whether each of the following statements concerning LTI systems is true or false. Justify your answers. (a) If h(t) is the impulse response of an LTI system and h(t) is periodic and nonzero, the system is unstable. (b) The inverse of a causal LTI system is always causal. (c) If lh[n]l :::::; K for each n, where K is a given number, then the LTI system with h[n] as its impulse response is stable. (d) If a discrete-time LTI system has an impulse response h[n] of finite duration, the system is stable. (e) If an LTI system is causal, it is stable. (f) The cascade of a non causal LTI system with a causal one is necessarily non- causal. (g) A continuous-time LTI system is stable if and only if its step response s(t) is absolutely integrable-that is, if and only if (h) A discrete-time LTI system is causal if and only if its step response s[n] is zero for n < 0. 2.49. In the text, we showed that if h[n] is absolutely summable, i.e., if +oc L, lh[kJI < oo, k= -oc then the LTI system with impulse response h[n] is stable. This means that absolute summability is a sufficient condition for stability. In this problem, we shall show that it is also a necessary condition. Consider an LTI system with impulse response h[n] that is not absolutely summable; that is, +oc L, ih[kJI = oo. k= -oc (a) Suppose that the input to this system is 0, if h[ -n] = 0 x[n] = { h[-n] if h[ -n] =I= 0 · lh[-nll' Does this input signal represent a bounded input? If so, what is the smallest number B such that lx[n]l :::::; B for all n? 154 Linear Time-Invariant Systems Chap.2 (b) Calculate the output at n = 0 for this particular choice of input. Does the re- sult prove the contention that absolute summability is a necessary condition for stability? (c) In a similar fashion, show that a continuous-time LTI system is stable if and only if its impulse response is absolutely integrable. 2.50. Consider the cascade of two systems shown in Figure P2.50. The first system, A, is known to be LTI. The second system, B, is known to be the inverse of system A. Let y 1 (t) denote the response of system A to x 1 (t), and let y2(t) denote the response of system A to x2(t). x(t) --1 Sy~!m I y(t) •I Sy~em 1----1•~ x(t) Figure P2.50 (a) What is the response of system B to the input ay1( t) + by2(t), where a and bare constants? (b) What is the response of system B to the input y1( t- T)? 2.51. In the text, we saw that the overall input-output relationship of the cascade of two LTI systems does not depend on the order in which they are cascaded. This fact, known as the commutativity property, depends on both the linearity and the time in variance of both systems. In this problem, we illustrate the point. (a) Consider two discrete-time systems A and B, where system A is an LTI system with unit sample response h[n] = (l/2)""u[n]. System B, on the other hand, is linear but time varying. Specifically, if the input to system B is w[n], its output is z[n] = nw[n]. Show that the commutativity property does not hold for these two systems by computing the impulse responses of the cascade combinations in Figures P2.5l(a) and P2.51(b), respectively. x[n] ....,_--t~ System y[n] x[n] ....,_--t~ System y[n] B A (a) (b) Figure P2. 51 (b) Suppose that we replace system B in each of the interconnected systems of Figure P2.51 by the system with the following relationship between its input w[n] and output z[n]: z[n] = w[n] + 2. Repeat the calculations of part (a) in this case. Chap. 2 Problems 155 2.52. Consider a discrete-time LTI system with unit sample response h[n] = (n + l)a""u[n], where Ia I < 1. Show that the step response of this system is = 1 a n a ) n] ] s[n] [ (a _ 1)2 - (a _ 1)2 a + (a _ 1) (n + 1 a u[n . (Hint: Note that 2.53. (a) Consider the homogeneous differential equation (P2.53-1) Show that if s0 is a solution of the equation N p(s) = L aksk = 0, (P2.53-2) k=O then Aesot is a solution of eq. (P2.53-1 ), where A is an arbitrary complex con- stant. (b) The polynomial p(s) in eq. (P2.53-2) can be factored in terms of its roots SJ, ... , Sr as where the Si are the distinct solutions of eq. (P2.53-2) and the ui are their multiplicities-that is, the number of times each root appears as a solution of the equation. Note that CT I + U2 + · · . + CT r = N. In general, if ui > 1, then not only is Aesit a solution of eq. (P2.53-1), but so is Atj e·1J, as long as j is an integer greater than or equal to zero and less than or equal to ui - 1. To illustrate this, show that if Ui = 2, then Atesit is a solution of eq. (P2.53-l). [Hint: Show that if sis an arbitrary complex number, then 156 Linear Time-Invariant Systems Chap.2 Thus, the most general solution of eq. (P2.53-l) is r fT 1 -] ~ ~ A··tjes;t LL I.J ' i= I j=O where the Ai.i are arbitrary complex constants. (c) Solve the following homogeneous differential equations with the specified aux- iliary conditions: (i) d~i~·~n + 3d~~n + 2y(t) = 0, y(O) = 0, y'(O) = 2 (ii) dd~y) + 3 d;;~n + 2y(t) = o, y(O) = I, y'(O) = -1 (iii) d~i~·;n + 3 d~;~n + 2y(t) = 0, y(O) = 0, y'(O) = 0 (iv) d~i~y) + 2 d:;~n + y(t) = 0, y(O) = 1, y'(O) = 1 (v) dd~·~n + d~i~;'l - d~;~n - y(t) = 0, y(O) = 1, y'(O) = 1, y""(O) = -2 2 (vi) d y~n + 2 dyUJ + 5y(t) = 0 v(O) = 1 y'(O) = dt- dt ' . ' 2.54. (a) Consider the homogeneous difference equation N Laky[n- k] = 0, (P2.54-1) k=O Show that if zo is a solution of the equation N Lakz-k = 0, (P2.54-2) k=O then Azg is a solution of eq. (P2.54-l ), where A is an arbitrary constant. (b) As it is more convenient for the moment to work with polynomials that have only nonnegative powers of z, consider the equation obtained by multiplying both sides of eq. (P2.54-2) by zN: N p(z) = L akzN-k = 0. (P2.54-3) k=O The polynomial p(z) can be factored as p(z) = ao(z - ZJ yr 1 ••• (z - z,.)u', where the z1, ••• , z,. are the distinct roots of p(z). Show that if y[n] = nzn-l, then ±a ky[n- k] = dp(z) zn-N + (n- N)p(z)zn-N-l. k=O dz Use this fact to show that if (Ji = 2, then both Az? and Bnz?- 1 are solutions of eq. (P2.54-1), where A and Bare arbitrary complex constants. More generally, one can use this same procedure to show that if O""i > 1, then Chap.2 Problems 157 A n! zn-r r!(n- r)! is a solution of eq. (P2.54-l) for r = 0, 1, ... , ui- 1.7 (c) Solve the following homogeneous difference equations with the specified aux- iliary conditions: (i) y[n] + ~y[n- 1] + iy[n- 2] = 0; y[O] = 1, y[ -1] = -6 (ii) y[n] - 2y[n- 1] + y[n - 2] = 0; y[O] = 1, y[l] = 0 (iii) y[n] - 2y[n- 1] + y[n- 2] = 0; y[O] = 1, y[10] = 21 (iv) y[n] - f!- y[n- 1] + ~y[n- 2] = 0; y[O] = 0, y[ -1] = 1 2.55. In the text we described one method for solving linear constant-coefficient difference equations, and another method for doing this was illustrated in Problem 2.30. If the assumption of initial rest is made so that the system described by the difference equation is LTI and causal, then, in principle, we can determine the unit impulse response h[n] using either of these procedures. In Chapter 5, we describe another method that allows us to determine h[n] in a more elegant way. In this problem we describe yet another approach, which basically shows that h[n] can be determined by solving the homogeneous equation with appropriate initial conditions. (a) Consider the system initially at rest and described by the equation 1 y[n]- ly[n- 1] = x[n]. (P2.55-1) Assuming that x[n] = S[n], what is y[O]? What equation does h[n] satisfy for n 2:: 1, and with what auxiliary condition? Solve this equation to obtain a closed-form expression for h[n]. (b) Consider next the LTI system initially at rest and described by the difference equation 1 y[n]- ly[n- 1] = x[n] + 2x[n- 1]. (P 2.55-2) This system is depicted in Figure P2.55(a) as a cascade of two LTI systems that are initially at rest. Because of the properties of LTI systems, we can reverse the order of the systems in the cascade to obtain an alternative representation of the same overall system, as illustrated in Figure P2.55(b ). From this fact, use the result of part (a) to determine the impulse response for the system de- scribed by eq. (P2.55-2). (c) Consider again the system of part (a), with h[n] denoting its impulse response. Show, by verifying that eq. (P2.55-3) satisfies the difference equation (P2.55- 1), that the response y[ n] to an arbitrary input x[ n] is in fact given by the con- volution sum +oc y[n] = L h[n - m]x[m]. (P2.55-3) 7Here, weareusingfactorialnotation-thatis, k! = k(k - l)(k- 2) ... (2)(1), whereO! is defined to be 1. 158 Linear Time-Invariant Systems Chap. 2 z[n] x[n] z[n] = x[n] + 2x[n-1] 1 ....,._~~ y[n]- 2y[n-1] = z[n] y[n] (a) w[n] x[n] ...... w[n]- ~w[n-1] = x[n] y[n] = w[n] + 2w[n-1] ~ y[n] (b) Figure P2.55 (d) Consider the LTI system initially at rest and described by the difference equa- tion N ~ aky[n - k] = x[n]. (P2.55-4) k=O Assuming that a0 =I= 0, what is y[O] if x[n] = B[n]? Using this result, specify the homogeneous equation and initial conditions that the impulse response of the system must satisfy. Consider next the causal LTI system described by the difference equation N M ~ aky[n - k] = ~ bkx[n - k]. (P2.55-5) k=O k=O Express the impulse response of this system in terms of that for the LTI system described by eq. (P2.55-4). (e) There is an alternative method for determining the impulse response of the LTI system described by eq. (P2.55-5). Specifically, given the condition of initial rest, i.e., in this case, y[-N] = y[-N + 1] = ... = y[ -1] = 0, solve eq. (P2.55-5) recursively when x[n] = B[n] in order to determine y[O], ... , y[M]. What equation does h[n] satisfy for n 2:: M? What are the appropriate initial conditions for this equation? (0 Using either of the methods outlined in parts (d) and (e), find the impulse re- sponses of the causal LTI systems described by the following equations: (i) y[n] - y[n - 2] = x[n] (ii) y[n] - y[n - 2] = x[n] + 2x[n - 1] (iii) y[n] - y[n - 2] = 2x[n] - 3x[n - 4] (iv) y[n] - ( ]3!2)y[n- 1] + ~y[n- 2] = x[n] 2.56. In this problem, we consider a procedure that is the continuous-time counterpart of the technique developed in Problem 2.55. Again, we will see that the problem of determining the impulse response h(t) for t > 0 for an LTI system initially at rest and described by a linear constant-coefficient differential equation reduces to the problem of solving the homogeneous equation with appropriate initial conditions. Chap.2 Problems 159 (a) Consider the LTI system initially at rest and described by the differential equa- tion ddy(tt) + 2y(t) = x(t). (P2.56-l) Suppose that x(t) = o(t). In order to determine the value of y(t) immediately after the application of the unit impulse, consider integrating eq. (P2.56-1) from t = o- to t = o+ (i.e., from ""just before"" to ""just after"" the application of the impulse). This yields o+ o+ y(O+)- y(O-) + 2 fa_ y(T)dT = fa_ D(T)dT = 1. (P2.56-2) Since the system is initially at rest and x(t) = 0 fort < 0, y(O-) = 0. To satisfy eq. (P2.56-2) we must have y(O+) = 1. Thus, since x(t) = 0 for t > 0, the impulse response of our system is the solution of the homogeneous differential equation dy(t) + 2y(t) = 0 dt with initial condition Solve this differential equation to obtain the impulse response h(t) for the sys- tem. Check your result by showing that +CX) y(t) = J-oc h(t - T)X( T) dT satisfies eq. (P2.56-l) for any input x(t). (b) To generalize the preceding argument, consider an LTI system initially at rest and described by the differential equation N dky(t) .L,ak-k- = x(t) (P2.56-3) k=O dt with x(t) = o(t). Assume the condition of initial rest, which, since x(t) = 0 for t < 0, implies that - dy - dN-1 y - y(O ) = d t (0 ) = .. . = d tN -I (0 ) = 0. (P2.56-4) Integrate both sides of eq. (P2.56-3) once from t = o- to t = o+, and use eq. (P2.56-4) and an argument similar to that used in part (a) to show that the 160 Linear Time-Invariant Systems Chap.2 resulting equation is satisfied with + dy + y(O ) = dt (0 ) = ... (P2.56-5a) and (P2.56-5b) Consequently, the system's impulse response fort> 0 can be obtained by solv- ing the homogeneous equation ~ dky(t) - 0 Lak--- k=O dtk with initial conditions given by eqs. (P2.56-5). (c) Consider now the causal LTI system described by the differential equation ~ dky(t) _ ~ b dkx(t) Lak k -Lk k"" (P2.56-6) k=O dt k=O dt Express the impulse response of this system in terms of that for the system of part (b). (Hint: Examine Figure P2.56.) __.. ~ dkw(t) w(t) M dkw(t) x(t) L ak -k- = x(t) y(t) = l bk -k- r---+- y(t) k = 0 dt k = 0 dt Figure P2. 56 (d) Apply the procedures outlined in parts (b) and (c) to find the impulse responses for the LTI systems initially at rest and described by the following differential equations: (i) d~i~~t) + 3 d~~n + 2y(t) = x(t) (ii) d~i~~t) + 2 d;;~tl + 2y(t) = x(t) (e) Use the results of parts (b) and (c) to deduce that if M ~ N in eq. (P2.56-6), then the impulse response h(t) will contain singularity terms concentrated at t = 0. In particular, h(t) will contain a term of the form M-N L lXrUr(t), r=O where the a,. are constants and the ur(t) are the singularity functions defined in Section 2.5. (f) Find the impulse responses of the causal LTI systems described by the following differential equations: (i) d;;~n + 2y(t) = 3 d~~;n + x(t) (ii) l('-y~t) + 5 dy(t) + 6y(t) = 2 d'x(t) + 2 d x~f) + 4 dx(t) + 3x(t) dt- dt dt 3 dt- dt Chap. 2 Problems 161 2.57. Consider a causal LTI systemS whose input x[n] and output y[n] are related by the difference equation y[n] = -ay[n- 1] + box[n] + b 1x[n- 1]. (a) Verify that S may be considered a cascade connection of two causal LTI systems sl and s2 with the following input-output relationship: sl : Yl [n] = box I [n] + bl X) [n- 1], s2 : Y2[n] = -ay2[n- 1] + X2[n]. (b) Draw a block diagram representation of S1 • (c) Draw a block diagram representation of S2• (d) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (f) Show that the two unit-delay elements in the block diagram representation of S obtained in part (e) may be collapsed into one unit-delay element. The result- ing block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (d) and (e) are referred to as Direct Form I realizations of S. 2.58. Consider a causal LTI systemS whose input x[n] and output y[n] are related by the difference equation 2y[n] - y[n - 1] + y[n - 3] = x[n] - 5x[n- 4]. (a) Verify that S may be considered a cascade connection of two causal LTI systems S 1 and S2 with the following input-output relationship: S1 : 2yl [n] = X1 [n] - 5XJ [n- 4], 1 1 S2 : Y2[n] = 2Y 2[n- 1] - 2Y 2[n- 3] + x2[n]. (b) Draw a block diagram representation of S1• (c) Draw a block diagram representation of S2 . (d) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (f) Show that the four delay elements in the block diagram representation of S obtained in part (e) may be collapsed to three. The resulting block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (d) and (e) are referred to as Direct Form I realizations of S. 162 Linear Time-Invariant Systems Chap. 2 2.59. Consider a causal LTI system 5 whose input x(t) and output y(t) are related by the differential equation dv(t) dx(t) a1-d· + aov(t) = box(t) + b1 -d-. t - t (a) Show that and express the constants A, B, and C in terms of the constants a0 , a 1, b0 , and b 1 • (b) Show that 5 may be considered a cascade connection of the following two causal LTI systems: sl : )'J(I) ~ Bx\(t) + c Lyx(T)dT, s, : y,(t) ~ A r/ '( T) dT + x,(r). (c) Draw a block diagram representation of 51• (d) Draw a block diagram representation of 52. (e) Draw a block diagram representation of 5 as a cascade connection of the block diagram representation of 51 followed by the block diagram representation of 52. (f) Draw a block diagram representation of 5 as a cascade connection of the block diagram representation of 52 followed by the block diagram of representa- tion 5 1• (g) Show that the two integrators in your answer to part (f) may be collapsed into one. The resulting block diagram is referred to as a Direct Form II realization of 5, while the block diagrams obtained in parts (e) and (f) are referred to as Direct Form I realizations of 5. 2.60. Consider a causal LTI system 5 whose input x(t) and output y(f) are related by the differential equation (a) Show that y( t) ~ A Ly( T) d T + B L ([y( <T) d <T) dT + Cx(t) + DL x(T)dT + EL ([x(<T)d<T )dT, Chap. 2 Problems 163 and express the constants A, B, C, D, and E in terms of the constants a0, a 1, a2 , bo, b,, and b2. (b) Show that S may be considered a cascade connection of the following two causal LTI systems: s, : y,(t) = ex, (I)+ of% x, (r)dT + E L~ u:%XJ(<T)d<T) dT, s2 : Y2(1) = A r%Y2( T) dT + B L% (fxY2(lT) d<T) dT + x2(1). (c) Draw a block diagram representation of S1• (d) Draw a block diagram representation of S2 . (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (f) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (g) Show that the four integrators in your answer to part (f) may be collapsed into two. The resulting block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (e) and (f) are referred to as Direct Form I realizations of S. EXTENSION PROBLEMS 2.61. (a) In the circuit shown in Figure P2.6l(a), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. L=1H 1 y(t) C =1F x(t) l (a) Figure P2.61 a (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form K1 eiw 1 t + K e.iw 21 2 • Specify the values of w 1 and w 2 . (iii) Show that, since the voltage and current are restricted to be real, the natural response of the system is sinusoidal. 164 Linear Time-Invariant Systems Chap.2 (b) In the circuit shown in Figure P2.61 (b), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. R = 10 x(t) + C =1F (b) Figure P2.61 b (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the natural response of this system has the form K e-ar, and spec- ify the value of a. (c) In the circuit shown in Figure P2.61(c), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. R = 2!1 L = 1H x(t) + r (c) Figure P2.61 c (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form e- 0 '{K 2 2 1 ei ' + K2e- i '}, and specify the value of a. (iii) Show that, since the voltage and current are restricted to be real, the natural response of the system is a decaying sinusoid. 2.62. (a) In the mechanical system shown in Figure P2.62(a), the force x(t) applied to the mass represents the input, while the displacement y(t) of the mass repre- sents the output. Determine the differential equation relating x(t) and y(t). Show that the natural response of this system is periodic. (b) Consider Figure P2.62(b ), in which the force x(t) is the input and the velocity y(t) is the output. The mass of the car ism, while the coefficient of kinetic fric- tion is p. Show that the natural response of tqis system decays with increasing time. (c) In the mechanical system shown in Figure P2.62(c), the force x(t) applied to the mass represents the input, while the displacement y(t) of the mass represents the output. Chap.2 Problems 165 m = 1,000 Kg p=0.1 N-s/m x(t) (a) (b) K = Spring constant = 2 N/m m =Mass= 1 Kg b = Damping constant= 2 N-s/m ! x(t) (c) Figure P2.62 (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form e-at{K1 ejt + K2e- jt}, and specify the value of a. (iii) Show that, since the force and displacement are restricted to be real, the natural response of the system is a decaying sinusoid. 166 Linear Time-Invariant Systems Chap. 2 2.63. A $100,000 mortgage is to be retired by equal monthly payments of D dollars. In- terest, compounded monthly, is charged at the rate of 12% per annum on the unpaid balance; for example, after the first month, the total debt equals 0 $100,000 + ( ;~2 )$100,000 = $101,000. The problem is to determine D such that after a specified time the mortgage is paid in full, leaving a net balance of zero. (a) To set up the problem, let y[n] denote the unpaid balance after the nth monthly payment. Assume that the principal is borrowed in month 0 and monthly pay- ments begin in month 1. Show that y[n] satisfies the difference equation y[n] - yy[n - 1] = - D n 2: 1 (P2.63-1) with initial condition y[O] = $100,000, where y is a constant. Determine y. (b) Solve the difference equation of part (a) to determine y[n] for n 2: 0. (Hint: The particular solution of eq. (P2.63-1) is a constant Y. Find the value of Y, and express y[n] for n 2: 1 as the sum of particular and homogeneous solutions. Determine the unknown constant in the homogeneous solution by directly calculating y[l] from eq. (P2.63-1) and comparing it to your solution.) (c) If the mortgage is to be retired in 30 years after 360 monthly payments of D dollars, determine the appropriate value of D. (d) What is the total payment to the bank over the 30-year period? (e) Why do banks make loans? 2.64. One important use of inverse systems is in situations in which one wishes to remove distortions of some type. A good example of this is the problem of removing echoes from acoustic signals. For example, if an auditorium has a perceptible echo, then an initial acoustic impulse will be followed by attenuated versions of the sound at regularly spaced intervals. Consequently, an often -used model for this phenomenon is an LTI system with an impulse response consisting of a train of impulses, i.e., h(t) = ~ hko(t - kT). (P2.64-1) k=O Here the echoes occur T seconds apart, and hk represents the gain factor on the kth echo resulting from an initial acoustic impulse. (a) Suppose that x(t) represents the original acoustic signal (the music produced by an orchestra, for example) and that y(t) = x(t) * h(t) is the actual signal that is heard if no processing is done to remove the echoes. In order to remove the distortion introduced by the echoes, assume that a microphone is used to sense y(t) and that the resulting signal is transduced into an electrical signal. We will Chap.2 Problems 167 also use y(t) to denote this signal, as it represents the electrical equivalent of the acoustic signal, and we can go from one to the other via acoustic-electrical conversion systems. The important point to note is that the system with impulse response given by eq. (P2.64-1) is invertible. Therefore, we can find an LTI system with im- pulse response g(t) such that y(t) * g(t) = x(t), and thus, by processing the electrical signal y(t) in this fashion and then con- verting back to an acoustic signal, we can remove the troublesome echoes. The required impulse response g(t) is also an impulse train: g(t) = L gk8(t- kT). k=O Determine the algebraic equations that the successive gk must satisfy, and solve these equations for go, g 1, and g2 in terms of hk· (b) Suppose that h0 = 1, h 1 = 1/2, and hi = 0 for all i ~ 2. What is g(t) in this case? (c) A good model for the generation of echoes is illustrated in Figure P2.64. Hence, each successive echo represents a fed-back version of y(t), delayed by T sec- onds and scaled by a. Typically, 0 < a < 1, as successive echoes are attenu- ated. x(t) _.,.. + y(t) a Delay """"""- T Figure P2. 64 (i) What is the impulse response of this system? (Assume initial rest, i.e., y(t) = 0 fort < 0 if x(t) = 0 fort < 0.) (ii) Show that the system is stable if 0 < a < 1 and unstable if a > 1. (iii) What is g(t) in this case? Construct a realization of the inverse system using adders, coefficient multipliers, and T-second delay elements. (d) Although we have phrased the preceding discussion in terms of continuous-time systems because of the application we have been considering, the same general ideas hold in discrete time. That is, the LTI system with impulse response h[n] = L, hk8[n - kN] k=O is invertible and has as its inverse an LTI system with impulse response g[n] = L, gk8[n- kN]. k=O 168 Linear Time-Invariant Systems Chap.2 It is not difficult to check that the gk satisfy the same algebraic equations as in part (a). Consider now the discrete-time LTI system with impulse response h[n] = L 8[n- kN]. k=-% This system is not invertible. Find two inputs that produce the same output. 2.65. In Problem 1.45, we introduced and examined some of the basic properties of cor- relation functions for continuous-time signals. The discrete-time counterpart of the correlation function has essentially the same properties as those in continuous time, and both are extremely important in numerous applications (as is discussed in Prob- lems 2.66 and 2.67). In this problem, we introduce the discrete-time correlation function and examine several more of its properties. Let x[n] and y[n] be two real-valued discrete-time signals. The autocorrela- tion functions <Pxx[n] and </>yy[n] of x[n] and y[n], respectively, are defined by the expressions +ex; <PxAn] = L x[m + n]x[m] m= -oo and +rye </>yy[n] = L y[m + n]y[m], m=-oc and the cross-correlation functions are given by +rye </>xy[n] = L x[m + n]y[m] m= -oc and +oo </>yx[n] = L y[m + n]x[m]. m= -oo As in continuous time, these functions possess certain symmetry properties. Specif- ically, <PxAn] and </>yy[n] are even functions, while </>xy[n] = </>yx[- n]. (a) Compute the autocorrelation sequences for the signals Xt [n], x2[n], x3[n], and x4 [n] depicted in Figure P2.65. (b) Compute the cross-correlation sequences </>x;x [n], i # j, i, j = 1, 2, 3, 4, 1 for xi[n], i = 1, 2, 3, 4, as shown in Figure P2.65. (c) Let x[n] be the input to an LTI system with unit sample response h[n], and let the corresponding output be y[n]. Find expressions for <f>xy[n] and </>yy[n] in terms of <PxAn] and h[n]. Show how <Pxy[n] and </>yy[n] can be viewed as the output Chap.2 Problems 169 • • • JII. 0 1 2 3 • • n • • • • n x4 [n] • • • • • • • J -1 0 1 n 0 • • • .r . .. 5 n Figure P2.65 of LTI systems with 4>xAn] as the input. (Do this by explicitly specifying the impulse response of each of the two systems.) (d) Let h[n] = x1 [n] in Figure P2.65, and let y[n] be the output of the LTI system with impulse response h[n] when the input x[n] also equals x 1 [n]. Calculate 4>x_v[n] and 4>_v_v[n] using the results of part (c). 2.66. Let h1( t), h2(t), and h3(t), as sketched in Figure P2.66, be the impulse responses of three LTI systems. These three signals are known as Walsh functions and are of considerable practical importance because they can be easily generated by digital logic circuitry and because multiplication by each of them can be implemented in a simple fashion by a polarity-reversing switch. -1 Figure P2.66 (a) Determine and sketch a choice for x 1 (t), a continuous-time signal with the fol- lowing properties: (i) x 1( t) is real. (ii) x1( t) = 0 fort < 0. (iii) \xi (t)\ ::; 1 fort ~ 0. (iv) y 1( t) = x1( t) * h(t) is as large as possible at t = 4. (b) Repeat part (a) for x2(t) and x3(t) by making Y2(t) = x2(t) * h2(t) and y3(t) = x3(t) * h3(t) each as large as possible at t = 4. (c) What is the value of Yij(l) = Xi(t) * h j(l), i =I= j at time t = 4 for i, j = 1, 2, 3? 170 Linear Time-Invariant Systems Chap.2 The system with impulse response hi(t) is known as the matched filter for the signal Xi(t) because the impulse response is tuned to xi(t) in order to produce the maximum output signal. In the next problem, we relate the concept of a matched filter to that of the correlation function for continuous-time signals. 2.67. The cross-correlation function between two continuous-time real signals x(t) and y(t) is c/>xy(t) = J_~ x(t + T)y(T)dT. (P2.67-l) The autocorrelation function of a signal x(t) is obtained by setting y(t) = x(t) in eq. (P2.67-1): c/>xx(t) = r: X(t + T)X(T)dT. (a) Compute the autocorrelation function for each of the two signals x 1 (t) and x2(t) depicted in Figure P2.67(a). 0 2 0 7 -1 (a) I I 4 -1t- -1 (b) Figure P2.67 (b) Let x(t) be a given signal, and assume that x(t) is of finite duration-i.e., that x(t) = 0 for t < 0 and t > T. Find the impulse response of an LTI system so that <f>xxU - T) is the output if x(t) is the input. (c) The system determined in part (b) is a matched filter for the signal x(t). That this definition of a matched filter is identical to the one introduced in Problem 2.66 can be seen from the following: Chap. 2 Problems 171 Let x(t) be as in part (b), and let y(t) denote the response to x(t) of an LTI system with real impulse response h(t). Assume that h(t) = 0 for t < 0 and fort> T. Show that the choice for h(t) that maximizes y(T), subject to the constraint that JoT h2(t)dt = M, a fixed positive number, (P2.67-2) is a scalar multiple of the impulse response determined in part (b). [Hint: Schwartz's inequality states that r Lb 2 ] I /2 [ Lb 2 ] 1/2 u(t)v(t)dt ,; u (t)dt v (t)dt [ for any two signals u(t) and v(t). Use this to obtain a bound on y(T).] (d) The constraint given by eq. (P2.67-2) simply provides a scaling to the impulse response, as increasing M merely changes the scalar multiplier mentioned in part (c). Thus, we see that the particular choice for h(t) in parts (b) and (c) is matched to the signal x(t) to produce maximum output. This is an extremely important property in a number of applications, as we will now indicate. In communication problems, one often wishes to transmit one of a small number of possible pieces of information. For example, if a complex message is encoded into a sequence of binary digits, we can imagine a system that trans- mits the information bit by bit. Each bit can then be transmitted by sending one signal, say, x0(t) , if the bit is a 0, or a different signal x1( t) if a 1 is to be com- municated. In this case, the receiving system for these signals must be capable of recognizing whether x0(t) or x 1 (t) has been received. Intuitively, what makes sense is to have two systems in the receiver, one tuned to x0(t) and one tuned to x1 (t), where, by ""tuned,"" we mean that the system gives a large output after the signal to which it is tuned is received. The property of producing a large output when a particular signal is received is exactly what the matched filter possesses. In practice, there is always distortion and interference in the transmission and reception processes. Consequently, we want to maximize the difference be- tween the response of a matched filter to the input to which it is matched and the response of the filter to one of the other signals that can be transmitted. To illustrate this point, consider the two signals x0 (t) and XI (t) depicted in Fig- ure P2.67(b). Let L0 denote the matched filter for x0(t), and let L1 denote the matched filter for x1( t). (i) Sketch the responses of Lo to x0(t) and x1 (t). Do the same for L1• (ii) Compare the values of these responses at t = 4. How might you modify x0 (t) so that the receiver would have an even easier job of distinguishing between x0(t) and x1 (t) in that the response of L0 to XI (t) and L1 to x0(t) would both be zero at t = 4? 2.68. Another application in which matched filters and correlation functions play an im- portant role is radar systems. The underlying principle of radar is that an electro- 172 Linear Time-Invariant Systems Chap.2 magnetic pulse transmitted at a target will be reflected by the target and will subse- quently return to the sender with a delay proportional to the distance to the target. Ideally, the received signal will simply be a shifted and possibly scaled version of the original transmitted signal. Let p(t) be the original pulse that is sent out. Show that c/>pp(O) = max c/>pp(t). t That is, c/>pp(O) is the largest value taken by c/>pp(t). Use this equation to deduce that, if the waveform that comes back to the sender is x(t) = a p(t - to), where a is a positive constant, then c/>xp(to) = max c/>xp(t). t (Hint: Use Schwartz's inequality.) Thus, the way in which simple radar ranging systems work is based on using a matched filter for the transmitted waveform p(t) and noting the time at which the output of this system reaches its maximum value. 2.69. In Section 2.5, we characterized the unit doublet through the equation +oo x(t) * u1(t) = J-o o x(t- r)u 1(r)dr = x'(t) (P2.69-1) for any signal x(t). From this equation, we derived the relationship +oo J-o o g(r)u 1(r)dr = -g'(O). (P2.69-2) (a) Show that eq. (P2.69-2) is an equivalent characterization of u1( t) by showing that eq. (P2.69-2) implies eq. (P2.69-1). [Hint: Fix t, and define the signal g( r) = x(t - r).] Thus, we have seen that characterizing the unit impulse or unit doublet by how it behaves under convolution is equivalent to characterizing how it be- haves under integration when multiplied by an arbitrary signal g(t). In fact, as indicated in Section 2.5, the equivalence of these operational definitions holds for all signals and, in particular, for all singularity functions. (b) Let f(t) be a given signal. Show that f(t)ul (t) = f(O)ui (t)- f'(O)o(t) by showing that both functions have the same operational definitions. (c) What is the value of Find an expression for f(t)u2(t) analogous to that in part (b) for f(t)u 1( t). Chap.2 Problems 173 2.70. In analogy with continuous-time singularity functions, we can define a set of discrete-time signals. Specifically, let u_ 1[ n] = u[n], uo[n] = o[n], and u, [n] = o[n] - o[n- 1], and define k>O k times and uk[n] = u_, [n] * u_, [n] * · · · * u_, [n], k < 0. lkl times Note that x[n] * 8 [n] = x[n], 00 x[n] * u[n] = L x[m], m= -oo and x[n] * u1 [n] = x[n] - x[n - 1], (a) What is 00 L x[m]u1 [m]? m=oo (b) Show that x[n]u 1 [n] = x[O]u 1 [n] - [x[1] - x[O]]o[n- 1] = x[1]u 1 [n] - [x[l] - x[O]]o[n]. (c) Sketch the signals u2 [n] and u3 [n]. (d) Sketch U-2[n] and U-3 [n]. (e) Show that, in general, for k > 0, (-l)nk! uk[n] = '(k _ ) 1 [u[n] - u[n - k- 1]]. (P2.70-1) n. n. (Hint: Use induction. From part (c), it is evident that uk[n] satisfies eq. (P2.70-1) fork = 2 and 3. Then, assuming that eq. (P2.70-1) satisfies uk[n], write uk+l [n] in terms of uk[n], and show that the equation also satisfies Uk+ I [n].) 174 Linear Time-Invariant Systems Chap.2 (f) Show that, in general, fork> 0, (n + k- 1)! u_k[n] = n!(k _ 1)! u[n]. (P2.70-2) (Hint: Again, use induction. Note that U-(k+ n[n] - U-(k+ l)[n- 1] = u_k[n]. (P2.70-3) Then, assuming that eq. (P2.70-2) is valid for u_k[n], use eq. (P2.70-3) to show that eq. (P2.70-2) is valid for U-(k+l)[n] as well.) 2.71. In this chapter, we have used several properties and ideas that greatly facilitate the analysis of LTI systems. Among these are two that we wish to examine a bit more closely. As we will see, in certain very special cases one must be careful in using these properties, which otherwise hold without qualification. (a) One of the basic and most important properties of convolution (in both con tin- uous and discrete time) is associativity. That is, if x(t), h(t), and g(t) are three signals, then x(t) * [g(t) * h(t)] = [x(t) * g(t)] * h(t) = [x(t) * h(t)] * g(t). (P2.71-l) This relationship holds as long as all three expressions are well defined and finite. As that is usually the case in practice, we will in general use the asso- ciativity property without comments or assumptions. However, there are some cases in which it does not hold. For example, consider the system depicted in Figure P2.71, with h(t) = u1( t) and g(t) = u(t). Compute the response of this system to the input x(t) = 1 for all t. x(t) -8--~E]----. y(t) x(t) -G~--~~ y(t) Figure P2. 71 Do this in the three different ways suggested by eq. (P2.71-l) and by the figure: (i) By first convolving the two impulse responses and then convolving the result with x(t). (ii) By first convolving x(t) with u1( t) and then convolving the result with u(t). (iii) By first convolving x(t) with u(t) and then convolving the result with u 1 (t). Chap.2 Problems 175 (b) Repeat part (a) for x(t) = e-t and h(t) = e -t u(t), g(t) = u1 (t) + o(t). (c) Do the same for x[n] = (U· h[n] = (4 )"" u[n], 1 g[n] = o[n] - 2o[n- 1]. Thus, in general, the associativity property of convolution holds if and only if the three expressions in eq. (P2.71-1) make sense (i.e., if and only if their interpretations in terms of LTI systems are meaningful). For example, in part (a) differentiating a constant and then integrating makes sense, but the process of integrating the constant from t = -oo and then differentiating does not, and it is only in such cases that associativity breaks down. Closely related to the foregoing discussion is an issue involving inverse systems. Consider the LTI system with impulse response h(t) = u(t). As we saw in part (a), there are inputs-specifically, x(t) = nonzero constant-for which the output of this system is infinite, and thus, it is meaningless to consider the question of inverting such outputs to recover the input. However, if we limit ourselves to inputs that do yield finite outputs, that is, inputs which satisfy (P2.71-2) then the system is invertible, and the LTI system with impulse response u1 (t) is its inverse. (d) Show that the LTI system with impulse response u1 (t) is not invertible. (Hint: Find two different inputs that both yield zero output for all time.) However, show that the system is invertible if we limit ourselves to inputs that satisfy eq. (P2.71-2). [Hint: In Problem 1.44, we showed that an LTI system is invertible if no input other than x(t) = 0 yields an output that is zero for all time; are there two inputs x(t) that satisfy eq. (P2.71-2) and that yield identically zero responses when convolved with u1 (t)?] What we have illustrated in this problem is the following: (1) If x(t), h(t), and g(t) are three signals, and if x(t) * g(t), x(t) * h(t), and h(t) * g(t) are all well defined and finite, then the associativity property, eq. (P2. 71-1 ), holds. 176 Linear Time-Invariant Systems Chap.2 (2) Let h(t) be the impulse response of an LTI system, and suppose that the impulse response g(t) of a second system has the property h(t) * g(t) = o(t). (P2.71-3) Then, from (1), for all inputs x(t) for which x(t) * h(t) and x(t) * g(t) are both well defined and finite, the two cascades of systems depicted in Fig- ure P2. 71 act as the identity system, and thus, the two LTI systems can be regarded as inverses of one another. For example, if h(t) = u(t) and g(t) = u1 (t), then, as long as we restrict ourselves to inputs satisfying eq. (P2.71-2), we can regard these two systems as inverses. Therefore, we see that the associativity property of eq. (P2.71-1) and the definition ofLTI inverses as given in eq. (P2.71-3) are valid, as long as all convolutions that are involved are finite. As this is certainly the case in any realistic problem, we will in general use these properties without comment or qualification. Note that, although we have phrased most of our discussion in terms of continuous-time signals and systems, the same points can also be made in discrete time [as should be evident from part (c)]. 2.72. Let 8L1(t) denote the rectangular pulse of height -k for 0 < t ::; Ll. Verify that d 1 dt oLl(t) = K [o(t) - o(t - L1)]. 2.73. Show by induction that tk-1 u_k(t) = (k _ )! u(t) fork = 1, 2, 3 ... 1 3 FouRIER SERIES REPRESENTATION OF PERIODIC SIGNALS 3.0 INTRODUCTION The representation and analysis of LTI systems through the convolution sum as developed in Chapter 2 is based on representing signals as linear combinations of shifted impulses. In this and the following two chapters, we explore an alternative representation for signals and LTI systems. As in Chapter 2, the starting point for our discussion is the development of a representation of signals as linear combinations of a set of basic signals. For this alternative representation we use complex exponentials. The resulting representations are known as the continuous-time and discrete-time Fourier series and transform. As we will see, these can be used to construct broad and useful classes of signals. We then proceed as we did in Chapter 2. That is, because of the superposition prop- erty, the response of an LTI system to any input consisting of a linear combination of basic signals is the same linear combination of the individual responses to each of the basic sig- nals. In Chapter 2, these responses were all shifted versions of the unit impulse response, leading to the convolution sum or integral. As we will find in the current chapter, the re- sponse of an LTI system to a complex exponential also has a particularly simple form, which then provides us with another convenient representation for LTI systems and with another way in which to analyze these systems and gain insight into their properties. In this chapter, we focus on the representation of continuous-time and discrete-time periodic signals referred to as the Fourier series. In Chapters 4 and 5, we extend the anal- ysis to the Fourier transform representation of broad classes of aperiodic, finite energy signals. Together, these representations provide one of the most powerful and important sets of tools and insights for analyzing, designing, and understanding signals and LTI sys- tems, and we devote considerable attention in this and subsequent chapters to exploring the uses of Fourier methods. 177"
3 Fourier Series Representation of Periodic Signals,"3 FouRIER SERIES REPRESENTATION OF PERIODIC SIGNALS 3.0 INTRODUCTION The representation and analysis of LTI systems through the convolution sum as developed in Chapter 2 is based on representing signals as linear combinations of shifted impulses. In this and the following two chapters, we explore an alternative representation for signals and LTI systems. As in Chapter 2, the starting point for our discussion is the development of a representation of signals as linear combinations of a set of basic signals. For this alternative representation we use complex exponentials. The resulting representations are known as the continuous-time and discrete-time Fourier series and transform. As we will see, these can be used to construct broad and useful classes of signals. We then proceed as we did in Chapter 2. That is, because of the superposition prop- erty, the response of an LTI system to any input consisting of a linear combination of basic signals is the same linear combination of the individual responses to each of the basic sig- nals. In Chapter 2, these responses were all shifted versions of the unit impulse response, leading to the convolution sum or integral. As we will find in the current chapter, the re- sponse of an LTI system to a complex exponential also has a particularly simple form, which then provides us with another convenient representation for LTI systems and with another way in which to analyze these systems and gain insight into their properties. In this chapter, we focus on the representation of continuous-time and discrete-time periodic signals referred to as the Fourier series. In Chapters 4 and 5, we extend the anal- ysis to the Fourier transform representation of broad classes of aperiodic, finite energy signals. Together, these representations provide one of the most powerful and important sets of tools and insights for analyzing, designing, and understanding signals and LTI sys- tems, and we devote considerable attention in this and subsequent chapters to exploring the uses of Fourier methods. 177"
3.0 Introduction,"3 FouRIER SERIES REPRESENTATION OF PERIODIC SIGNALS 3.0 INTRODUCTION The representation and analysis of LTI systems through the convolution sum as developed in Chapter 2 is based on representing signals as linear combinations of shifted impulses. In this and the following two chapters, we explore an alternative representation for signals and LTI systems. As in Chapter 2, the starting point for our discussion is the development of a representation of signals as linear combinations of a set of basic signals. For this alternative representation we use complex exponentials. The resulting representations are known as the continuous-time and discrete-time Fourier series and transform. As we will see, these can be used to construct broad and useful classes of signals. We then proceed as we did in Chapter 2. That is, because of the superposition prop- erty, the response of an LTI system to any input consisting of a linear combination of basic signals is the same linear combination of the individual responses to each of the basic sig- nals. In Chapter 2, these responses were all shifted versions of the unit impulse response, leading to the convolution sum or integral. As we will find in the current chapter, the re- sponse of an LTI system to a complex exponential also has a particularly simple form, which then provides us with another convenient representation for LTI systems and with another way in which to analyze these systems and gain insight into their properties. In this chapter, we focus on the representation of continuous-time and discrete-time periodic signals referred to as the Fourier series. In Chapters 4 and 5, we extend the anal- ysis to the Fourier transform representation of broad classes of aperiodic, finite energy signals. Together, these representations provide one of the most powerful and important sets of tools and insights for analyzing, designing, and understanding signals and LTI sys- tems, and we devote considerable attention in this and subsequent chapters to exploring the uses of Fourier methods. 177 178 Fourier Series Representation of Periodic Signals Chap. 3 We begin in the next section with a brief historical perspective in order to provide some insight into the concepts and issues that we develop in more detail in the sections and chapters that follow. 3.1 A HISTORICAL PERSPECTIVE The development of Fourier analysis has a long history involving a great many individ- uals and the investigation of many different physical phenomena. 1 The concept of using ""trigonometric sums""-that is, sums of harmonically related sines and cosines or periodic complex exponentials-to describe periodic phenomena goes back at least as far as the Babylonians, who used ideas of this type in order to predict astronomical events.2 The modern history of the subject begins in 1748 with L. Euler, who examined the motion of a vibrating string. In Figure 3.1, we have indicated the first few of what are known as the ""normal modes"" of such a string. If we consider the vertical deflection j(t, x) of the string at time t and at a distance x along the string, then for any fixed instant of time, the normal modes are harmonically related sinusoidal functions of x. What Euler noted was that if the configuration of a vibrating string at some point in time is a linear combination of these normal modes, so is the configuration at any subsequent time. Furthermore, Euler showed that one could calculate the coefficients for the linear combination at the later time in a very straightforward manner from the coefficients at the earlier time. In doing this, Euler performed the same type of calculation as we will in the next section in deriving one of the properties of trigonometric sums that make them so useful for the analysis of LTI systems. Specifically, we will see that if the input to an LTI system is expressed as a linear combination of periodic complex exponentials or sinusoids, the output can also be expressed in this form, with coefficients that are related in a straightforward way to those oftheinput. The property described in the preceding paragraph would not be particularly useful, unless it were true that a large class of interesting functions could be represented by linear combinations of complex exponentials. In the middle of the 18th century, this point was the subject of heated debate. In 1753, D. Bernoulli argued on physical grounds that all physi- cal motions of a string could be represented by linear combinations of normal modes, but he did not pursue this mathematically, and his ideas were not widely accepted. In fact, Eu- ler himself discarded trigonometric series, and in 1759 J. L. Lagrange strongly criticized the use of trigonometric series in the examination of vibrating strings. His criticism was based on his own belief that it was impossible to represent signals with comers (i.e., with discontinuous slopes) using trigonometric series. Since such a configuration arises from 1 The historical material in this chapter was taken from the following references: I. Grattan-Guiness. Joseph Fourier, !76S-JR30 (Cambridge, MA: The MIT Press, 1972): G. F. Simmons, Differential Equations: With Applications and Historical Notes (New York: McGraw-Hill Book Company, 1972): C. Lanczos, Dis- course 011 Fourier Series (London: Oliver and Boyd, 1966): R. E. Edwards, Fourier Series: A Modem Intro- duction (New York: Springer-Verlag, 2nd ed., 1970): and A. D. Aleksandrov, A. N. Kolmogorov, and M.A. Lavrent'ev, Mathematics: Its Content, Methods. and Mean in[?, trans. S. H. Gould, Vol. II: trans. K. Hirsch, Vol. III (Cambridge, MA: The MIT Press, 1969). Of these, Grattan-Guiness' work offers the most complete account of Fourier's life and contributions. Other references are cited in several places in the chapter. ' H. Dym and H. P. McKean, Fourier Series and lntef?rals (New York: Academic Press, 1972). This text and the book of Simmons cited in footnote I also contain discussions of the vibrating-string problem and its role in the development of Fourier analysis."
3.1 A Historical Perspective,"178 Fourier Series Representation of Periodic Signals Chap. 3 We begin in the next section with a brief historical perspective in order to provide some insight into the concepts and issues that we develop in more detail in the sections and chapters that follow. 3.1 A HISTORICAL PERSPECTIVE The development of Fourier analysis has a long history involving a great many individ- uals and the investigation of many different physical phenomena. 1 The concept of using ""trigonometric sums""-that is, sums of harmonically related sines and cosines or periodic complex exponentials-to describe periodic phenomena goes back at least as far as the Babylonians, who used ideas of this type in order to predict astronomical events.2 The modern history of the subject begins in 1748 with L. Euler, who examined the motion of a vibrating string. In Figure 3.1, we have indicated the first few of what are known as the ""normal modes"" of such a string. If we consider the vertical deflection j(t, x) of the string at time t and at a distance x along the string, then for any fixed instant of time, the normal modes are harmonically related sinusoidal functions of x. What Euler noted was that if the configuration of a vibrating string at some point in time is a linear combination of these normal modes, so is the configuration at any subsequent time. Furthermore, Euler showed that one could calculate the coefficients for the linear combination at the later time in a very straightforward manner from the coefficients at the earlier time. In doing this, Euler performed the same type of calculation as we will in the next section in deriving one of the properties of trigonometric sums that make them so useful for the analysis of LTI systems. Specifically, we will see that if the input to an LTI system is expressed as a linear combination of periodic complex exponentials or sinusoids, the output can also be expressed in this form, with coefficients that are related in a straightforward way to those oftheinput. The property described in the preceding paragraph would not be particularly useful, unless it were true that a large class of interesting functions could be represented by linear combinations of complex exponentials. In the middle of the 18th century, this point was the subject of heated debate. In 1753, D. Bernoulli argued on physical grounds that all physi- cal motions of a string could be represented by linear combinations of normal modes, but he did not pursue this mathematically, and his ideas were not widely accepted. In fact, Eu- ler himself discarded trigonometric series, and in 1759 J. L. Lagrange strongly criticized the use of trigonometric series in the examination of vibrating strings. His criticism was based on his own belief that it was impossible to represent signals with comers (i.e., with discontinuous slopes) using trigonometric series. Since such a configuration arises from 1 The historical material in this chapter was taken from the following references: I. Grattan-Guiness. Joseph Fourier, !76S-JR30 (Cambridge, MA: The MIT Press, 1972): G. F. Simmons, Differential Equations: With Applications and Historical Notes (New York: McGraw-Hill Book Company, 1972): C. Lanczos, Dis- course 011 Fourier Series (London: Oliver and Boyd, 1966): R. E. Edwards, Fourier Series: A Modem Intro- duction (New York: Springer-Verlag, 2nd ed., 1970): and A. D. Aleksandrov, A. N. Kolmogorov, and M.A. Lavrent'ev, Mathematics: Its Content, Methods. and Mean in[?, trans. S. H. Gould, Vol. II: trans. K. Hirsch, Vol. III (Cambridge, MA: The MIT Press, 1969). Of these, Grattan-Guiness' work offers the most complete account of Fourier's life and contributions. Other references are cited in several places in the chapter. ' H. Dym and H. P. McKean, Fourier Series and lntef?rals (New York: Academic Press, 1972). This text and the book of Simmons cited in footnote I also contain discussions of the vibrating-string problem and its role in the development of Fourier analysis. Sec. 3.1 A Historical Perspective 179 >------------x---+- Position along the string 0 ----- ...... ....... Vertical deflection t ........ f(t,x) Figure 3. 1 Normal modes of a vi- brating string. (Solid lines indicate the configuration of each of these modes at some fixed instant of time, t.) the plucking of a string (i.e., pulling it taut and then releasing it), Lagrange argued that trigonometric series were of very limited use. It was in this somewhat hostile and skeptical environment that Jean Baptiste Joseph Fourier (Figure 3.2) presented his ideas half a century later. Fourier was born on March Figure 3.2 Jean Baptiste Joseph Fourier [picture from J. B. J. Fourier, Oeuvres de Fourier, Vol. II (Paris: Gauthier-Villars et Fils, 1980)]. 180 Fourier Series Representation of Periodic Signals Chap. 3 21, 1768, in Auxerre, France, and by the time of his entrance into the controversy con- cerning trigonometric series, he had already had a lifetime of experiences. His many contributions-in particular, those concerned with the series and transform that carry his name-are made even more impressive by the circumstances under which he worked. His revolutionary discoveries, although not completely appreciated during his own life- time, have had a major impact on the development of mathematics and have been and still are of great importance in an extremely wide range of scientific and engineering disci- plines. In addition to his studies in mathematics, Fourier led an active political life. In fact, during the years that followed the French Revolution, his activities almost led to his down- fall, as he narrowly avoided the guillotine on two separate occasions. Subsequently, Fourier became an associate of Napoleon Bonaparte, accompanied him on his expeditions to Egypt (during which time Fourier collected the information he would use later as the basis for his treatises on Egyptology), and in 1802 was appointed by Bonaparte to the position of prefect of a region of France centered in Grenoble. It was there, while serving as prefect, that Fourier developed his ideas on trigonometric series. The physical motivation for Fourier's work was the phenomenon of heat propaga- tion and diffusion. This in itself was a significant step in that most previous research in mathematical physics had dealt with rational and celestial mechanics. By 1807, Fourier had completed a work, Fourier had found series of harmonically related sinusoids to be useful in representing the temperature distribution through a body. In addition, he claimed that ""any"" periodic signal could be represented by such a series. While his treatment of this topic was significant, many of the basic ideas behind it had been discovered by oth- ers. Also, Fourier's mathematical arguments were still imprecise, and it remained for P. L. Dirichlet in 1829 to provide precise conditions under which a periodic signal could be rep- resented by a Fourier series. 3 Thus, Fourier did not actually contribute to the mathematical theory of Fourier series. However, he did have the clear insight to see the potential for this series representation, and it was to a great extent his work and his claims that spurred much of the subsequent work on Fourier series. In addition, Fourier took this type of representa- tion one very large step farther than any of his predecessors: He obtained a representation for aperiodic signals-not as weighted sums of harmonically related sinusoids-but as weighted integrals of sinusoids that are not all harmonically related. It is this extension from Fourier series to the Fourier integral or transform that is the focus of Chapters 4 and 5. Like the Fourier series, the Fourier transform remains one of the most powerful tools for the analysis of LTI systems. Four distinguished mathematicians and scientists were appointed to examine the 1807 paper of Fourier. Three of the four-S. F. Lacroix, G. Monge, and P. S. de Laplace- were in favor of publication of the paper, but the fourth, J. L. Lagrange, remained adamant in rejecting trigonometric series, as he had done 50 years earlier. Because of Lagrange's vehement objections, Fourier's paper never appeared. After several other attempts to have his work accepted and published by the Institut de France, Fourier undertook the writing of another version of his work, which appeared as the text Theorie analytique de Ia chaleur:!, 'Both S.D. Poisson and A. L. Cauchy had obtained results about the convergence of Fourier series before 1829, but Dirichlet's work represented such a significant extension of their results that he is usually credited with being the first to consider Fourier series convergence in a rigorous fashion. 4See J. B. J. Fourier, The Analytical Theory of Heat, trans. A. Freeman (New York: Dover, 1955). Sec. 3.1 A Historical Perspective 181 This book was published in 1822, 15 years after Fourier had first presented his results to the Institut. Toward the end of his life Fourier received some of the recognition he deserved, but the most significant tribute to him has been the enormous impact of his work on so many disciplines within the fields of mathematics, science, and engineering. The theory of integration, point-set topology, and eigenfunction expansions are just a few examples of topics in mathematics that have their roots in the analysis of Fourier series and inte- grals. 5 Furthermore, in addition to the original studies of vibration and heat diffusion, there are numerous other problems in science and engineering in which sinusoidal signals, and therefore Fourier series and transforms, play an important role. For example, sinusoidal signals arise naturally in describing the motion of the planets and the periodic behavior of the earth's climate. Alternating-current sources generate sinusoidal voltages and currents, and, as we will see, the tools of Fourier analysis enable us to analyze the response of an LTI system, such as a circuit, to such sinusoidal inputs. Also, as illustrated in Figure 3.3, waves in the ocean consist of the linear combination of sinusoidal waves with different spatial periods or wavelengths. Signals transmitted by radio and television stations are si- nusoidal in nature as well, and as a quick perusal of any text on Fourier analysis will show, the range of applications in which sinusoidal signals arise and in which the tools of Fourier analysis are useful extends far beyond these few examples. - Wavelength 150 ft ----Wavelenght 500ft - · - • -Wavelength 800 ft Figure 3.3 Ship encountering the superposition of three wave trains, each with a different spatial period. When these waves reinforce one another, a very large wave can result. In more severe seas, a giant wave indicated by the dotted line could result. Whether such a reinforcement occurs at any location depends upon the relative phases of the components that are superposed. [Adapted from an illustration by P. Mion in ""Nightmare Waves Are All Too Real to Deepwater Sailors,"" by P. Britton, Smithsonian 8 (February 1978), pp. 64-65]. While many of the applications in the preceding paragraph, as well as the original work of Fourier and his contemporaries on problems of mathematical physics, focus on phenomena in continuous time, the tools of Fourier analysis for discrete-time signals and systems have their own distinct historical roots and equally rich set of applications. In par- ticular, discrete-time concepts and methods are fundamental to the discipline of numerical analysis. Formulas for the processing of discrete sets of data points to produce numerical approximations for interpolation, integration, and differentiation were being investigated as early as the time of Newton in the 1600s. In addition, the problem of predicting the motion of a heavenly body, given a sequence of observations of the body, spurred the 5For more on the impact of Fourier's work on mathematics, see W. A. Coppel, ""J. B. Fourier-on the occasion of His Two Hundredth Birthday,"" American Mathematical Monthly, 76 ( 1969), 46l:l-83. 182 Fourier Series Representation of Periodic Signals Chap. 3 investigation of harmonic time series in the 18th and 19th centuries by eminent scientists and mathematicians, including Gauss, and thus provided a second setting in which much of the initial work was done on discrete-time signals and systems. In the mid-1960s an algorithm, now known as the fast Fourier transform, or FFT, was introduced. This algorithm, which was independently discovered by Cooley and Tukey in 1965, also has a considerable history and can, in fact, be found in Gauss' notebooks.6 What made its modem discovery so important was the fact that the FFT proved to be perfectly suited for efficient digital implementation, and it reduced the time required to compute transforms by orders of magnitude. With this tool, many interesting but previ- ously impractical ideas utilizing the discrete-time Fourier series and transform suddenly became practical, and the development of discrete-time signal and system analysis tech- niques moved forward at an accelerated pace. What has emerged out of this long history is a powerful and cohesive framework for the analysis of continuous-time and discrete-time signals and systems and an extraordinar- ily broad array of existing and potential applications. In this and the following chapters, we will develop the basic tools of that framework and examine some of its important im- plications. 3.2 THE RESPONSE OF LTI SYSTEMS TO COMPLEX EXPONENTIALS As we indicated in Section 3.0, it is advantageous in the study of LTI systems to represent signals as linear combinations of basic signals that possess the following two properties: 1. The set of basic signals can be used to construct a broad and useful class of signals. 2. The response of an LTI system to each signal should be simple enough in structure to provide us with a convenient representation for the response of the system to any signal constructed as a linear combination of the basic signals. Much of the importance of Fourier analysis results from the fact that both of these prop- erties are provided by the set of complex exponential signals in continuous and discrete time-i.e., signals of the form e'1 in continuous time and z"" in discrete time, where s and z are complex numbers. In subsequent sections of this and the following two chapters, we will examine the first property in some detail. In this section, we focus on the second property and, in this way, provide motivation for the use of Fourier series and transforms in the analysis of LTI systems. The importance of complex exponentials in the study of LTI systems stems from the fact that the response of an LTI system to a complex exponential input is the same complex exponential with only a change in amplitude; that is, continuous time: eH --> H(s)e' 1 , (3.1) discrete time: z""--> H(z)z"", (3.2) where the complex amplitude factor H(s) or H(z) will in general be a function of the complex variable s or z. A signal for which the system output is a (possibly complex) ""M. T. Heideman, D. H. Johnson, and C. S. Burrus, ""Gauss and the History of the Fast Fourier Trans- form,"" The IEEE ASSP Magazi11e I (1984). pp. 14-21."
3.2 The Response of LTI Systems to Complex Exponentials,"182 Fourier Series Representation of Periodic Signals Chap. 3 investigation of harmonic time series in the 18th and 19th centuries by eminent scientists and mathematicians, including Gauss, and thus provided a second setting in which much of the initial work was done on discrete-time signals and systems. In the mid-1960s an algorithm, now known as the fast Fourier transform, or FFT, was introduced. This algorithm, which was independently discovered by Cooley and Tukey in 1965, also has a considerable history and can, in fact, be found in Gauss' notebooks.6 What made its modem discovery so important was the fact that the FFT proved to be perfectly suited for efficient digital implementation, and it reduced the time required to compute transforms by orders of magnitude. With this tool, many interesting but previ- ously impractical ideas utilizing the discrete-time Fourier series and transform suddenly became practical, and the development of discrete-time signal and system analysis tech- niques moved forward at an accelerated pace. What has emerged out of this long history is a powerful and cohesive framework for the analysis of continuous-time and discrete-time signals and systems and an extraordinar- ily broad array of existing and potential applications. In this and the following chapters, we will develop the basic tools of that framework and examine some of its important im- plications. 3.2 THE RESPONSE OF LTI SYSTEMS TO COMPLEX EXPONENTIALS As we indicated in Section 3.0, it is advantageous in the study of LTI systems to represent signals as linear combinations of basic signals that possess the following two properties: 1. The set of basic signals can be used to construct a broad and useful class of signals. 2. The response of an LTI system to each signal should be simple enough in structure to provide us with a convenient representation for the response of the system to any signal constructed as a linear combination of the basic signals. Much of the importance of Fourier analysis results from the fact that both of these prop- erties are provided by the set of complex exponential signals in continuous and discrete time-i.e., signals of the form e'1 in continuous time and z"" in discrete time, where s and z are complex numbers. In subsequent sections of this and the following two chapters, we will examine the first property in some detail. In this section, we focus on the second property and, in this way, provide motivation for the use of Fourier series and transforms in the analysis of LTI systems. The importance of complex exponentials in the study of LTI systems stems from the fact that the response of an LTI system to a complex exponential input is the same complex exponential with only a change in amplitude; that is, continuous time: eH --> H(s)e' 1 , (3.1) discrete time: z""--> H(z)z"", (3.2) where the complex amplitude factor H(s) or H(z) will in general be a function of the complex variable s or z. A signal for which the system output is a (possibly complex) ""M. T. Heideman, D. H. Johnson, and C. S. Burrus, ""Gauss and the History of the Fast Fourier Trans- form,"" The IEEE ASSP Magazi11e I (1984). pp. 14-21. Sec. 3.2 The Response of LTI Systems to Complex Exponentials 183 constant times the input is referred to as an eigenfunction of the system, and the amplitude factor is referred to as the system's eigenvalue. To show that complex exponentials are indeed eigenfunctions of LTI systems, let us consider a continuous-time LTI system with impulse response h(t). For an input x(t), we can determine the output through the use of the convolution integral, so that with x(t) = est y(t) = I+w _ h( r)x(t - r) dr 00 I (3.3) +w = -x h( T)es(t-T) dT. Expressing es<t-T) as es1e-s7 , and noting that est can be moved outside the integral, we see that eq. (3.3) becomes +x y(t) = est I-oo h(r)e- 17 dr. (3.4) Assuming that the integral on the right-hand side of eq. (3.4) converges, the response to est is of the form y(t) = H(s)es1, (3.5) where H(s) is a complex constant whose value depends on sand which is related to the system impulse response by I+w H(s) = -oo h(r)e-s7 dT. (3.6) Hence, we have shown that complex exponentials are eigenfunctions of LTI systems. The constant H (s) for a specific value of s is then the eigenvalue associated with the eigen - function est. In an exactly parallel manner, we can show that complex exponential sequences are eigenfunctions of discrete-time LTI systems. That is, suppose that an LTI system with impulse response h[n] has as its input the sequence (3.7) where z is a complex number. Then the output of the system can be determined from the convolution sum as +x y[n] L h[k]x[n - k] k= -00 (3.8) +oo +oo = L h[k]zn-k = Zn L h[k]z-k. k=-X k= -00 From this expression, we see that if the input x[n] is the complex exponential given by eq. (3.7), then, assuming that the summation on the right-hand side of eq. (3.8) converges, the output is the same complex exponential multiplied by a constant that depends on the 184 Fourier Series Representation of Periodic Signals Chap. 3 value of z. That is, y[n] = H(z)zn, (3.9) where +x H(z) = L h[k]z-k. (3.10) k=-X Consequently, as in the continuous-time case, complex exponentials are eigenfunctions of discrete-time LTI systems. The constant H(z) for a specified value of z is the eigenvalue associated with the eigenfunction zn. For the analysis of LTI systems, the usefulness of decomposing more general signals in terms of eigenfunctions can be seen from an example. Let x(t) correspond to a linear combination of three complex exponentials; that is, (3.11) From the eigenfunction property, the response to each separately is a1esit ~ a1H(s1)es 11, a2es2t ~ a2H(s2)es2t, a3es3t ~ a3H(s3)es31 , and from the superposition property the response to the sum is the sum of the responses, so that (3.12) More generally, in continuous time, eq. (3.5), together with the superposition property, implies that the representation of signals as a linear combination of complex exponentials leads to a convenient expression for the response of an LTI system. Specifically, if the input to a continuous-time LTI system is represented as a linear combination of complex exponentials, that is, if x(t) = L akeskt, (3.13) k then the output will be y(t) = L akH(sk)e""'k1• (3.14) k In an exactly analogous manner, if the input to a discrete-time LTI system is represented as a linear combination of complex exponentials, that is, if (3.15) then the output will be y[n] = L akH(zk)z'k. (3.16) k Sec. 3.2 The Response of LTI Systems to Complex Exponentials 185 In other words, for both continuous time and discrete time, if the input to an LTI system is represented as a linear combination of complex exponentials, then the output can also be represented as a linear combination of the same complex exponential signals. Each coefficient in this representation of the output is obtained as the product of the corre- sponding coefficient a, of the input and the system's eigenvalue H(sk) or H(z.,) associated with the eigenfunction e''' or z.'k, respectively. It was precisely this fact that Euler discov- ered for the problem of the vibrating string, that Gauss and others used in the analysis of time series, and that motivated Fourier and others after him to consider the question of how broad a class of signals could be represented as a linear combination of complex exponentials. In the next few sections we examine this question for periodic signals, first in continuous time and then in discrete time, and in Chapters 4 and 5 we consider the extension of these representations to aperiodic signals. Although in general, the variables sand z in eqs. (3.1 )-(3.16) may be arbitrary complex numbers, Fourier analysis involves restricting our attention to particular forms for these variables. In particular, in continuous time we focus on purely imaginary values of s-i.e., s = }w-and thus, we consider only complex exponentials of the form e.i<ur. Similarly, in discrete time we restrict the range of values of z. to those of unit magnitude-i.e., z. = e.iw_so that we focus on complex exponentials of the form e.iwn. Example 3.1 As an illustration of eqs. (3.5) and (3.6), consider an LTI system for which the input x(t) and output y(t) are related by a time shift of 3, i.e., y(t) = x(t- 3). (3.17) If the input to this system is the complex exponential signal x(t) = ei2', then, from eq. (3.17), (3.18) Equation (3.18) is in the form of eq. (3.5), as we would expect, since ei2' is an eigen- function. The associated eigenvalue is H(j2) = e-16 . It is straightforward to confirm eq. (3.6) for this example. Specifically, from eq. (3.17), the impulse response of the sys- tem is h(t) = 8(t - 3 ). Substituting into eq. (3.6), we obtain H(s) = [,'l D(T- 3)e-"" dT = e- 3', so that H(j2) = e-'6 . As a second example, in this case illustrating eqs. (3.11) and (3.12), consider the input signal x(t) = cos(4t) + cos(7t). From eq. (3.17), y(t) will of course be y(t) = cos(4(t- 3)) + cos(7(t- 3)). (3.19) To see that this will also result from eq. (3.12). we first expand x(t) using Euler's relation: x(t) = !ei~' + !e-i4I + !ei7I +! e-J7I 2 2 (3.20) 2 2 • From eqs. (3.11) and (3.12), 186 Fourier Series Representation of Periodic Signals Chap. 3 or )'(t) = lei4U-3) + le-J411->1 + lei7U-3! + le-J7U-3) 2 2 2 2 = cos(4(t - 3)) + cos(7(t - 3)). For this simple example, multiplication of each periodic exponential component of x(t)-for example, ~ei4'-by the corresponding eigenvalue---e.g., H(j4) = e- i 12- effectively causes the input component to shift in time by 3. Obviously, in this case we can determine y(t) in eq. (3.19) by inspection rather than by employing eqs. (3.11) and (3.12). However, as we will see, the general property embodied in eqs. (3.11) and (3.12) not only allows us to calculate the responses of more complex LTI systems, but also provides the basis for the frequency domain representation and analysis of LTI systems. 3.3 FOURIER SERIES REPRESENTATION OF CONTINUOUS-TIME PERIODIC SIGNALS 3.3.1 Linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a signal is periodic if, for some positive value of T, x(t) = x(t + T) for all t. (3.21) The fundamental period of x(t) is the minimum positive, nonzero value of T far which eq. (3.21) is satisfied, and the value w 0 = 27r!T is referred to as the fundamental fre- quency. In Chapter 1 we also introduced two basic periodic signals, the sinusoidal signal x(t) = cos wot (3.22) and the periodic complex exponential (3.23) Both of these signals are periodic with fundamental frequency w0 and fundamental period T = 27rlwo. Associated with the signal in eq. (3.23) is the set of harmonically related complex exponentials <fJk(t) = eikwot = ejk(2rr!T)t, k = 0, ::±:: 1, ±2, .... (3.24) Each of these signals has a fundamental frequency that is a multiple of w 0 , and therefore, each is periodic with period T (although for lkl 2'.: 2, the fundamental period of <fJk(t) is a fraction of n. Thus, a linear combination of harmonically related complex exponentials of the form +x L L+x x(t) = akejkwot akeik(2rr!T)t (3.25) k=-X k=-X"
3.3 Fourier Series Representation of Continuous-Time Periodic Signals,"186 Fourier Series Representation of Periodic Signals Chap. 3 or )'(t) = lei4U-3) + le-J411->1 + lei7U-3! + le-J7U-3) 2 2 2 2 = cos(4(t - 3)) + cos(7(t - 3)). For this simple example, multiplication of each periodic exponential component of x(t)-for example, ~ei4'-by the corresponding eigenvalue---e.g., H(j4) = e- i 12- effectively causes the input component to shift in time by 3. Obviously, in this case we can determine y(t) in eq. (3.19) by inspection rather than by employing eqs. (3.11) and (3.12). However, as we will see, the general property embodied in eqs. (3.11) and (3.12) not only allows us to calculate the responses of more complex LTI systems, but also provides the basis for the frequency domain representation and analysis of LTI systems. 3.3 FOURIER SERIES REPRESENTATION OF CONTINUOUS-TIME PERIODIC SIGNALS 3.3.1 Linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a signal is periodic if, for some positive value of T, x(t) = x(t + T) for all t. (3.21) The fundamental period of x(t) is the minimum positive, nonzero value of T far which eq. (3.21) is satisfied, and the value w 0 = 27r!T is referred to as the fundamental fre- quency. In Chapter 1 we also introduced two basic periodic signals, the sinusoidal signal x(t) = cos wot (3.22) and the periodic complex exponential (3.23) Both of these signals are periodic with fundamental frequency w0 and fundamental period T = 27rlwo. Associated with the signal in eq. (3.23) is the set of harmonically related complex exponentials <fJk(t) = eikwot = ejk(2rr!T)t, k = 0, ::±:: 1, ±2, .... (3.24) Each of these signals has a fundamental frequency that is a multiple of w 0 , and therefore, each is periodic with period T (although for lkl 2'.: 2, the fundamental period of <fJk(t) is a fraction of n. Thus, a linear combination of harmonically related complex exponentials of the form +x L L+x x(t) = akejkwot akeik(2rr!T)t (3.25) k=-X k=-X Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 187 is also periodic with period T. In eq. (3.25), the term fork = 0 is a constant. The terms for k = + 1 and k = - I both have fundamental frequency equal to wo and are collectively referred to as the fundamental components or the first harmonic components. The two terms fork = +2 and k = -2 are periodic with half the period (or, equivalently, twice the frequency) of the fundamental components and are referred to as the second harmonic components. More generally, the components for k = + N and k = - N are referred to as the Nth harmonic components. The representation of a periodic signal in the form of eq. (3.25) is referred to as the Fourier series representation. Before developing the properties of this representation, let us consider an example. Example 3.2 Consider a periodic signal x(t), with fundamental frequency 211"", that is expressed in the form of eq. (3.25) as +3 x(t) = L akeik27TI, (3.26) k~-3 where a0 = 1, 4' Rewriting eq. (3.26) and collecting each of the harmonic components which have the same fundamental frequency, we obtain x(t) l + -1( e1·2 = 7T/ + e-1?- '"") + -1( e'·4 1Tt + e-1· 4 1T1) 4 2 (3.27) + 1 .6 .6 3(el 1Tt + e-1 1T'). Equivalently, using Euler's relation, we can write x(t) in the form + 1 2 x(t) = 1 2 cos 211""t + cos 411""t + } cos 611""t. (3.28) In Figure 3.4, we illustrate graphically how the signal x(t) is built up from its harmonic components. 188 Fourier Series Representation of Periodic Signals Chap. 3 x1(t) = ~ cos 2Tit x0(t) + x1 (t) /\/\1/\/\ ~ /\TVV\1~ t Figure 3.4 Construction of the signal x(t) in Example 3.2 as a linear com- bination of harmonically related sinusoidal signals. Equation (3.28) is an example of an alternative form for the Fourier series of real periodic signals. Specifically, suppose that x(t) is real and can be represented in the form of eq. (3.25). Then, since x*(t) = x(t), we obtain t-<f. x(t) = .:2: ale-ik<uot. 1.~-C£ Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 189 Replacing k by - k in the summation, we have +x x(t) = L a*_kejkwor, k=-X which, by comparison with eq. (3.25), requires that ak = a*_k, or equivalently, that (3.29) Note that this is the case in Example 3.2, where the ak's are in fact real and ak = a-k· To derive the alternative forms of the Fourier series, we first rearrange the summation in eq. (3.25) as x(t) = ao + L[akejkwor + a_ke- jkwor]. k=i Substituting a~ for a_k from eq. (3.29), we obtain x(t) = ao + L[akejkwor + ake-jkwor]. k=i Since the two terms inside the summation are complex conjugates of each other, this can be expressed as x(t) = ao + L 2CR-e{akejkwo1 }. (3.30) k=i If ak is expressed in polar form as then eq. (3.30) becomes x(t) = ao + L 2CR-e{Akej(kwot+OA)}. k=i That is, x(t) = ao + 2 L Ak cos(kwot + fh). (3.31) k=i Equation (3.31) is one commonly encountered form for the Fourier series of real periodic signals in continuous time. Another form is obtained by writing ak in rectangular form as ak = Bk + }Ck> where Bk and Ck are both real. With this expression for ak. eq. (3.30) takes the form x(t) = ao + 2 L [Bk cos kwot- ck sin kwot]. (3.32) k=i In Example 3.2 the ak's are all real, so that ak = Ak = Bk. and therefore, both represen- tations, eqs. (3.31) and (3.32), reduce to the same form, eq. (3.28). 190 Fourier Series Representation of Periodic Signals Chap. 3 Thus, for real periodic functions, the Fourier series in terms of complex exponentials, as given in eq. (3.25), is mathematically equivalent to either of the two forms in eqs. (3.31) and (3.32) that use trigonometric functions. Although the latter two are common forms for Fourier series,7 the complex exponential form of eq. (3.25) is particularly convenient for our purposes, so we will use that form almost exclusively. Equation (3.29) illustrates one of many properties associated with Fourier series. These properties are often quite useful in gaining insight and for computational purposes, and in Section 3.5 we collect together the most important of them. The derivation of several of them is considered in problems at the end of the chapter. In Section 4.3, we also will develop the majority of the properties within the broader context of the Fourier transform. 3.3.2 Determination of the Fourier Series Representation of a Continuous-time Periodic Signal Assuming that a given periodic signal can be represented with the series of eq. (3.25), we need a procedure for determining the coefficients ak. Multiplying both sides of eq. (3.25) by e-jn<v""1, we obtain x(t)e i~~<v""r = L ake 1kw""re- 1nw11 r (3.33) k~ -X Integrating both sides from 0 to T = 27T/w0, we have Here, Tis the fundamental period of x(t), and consequently, we are integrating over one period. Interchanging the order of integration and summation yields rT xcr>e-j""w""/ dr = L+x ak [ rT ej(i.-n)'""""1 drl . (3.34) Jo k~-x Jo The evaluation of the bracketed integral is straightforward. Rewriting this integral using Euler's formula, we obtain ( ei!k-nl<'-'""1d t = {T cos(k- n)w0tdt + j {T sin(k- n)wotdt. Jo Jo Jo (3.35) For k ~ n, cos( k- n )w0 t and sin( k -n )wot are periodic sinusoids with fundamental period (Tifk- nf). Therefore, in eq. (3.35), we are integrating over an interval (of length D that is an integral number of periods of these signals. Since the integral may be viewed as measuring the total area under the functions over the interval, we see that for k ~ n, both of the integrals on the right-hand side of eq. (3.35) are zero. For k = n, the integrand on the left-hand side of eq. (3.35) equals I, and thus, the integral equals T. In sum, we then have T { ej!k-n)w11 r dt = { T, k = n Jo 0, k~n· 7 ln fact, in his original work, Fourier used the sine-cosine form of the Fourier series given in eq. (3.32). Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 191 and consequently, the right-hand side of eq. (3.34) reduces to T a11 • Therefore, a = -l iT x(t)e- J.nw ot dt II T () . (3.36) which provides the equation for determining the coefficients. Furthermore, note that in evaluating eq. (3.35), the only fact that we used concerning the interval of integration was that we were integrating over an interval of length T, which is an integral number of periods of cos(k- n)w0t and sin(k- n)w0 t. Therefore, we will obtain the same result if we integrate over any interval of length T. That is, if we denote integration over any interval of length T by JT ' we have I ei<k-ll)w0 t dt = { T, k = n T 0, k=i'n' and consequently, a"" = T1 IT x(t)e-.!'.""v ot dt. (3.37) To summarize, if x(t) has a Fourier series representation [i.e., if it can be expressed as a linear combination of harmonically related complex exponentials in the form of eq. (3.25)], then the coefficients are given by eq. (3.37). This pair of equations, then, defines the Fourier series of a periodic continuous-time signal: +x +x x(t) = L akejkwot = L akejk(27TIT)t, (3.38) k~ -X k~ -X ak = -I I x(t)e-.1"" k wot dt = -1 I x(t)e- 1"" k· (_~7 T IT !t dt. (3.39) T T T T Here, we have written equivalent expressions for the Fourier series in terms of the fun- damnetal frequency w 0 and the fundamental period T. Equation (3.38) is referred to as the synthesis equation and eq. (3.39) as the analysis equation. The set of coefficients {ak} are often called the Fourier series coefficients or the spectral coefficients of x(t). 8 These complex coefficients measure the portion of the signal x(t) that is at each harmonic of the fundamental component. The coefficient a0 is the de or constant component of x(t) and is given by eq. (3.39) with k = 0. That is, ao = ~ Lx (t)dt, (3.40) which is simply the average value of x(t) over one period. Equations (3.38) and (3.39) were known to both Euler and Lagrange in the mid- dle of the 18th century. However, they discarded this line of analysis without having 'The term ""spectral coefficient"" is derived from problems such as the spectroscopic decomposition of light into spectral lines (i.e., into its elementary components at different frequencies). The intensity of any line in such a decomposition is a direct measure of the fraction of the total light energy at the frequency corresponding to the line. 192 Fourier Series Representation of Periodic Signals Chap. 3 examined the question of how large a class of periodic signals could, in fact, be represented in such a fashion. Before we turn to this question in the next section, let us illustrate the continuous-time Fourier series by means of a few examples. Example 3.3 Consider the signal x(t) = sin wot. whose fundamental frequency is w11 • One approach to determining the Fourier series coefficients for this signal is to apply eq. (3.39). For this simple case, however, it is easier to expand the sinusoidal signal as a linear combination of complex exponentials and identify the Fourier series coefficients by inspection. Specifically, we can express sinw 11 t as = I . I . sinW()f -ef(Uof- -e- jtuol. 2j 2j Comparing the right-hand sides of this equation and eq. (3.38), we obtain I a -I = - j' 2 k#+lor-1. Example 3.4 Let x(t) = I + sin wot + 2 cos wot + cos (2wot + i). which has fundamental frequency w11 • As with Example 3.3, we can again expand x(t) directly in terms of complex exponentials, so that I x(t) = I+ 2j[el""'o'- e I. '""""']+ Lel""'o' + e I. '""""']+ I 0 I' . 0 I' 2[el'-""'o'+""~l + e il-wor+7T~']. Collecting terms, we obtain x(t) =I+ (I+ L)e i'""o' +(I- L)e i•""o' + OeJ!7TI4}j2wol + Ge-ji7TI4} Thus, the Fourier series coefficients for this example are ao = I. I . OJ =(I+ L) = I- 21· I . a 1 = (I- 21j) = I+ 21· 02 ~eJI7TI4I h4 (l + 1'). 2 Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 193 = ~e- ii7T/4i = j2 (I _ 2 4 1') ' ak = 0, lkl > 2. In Figure 3.5, we show a bar graph of the magnitude and phase of ak. -3 -2 -1 0 2 3 k -2 -3 -1 0 2 3 • • k Figure 3.5 Plots of the magnitude and phase of the Fourier coefficients of the signal considered in Example 3.4. Example 3.5 The periodic square wave, sketched in Figure 3.6 and defined over one period as x(t) 1, ltl < T1 = { ltl (3.41) 0, T1 < < T/2' is a signal that we will encounter a number of times throughout this book. This signal is periodic with fundamental period T and fundamental frequency w0 = 27r!T. To determine the Fourier series coefficients for x(t), we use eq. (3.39). Because of the symmetry of x(t) about t = 0, it is convenient to choose - T/2 ::s t < T/2 as the x(t) .. . J [1 [1 ~ [1 [1 [ ... I I -2T -T l -T, T, l T 2T 2 2 Figure 3.6 Periodic square wave. 194 Fourier Series Representation of Periodic Signals Chap. 3 interval over which the integration is performed, although any interval of length T is equally valid and thus will lead to the same result. Using these limits of integration and substituting from eq. (3.41), we have first, fork = 0, ao = _!_ IT 1 dt = 2Ti . (3.42) T -Ti T As mentioned previously, a0 is interpreted to be the average value of x(t), which in this case equals the fraction of each period during which x(t) = 1. For k # 0, we obtain which we may rewrite as (3.43) Noting that the term in brackets is sin kw0 Ti, we can express the coefficients ak as 2 sin(kwoTi) k # 0, (3.44) kwoT where we have used the fact that w 0 T = 27r. Figure 3.7 is a bar graph of the Fourier series coefficients for this example. In particular, the coefficients are plotted for a fixed value of Ti and several values of T. For this specific example, the Fourier coefficients are real, and consequently, they can be depicted graphically with only a single graph. More generally, of course, the Fourier coefficients are complex, so that two graphs, corresponding to the real and imaginary parts, or magnitude and phase, of each coefficient, would be required. For T = 4 Ti, x(t) is a square wave that is unity for half the period and zero for half the period. In this case, woTi = 7r/2, and from eq. (3.44), sin( 1T k/2) ak = k# 0, (3.45) k1T while ao = 2· (3.46) From eq. (3.45), ak = 0 fork even and nonzero. Also, sin( 1T k/2) alternates between :±:: 1 for successive odd values of k. Therefore, ai G-i 1T ' a3 = a_3 = 37r ' as G-5 57r Sec. 3.4 Convergence of the Fourier Series 195 k (a) ..... ..... .1111111. ..... . .... I • I I I I 11-4 0 4 I II I I I • I k (b) • I I . ..111111111111... . 1 I 1 I I Jill ~8 0 8 I I II II I k (c) Figure 3.7 Plots of the scaled Fourier series coefficients Tak for the pe- riodic square wave with T1 fixed and for several values of T: (a) T = 4 T;; (b) T = 8 T1; (c) T = 16 T;. The coefficients are regularly spaced samples of the envelope (2 sin wT; )lw, where the spacing between samples, 2TTIT, de- creases as T increases. 3. 4 CONVERGENCE OF THE FOURIER SERIES Although Euler and Lagrange would have been happy with the results of Examples 3.3 and 3.4, they would have objected to Example 3.5, since x(t) is discontinuous while each of its harmonic components is continuous. Fourier, on the other hand, considered the same example and maintained that the Fourier series representation of the square wave is valid. In fact, Fourier maintained that any periodic signal could be represented by a Fourier series. Although this is not quite true, it is true that Fourier series can be used to represent an extremely large class of periodic signals, including the square wave and all other periodic signals with which we will be concerned in this book and which are of interest in practice. To gain an understanding of the square-wave example and, more generally, of the question of the validity of Fourier series representations, let us examine the problem of approximating a given periodic signal x(t) by a linear combination of a finite number of harmonically related complex exponentials-that is, by a finite series of the form"
"3.4 Convergence of the Fourier Series ","Sec. 3.4 Convergence of the Fourier Series 195 k (a) ..... ..... .1111111. ..... . .... I • I I I I 11-4 0 4 I II I I I • I k (b) • I I . ..111111111111... . 1 I 1 I I Jill ~8 0 8 I I II II I k (c) Figure 3.7 Plots of the scaled Fourier series coefficients Tak for the pe- riodic square wave with T1 fixed and for several values of T: (a) T = 4 T;; (b) T = 8 T1; (c) T = 16 T;. The coefficients are regularly spaced samples of the envelope (2 sin wT; )lw, where the spacing between samples, 2TTIT, de- creases as T increases. 3. 4 CONVERGENCE OF THE FOURIER SERIES Although Euler and Lagrange would have been happy with the results of Examples 3.3 and 3.4, they would have objected to Example 3.5, since x(t) is discontinuous while each of its harmonic components is continuous. Fourier, on the other hand, considered the same example and maintained that the Fourier series representation of the square wave is valid. In fact, Fourier maintained that any periodic signal could be represented by a Fourier series. Although this is not quite true, it is true that Fourier series can be used to represent an extremely large class of periodic signals, including the square wave and all other periodic signals with which we will be concerned in this book and which are of interest in practice. To gain an understanding of the square-wave example and, more generally, of the question of the validity of Fourier series representations, let us examine the problem of approximating a given periodic signal x(t) by a linear combination of a finite number of harmonically related complex exponentials-that is, by a finite series of the form 196 Fourier Series Representation of Periodic Signals Chap. 3 LN XN(t) = ake;kwot (3.47) k=-N Let eN(t) denote the approximation error; that is, +N eN(t) = x(t) - XN(t) = x(t) - L ake;kwot (3.48) k=-N In order to determine how good any particular approximation is, we need to specify a quantitative measure of the size of the approximation error. The criterion that we will use is the energy in the error over one period: (3.49) As shown in Problem 3.66, the particular choice for the coefficients in eq. (3.47) that minimize the energy in the error is (3.50) Comparing eqs. (3.50) and (3.39), we see that eq. (3.50) is identical to the expression used to determine the Fourier series coefficients. Thus, if x(t) has a Fourier series representa- tion, the best approximation using only a finite number of harmonically related complex exponentials is obtained by truncating the Fourier series to the desired number of terms. As N increases, new terms are added and EN decreases. If, in fact, x(t) has a Fourier series representation, then the limit of EN as N ~ oo is zero. Let us tum now to the question of when a periodic signal x(t) does in fact have a Fourier series representation. Of course, for any signal, we can attempt to obtain a set of Fourier coefficients through the use of eq. (3.39). However, in some cases, the integral in eq. (3.39) may diverge; that is, the value obtained for some of the ak may be infinite. Moreover, even if all of the coefficients obtained from eq. (3.39) are finite, when these coefficients are substituted into the synthesis equation (3.38), the resulting infinite series may not converge to the original signal x(t). Fortunately, there are no convergence difficulties for large classes of periodic signals. For example, every continuous periodic signal has a Fourier series representation for which the energy EN in the approximation error approaches 0 as N goes to oo. This is also true for many discontinuous signals. Since we will find it very useful to include discontinuous signals such as square waves in our discussions, it is worthwhile to investigate the issue of convergence in a bit more detail. Specifically, there are two somewhat different classes of conditions that a periodic signal can satisfy to guarantee that it can be represented by a Fourier series. In discussing these, we will not attempt to provide a complete mathematical justification; more rigorous treatments can be found in many texts on Fourier analysis.9 9See, for example, R. V. Churchill, Fourier Series and Boundary Value Problems, 3rd ed. (New York: McGraw-Hill Book Company, 1978); W. Kaplan, Operational Methods for Linear Systems (Reading, MA: Addison-Wesley Publishing Company, 1962); and the book by Dym and McKean referenced in footnote 2 of this chapter. Sec. 3.4 Convergence of the Fourier Series 197 One class of periodic signals that are representable through the Fourier series is those signals which have finite energy over a single period, i.e., signals for which (3.51) When this condition is satisfied, we are guaranteed that the coefficients ak obtained from eq. (3.39) are finite. Furthermore, let XN(t) be the approximation to x(t) obtained by using these coefficients for Ik l :::; N: +N XN(l) = L akeJkwot (3.52) k=-N Then we are guaranteed that the energy EN in the approximation error, as defined in eq. (3.49), converges to 0 as we add more and more terms, i.e., as N ~ oo. That is, if we define +x e(t) = x(t) - L akejkwot, (3.53) k=-% then t le(t)l 2 dt = 0. (3.54) As we will see in an example at the end of this section, eq. (3.54) does not imply that the signal x(t) and its Fourier series representation L+x akejkwot (3.55) k= -ex are equal at every value oft. What it does say is that there is no energy in their difference. The type of convergence guaranteed when x(t) has finite energy over a single pe- riod is quite useful. In this case eq. (3.54) states that the difference between x(t) and its Fourier series representation has zero energy. Since physical systems respond to signal en- ergy, from this perspective x(t) and its Fourier series representation are indistinguishable. Because most of the periodic signals that we consider do have finite energy over a single period, they have Fourier series representations. Moreover, an alternative set of conditions, developed by P. L. Dirichlet and also satisfied by essentially all of the signals with which we will be concerned, guarantees that x(t) equals its Fourier series representation, except at isolated values oft for which x(t) is discontinuous. At these values, the infinite series of eq. (3.55) converges to the average of the values on either side of the discontinuity. The Dirichlet conditions are as follows: Condition 1. Over any period, x(t) must be absolutely integrable; that is, t lx(t)l dt < oo. (3.56) 198 Fourier Series Representation of Periodic Signals Chap.3 As with square integrability, this guarantees that each coefficient ak will be finite, since So if fr [x(t)[ dt < oo, then A periodic signal that violates the first Dirichlet condition is 1 x(t) = t' 0 < t :S l; that is, x(t) is periodic with period 1. This signal is illustrated in Figure 3.8(a). Condition 2. In any finite interval of time, x(t) is of bounded variation; that is, there are no more than a finite number of maxima and minima during any single period of the signal. An example of a function that meets Condition 1 but not Condition 2 is x(t) = sin ( 2;} 0 < t :S 1, (3.57) as illustrated in Figure 3.8(b). For this function, which is periodic with T = 1, L' /x(t)/ dt < 1. The function has, however, an infinite number of maxima and minima in the interval. Condition 3. In any finite interval of time, there are only a finite number of discontinu- ities. Furthermore, each of these discontinuities is finite. An example of a function that violates Condition 3 is illustrated in Figure 3.8(c). The signal, of period T = 8, is composed of an infinite number of sections, each of which is half the height and half the width of the previous section. Thus, the area under one period of the function is clearly less than 8. However, there are an infinite number of discontinuities in each period, thereby violating Condition 3. As can be seen from the examples given in Figure 3.8, signals that do not satisfy the Dirichlet conditions are generally pathological in nature and consequently do not typically arise in practical contexts. For this reason, the question of the convergence of Fourier series will not play a particularly significant role in the remainder of the book. For a periodic signal that has no discontinuities, the Fourier series representation converges and equals the original signal at every value oft. For a periodic signal with a finite number of discontinuities in each period, the Fourier series representation equals the signal every- Sec. 3.4 Convergence of the Fourier Series 199 x(t) ~ I I -1 0 2 (a) x(t) 2 (b) Figure 3.8 Signals that violate the Dirichlet conditions: (a) the signal x(t) = 1/t for 0 <ts 1, a peri- x(t) odic signal with period 1 (this signal violates the first Dirichlet condition); (b) the periodic signal of eq. (3.57), which violates the second Dirichlet condition; (c) a signal periodic with period 8 that violates the third Dirichlet condition [for 0 s t < 8, the value of x( t) decreases by a factor of 2 when- ever the distance from t to 8 decreases by a factor of 2; that is, 8 16 t x(t) = 1, 0 s t < 4, x(t) = 1/2, 4 s t < 6, x(t) = 1/4, 6 s t < 7, (c) x(t) = 1/8, 7 s t < 7.5, etc.]. where except at the isolated points of discontinuity, at which the series converges to the average value of the signal on either side of the discontinuity. In this case the difference between the original signal and its Fourier series representation contains no energy, and consequently, the two signals can be thought of as being the same for all practical pur- 200 Fourier Series Representation of Periodic Signals Chap.3 poses. Specifically, since the signals differ only at isolated points, the integrals of both signals over any interval are identical. For this reason, the two signals behave identically under convolution and consequently are identical from the standpoint of the analysis of LTI systems. To gain some additional understanding of how the Fourier series converges for a periodic signal with discontinuities, let us return to the example of a square wave. In particular, in 1898, 10 an American physicist, Albert Michelson, constructed a harmonic analyzer, a device that, for any periodic signal x(t), would compute the truncated Fourier series approximation of eq. (3.52) for values of N up to 80. Michelson tested his device on many functions, with the expected result that XN(t) looked very much like x(t). However, when he tried the square wave, he obtained an important and, to him, very surprising re- sult. Michelson was concerned about the behavior he observed and thought that his device might have had a defect. He wrote about the problem to the famous mathematical physicist Josiah Gibbs, who investigated it and reported his explanation in 1899. What Michelson had observed is illustrated in Figure 3.9, where we have shown xN(t) for several values of N for x(t), a symmetric square wave (T = 4T1) . In each case, the partial sum is superimposed on the original square wave. Since the square wave satis- fies the Dirichlet conditions, the limit as N -7 oo of XN(t) at the discontinuities should be the average value of the discontinuity. We see from the figure that this is in fact the case, since for any N, XN(t) has exactly that value at the discontinuities. Furthermore, for any other value oft, say, t = t 1, we are guaranteed that lim XN(t1) = x(t1). N -""oo Therefore, the squared error in the Fourier series representation of the square wave has zero area, as in eqs. (3.53) and (3.54). For this example, the interesting effect that Michelson observed is that the behavior of the partial sum in the vicinity of the discontinuity exhibits ripples and that the peak am- plitude of these ripples does not seem to decrease with increasing N. Gibbs.showed that these are in fact the case. Specifically, for a discontinuity of unity height, thep artial sum exhibits a maximum value of 1.09 (i.e., an overshoot of 9% of the height of the discon- tinuity), no matter how large N becomes. One must be careful to interpret this c~ectly, however. As stated before, for any fixed value oft, say, t = t 1, the partial sums will con- verge to the correct value, and at the discontinuity they will converge to one-half the sum of the values of the signal on either side of the discontinuity. However, the closer t1 is cho- sen to the point of discontinuity, the larger N must be in order to reduce the error below a specified amount. Thus, as N increases, the ripples in the partial sums become compressed toward the discontinuity, but for any finite value of N, the peak amplitude of the ripples remains constant. This behavior has come to be known as the Gibbs phenomenon. The im- plication is that the truncated Fourier series approximation xN(t) of a discontinuous signal x(t) will in general exhibit high-frequency ripples and overshoot x(t) near the disconti- nuities. If such an approximation is used in practice, a large enough value of N should be chosen so as to guarantee that the total energy in these ripples is insignificant. In the limit, of course, we know that the energy in the approximation error vanishes and that the Fourier series representation of a discontinuous signal such as the square wave converges. 10The historical information used in this example is taken from the book by Lanczos referenced in foot- note I of this chapter. Sec. 3.4 Convergence of the Fourier Series 201 N=19 0 (d) - T, 0 T, (e) Figure 3. 9 Convergence of the Fourier series representation of a square wave: an illustration of the Gibbs phenomenon. Here, we have depicted the finite series approximation xN(t) = ~~ N ake1k""'o 1 for several values of N. 202 Fourier Series Representation of Periodic Signals Chap. 3 3.5 PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES As mentioned earlier, Fourier series representations possess a number of important prop- erties that are useful for developing conceptual insights into such representations, and they can also help to reduce the complexity of the evaluation of the Fourier series of many sig- nals. In Table 3.1 we have summarized these properties, several of which are considered in the problems at the end of this chapter. In Chapter 4, in which we develop the Fourier transform, we will see that most of these properties can be deduced from corresponding properties of the continuous-time Fourier transform. Consequently we limit ourselves here to the discussion of several of these properties to illustrate how they may be derived, in- terpreted, and used. Throughout the following discussion of selected properties from Table 3.1, we will find it convenient to use a shorthand notation to indicate the relationship between a peri- odic signal and its Fourier series coefficients. Specifically, suppose that x(t) is a periodic signal with period T and fundamental frequency w 0 = 27T!T. Then if the Fourier series coefficients of x(t) are denoted by ab we will use the notation 3'S x(t) <-------+ a k to signify the pairing of a periodic signal with its Fourier series coefficients. 3.5.1 Linearity Let x(t) and y(t) denote atw, o periodic signals with period T and which have Fourier series coefficients denoted by and b"" respectively. That is, 3'S x(t) <-------+ a"" 3'S y (t) <-------+ b k . Since x(t) and y(t) have the same period T, it easily follows that any linear combination of the two scig, nals will also be periodic with period T. Furthermore, the Fourier series coefficients of the linear combination of x(t) and y(t), z(t) = Ax(t) + By(t), are given by the same linear combination of the Fourier series coefficients for x(t) and y(t). That is, ;rs z(t) = Ax(t) + By(t) <-------+ c, = Aa, + Bb,. (3.58) The proof of this follows directly from the application of eq. (3.39). We also note that the linearity property is easily extended to a linear combination of an arbitrary number of signals with period T. 3.5.2 Time Shifting When a time shift is applied to a periodic signal x(t ), the period T of the signal is preserved. The Fourier series coefficients bk of the resulting signal y(t) = x(t- to) may be expressed as b k = -] I x(t - to )e - ·j kw 0 td t. (3.59) T T"
3.5 Properties of Continuous-Time Fourier Series,"202 Fourier Series Representation of Periodic Signals Chap. 3 3.5 PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES As mentioned earlier, Fourier series representations possess a number of important prop- erties that are useful for developing conceptual insights into such representations, and they can also help to reduce the complexity of the evaluation of the Fourier series of many sig- nals. In Table 3.1 we have summarized these properties, several of which are considered in the problems at the end of this chapter. In Chapter 4, in which we develop the Fourier transform, we will see that most of these properties can be deduced from corresponding properties of the continuous-time Fourier transform. Consequently we limit ourselves here to the discussion of several of these properties to illustrate how they may be derived, in- terpreted, and used. Throughout the following discussion of selected properties from Table 3.1, we will find it convenient to use a shorthand notation to indicate the relationship between a peri- odic signal and its Fourier series coefficients. Specifically, suppose that x(t) is a periodic signal with period T and fundamental frequency w 0 = 27T!T. Then if the Fourier series coefficients of x(t) are denoted by ab we will use the notation 3'S x(t) <-------+ a k to signify the pairing of a periodic signal with its Fourier series coefficients. 3.5.1 Linearity Let x(t) and y(t) denote atw, o periodic signals with period T and which have Fourier series coefficients denoted by and b"" respectively. That is, 3'S x(t) <-------+ a"" 3'S y (t) <-------+ b k . Since x(t) and y(t) have the same period T, it easily follows that any linear combination of the two scig, nals will also be periodic with period T. Furthermore, the Fourier series coefficients of the linear combination of x(t) and y(t), z(t) = Ax(t) + By(t), are given by the same linear combination of the Fourier series coefficients for x(t) and y(t). That is, ;rs z(t) = Ax(t) + By(t) <-------+ c, = Aa, + Bb,. (3.58) The proof of this follows directly from the application of eq. (3.39). We also note that the linearity property is easily extended to a linear combination of an arbitrary number of signals with period T. 3.5.2 Time Shifting When a time shift is applied to a periodic signal x(t ), the period T of the signal is preserved. The Fourier series coefficients bk of the resulting signal y(t) = x(t- to) may be expressed as b k = -] I x(t - to )e - ·j kw 0 td t. (3.59) T T Sec. 3.5 Properties of Continuous-Time Fourier Series 203 Letting T = t - to in the integral, and noting that the new variable T will also range over an interval of duration T, we obtain (3.60) where ak is the kth Fourier series coefficient of x(t). That is, if ~5 x(t) ~ ak> then One consequence of this property is that, when a periodic signal is shifted in time, the magnitudes of its Fourier series coefficients remain unaltered. That is, lbkl = lakl· 3.5.3 Time Reversal The period T of a periodic signal x(t) also remains unchanged when the signal undergoes time reversal. To determine the Fourier series coefficients of y(t) = x(- t), let us consider the effect of time reversal on the synthesis equation (3.38): x(-t) = L ake-jk2m!T. (3.61) k=-X Making the substitution k = - m, we obtain y(t) = x(- t) = L a-meJm2m!T (3.62) m=-oc We observe that the right-hand side of this equation has the form of a Fourier series syn- thesis equation for x( -t), where the Fourier series coefficients bk are (3.63) That is, if then ~5 x(-t) ~ a-k· In other words time reversal applied to a continuous-time signal results in a time reversal of the corresponding sequence of Fourier series coefficients. An interesting consequence of the time-reversal property is that if x(t) is even-that is, if x( -t) = x(t)-then its Fourier series coefficients are also even-i.e., a_k = ak. Similarly, if x(t) is odd, so that x( -t) = - x(t), then so are its Fourier series coefficients-i.e., a_k = -ak. 204 Fourier Series Representation of Periodic Signals Chap. 3 3.5.4 Time Scaling Time scaling is an operation that in general changes the period of the underlying signal. Specifically, if x(t) is periodic with period T and fundamental frequency w 0 = 27r!T, then x(at), where a is a positive real number, is periodic with period Tla and fundamen- tal frequency aw0 . Since the time-scaling operation applies directly to each of the har- monic components of x(t), we may easily conclude that the Fourier coefficients for each of those components remain the same. That is, if x(t) has the Fourier series representation in eq. (3.38), then +""- x(at) = ~ a,ejk(uwo)l k~-x is the Fourier series representation of x(at). We emphasize that, while the Fourier coef- ficients have not changed, the Fourier series representation has changed because of the change in the fundamental frequency. 3.5.5 Multiplication Suppose that x(t) and y(t) are both periodic with period T and that ~s x(t) <-------? a k> OJ'S y( t) <-------? b k. Since the product x(t)y(t) is also periodic with period T, we can expand it in a Fourier series with Fourier series coefficients h, expressed in terms of those for x(t) and y(t). The result is ~s x( t) y( t) <-------? h k (3.64) One way to derive this relationship (see Problem 3.46) is to multiply the Fourier series representations of x(t) and y(t) and to not~ that the kth harmonic component in the product will have a coefficient which is the sum of terms of the form a1bk-t. Observe that the sum on the right-hand side of eq. (3.64) may be interpreted as the discrete-time convolution of the sequence representing the Fourier coefficients of x(t) and the sequence representing the Fourier coefficients of y(t). 3.5.6 Conjugation and Conjugate Symmetry Taking the complex conjugate of a periodic signal x(t) has the effect of complex conjuga- tion and time reversal on the corresponding Fourier series coefficients. That is, if H x( t) <-------? a k, then (3.65) Sec. 3.5 Properties of Continuous-Time Fourier Series 205 This property is easily proved by applying complex conjugation to both sides of eq. (3.38) and replacing the summation variable k by its negative. Some interesting consequences of this property may be derived for x(t) real-that is, when x(t) = x*(t). In particular, in this case, we see from eq. (3.65) that the Fourier series coefficients will be conjugate symmetric, i.e., a-k = a~, (3.66) as we previously saw in eq. (3.29). This in tum implies various symmetry properties (listed in Table 3.1) for the magnitudes, phases, real parts, and imaginary parts of the Fourier series coefficients of real signals. For example, from eq. (3.66), we see that if x(t) is real, then a0 is real and lakl = la-kl· Also, if x(t) is real and even, then, from Section 3.5.3, ak = a-k· However, from eq. (3.66) we see that a~ = a_b so that ak = a~. That is, if x(t) is real and even, then so are its Fourier series coefficients. Similarly, it can be shown that if x(t) is real and odd, then its Fourier series coefficients are purely imaginary and odd. Thus, for example, a0 = 0 if x(t) is real and odd. This and the other symmetry properties of the Fourier series are examined further in Problem 3.42. 3.5.7 Parseval's Relation for Continuous-Time Periodic Signals As shown in Problem 3.46, Parseval's relation for continuous-time periodic signals is (3.67) where the ak are the Fourier series coefficients of x(t) and Tis the period of the signal. Note that the left-hand side of eq. (3.67) is the average power (i.e., energy per unit time) in one period of the periodic signal x(t). Also, ~ t 2 lakejkwot 1 dt = ~ t lakl 2dt = lakl2 , (3.68) so that lakl2 is the average power in the kth harmonic component of x(t). Thus, what Par- seval's relation states is that the total average power in a periodic signal equals the sum of the average powers in all of its harmonic components. 3.5.8 Summary of Properties of the Continuous-Time Fourier Series In Table 3.1, we summarize these and other important properties of continuous-time Fourier series. 3.5.9 Examples Fourier series properties, such as those listed in Table 3.1, may be used to circumvent some of the algebra involved in determining the Fourier coefficients of a given signal. In the next 206 Fourier Series Representation of Periodic Signals Chap. 3 TABLE 3.1 PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES Property Section Periodic Signal Fourier Series Coefficients x(t)} Periodic with period T and y(t) fundamental frequency w0 = 21TIT Linearity 3.5.1 Ax(t) + By(t) Aak + Bbk Time Shifting 3.5.2 x(t - to) ake- jkwoto = ake- jk(27TIT)to Frequency Shifting efMwot = efM(2rriT)t x(t) Conjugation 3.5.6 x""(t) Time Reversal 3.5.3 x( -t) Time Scaling 3.5.4 x(at), a > 0 (periodic with period T/a) Periodic Convolution t x(r)y(t- r)dr Multiplication 3.5.5 x(t)y(t) dx(t) 2 Differentiation 'k 'k 1T ----;It J woak = J Tak ' d (finite valued and Integration J x(t) t -oo periodic only if a = 0) CLo )ak = Ck( 2~1T) )a, 0 :.:~~~._~ CR.:{a-k} Conjugate Symmetry for 3.5.6 x(t) real 9'm{ak} = -9'm{a_d Real Signals ! \ak\ = \a-1\ <r:ak = -<r:a-k Real and Even Signals 3.5.6 x(t) real and even a1 real and even Real and Odd Signals 3.5.6 x( t) real and odd a1 purely imaginary and odd Even-Odd Decomposition { x,(t) = Sv{x(t)} [x(t) real] CR.:{a1} of Real Signals X0 (t) = 0d{x(t)} [x(t) real] j9'm{ak} Parseval's Relation for Periodic Signals three examples, we illustrate this. The last example in this section then demonstrates how properties of a signal can be used to characterize the signal in great detail. Example 3.6 Consider the signal g(t) with a fundamental period of 4, shown in Figure 3.10. We could determine the Fourier series representation of g(t) directly from the analysis equa- tion (3.39). Instead, we will use the relationship of g(t) to the symmetric periodic square wave x(t) in Example 3.5. Referring to that example, we see that, with T = 4 and T1 = 1, g(t) = x(t- 1)- 1/2. (3.69) Sec. 3.5 Properties of Continuous-Time Fourier Series 207 g(t) 1 2 -2 -1 1 2 - 1 2 Figure 3. 1 o Periodic signal for Example 3.6. The time-shift property in Table 3.1 indicates that, if the Fourier Series coefficients of x(t) are denoted by ako the Fourier coefficients of x(t - 1) may be expressed as (3.70) The Fourier coefficients of the de offset in g(t)-i.e., the term -1/2 on the right-hand side of eq. (3.69)-are given by _ { 0-,k , fork~ 0 fork = 0 . (3.71) Ck - Applying the linearity property in Table 3.1, we conclude that the coefficients for g(t) may be expressed as where each ak may now be replaced by the corresponding expression from eqs. (3.45) and (3.46), yielding (3.72) Example 3.7 Consider the triangular wave signal x(t) with period T = 4 and fundamental frequency w 0 = 7T/2 shown in Figure 3.11. The derivative ofthis signal is the signal g(t) in Exam- x(t) -2 2 Figure 3. 11 Triangular wave signal in Example 3.7. 208 Fourier Series Representation of Periodic Signals Chap. 3 ple 3.6. Denoting the Fourier coefficients of g(t) by d, and those of x(t) by ek, we see that the differentiation property in Table 3.1 indicates that (3.73) This equation can be used to express ek in terms of d"" except when k = 0. Specifically, from eq. (3.72), , _ 2dk _ 2sin(7Tk/2) -jbr/1 ek - jk1T - j(k1T)2 e ' k # 0. (3.74) Fork = 0, e0 can be determined by finding the area under one period of x(t) and dividing by the length of the period: Example 3.8 Let us examine some properties of the Fourier series representation of a periodic train of impulses, or impulse train. This signal and its representation in terms of complex expo- nentials will play an important role when we discuss the topic of sampling in Chapter 7. The impulse train with period T may be expressed as x(t) = L 8(t - kT); (3.75) A~ o. it is illustrated in Figure 3.12(a). To determine the Fourier series coefficients ak, we use eq. (3.39) and select the interval of integration to be -T/2 :<::: t :<::: T/2, avoiding the placement of impulses at the integration limits. Within this interval, x(t) is the same as 8(t), ~it followWhat = -l f T/2 . . 1 ak 8(t)e- 1k2m!T dt = -. (3.76) T -m T In other words, all the Fourier series coefficients of the impulse train are identical. These coefficients are also real valued and even (with respect to the index k). This is to be expected, since, according to Table 3.1, any real and even signal (such as our impulse train) should have real and even Fourier coefficients. The impulse train also has a straightforward relationship to square-wave signals such as g(t) in Figure 3.6, which we repeat in Figure 3.12(b). The derivative of g(t) is the signal q(t) illustrated in Figure 3.12(c). We may interpret q(t) as the difference of two shifted versions of the impulse train x(t). That is, (3.77) Using the properties of Fourier series, we can now compute the Fourier series coeffi- cients of q(t) and g(t) without any further direct evaluation of the Fourier series analysis equation. First, from the time-shifting and linearity properties, we see from eq. (3.77) that the Fourier series coefficients bk of q(t) may be expressed in terms of the Fourier series coefficients a, of x(t); that is, Sec. 3.5 Properties of Continuous-Time Fourier Series 209 x(t) -T T (a) g(t) ' -T/2 -T, T, T/2 T (b) q(t) -T/2 T/2 -1 (c) Figure 3.12 (a) Periodic train of impulses; (b) periodic square wave; (c) derivative of the periodic square wave in (b). where wo = 27TIT. Using eq. (3.76), we then have Finally, since q(t) is the derivative of g(t), we can use the differentiation property in Table 3.1 to write bk = jkwocb (3.78) where the ck are the Fourier series coefficients of g(t). Thus, bk 2} sin(kwoTI) sin(kwoTJ) Ck = -- = ---,---- k ¥: 0, (3.79) jkwo jkwoT k7T 210 Fourier Series Representation of Periodic Signals Chap. 3 where we have used the fact that w 0 T = 27T. Note that eq. (3.79) is valid for k ¥- 0, since we cannot solve for c0 from eq. (3.78) with k = 0. However, since c0 is just the average value of g(t) over one period, we can determine it by inspection from Figure 3.12(b): 2TI Co= T' (3.80) Eqs. (3.80) and (3.79) are identical to eqs. (3.42) and (3.44), respectively, for the Fourier series coefficients of the square wave derived in Example 3.5. The next example is chosen to illustrate the use of many of the properties in Table 3.1. Example 3.9 Suppose we are given the following facts about a signal x(t): 1. x(t) is a real signal. 2. x(t) is periodic with period T = 4, and it has Fourier series coefficients GJ.:. 3. a k = 0 for \k \ > 1. 4. The signal with Fourier coefficients bk = e-Frrkl2a_k is odd. 5. ~ f4 \x(t)\ 2dt = 112. Let us show that this information is sufficient to determine the signal x(t) to within a sign factor. According to Fact 3, x(t) has at most three nonzero Fourier series coefficients at:: a0 , a 1, and a_ 1• Then, since x(t) has fundamental frequency w 0 = 27T/4 = 7T/2, it follows that Since x(t) is real (Fact 1), we can use the symmetry properties in Table 3.1 to conclude that a0 is real and a 1 = a*_ 1 • Consequently, (3.81) Let us now determine the signal corresponding to the Fourier coefficients bt: given in Fact 4. Using the time-reversal property from Table 3.1, we note that G-J.: corresponds to the signal x( -t). Also, the time-shift property in the table indicates that multiplication of the kth Fourier coefficient by e- Jk7rl2 = e- Jkwo corresponds to the underly~ng signal being shifted by 1 to the right (i.e., having t replaced by ~ - 1). We conclude that the coefficients bk correspond to the signal x( -(t- 1)) = x( -t + 1), which, according to Fact 4, must be odd. Since x(t) is real, x( -t + 1) must also be real. From Table 3.1, it then follows that the Fourier coefficients of x( -t + 1) must be purely imaginary and odd. Thus, b0 = 0 and b -I = - b1. Since time-reversal and time-shift operations cannot change the average power per period, Fact 5 holds even if x(t) is replaced by x( -t + 1). That is, ~ L\x ( -t + 1)\2dt = 112. (3.82) Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 211 We can now use Parseval's relation to conclude that (3.83) Substituting b1 = - b_ 1 in this equation, we obtain jb1l = 112. Since b1 is also known to be purely imaginary, it must be either j/2 or- j/2. Now we can translate these conditions on b0 and b1 into equivalent statements on a0 and a 1• First, since b0 = 0, Fact 4 implies that a0 = 0. With k = 1, this condition implies that a 1 = e- j7TI2b_ 1 = - jb_ 1 = jb 1• Thus, if we take b1 = j/2, then a 1 -112, and therefore, from eq. (3.81), x(t) = -cos(7 Tt/2). Alternatively, if we take b1 - j/2, then a 1 = 112, and therefore, x(t) = cos( 7Ttl2). 3.6 FOURIER SERIES REPRESENTATION OF DISCRETE-TIME PERIODIC SIGNALS In this section, we consider the Fourier series representation of discrete-time periodic sig- nals. While the discussion closely parallels that of Section 3.3, there are some important differences. In particular, the Fourier series representation of a discrete-time periodic sig- nal is a .finite series, as opposed to the infinite series representation required for continuous- time periodic signals. As a consequence, there are no mathematical issues of convergence such as those discussed in Section 3.4. 3.6.1 linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a discrete-time signal x[n] is periodic with period N if x[n] = x[n + N]. (3.84) The fundamental period is the smallest positive integer N for which eq. (3.84) holds, and w 0 = 27T/ N is the fundamental frequency. For example, the complex exponential e1<27TIN)n is periodic with period N. Furthermore, the set of all discrete-time complex exponential signals that are periodic with period N is given by cpk(n] = ejkwon = ejk(27TIN)n, k = 0, ± 1, ±2, .... (3.85) All of these signals have fundamental frequencies that are multiples of27r/N and thus are harmonically related. As mentioned in Section 1.3.3, there are only N distinct signals in the set given by eq. (3.85). This is a consequence of the fact that discrete-time complex exponen- tials which differ in frequency by a multiple of 27r are identical. Specifically, cp0 [n] = </>N[n], <!>1 [n] = <I>N+l [n], and, in general, (3.86) That is, when k is changed by any integer multiple of N, the identical sequence is gener- ated. This differs from the situation in continuous time in which the signals </>k(t) defined in eq. (3 .24) are all different from one another."
3.6 Fourier Series Representation of Discrete-Time Periodic Signals,"Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 211 We can now use Parseval's relation to conclude that (3.83) Substituting b1 = - b_ 1 in this equation, we obtain jb1l = 112. Since b1 is also known to be purely imaginary, it must be either j/2 or- j/2. Now we can translate these conditions on b0 and b1 into equivalent statements on a0 and a 1• First, since b0 = 0, Fact 4 implies that a0 = 0. With k = 1, this condition implies that a 1 = e- j7TI2b_ 1 = - jb_ 1 = jb 1• Thus, if we take b1 = j/2, then a 1 -112, and therefore, from eq. (3.81), x(t) = -cos(7 Tt/2). Alternatively, if we take b1 - j/2, then a 1 = 112, and therefore, x(t) = cos( 7Ttl2). 3.6 FOURIER SERIES REPRESENTATION OF DISCRETE-TIME PERIODIC SIGNALS In this section, we consider the Fourier series representation of discrete-time periodic sig- nals. While the discussion closely parallels that of Section 3.3, there are some important differences. In particular, the Fourier series representation of a discrete-time periodic sig- nal is a .finite series, as opposed to the infinite series representation required for continuous- time periodic signals. As a consequence, there are no mathematical issues of convergence such as those discussed in Section 3.4. 3.6.1 linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a discrete-time signal x[n] is periodic with period N if x[n] = x[n + N]. (3.84) The fundamental period is the smallest positive integer N for which eq. (3.84) holds, and w 0 = 27T/ N is the fundamental frequency. For example, the complex exponential e1<27TIN)n is periodic with period N. Furthermore, the set of all discrete-time complex exponential signals that are periodic with period N is given by cpk(n] = ejkwon = ejk(27TIN)n, k = 0, ± 1, ±2, .... (3.85) All of these signals have fundamental frequencies that are multiples of27r/N and thus are harmonically related. As mentioned in Section 1.3.3, there are only N distinct signals in the set given by eq. (3.85). This is a consequence of the fact that discrete-time complex exponen- tials which differ in frequency by a multiple of 27r are identical. Specifically, cp0 [n] = </>N[n], <!>1 [n] = <I>N+l [n], and, in general, (3.86) That is, when k is changed by any integer multiple of N, the identical sequence is gener- ated. This differs from the situation in continuous time in which the signals </>k(t) defined in eq. (3 .24) are all different from one another. 212 Fourier Series Representation of Periodic Signals Chap.3 We now wish to consider the representation of more general periodic sequences in terms of linear combinations of the sequences <f>dn] in eq. (3.85). Such a linear combina- tion has the form x[n] = L ak<f>k[n] = L akejkwon = L a~.;ejk(2TTIN)n. (3.87) k k k Since the sequences <f>dn] are distinct only over a range of N successive values of k, the summation in eq. (3.87) need only include terms over this range. Thus, the summation is on k, as k varies over a range of N successive integers, beginning with any value of k. We indicate this by expressing the limits of the summation as k = (N). That is, x[n] = L ak<f>k[n] = L akejkwon = L akejk(2TriNJn. (3.88) k=(N) k=(N) k=(N) For example, k could take on the values k = 0, 1, ... , N- 1, or k = 3, 4, ... , N + 2. In either case, by virtue of eq. (3.86), exactly the same set of complex exponential sequences appears in the summation on the right-hand side of eq. (3.88). Equation (3.88) is referred to as the discrete-time Fourier series and the coefficients a~.; as the Fourier series coeffi- cients. 3.6.2 Determination of the Fourier Series Representation of a Periodic Signal Suppose now that we are given a sequence x[n] that is periodic with fundamental period N. We would like to determine whether a representation of x[n] in the form of eq. (3.88) exists and, if so, what the values of the coefficients ak are. This question can be phrased in terms of finding a solution to a set oflinear equations. Specifically, if we evaluate eq. (3.88) for N successive values of n corresponding to one period of x[n], we obtain x[O] = L a~.;, k=(N) x[l] = L akej2TrkiN, k=(N) (3.89) x[N _ l] = L akej2Trk(N-I)IN. k=(N) Thus, eq. (3.89) represents a set of N linear equations for theN unknown coefficients ak as k ranges over a set of N successive integers. It can be shown that this set of equations is linearly independent and consequently can be solved to obtain the coefficients a~.; in terms of the given values of x[n]. In Problem 3.32, we consider an example in which the Fourier series coefficients are obtained by explicitly solving the set of N equations given in eq. (3.89). However, by following steps parallel to those used in continuous time, it is possible to obtain a closed-form expression for the coefficients ak in terms of the values of the sequence x[n]. Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 213 The basis for this result is the fact, shown in Problem 3.54, that L ejk(2n/N)n = { N, k = 0, ±N, ±2N, ... (3.90) n=(N) O, otherwise Equation (3.90) states that the sum over one period of the values of a periodic complex exponential is zero, unless that complex exponential is a constant. Now consider the Fourier series representation of eq. (3.88). Multiplying both sides bye- jr(2rriN)n and summing over N terms, we obtain L x[n]e- jr(2rr/N)n = L L akej(k-r)(2rr!N)n. (3.91) n=(N) n=(N) k=(N) Interchanging the order of summation on the right-hand side, we have L x[n]e- jr(2rr/N)n = L ak L ej(k-r)(2rr/N)n. (3.92) n=(N) k=(N) n=(N) From the identity in eq. (3.90), the innermost sum on non the right-hand side of eq. (3.92) is zero, unless k - r is zero or an integer multiple of N. Therefore, if we choose values for r over the same range as that over which k varies in the outer summation, the innermost sum on the right-hand side of eq. (3.92) equals N if k = rand 0 if k =I= r. The right-hand side of eq. (3.92) then reduces toNa ,, and we have a, = ~ L x[n]e-jr(2rr!N)n. (3.93) n=(N) This provides a closed-form expression for obtaining the Fourier series coefficients, and we have the discrete-time Fourier series pair: x[n] L akejkwon = L akejk(2rr/N)n, (3.94) k=(N) k=(N) ak = N L x[n]e- jkwon = ~ L x[n]e- jk(2rr!N)n. (3.95) n=(N) n=(N) These equations play the same role for discrete-time periodic signals that eqs. (3.38) and (3.39) play for continuous-time periodic signals, with eq. (3.94) the synthesis equation and eq. (3.95) the analysis equation. As in continuous time, the discrete-time Fourier series coefficients ak are often referred to as the spectral coefficients of x[n]. These coefficients specify a decomposition of x[ n] into a sum of N harmonically related complex exponen- tials. Referring to eq. (3.88), we see that if we take kin the range from 0 toN - 1, we have (3.96) 214 Fourier Series Representation of Periodic Signals Chap.3 Similarly, if k ranges from 1 toN, we obtain (3.97) From eq. (3.86), <f>0[n] = <f>N[n], and therefore, upon comparing eqs. (3.96) and (3.97), we conclude that a0 = aN. Similarly, by letting k range over any set of N consecutive integers and using eq. (3.86), we can conclude that (3.98) That is, if we consider more than N sequential values of k, the values ak repeat periodically with period N. It is important that this fact be interpreted carefully. In particular, since there are only N distinct complex exponentials that are periodic with period N, the discrete- time Fourier series representation is a finite series with N terms. Therefore, if we fix the N consecutive values of k over which we define the Fourier series in eq. (3.94), we will obtain a set of exactly N Fourier coefficients from eq. (3.95). On the other hand, at times it will be convenient to use different sets of N values of k, and consequently, it is useful to regard eq. (3.94) as a sum over any arbitrary set of N successive values of k. For this reason, it is sometimes convenient to think of ak as a sequence defined for all values of k, but where only N successive elements in the sequence will be used in the Fourier series representation. Furthermore, since the <f>k[n] repeat periodically with period N as we vary k [eq. (3.86)], so must the ak [eq. (3.98)]. This viewpoint is illustrated in the next example. Example 3. 1 0 Consider the signal x[n] = sinwon, (3.99) which is the discrete-time counterpart of the signal x(t) = sin w0 t of Example 3.3. x[n] is periodic only if 27T/w0 is an integer or a ratio of integers. For the case when 27T/w0 is an integer N, that is, when 27T wo = N' x[n] is periodic with fundamental period N, and we obtain a result that is exactly analo- gous to the continuous-time case. Expanding the signal as a sum of two complex expo- nentials, we get x[n] = _!_ ej(21T!N)n _ _!_ e- j(21T!N)n (3.100) 2j 2j . Comparing eq. (3.100) with eq. (3.94), we see by inspection that 1 at = j' (3.101) 2 Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 215 and the remaining coefficients over the interval of summation are zero. As described previously, these coefficients repeat with period N; thus, aN+ 1 is also equal to ( 1/2j) and aN-t equals ( -112j). The Fourier series coefficients for this example with N = 5 are illustrated in Figure 3.13. The fact that they repeat periodically is indicated. However, only one period is utilized in the synthesis equation (3.94). 1 2] -6 -1 -8 -7 -5-4-3 -2 0 1 2 3 k Figure 3.13 Fourier coefficients for x[n] = sin(27T/5)n. Consider now the case when 27T/w0 is a ratio of integers-that is, when Assuming that M and N do not have any common factors, x[n] has a fundamental period of N. Again expanding x[n] as a sum of two complex exponentials, we have x[n] = _!_ejM(27TIN)n _ _!_e-jM(27T!N)n 2j 2j ' from which we can determine by inspection that aM = (l/2j), a-M = ( -112j), and the remaining coefficients over one period of length N are zero. The Fourier coefficients for this example with M = 3 and N = 5 are depicted in Figure 3.14. Again, we have indicated the periodicity of the coefficients. For example, for N = 5, a2 = a_ 3, which in our example equals ( -l/2j). Note, however, that over any period of length 5 there are only two nonzero Fourier coefficients, and therefore there are only two nonzero terms in the synthesis equation. 1 21 k Figure 3.14 Fourier coefficients for x[n] = sin 3(27T/5)n. 216 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 11 Consider the signal x[n] = 1 + sm. (2N7T ) n + 3 cos (2N7T ) n + cos (4N7T n + 27T ) . This signal is periodic with period N, and, as in Example 3.10, we can expand x[n] directly in terms of complex exponentials to obtain x[n] = 1 + 1j [ej(27TIN)n _ e- j(27TIN)n] + ~ [ej(27TIN)n + e- j(27TIN)n] 2 + ~ [ej(47Tn/N+7TI2) + e- j(47Tn/N+7T!2)]. Collecting terms, we find that x[n) ~ I + (~ + 21j )ej(2w/N)• + (~ - 2~ )e- j(2w/N)• + (iejwa )ej2(2w/N)• + (ie- jwa )e- j2(2w/Nl•, Thus the Fourier series coefficients for this example are a0 = 1, 3 1 3 1 . a 1 = 2 + 2j = 2- 21' 3 1 3 1 . a-l = 2 - 2j = 2 + 21 ' 1 . a2 = 21· 1 . a-2 = -2], with ak = 0 for other values of kin the interval of summation in the synthesis equa- tion (3.94). Again, the Fourier coefficients are periodic with period N, so, for example, aN = 1, a3N-l = ~ + !i, and a2-N = !i· In Figure 3.15(a) we have plotted the real and imaginary parts of these coefficients for N = 10, while the magnitude and phase of the coefficients are depicted in Figure 3.15(b). Note that in Example 3.11, a-k = a~ for all values of k. In fact, this equality holds whenever x[n] is real. The property is identical to one that we discussed in Section 3.3 for continuous-time periodic signals, and as in continuous time, one implication is that there are two alternative forms for the discrete-time Fourier series of real periodic sequences. These forms are analogous to the continuous-time Fourier series representations given in eqs. (3.31) and (3.32) and are examined in Problem 3.52. For our purposes, the exponential form of the Fourier series, as given in eqs. (3.94) and (3.95), is particularly convenient, and we will use it exclusively. Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 217 .... 111 ....... 111 ....... 11 1.~. ..... 111 ....... 111 ... . -2N -N 0 N 2N k (a) k 1T/2 k -1T/2 (b) Figure 3.1 s (a) Real and imaginary parts of the Fourier series coefficients in Example 3.11; (b) magnitude and phase of the same coefficients. 218 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 1 2 In this example, we consider the discrete-time periodic square wave shown in Fig- ure 3.16. We can evaluate the Fourier series for this signal using eq. (3.95). Because x[ n] = 1 for - N1 ::; n ::; N1, it is particularly convenient to choose the length-N interval of summation in eq. (3.95) so that it includes the range -N1 ::; n ::; N1• In this case, we can express eq. (3.95) as (3.102) .IIIII ..... :IIIII ...... IIIII ..· ·· -N -N1 0 N1 N n Figure 3. 16 Discrete-time periodic square wave. Letting m = n + N1, we observe that eq. (3.102) becomes 1 2Nt = _N L"".'."".'. e- jk(2rr!N)(m-N1l ak m=O (3.103) 1 2Nt Nejk(2rr!N)N1 L e- jk(2TT/N)m. m=O The summation in eq. (3.103) consists of the sum of the first 2N1 + 1 terms in a geometric series, which can be evaluated using the result of Problem 1.54. This yields 1 (1 _e - jk2rr(2N1 + 1)/N) a -ejk(2TTINlNt k = N 1 - e- jk(2rr!N) 1 e- jk(2TT/2N)[ejk2rr(N1+ 1/2)/N _ e- jk2TT(N1 + 1/2)/N] (3.104) N e- jk(2rr/2Nl[ejk(2rr12N) _ e- jk(2rr/2Nl] 1 sin[2?Tk(N1 + 112)/N] k =/= 0, ±N, ±2N, ... N sin( ?T k/N ) and 2NI + 1 k = 0, ±N, ±2N, .... (3.105) N The coefficients ak for 2N1 + 1 = 5 are sketched for N = 10, 20, and 40 in Figures 3.17(a), (b), and (c), respectively. In discussing the convergence of the continuous-time Fourier series in Section 3.4, we considered the example of a symmetric square wave and observed how the finite sum in eq. (3.52) converged to the square wave as the number of terms approached infinity.ln par- Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 219 (a) (b) 1 olllllllllllllo I 1 I 11!10 I 1 olllllllllllllo 1 I o!llo 1 lolllllllllllllol 'I ill I' 'I 1111_1 8 _ 4 O 4 8 'I Ill I' 'I ill I' 'I k (c) Figure 3.17 Fourier series coefficients for the periodic square wave of Ex- ample 3.12; plots of Nak for 2N1 + 1 = 5 and (a) N = 10 ; (b) N = 20; and (c) N = 40. ticular, we observed the Gibbs phenomenon at the discontinuity, whereby, as the number of terms increased, the ripples in the partial sum (Figure 3.9) became compressed toward the discontinuity, with the peak amplitude of the ripples remaining constant independently of the number of terms in the partial sum. Let us consider the analogous sequence of partial sums for the discrete-time square wave, where, for convenience, we will assume that the period N is odd. In Figure 3.18, we have depicted the signals M x[n] L akejk(2TTIN)n (3.1 06) k=-M for the example of Figure 3.16 with N = 9, 2N1 + 1 = 5, and for several values of M. ForM = 4, the partial sum exactly equals x[n]. We see in particular that in contrast to the continuous-time case, there are no convergence issues and there is no Gibbs phenomenon. In fact, there are no convergence issues with the discrete-time Fourier series in general. The reason for this stems from the fact that any discrete-time periodic sequence x[n] is completely specified by a finite number N of parameters, namely, the values of the se- quence over one period. The Fourier series analysis equation (3.95) simply transforms this set of N parameters into an equivalent set-the values of theN Fourier coefficients-and 220 Fourier Series Representation of Periodic Signals Chap. 3 ~[n] M=1 JJ]IJr .. rJJ!JJr .. rJJ!JJr .. rJl!JJr .. rJJ!JJ -18 -9 0 ·9 18 n (a) ~[n] M=2 JJ[JJt,,tJJ!JJr,,tJJ!JJt,,tiJlJir,,tJJ!JJ -18 -9 0 9 18 n (b) ~[n] M=3 J[J[l ....l llll. ...l llll. ... l!Ill .... l[J[l -18 -9 0 9 18 n (c) ~[n] M=4 Figure 3.18 Partial sums of eqs. lllll .... IIJII ....I IJII ....I IJII ....I IIII (3.1 06) and (3.1 07) for the periodic -18 -9 0 9 18 n square wave of Figure 3.16 with N = 9 and 2N1 + 1 = 5: (a) M = 1; (d) (b) M = 2; (c) M = 3; (d) M = 4. the synthesis equation (3.94) tells us how to recover the values of the original sequence in terms of a .finite series. Thus, if N is odd and we take M = (N - 1) /2 in eq. (3.106), the sum includes exactly N terms, and consequently, from the synthesis equations, we have x[n] = x[n]. Similarly, if N is even and we let M .t[n] = L akejkC'2Tr!Nln, (3.1 07) k=-M+I then with M = N 12, this sum consists of N terms, and again, we can conclude from eq. (3.94) that x[n] = x[n]. In contrast, a continuous-time periodic signal takes on a continuum of values over a single period, and an infinite number of Fourier coefficients are required to represent it. Sec. 3.7 Properties of Discrete-Time Fourier Series 221 Thus, in general, none of the finite partial sums in eq. (3.52) yield the exact values of x(t), and convergence issues, such as those considered in Section 3.4, arise as we consider the problem of evaluating the limit as the number of terms approaches infinity. 3.7 PROPERTIES OF DISCRETE-TIME FOURIER SERIES There are strong similarities between the properties of discrete-time and continuous-time Fourier series. This can be readily seen by comparing the discrete-time Fourier series properties summarized in Table 3.2 with their continuous-time counterparts in Table 3.1. TABLE 3.2 PROPERTIES OF DISCRETE-TIME FOURIER SERIES Property Periodic Signal Fourier Series Coefficients x[n] } Periodic with period Nand ak } Periodic with y[n] fundamental frequency w0 = 2nlN bk period N Linearity Ax[n] + By[n] Aak + Bbk Time Shifting x[n- no] ake-jk(27TIN)n0 Frequency Shifting eiM<27TIN)n x[n] Conjugation x*[n] a*_k Time Reversal x[-n] a_k x[nlm], if n is a multiple of m Time Scaling 1 (viewed as periodic) X(m)[n] = { . . . -ak 0, If n IS not a multiple of m m with period mN (periodic with period mN) Periodic Convolution L x[r]y[n- r] r=(N) Multiplication x[n]y[n] L a1bk-1 l=(N) First Difference x[n] - x[n - 1] (1 - e-Jk(27T!NJ)ak Running Sum """""""" (finite valued and periodic only) L x[k]. . If ao 0 ( (1 - e-Iik(27TINJ) )ak k=-x = ak = a*_k ffi-e{ak} = ffi-e{a-k} Conjugate Symmetry for x[n] real 9m{ad = -9m{a-k} Real Signals liaki = ia-kl <rak = - <ta-k Real and Even Signals x[n] real and even ak real and even Real and Odd Signals x[n] real and odd ak purely imaginary and odd Even-Odd Decomposition Xe[n] = 8v{x[n]} [x[n] real] ffi-e{ak} of Real Signals { X0 [n] = 0d{x[n]} [x[n] real] j9m{ad Parseval's Relation for Periodic Signals ~ L L 2 lx[n]il = lakl n=(N) k=(N)"
"3.7 Properties of Discrete-Time Fourier Series ","Sec. 3.7 Properties of Discrete-Time Fourier Series 221 Thus, in general, none of the finite partial sums in eq. (3.52) yield the exact values of x(t), and convergence issues, such as those considered in Section 3.4, arise as we consider the problem of evaluating the limit as the number of terms approaches infinity. 3.7 PROPERTIES OF DISCRETE-TIME FOURIER SERIES There are strong similarities between the properties of discrete-time and continuous-time Fourier series. This can be readily seen by comparing the discrete-time Fourier series properties summarized in Table 3.2 with their continuous-time counterparts in Table 3.1. TABLE 3.2 PROPERTIES OF DISCRETE-TIME FOURIER SERIES Property Periodic Signal Fourier Series Coefficients x[n] } Periodic with period Nand ak } Periodic with y[n] fundamental frequency w0 = 2nlN bk period N Linearity Ax[n] + By[n] Aak + Bbk Time Shifting x[n- no] ake-jk(27TIN)n0 Frequency Shifting eiM<27TIN)n x[n] Conjugation x*[n] a*_k Time Reversal x[-n] a_k x[nlm], if n is a multiple of m Time Scaling 1 (viewed as periodic) X(m)[n] = { . . . -ak 0, If n IS not a multiple of m m with period mN (periodic with period mN) Periodic Convolution L x[r]y[n- r] r=(N) Multiplication x[n]y[n] L a1bk-1 l=(N) First Difference x[n] - x[n - 1] (1 - e-Jk(27T!NJ)ak Running Sum """""""" (finite valued and periodic only) L x[k]. . If ao 0 ( (1 - e-Iik(27TINJ) )ak k=-x = ak = a*_k ffi-e{ak} = ffi-e{a-k} Conjugate Symmetry for x[n] real 9m{ad = -9m{a-k} Real Signals liaki = ia-kl <rak = - <ta-k Real and Even Signals x[n] real and even ak real and even Real and Odd Signals x[n] real and odd ak purely imaginary and odd Even-Odd Decomposition Xe[n] = 8v{x[n]} [x[n] real] ffi-e{ak} of Real Signals { X0 [n] = 0d{x[n]} [x[n] real] j9m{ad Parseval's Relation for Periodic Signals ~ L L 2 lx[n]il = lakl n=(N) k=(N) 222 Fourier Series Representation of Periodic Signals Chap. 3 The derivations of many of these properties are very similar to those of the corresponding properties for continuous-time Fourier series, and several such derivations are considered in the problems at the end of the chapter. In addition, in Chapter 5 we will see that most of the properties can be inferred from corresponding properties of the discrete-time Fourier transform. Consequently, we limit the discussion in the following subsections to only a few of these properties, including several that have important differences relative to those for continuous time. We also provide examples illustrating the usefulness of various discrete- time Fourier series properties for developing conceptual insights and helping to reduce the complexity of the evaluation of the Fourier series of many periodic sequences. As with continuous time, it is often convenient to use a shorthand notation to indicate the relationship between a periodic signal and its Fourier series coefficients. Specifically, if x[n] is a periodic signal with period N and with Fourier series coefficients denoted by ak> then we will write 3. 7. 1 Multiplication The multiplication property of the Fourier series representation is one example of a prop- erty that reflects the difference between continuous time and discrete time. From Table 3.1, the product of two continuous-time signals of period T results in a periodic signal with pe- riod T whose sequence of Fourier series coefficients is the convolution of the sequences of Fourier series coefficients of the two signals being multiplied. In discrete time, suppose that and ~s y[n] ~ bk are both periodic with period N. Then the product x[n]y[n] is also periodic with period N, and, as shown in Problem 3.57, its Fourier coefficients, db are given by ~s x[n]y[n] ~ dk = L a,bk-1· (3.108) /=(N) Equation (3 .1 08) is analogous to the definition of convolution, except that the summation variable is now restricted to an interval of N consecutive samples. As shown in Problem 3.57, the summation can be taken over any set of N consecutive values of l. We refer to this tyr~ of operation as a periodic convolution between the two periodic sequences of Fourier coefficients. The usual form of the convolution sum (where the summation variable ranges from - oo to oo) is sometimes referred to as aperiodic convolution, to distinguish it from periodic convolution. 3. 7.2 First Difference The discrete-time parallel to the differentiation property of the continuous-time Fourier series involves the use of the first-difference operation, which is defined as x[n]- x[n -1]. Sec. 3.7 Properties of Discrete-Time Fourier Series 223 If x[n] is periodic with period N, then so is y[n], since shifting x[n] or linearly combining x[n] with another periodic signal whose period is N always results in a periodic signal with period N. Also, if then the Fourier coefficients corresponding to the first difference of x[n] may be expressed as (3.109) which is easily obtained by applying the time-shifting and linearity properties in Table 3.2. A common use of this property is in situations where evaluation of the Fourier series co- efficients is easier for the first difference than for the original sequence. (See Problem 3.31.) 3.7.3 Parseval's Relation for Discrete-Time Periodic Signals As shown in Problem 3.57, Parseval's relation for discrete-time periodic signals is given by (3.110) where the ak are the Fourier series coefficients of x[n] and N is the period. As in the continuous-time case, the left -hand side of Parse val's relation is the average power in one period for the periodic signal x[n]. Similarly, iaki2 is the average power in the kth harmonic component of x[n]. Thus, once again, Parseval's relation states that the average power in a periodic signal equals the sum of the average powers in all of its harmonic components. In discrete time, of course, there are only N distinct harmonic components, and since the ak are periodic with period N, the sum on the right-hand side of eq. (3.110) can be taken over any N consecutive values of k. 3.7.4 Examples In this subsection, we present several examples illustrating how properties of the discrete- time Fourier series can be used to characterize discrete-time periodic signals and to com- pute their Fourier series representations. Specifically, Fourier series properties, such as those listed in Table 3.2, may be used to simplify the process of determining the Fourier series coefficients of a given signal. This involves first expressing the given signal in terms of other signals whose Fourier series coefficients are already known or are simpler to com- pute. Then, using Table 3 .2, we can express the Fourier series coefficients of the given signal in terms of the Fourier series coefficients of the other signals. This is illustrated in Example 3.13. Example 3.14 then illustrates the determination of a sequence from some partial information. In Example 3.15 we illustrate the use of the periodic convolution prop- erty in Table 3.2. 224 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 13 Let us consider the problem of finding the Fourier series coefficients ak of the sequence x[n] shown in Figure 3.19(a). This sequence has a fundamental period of 5. We observe that x[ n] may be viewed as the sum of the square wave x 1 [ n] in Figure 3 .19(b) and the de sequence x2[n] in Figure 3.19(c). Denoting the Fourier series coefficients of x1 [n] by bk and those of x 2 [n] by ck. we use the linearity property of Table 3.2 to conclude that ak=bk+ck. (3.111) 2 x[n] ... I I Ill I i lll I I lll -5 0 5 n (a) I I I . x~[n] • • I I I • • I I I 0 • • n (b) I I I I I I I I I I I I I I I I 0 n (c) Figure 3.19 (a) Periodic sequence x[n] for Example 3.13 and its represen- tation as a sum of (b) the square wave x1 [n] and (c) the de sequence x2[n]. From Example 3.12 (with N1 = 1 and N = 5), the Fourier series coefficients bk corre- sponding to x1 [n] can be expressed as ! sin(37T k/5) _ 5 sin(1rk/5) , fork# 0, ±5, ± 10, ... b k - 3 (3.112) \ 5, fork= 0, ±5, ±10, ... The sequence x2 [n] has only a de value, which is captured by its zeroth Fourier series coefficient: 1 4 co = 5 L x2[n] = 1. (3.113) n=O Since the discrete-time Fourier series coefficients are periodic, it follows that c k = 1 whenver k is an integer multiple of 5. The remaining coefficients of x2 [n] must be zero, because x2 [n] contains only a de component. We can now substitute the expressions for bk and ck into eq. (3.111) to obtain bk = ! sin(37T k/5) _ 5 sin( 1rk/5) , fork# 0, ±5, ± 10, ... ak - (3.114) 8 5, fork= 0, ±5, ±10, ... \ Sec. 3.7 Properties of Discrete-Time Fourier Series 225 Example 3. 1 4 Suppose we are given the following facts about a sequence x[n]: 1. x[n] is periodic with period N = 6. 2. L~=o x[n] = 2. 3. L:= 2( -1) 11 x[n] = 1. 4. x[n] has the minimum power per period among the set of signals satisfying the preceding three conditions. Let us determine the sequence x[n]. We denote the Fourier series coefficients of x[n] by ak. From Fact 2, we conclude that a0 = 1/3. Noting that (-1) 11 = e-}1rn = e-J(21TI6 )3n, we see from Fact 3 that a3 116. From Parseval's relation (see Table 3.2), the average power in x[n] is 5 P = Liakl2 • (3.115) k=O Since each nonzero coefficient contributes a positive amount toP, and since the values of a0 and a3 are prespecified, the value of P is minimized by choosing a 1 = a2 = a4 = a5 = 0. It then follows that x[n] = ao + a3eJm' = (1/3) + (1/6)( -1)"", (3.116) which is sketched in Figure 3.20. x[n] ~ l i l l l I -2 -1 0 2 3 n Figure 3.20 Sequence x[n] that is consistent with the properties specified in Example 3.14. Example 3.15 In this example we determine and sketch a periodic sequence, given an algebraic expres- sion for its Fourier series coefficients. In the process, we will also exploit the periodic convolution property (see Table 3.2) of the discrete-time Fourier series. Specifically, as stated in the table and as shown in Problem 3.58, if x[n] and y[n] are periodic with period N, then the signal w[n] = L x[r]y[n- r] r=(N) is also periodic with period N. Here, the summation may be taken over any set of N consecutive values of r. Furthermore, the Fourier series coefficients of w[n] are equal to Nakbb where ak and bk are the Fourier coefficients of x[n] and y[n], respectively. 226 Fourier Series Representation of Periodic Signals Chap.3 Suppose now that we are told that a signal w[n] is periodic with a fundamental period of N = 7 and with Fourier series coefficients sin2 (3Trkl7) (3.117) ck = 7sin2(Trkl7). We observe that ck = 7d~, where dk denotes the sequence of Fourier series coefficients of a square wave x[n], as in Example 3.12, with N1 = 1 and N = 7. Using the periodic convolution property, we see that 3 w[n] = L x[r]x[n- r] = L x[r]x[n- r], (3.118) r=m r= -3 where, in the last equality, we have chosen to sum over the interval -3 :::; r :::; 3. Except for the fact that the sum is limited to a finite interval, the product-and-sum method for evaluating convolution is applicable here. In fact, we can convert eq. (3.118) to an ordi- nary convolution by defining a signal x[nl that equals x[n] for -3 :::; n :::; 3 and is zero otherwise. Then, from eq. (3.118), 3 +x w[n] = L x[r]x[n- r] = L x[r]x[n- r]. r= -3 r= -'X That is, w[n] is the aperiodic convolution of the sequences x[n] and x[n]. The sequences x[r], x[r], and x[n- r] are sketched in Figure 3.21 (a)-(c ). From the figure we can immediately calculate w[n]. In particular we see that w[O] = 3; w[ -1] = w[1] = 2; w[-2] = w[2] = 1; and w[-3] = w[3] = 0. Since w[n] is periodic with period 7, we can then sketch w[n] as shown in Figure 3.21(d). 3.8 FOURIER SERIES AND LTI SYSTEMS In the preceding few sections, we have seen that the Fourier series representation can be used to construct any periodic signal in discrete time and essentially all periodic continuous-time signals of practical importance. In addition, in Section 3.2 we saw that the response of an LTI system to a linear combination of complex exponentials takes a particularly simple form. Specifically, in continuous time, if x(t) = e-~1 is the input to a continuous-time LTI system, then the output is given by y(t) = H(s)e·11 , where, from eq. (3.6), (3.119) in which h(t) is the impulse response of the LTI system. Similarly, if x[n] = zn is the input to a discrete-time LTI system, then the output is given by y[n] = H(z)zn, where, from eq. (3.10), +x H(z) = L h[k]z-k, (3.120) k= -Cfj in which h[n] is the impulse response of the LTI system."
3.8 Fourier Series and LTI Systems,"226 Fourier Series Representation of Periodic Signals Chap.3 Suppose now that we are told that a signal w[n] is periodic with a fundamental period of N = 7 and with Fourier series coefficients sin2 (3Trkl7) (3.117) ck = 7sin2(Trkl7). We observe that ck = 7d~, where dk denotes the sequence of Fourier series coefficients of a square wave x[n], as in Example 3.12, with N1 = 1 and N = 7. Using the periodic convolution property, we see that 3 w[n] = L x[r]x[n- r] = L x[r]x[n- r], (3.118) r=m r= -3 where, in the last equality, we have chosen to sum over the interval -3 :::; r :::; 3. Except for the fact that the sum is limited to a finite interval, the product-and-sum method for evaluating convolution is applicable here. In fact, we can convert eq. (3.118) to an ordi- nary convolution by defining a signal x[nl that equals x[n] for -3 :::; n :::; 3 and is zero otherwise. Then, from eq. (3.118), 3 +x w[n] = L x[r]x[n- r] = L x[r]x[n- r]. r= -3 r= -'X That is, w[n] is the aperiodic convolution of the sequences x[n] and x[n]. The sequences x[r], x[r], and x[n- r] are sketched in Figure 3.21 (a)-(c ). From the figure we can immediately calculate w[n]. In particular we see that w[O] = 3; w[ -1] = w[1] = 2; w[-2] = w[2] = 1; and w[-3] = w[3] = 0. Since w[n] is periodic with period 7, we can then sketch w[n] as shown in Figure 3.21(d). 3.8 FOURIER SERIES AND LTI SYSTEMS In the preceding few sections, we have seen that the Fourier series representation can be used to construct any periodic signal in discrete time and essentially all periodic continuous-time signals of practical importance. In addition, in Section 3.2 we saw that the response of an LTI system to a linear combination of complex exponentials takes a particularly simple form. Specifically, in continuous time, if x(t) = e-~1 is the input to a continuous-time LTI system, then the output is given by y(t) = H(s)e·11 , where, from eq. (3.6), (3.119) in which h(t) is the impulse response of the LTI system. Similarly, if x[n] = zn is the input to a discrete-time LTI system, then the output is given by y[n] = H(z)zn, where, from eq. (3.10), +x H(z) = L h[k]z-k, (3.120) k= -Cfj in which h[n] is the impulse response of the LTI system. Sec. 3.8 Fourier Series and LTI Systems 227 x[r] • I I I • • • • I I r • • • • I I I • -3 -2 -1 0 2 3 (a) Q[r] I 1 • • • • • • • • I I • • • • • • • • ' -1 0 (b) x[n-r] I I I I I I1 • • • • • • • • • • • I I ' n-7 n-1 n n+1 (c) 3 w[n] 2 -7 -3 -2 -1 0 2 3 7 n (d) Figure 3.21 (a) The square-wave sequence x[r] in Example 3.15; (b) the sequence x[r] equal to x[r] for -3 :::; r :::; 3 and zero otherwise; (c) the sequence x[n- r]; (d) the sequence w[n] equal to the periodic convolution of x[n] with itself and to the aperiodic convolution of x[n] with x[n]. When s or z are general complex numbers, H(s) and H(z) are referred to as the system functions of the corresponding systems. For continuous-time signals and systems in this and the following chapter, we focus on the specific case in which CRe{s} = 0, so that s = jw, and consequently, est is of the form ejwr. This input is a complex exponential at frequency w. The system function of the forms = jw-i.e., H(jw) viewed as a function of w-is referred to as the frequency response of the system and is given by +oc H(jw) = J-o c h(t)e-jwtdt. (3.121) 228 Fourier Series Representation of Periodic Signals Chap.3 Similarly, for discrete-time signals and systems, we focus in this chapter and in Chapter 5 on values of z for which lzl = 1, so that z = eiw and z"" is of the form eJwn. Then the system function H(z) for z restricted to the form z = eiw is referred to as the frequency response of the system and is given by +x H(eiw) = L h[n]e- jwn. (3.122) n= -oo The response of an LTI system to a complex exponential signal of the form eiwt (in continuous time) or eJwn (in discrete time) is particularly simple to express in terms of the frequency response of the system. Furthermore, as a result of the superposition property for LTI systems, we can express the response of an L TI system to a linear combination of complex exponentials with equal ease. In Chapters 4 and 5, we will see how we can use these ideas together with continuous-time and discrete-time Fourier transforms to an- alyze the response of LTI systems to aperiodic signals. In the remainder of this chapter, as a first look at this important set of concepts and results, we focus on interpreting and understanding this notion in the context of periodic signals. Consider first the continuous-time case, and let x(t) be a periodic signal with a Fourier series representation given by +oo x(t) = L akejkwot. (3.123) k=-X Suppose that we apply this signal as the input to an LTI system with impulse response h(t). Then, since each of the complex exponentials in eq. (3.123) is an eigenfunction of the system, as in eq. (3.13) with sk = jkw0 , it follows that the output is +oc y(t) = L akHUkwo)eJkwot. (3.124) k = -oc Thus, y(t) is also periodic with the same fundamental frequency as x(t). Furthermore, if {ak} is the set of Fourier series coefficients for the input x(t), then {akH(j kwo)} is the set of coefficients for the output y(t). That is, the effect of the LTI system is to modify individually each of the Fourier coefficients of the input through multiplication by the value of the frequency response at the corresponding frequency. Example 3. 16 Suppost: that the periodic signal x(t) discussed in Example 3.2 is the input signal to an LTI system with impulse response h(t) = e-r u(t). Sec. 3.8 Fourier Series and LTI Systems 229 To calculate the Fourier series coefficients of the output y(t), we first compute the fre- quency response: H(jw) = Le -re- Jwr dT ---1- e- r e -;·wr I""' (3.125) 1 + jw 0 1 1 + jw"" Therefore, using eqs. (3.124) and (3.125), together with the fact that w 0 = 21T in this example, we obtain +3 y(t) = L bkejk2m, (3.126) k= -3 with bk = akH(j k21T), so that bo = 1, bl ~ H,+,j27T). b-1 H~-~j27T). b, ~ HI +lj 47T). h-2 ~ HI -lj47T). (3.127) Note that y(t) must be a real-valued signal, since it is the convolution of x(t) and h(t), which are both real. This can be verified by examining eq. (3.127) and observing that b~ = b- k. Therefore, y(t) can also be expressed in either ofthe forms given in eqs. (3 .31) and (3.32); that is, 3 y(t) = 1 + 2 L Dk cos (21Tkt + fh), (3.128) k=l or 3 y(t) = 1 + 2 L [Ek cos 21Tkt- Fk sin 21Tkt], (3.129) k=l where (3.130) These coefficients can be evaluated directly from eq. (3.127). For example, 1 1 D1 = lbii = 01 = <tb1 = - tan- (27T), 4Jl+47T2 ' 1 1T E1 = cR-e{bi} = 4(1 + 47T2)' F1 = tfm{bi} = 2(1 + 47T2)"" 230 Fourier Series Representation of Periodic Signals Chap.3 In discrete time, the relationship between the Fourier series coefficients of the input and output of an LTI system exactly parallels eqs. (3.123) and (3.124). Specifically, let x[n] be a periodic signal with Fourier series representation given by x[n] = L akejk(2rr!Nln. k=(N) If we apply this signal as the input to an LTI system with impulse response h[n], then, as in eq. (3 .16) with Zk = ei k(2rr1N ), the output is y[n] = ~ akH(ej2rrk!N)ejk(2rr!N)n. (3.131) k=(N) Thus, y[n] is also periodic with the same period as x[n], and the kth Fourier coefficient of y[n] is the product of the kth Fourier coefficient of the input and the value of the frequency response of the LTI system, H(ei2rrk!N), at the corresponding frequency. Example 3.17 Consider an LTI system with impulse response h[n] = a 11 u[n], -1 <a< 1, and with the input x[n] =cos (N27 Tn) . (3.132) As in Example 3.10, x[n] can be written in Fourier series form as Also, from eq. (3.122), H(eiw) = fane- Jwn = f (ae- Jw r. (3.133) n=O n=O This geometric series can be evaluated using the result of Problem 1.54, yielding (3.134) Using eq. (3.131), we then obtain the Fourier series for the output: (3.135) Sec. 3.9 Filtering 231 If we write then eq. (3.135) reduces to y[n] ~ rcos(~ n + 8). (3.136) For example, if N = 4, and thus, y[n] = ~c1 os (7Tn - tan- 1(a) ). We note that for expressions such as eqs. (3.124) and (3.131) to make sense, the frequency responses H(jw) and H(eiw) in eqs. (3.121) and (3.122) must be well defined and finite. As we will see in Chapters 4 and 5, this will be the case if the LTI systems under consideration are stable. For example, the LTI system in Example 3 .16, with impulse response h(t) = e-1 u(t), is stable and has a well-defined frequency response given by eq. (3.125). On the other hand, an LTI system with impulse response h(t) = e1 u(t) is unstable, and it is easy to check that the integral in eq. (3.121) for H(jw) diverges for any value of w. Similarly, the LTI system in Example 3.17, with impulse response h[n] = anu[n], is stable for jaj < 1 and has frequency response given by eq. (3.134). However, if jaj > 1, the system is unstable, and then the summation in eq. (3.133) diverges. 3. 9 FILTERING In a variety of applications, it is of interest to change the relative amplitudes of the fre- quency components in a signal or perhaps eliminate some frequency components entirely, a process referred to as .filtering. Linear time-invariant systems that change the shape of the spectrum are often referred to as frequency-shaping filters. Systems that are designed to pass some frequencies essentially undistorted and significantly attenuate or eliminate oth- ers are referred to as frequency-selective filters. As indicated by eqs. (3.124) and (3.131), the Fourier series coefficients of the output of an LTI system are those of the input multi- plied by the frequency response of the system. Consequently, filtering can be conveniently accomplished through the use of LTI systems with an appropriately chosen frequency re- sponse, and frequency-domain methods provide us with the ideal tools to examine this very important class of applications. In this and the following two sections, we take a first look at filtering through a few examples."
3.9 Filtering,"Sec. 3.9 Filtering 231 If we write then eq. (3.135) reduces to y[n] ~ rcos(~ n + 8). (3.136) For example, if N = 4, and thus, y[n] = ~c1 os (7Tn - tan- 1(a) ). We note that for expressions such as eqs. (3.124) and (3.131) to make sense, the frequency responses H(jw) and H(eiw) in eqs. (3.121) and (3.122) must be well defined and finite. As we will see in Chapters 4 and 5, this will be the case if the LTI systems under consideration are stable. For example, the LTI system in Example 3 .16, with impulse response h(t) = e-1 u(t), is stable and has a well-defined frequency response given by eq. (3.125). On the other hand, an LTI system with impulse response h(t) = e1 u(t) is unstable, and it is easy to check that the integral in eq. (3.121) for H(jw) diverges for any value of w. Similarly, the LTI system in Example 3.17, with impulse response h[n] = anu[n], is stable for jaj < 1 and has frequency response given by eq. (3.134). However, if jaj > 1, the system is unstable, and then the summation in eq. (3.133) diverges. 3. 9 FILTERING In a variety of applications, it is of interest to change the relative amplitudes of the fre- quency components in a signal or perhaps eliminate some frequency components entirely, a process referred to as .filtering. Linear time-invariant systems that change the shape of the spectrum are often referred to as frequency-shaping filters. Systems that are designed to pass some frequencies essentially undistorted and significantly attenuate or eliminate oth- ers are referred to as frequency-selective filters. As indicated by eqs. (3.124) and (3.131), the Fourier series coefficients of the output of an LTI system are those of the input multi- plied by the frequency response of the system. Consequently, filtering can be conveniently accomplished through the use of LTI systems with an appropriately chosen frequency re- sponse, and frequency-domain methods provide us with the ideal tools to examine this very important class of applications. In this and the following two sections, we take a first look at filtering through a few examples. 232 Fourier Series Representation of Periodic Signals Chap.3 3.9.1 Frequency-Shaping Filters One application in which frequency-shaping filters are often encountered is audio sys- tems. For example, LTI filters are typically included in such systems to permit the listener to modify the relative amounts of low-frequency energy (bass) and high-frequency en- ergy (treble). These filters correspond to LTI systems whose frequency responses can be changed by manipulating the tone controls. Also, in high-fidelity audio systems, a so-called equalizing filter is often included in the preamplifier to compensate for the frequency- response characteristics of the speakers. Overall, these cascaded filtering stages are fre- quently referred to as the equalizing or equalizer circuits for the audio system. Figure 3.22 illustrates the three stages of the equalizer circuits for one particular series of audio speak- ers. In this figure, the magnitude of the frequency response for each of these stages is shown on a log-log plot. Specifically, the magnitude is in units of 20 log 10 )H(jw )), referred to as decibels or dB. The frequency axis is labeled in Hz (i.e., w/27T) along a logarithmic scale. As will be discussed in more detail in Section 6.2.3, a logarithmic display of the magnitude of the frequency response in this form is common and useful. Taken together, the equalizing circuits in Figure 3.22 are designed to compensate for the frequency response of the speakers and the room in which they are located and to allow the listener to control the overall frequency response. In particular, since the three systems are connected in cascade, and since each system modifies a complex exponential input K e.iwt by multiplying it by the system frequency response at that frequency, it follows that the overall frequency response of the cascade of the three systems is the product of the three frequency responses. The first two filters, indicated in Figures 3.22(a) and (b), together make up the control stage of the system, as the frequency behavior of these filters can be adjusted by the listener. The third filter, illustrated in Figure 3.22(c), is the equalizer stage, which has the fixed frequency response indicated. The filter in Figure 3.22(a) is a low- frequency filter controlled by a two-position switch, to provide one of the two frequency responses indicated. The second filter in the control stage has two continuously adjustable slider switches to vary the frequency response within the limits indicated in Figure 3.22(b). Another class of frequency-shaping filters often encountered is that for which the filter output is the derivative of the filter input, i.e., y(t) = d x(t)ldt. With x(t) of the form x(t) = e.iwt, y(t) will be y(t) = jwe.iwt, from which it follows that the frequency response is H(jw) = jw. (3.137) The frequency response characteristics of a differentiating filter are shown in Figure 3.23. Since H(jw) is complex in general, and in this example in particular, H(jw) is frequently displayed (as in the figure) as separate plots of )H(jw )) and <XH(jw ). The shape ofthis fre- quency response implies that a complex exponential input e.iwr will receive greater ampli- fication for larger values of w. Consequently, differentiating filters are useful in enhancing rapid variations or transitions in a signal. One purpose for which differentiating filters are often used is to enhance edges in picture processing. A black-and-white picture can be thought of as a two-dimensional ""continuous-time"" signal x(t1, t2), where t1 and t2 are the horizontal and vertical coordi- nates, respectively, and x(t1, t2) is the brightness of the image. If the image is repeated periodically in the horizontal and vertical directions, then it can be represented by a two- dimensional Fourier series (see Problem 3.70) consisting of sums of products of complex +25 ~~~--~~~--~----~~--~--~--~~~~~--~ +20 +15 :m3. +10 r------------====-------------------------------~ 3l +5 ~-----~ c g_ 0 Switch position 2 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 100 200 400 600 1kHz 2 3 4 6 8 10 20 Frequency (a) +25 --~~--~~----~------~--~--~--~~~~~--~ +20 +15 m +10 :3. 3l +5 c g_ 0 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 1 00 200 400 600 1 kHz 2 3 4 6 8 10 20 Frequency (b) +25 .--.-.--.-.-.----.----.--.--.----.--.-.--.-.-.--~ +20 +15 m +1o :3. 3l +5 c g_ 0 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 1 00 200 400 600 1kHz 2 3 4 6 8 10 20 Frequency (c) Figure 3.22 Magnitudes of the frequency responses of the equalizer circuits for one particular series of audio speakers, shown on a scale of 201og10 IH(jw)l, which is referred to as a decibel (or dB) scale. (a) Low- frequency filter controlled by a two-positior~ switch; (b) upper and lower frequency limits on a continuously adjustable shaping filter; (c) fixed frequency response of the equalizer stage. 233 234 Fourier Series Representation of Periodic Signals Chap.3 I HOw) I w ~r------ Figure 3.23 Characteristics of the w frequency response of a filter for which 1T 2 the output is the derivative of the in- put. exponentials, ejw 1 ' 1 and ejw 2t2 , that oscillate at possibly different frequencies in each of the two coordinate directions. Slow variations in brightness in a particular direction are represented by the lower harmonics in that direction. For example, consider an edge cor- responding to a sharp transition in brightness that runs vertically in an image. Since the brightness is constant or slowly varying along the edge, the frequency content of the edge in the vertical direction is concentrated at low frequencies. In contrast, since there is an abrupt variation in brightness across the edge, the frequency content of the edge in the horizontal direction is concentrated at higher frequencies. Figure 3.24 illustrates the effect on an image of the two-dimensional equivalent of a differentiating filter. 11 Figure 3.24(a) shows two original images and Figure 3.24(b) the result of processing those images with the filter. Since the derivative at the edges of a picture is greater than in regions where the brightness varies slowly with distance, the effect of the filter is to enhance the edges. Discrete-time LTI filters also find a broad array of applications. Many of these in- volve the use of discrete-time systems, implemented using general- or special-purpose digital processors, to process continuous-time signals, a topic we discuss at some length in Chapter 7. In addition, the analysis of time series information, including demographic data and economic data sequences such as the stock market average, commonly involves the use of discrete-time filters. Often the long-term variations (which correspond to low fre- quencies) have a different significance than the short-term variations (which correspond to high frequencies), and it is useful to analyze these components separately. Reshaping the relative weighting of the components is typically accomplished using discrete-time filters. As one example of a simple discrete-time filter, consider an LTI system that succes- sively takes a two-point average of the input values: 1 y[n] = 2(x[n] + x[n - 1] ). (3.138) 11 Specifically each image in Figure 3.24(b) is the magnitude of the two-dimensional gradient of its counterpart image in Figure 3.24(a) where the magnitude of the gradient off (x, y) is Sec. 3.9 Filtering 235 (a) (b) Figure 3.24 Effect of a differentiating filter on an image: (a) two original images; (b) the result of processing the original images with a differentiating filter. In this case h[n] = ~(8[n] + 8[n- 1]), and from eq. (3.122), we see that the frequency response of the system is . 1 + . . n H(e1w) = [1 e- 1w] = e-1w ~ cos(w/2). 2 (3.139) The magnitude of H(e.iw) is plotted in Figure 3.25(a), and <t-H(e.iw) is shown in Figure 3.25(b ). As discussed in Section 1.3.3, low frequencies for discrete-time complex expo- nentialsoccurnearw = 0, ±27T, ±47T, ... , andhighfrequenciesnearw = ±1r, ±37T, .... This is a result of the fact that e.i(w + 21r)n = e.iwn, so that in discrete time we need only con- sider a 27T interval of values of w in order to cover a complete range of distinct discrete- time frequencies. As a consequence, any discrete-time frequency responses H(e.iw) must be periodic with period 27T, a fact that can also be deduced directly from eq. (3.122). For the specific filter defined in eqs. (3.138) and (3.139), we see from Figure 3.25(a) that IH(e.iw)l is large for frequencies near w = 0 and decreases as we increase lwl toward 7T, indicating that higher frequencies are attentuated more than lower ones. For exam- ple, if the input to this system is constant-i.e., a zero-frequency complex exponential 236 Fourier Series Representation of Periodic Signals Chap. 3 IH(ei""')l 0 'lT (!) (a) Figure 3.25 (a) Magnitude and (b) phase for the frequency response of the discrete-time LTI system (b) y[n] = 1/2(x[n] + x[n- 1] ). x[n] = KejO·n = K-then the output will be y[n] = H(ej·O)KejwO·n = K = x[n]. On the other hand, if the input is the high-frequency signal x[n] Kej1rn then the output will be y[n] = H(ej1r)K ej1r·n = 0. Thus, this system separates out the long-term constant value of a signal from its high- frequency fluctuations and, consequently, represents a first example of frequency-selective filtering, a topic we look at more carefully in the next subsection. 3. 9. 2 Frequency-Selective Filters Frequency-selective filters are a class of filters specifically intended to accurately or approximately select some bands of frequencies and reject others. The use of frequency- selective filters arises in a variety of situations. For example, if noise in an audio recording is in a higher frequency band than the music or voice on the recording is, it can be removed by frequency-selective filtering. Another important application of frequency- selective filters is in communication systems. As we discuss in detail in Chapter 8, the basis for amplitude modulation (AM) systems is the transmission of information from many different sources simultaneously by putting the information from each channel into a separate frequency band and extracting the individual channels or bands at the receiver using frequency-selective filters. Frequency-selective filters for separating the individual Sec. 3.9 Filtering 237 channels and frequency-shaping filters (such as the equalizer illustrated in Figure 3.22) for adjusting the quality of the tone form a major part of any home radio and television receiver. While frequency selectivity is not the only issue of concern in applications, its broad importance has led to a widely accepted set of terms describing the characteristics of frequency-selective filters. In particular, while the nature of the frequencies to be passed by a frequency-selective filter varies considerably from application to application, several basic types of filter are widely used and have been given names indicative of their func- tion. For example, a lowp ass filter is a filter that passes low frequencies-i.e., frequencies around w = 0-and attenuates or rejects higher frequencies. A highpass filter is a filter that passes high frequencies and attentuates or rejects low ones, and a bandpass filter is a filter that passes a band of frequencies and attenuates frequencies both higher and lower than those in the band that is passed. In each case, the cutofff requencies are the frequen- cies defining the boundaries between frequencies that are passed and frequencies that are rejected-i.e., the frequencies in the passband and stopband. Numerous questions arise in defining and assessing the quality of a frequency- selective filter. How effective is the filter at passing frequencies in the passband? How effective is it at attentuating frequencies in the stopband? How sharp is the transition near the cutoff frequency-i.e., from nearly free of distortion in the passband to highly attenuated in the stopband? Each of these questions involves a comparison of the charac- teristics of an actual frequency-selective filter with those of a filter with idealized behavior. Specifically, an ideal frequency-selective filter is a filter that exactly passes complex ex- ponentials at one set of frequencies without any distortion and completely rejects signals at all other frequencies. For example, a continuous-time ideal lowpass filter with cutoff frequency we is an LTI system that passes complex exponentials efwt for values of win the range -we :::; w :::; we and rejects signals at all other frequencies. That is, the frequency response of a continuous-time ideal lowpass filter is 1 H(jw) = { ' (3.140) 0, as shown in Figure 3.26. H(jw) 11 0 w -+---Stopband-!-Passband-!-Stopband----+- Figure 3.26 Frequency response of an ideal lowpass filter. Figure 3.27(a) depicts the frequency response of an ideal continuous-time highpass filter with cutoff frequency We, and Figure 3.27(b) illustrates an ideal continuous-time bandpass filter with lower cutoff frequency wc1 and upper cutoff frequency wc2 . Note that each of these filters is symmetric about w = 0, and thus, there appear to be two pass bands for the highpass and bandpass filters. This is a consequence of our having adopted the 238 Fourier Series Representation of Periodic Signals Chap.3 H(jw} 1! -we We w (a) H(jw) 1! Figure 3.27 (a) Frequency re- -we2 -we1 We1 w sponse of an ideal highpass filter; (b) frequency response of an ideal (b) bandpass filter. use of the complex exponential signal e.iwt, rather than the sinusoidal signals sin wt and cos wt, at frequency w. Since e.iwt = cos wt + j sin wt and e-.iwt = cos wt- j sin wt, both of these complex exponentials are composed of sinusoidal signals at the same frequency w. For this reason, we usually define ideal filters so that they have the symmetric frequency response behavior seen in Figures 3.26 and 3.27. In a similar fashion, we can define the corresponding set of ideal discrete-time frequency-selective filters, the frequency responses for which are depicted in Figure 3.28. H(eiw) ,=] 11 -2TI -we 0 We 2TI W (a) H(eiw) I -2TI 2TI W (b) H(eiw) I II -2TI 2TI Figure 3.28 Discrete-time ideal W frequency-selective filters: (a) lowpass; (c) (b) highpass; (c) bandpass. Sec. 3.10 Examples of Continuous-Time Filters Described by Differential Equations 239 In particular, Figure 3.28(a) depicts an ideal discrete-time lowpass filter, Figure 3.28(b) is an ideal highpass filter, and Figure 3.28(c) is an ideal bandpass filter. Note that, as discussed in the preceding section, the characteristics of the continuous-time and discrete- time ideal filters differ by virtue of the fact that, for discrete-time filters, the frequency response H(ejw) must be periodic with period 27T, with low frequencies near even multi- ples of 1T and high frequencies near odd multiples of 1T. As we will see on numerous occasions, ideal filters are quite useful in describing ide- alized system configurations for a variety of applications. However, they are not realizable in practice and must be approximated. Furthermore, even if they could be realized, some of the characteristics of ideal filters might make them undesirable for particular applications, and a nonideal filter might in fact be preferable. In detail, the topic of filtering encompasses many issues, including design and imple- mentation. While we will not delve deeply into the details of filter design methodologies, in the remainder of this chapter and the following chapters we will see a number of other examples of both continuous-time and discrete-time filters and will develop the concepts and techniques that form the basis of this very important engineering discipline. 3. 1 0 EXAMPLES OF CONTINUOUS-TIME FILTERS DESCRIBED BY DIFFERENTIAL EQUATIONS In many applications, frequency-selective filtering is accomplished through the use of LTI systems described by linear constant-coefficient differential or difference equations. The reasons for this are numerous. For example, many physical systems that can be inter- preted as performing filtering operations are characterized by differential or difference equations. A good example of this that we will examine in Chapter 6 is an automobile suspension system, which in part is designed to filter out high-frequency bumps and ir- regularities in road surfaces. A second reason for the use of filters described by differen- tial or difference equations is that they are conveniently implemented using either analog or digital hardware. Furthermore, systems described by differential or difference equa- tions offer an extremely broad and flexible range of designs, allowing one, for example, to produce filters that are close to ideal or that possess other desirable characteristics. In this and the next section, we consider several examples that illustrate the implementation of continuous-time and discrete-time frequency-selective filters through the use of dif- ferential and difference equations. In Chapters 4-6, we will see other examples of these classes of filters and will gain additional insights into the properties that make them so use- ful. 3.1 0.1 A Simple RC Lowpass Filter Electrical circuits are widely used to implement continuous-time filtering operations. One of the simplest examples of such a circuit is the first-order RC circuit depicted in Fig- ure 3.29, where the source voltage vs(t) is the system input. This circuit can be used to perform either a lowpass or highpass filtering operation, depending upon what we take as the output signal. In particular, suppose that we take the capacitor voltage vc(t) as the output. In this case, the output voltage is related to the input voltage through the linear"
3.10 Examples of Continuous-Time Filters Described by Differential Equations,"Sec. 3.10 Examples of Continuous-Time Filters Described by Differential Equations 239 In particular, Figure 3.28(a) depicts an ideal discrete-time lowpass filter, Figure 3.28(b) is an ideal highpass filter, and Figure 3.28(c) is an ideal bandpass filter. Note that, as discussed in the preceding section, the characteristics of the continuous-time and discrete- time ideal filters differ by virtue of the fact that, for discrete-time filters, the frequency response H(ejw) must be periodic with period 27T, with low frequencies near even multi- ples of 1T and high frequencies near odd multiples of 1T. As we will see on numerous occasions, ideal filters are quite useful in describing ide- alized system configurations for a variety of applications. However, they are not realizable in practice and must be approximated. Furthermore, even if they could be realized, some of the characteristics of ideal filters might make them undesirable for particular applications, and a nonideal filter might in fact be preferable. In detail, the topic of filtering encompasses many issues, including design and imple- mentation. While we will not delve deeply into the details of filter design methodologies, in the remainder of this chapter and the following chapters we will see a number of other examples of both continuous-time and discrete-time filters and will develop the concepts and techniques that form the basis of this very important engineering discipline. 3. 1 0 EXAMPLES OF CONTINUOUS-TIME FILTERS DESCRIBED BY DIFFERENTIAL EQUATIONS In many applications, frequency-selective filtering is accomplished through the use of LTI systems described by linear constant-coefficient differential or difference equations. The reasons for this are numerous. For example, many physical systems that can be inter- preted as performing filtering operations are characterized by differential or difference equations. A good example of this that we will examine in Chapter 6 is an automobile suspension system, which in part is designed to filter out high-frequency bumps and ir- regularities in road surfaces. A second reason for the use of filters described by differen- tial or difference equations is that they are conveniently implemented using either analog or digital hardware. Furthermore, systems described by differential or difference equa- tions offer an extremely broad and flexible range of designs, allowing one, for example, to produce filters that are close to ideal or that possess other desirable characteristics. In this and the next section, we consider several examples that illustrate the implementation of continuous-time and discrete-time frequency-selective filters through the use of dif- ferential and difference equations. In Chapters 4-6, we will see other examples of these classes of filters and will gain additional insights into the properties that make them so use- ful. 3.1 0.1 A Simple RC Lowpass Filter Electrical circuits are widely used to implement continuous-time filtering operations. One of the simplest examples of such a circuit is the first-order RC circuit depicted in Fig- ure 3.29, where the source voltage vs(t) is the system input. This circuit can be used to perform either a lowpass or highpass filtering operation, depending upon what we take as the output signal. In particular, suppose that we take the capacitor voltage vc(t) as the output. In this case, the output voltage is related to the input voltage through the linear 240 Fourier Series Representation of Periodic Signals Chap. 3 + v,(t)- R + c Figure 3.29 First-order RC filter. constant-coefficient differential equation (3.141) Assuming initial rest, the system described by eq. (3.141) is LTI. In order to determine its frequency response H(jw ), we note that, by definition, with input voltage v,(t) = e.i'"" 1, we must have the output voltage vc(t) = H(jw )e.iwt. If we substitute these expressions into eq. (3.141), we obtain d . . . RC- [H(jw )e1'""1) + H(jw )ei'"" 1 = el'"" 1, (3.142) dt or RC jwH(jw )ejvJt + H(jw )ejcvt = ejwt, (3.143) from which it follows directly that H(jw )ejwt 1 ejcvl (3.144) 1 + RC}w ' or H(jw) = 1 + RC . . (3.145) JW The magnitude and phase of the frequency response H(jw) for this example are shown in Figure 3.30. Note that forfrequencies near w = 0, IH (jw )I = 1, while for larger values of w (positive or negative), IH(}w)i is considerably smaller and in fact steadily decreases as lwl increases. Thus, this simple RC filter (with Vc(t) as output) is a nonideal lowpass filter. To provide a first glimpse at the trade-offs involved in filter design, let us briefly consider the time-domain behavior of the circuit. In particular, the impulse response of the system described by eq. (3.141) is (3.146) Sec. 3.10 Examples of Continuous-Time Filters Described by Differential Equations 241 !H(jw)l -1/RC 0 1/RC w (a) <tH(jw) Tr/2 w (b) Figure 3.30 (a) Magnitude and (b) phase plots for the frequency response for the RC circuit of Figure 3.29 with output Vc(t). and the step response is s(t) = [1 - e -r!RC]u(t), (3.147) both of which are plotted in Figure 3.31 (where T = RC). Comparing Figures 3.30 and 3.31, we see a fundamental trade-off. Specifically, suppose that we would like our filter to pass only very low frequencies. From Figure 3.30(a), this implies that 1/ RC must be small, or equivalently, that RC is large, so that frequencies other than the low ones of interest will be attentuated sufficiently. However, looking at Figure 3.3l(b), we see that if RC is large, then the step response will take a considerable amount of time to reach its long-term value of 1. That is, the system responds sluggishly to the step input. Conversely, if we wish to have a faster step response, we need a smaller value of RC, which in tum implies that the filter will pass higher frequencies. This type of trade-off between behavior in the frequency domain and in the time domain is typical of the issues arising in the design and analysis of LTI systems and filters and is a subject we will look at more carefully in Chapter 6. 3.1 0.2 A Simple RC Highpass Filter As an alternative to choosing the capacitor voltage as the output in our RC circuit, we can choose the voltage across the resistor. In this case, the differential equation relating input 242 Fourier Series Representation of Periodic Signals Chap. 3 h(t) (a) s(t) 1- .!_ e Figure 3.31 (a) Impulse response of the first-order RC lowpass filter with r = RC; (b) step response of RC low- (b) pass filter with r = RC. and output is Rc dv,(t) () = RCdvs(t) dt + Vr t dt . (3.148) We can find the frequency response G(jw) of this system in exactly the same way we did in the previous case: If Vs(t) = eJwt, then we must have v,(t) = G(jw )eiwt; substituting these expressions into eq. (3.148) and performing a bit of algebra, we find that G(. ) = jwRC JW 1 + (3.149) jwRC The magnitude and phase of this frequency response are shown in Figure 3.32. From the figure, we see that the system attenuates lower frequencies and passes higher frequencies- i.e., those for which Jwl >> 11 RC-with minimal attenuation. That is, this system acts as a nonideal highpass filter. As with the lowpass filter, the parameters of the circuit control both the frequency response of the high pass filter and its time response characteristics. For example, consider the step response for the filter. From Figure 3.29, we see that v,(t) = v,(t) - Vc(t). Thus, if v,(t) = u(t), Vc(t) must be given by eq. (3.147). Consequently, the step response of the highpass filter is (3.150) which is depicted in Figure 3.33. Consequently, as RC is increased, the response becomes more sluggish-i.e., the step response takes a longer time to reach its long-term value Sec.3.10 Examples of Continuous-Time Filters Described by Differential Equations 243 IG(jw)l -1/RC 0 1/RC (a) <l:G(jw) (b) Figure 3.32 (a) Magnitude and (b) phase plots for the frequency response of the RC circuit of Figure 3.29 with output v,(t). v,(t) Figure 3.33 Step response of the RC first-order RC highpass filter with T = RC. of 0. From Figure 3.32, we see that increasing RC (or equivalently, decreasing 11 RC) also affects the frequency response, specifically, it extends the passband down to lower frequencies. We observe from the two examples in this section that a simple RC circuit can serve as a rough approximation to a high pass or a lowpass filter, depending upon the choice of the physical output variable. As illustrated in Problem 3.71, a simple mechanical system using a mass and a mechanical damper can also serve as a lowpass or highp ass filter described by 244 Fourier Series Representation of Periodic Signals Chap.3 analogous first-order differential equations. Because of their simplicity, these examples of electrical and mechanical filters do not have a sharp transition from passband to stopband and, in fact, have only a single parameter (namely, RC in the electrical case) that con- trols both the frequency response and time response behavior of the system. By designing more complex filters, implemented using more energy storage elements (capacitances and inductances in electrical filters and springs and damping devices in mechanical filters), we obtain filters described by higher order differential equations. Such filters offer con- siderably more flexibility in terms of their characteristics, allowing, for example, sharper passband-stopband transition or more control over the trade-offs between time response and frequency response. 3.11 EXAMPLES OF DISCRETE-TIME FILTERS DESCRIBED BY DIFFERENCE EQUATIONS As with their continuous-time counterparts, discrete-time filters described by linear constant-coefficient difference equations are of considerable importance in practice. In- deed, since they can be efficiently implemented in special- or general-purpose digital systems, filters described by difference equations are widely used in practice. As in al- most all aspects of signal and system analysis, when we examine discrete-time filters described by difference equations, we find both strong similarities and important differ- ences with the continuous-time case. In particular, discrete-time LTI systems described by difference equations can either be recursive and have impulse responses of infinite length (IIR systems) or be nonrecursive and have finite-length impulse responses (FIR systems). The former are the direct counterparts of continuous-time systems described by differential equations illustrated in the previous section, while the latter are also of considerable practical importance in digital systems. These two classes have distinct sets of advantages and disadvantages in terms of ease of implementation and in terms of the order of filter or the complexity required to achieve particular design objectives. In this section we limit ourselves to a few simple examples of recursive and nonrecursive filters, while in Chapters 5 and 6 we develop additional tools and insights that allow us to analyze and understand the properties of these systems in more detail. 3.11.1 First-Order Recursive Discrete-Time Filters The discrete-time counterpart of each of the first -order filters considered in Section 3.10 is the LTI system described by the first-order difference equation y[n] - ay[n- 1] = x[n]. (3.151) From the eigenfunction property of complex exponential signals, we know that if x[n] = eiwn, then y[n] = H(ei""')eiwn, where H(ei""') is the frequency response of the system. Substituting into eq. (3.151), we obtain (3.152) or (3.153)"
3.11 Examples of Discrete-Time Filters Described by Difference Equations,"244 Fourier Series Representation of Periodic Signals Chap.3 analogous first-order differential equations. Because of their simplicity, these examples of electrical and mechanical filters do not have a sharp transition from passband to stopband and, in fact, have only a single parameter (namely, RC in the electrical case) that con- trols both the frequency response and time response behavior of the system. By designing more complex filters, implemented using more energy storage elements (capacitances and inductances in electrical filters and springs and damping devices in mechanical filters), we obtain filters described by higher order differential equations. Such filters offer con- siderably more flexibility in terms of their characteristics, allowing, for example, sharper passband-stopband transition or more control over the trade-offs between time response and frequency response. 3.11 EXAMPLES OF DISCRETE-TIME FILTERS DESCRIBED BY DIFFERENCE EQUATIONS As with their continuous-time counterparts, discrete-time filters described by linear constant-coefficient difference equations are of considerable importance in practice. In- deed, since they can be efficiently implemented in special- or general-purpose digital systems, filters described by difference equations are widely used in practice. As in al- most all aspects of signal and system analysis, when we examine discrete-time filters described by difference equations, we find both strong similarities and important differ- ences with the continuous-time case. In particular, discrete-time LTI systems described by difference equations can either be recursive and have impulse responses of infinite length (IIR systems) or be nonrecursive and have finite-length impulse responses (FIR systems). The former are the direct counterparts of continuous-time systems described by differential equations illustrated in the previous section, while the latter are also of considerable practical importance in digital systems. These two classes have distinct sets of advantages and disadvantages in terms of ease of implementation and in terms of the order of filter or the complexity required to achieve particular design objectives. In this section we limit ourselves to a few simple examples of recursive and nonrecursive filters, while in Chapters 5 and 6 we develop additional tools and insights that allow us to analyze and understand the properties of these systems in more detail. 3.11.1 First-Order Recursive Discrete-Time Filters The discrete-time counterpart of each of the first -order filters considered in Section 3.10 is the LTI system described by the first-order difference equation y[n] - ay[n- 1] = x[n]. (3.151) From the eigenfunction property of complex exponential signals, we know that if x[n] = eiwn, then y[n] = H(ei""')eiwn, where H(ei""') is the frequency response of the system. Substituting into eq. (3.151), we obtain (3.152) or (3.153) Sec.3.11 Examples of Discrete-Time Filters Described by Difference Equations 245 so that H(eiw) = (3.154) 1-ae~jw' The magnitude and phase of H(eiw) are shown in Figure 3.34(a) for a = 0.6 and in Figure 3.34(b) for a = -0.6. We observe that, for the positive value of a, the difference equation (3.151) behaves like a lowpass filter with minimal attenuation of low frequencies near w = 0 and increasing attenuation as we increase w toward w = 7T. For the negative value of a, the system is a highpass filter, passing frequencies near w = 7T and attenuating lower frequencies. In fact, for any positive value of a < 1, the system approximates a lowpass filter, and for any negative value of a > -1, the system approximates a highpass filter, where Ia I controls the size of the filter passband, with broader pass bands as Ia I is decreased. As with the continuous-time examples, we again have a trade-off between time do- main and frequency domain characteristics. In particular, the impulse response of the sys- tem described by eq. (3.151) is h[n] = a 11 u[n]. (3.155) The step response s[n] u[n] * h[n] is 1 -an+ I s[n] = 1 u[n]. (3.156) -a From these expressions, we see that lal also controls the speed with which the impulse and step responses approach their long-term values, with faster responses for smaller values of lai, and hence, for filters with smaller passbands. Just as with differential equations, higher order recursive difference equations can be used to provide sharper filter charac- teristics and to provide more flexibility in balancing time-domain and frequency-domain constraints. Finally, note from eq. (3.155) that the system described by eq. (3.151) is unstable if lal ::::: 1 and thus does not have a finite response to complex exponential inputs. As we stated previously, Fourier-based methods and frequency domain analysis focus on systems with finite responses to complex exponentials; hence, for examples such as eq. (3.151), we restrict ourselves to stable systems. 3. 11.2 Nonrecursive Discrete-Time Filters The general form of an FIR nonrecursive difference equation is M y[n] = L bkx[n - k]. (3.157) k= ~N That is, the output y[n] is a weighted average of the (N + M + 1) values of x[n] from x[n - M] through x[n + N], with the weights given by the coefficients bk. Systems of this form can be used to meet a broad array of filtering objectives, including frequency- selective filtering. One frequently used example of such a filter is a moving-average filter, where the output y[n] for any n-say, n0-is an average of values of x[n] in the vicinity of no. The -1T 1T w 7r 2 w 7r 2 (a) I H(ei""') I -1T Figure 3.34 Frequency response 7r 2 of the first-order recursive discrete- time filter of eq. (3.151 ): (a) a = 0.6; (b) (b) a = -0.6. 246 Sec. 3.11 Examples of Discrete-Time Filters Described by Difference Equations 247 basic idea is that by averaging values locally, rapid high-frequency components of the in- put will be averaged out and lower frequency variations will be retained, corresponding to smoothing or lowpass filtering the original sequence. A simple two-point moving-average filter was briefly introduced in Section 3.9 [eq. (3.138)]. An only slightly more complex example is the three-point moving-average filter, which is of the form 1 y[n] = 3(x[n- 1] + x[n] + x[n + 1]), (3.158) so that each output y[n] is the average of three consecutive input values. In this case, 1 h[n] = '3[8[n + 1] + 8[n] + 8[n- 1]], and thus, from eq. (3.122), the corresponding frequency response is (3.159) The magnitude of H(eiw) is sketched in Figure 3.35. We observe that the filter has the general characteristics of a lowpass filter, although, as with the first-order recursive filter, it does not have a sharp transition from passband to stopband. Figure 3.35 Magnitude of the fre- 0 :rr 1T 21T w quency response of a three-point 2 moving-average lowpass filter. The three-point moving-average filter in eq. (3.158) has no parameters that can be changed to adjust the effective cutoff frequency. As a generalization of this moving- average filter, we can consider averaging over N + M + 1 neighboring points-that is, using a difference equation of the form 1 M y[n] = N + M + 1 L x[n- k]. (3.160) k=-N The corresponding impulse response is a rectangular pulse (i.e., h[n] = li(N + M + 1) for -N ::::; n ::::; M, and h[n] = 0 otherwise). The filter's frequency response is M H(eiw) = 1 L e- }wk. (3.161) N + M + 1 k=-N 248 Fourier Series Representation of Periodic Signals Chap.3 The summation in eq. (3.161) can be evaluated by performing calculations similar to those in Example 3.12, yielding H( jw) = 1 jw[(N-M)/2] sin[w(M + N + 1)/2] + + (3.162) e N M 1 e sin(w/2) · By adjusting the size, N + M + 1, of the averaging window we can vary the cutoff fre- quency. For example, the magnitude of H(ejw) is shown in Figure 3.36for M +N + 1 = 33 and M + N + 1 = 65. 0 11'12 w (a) 0 w (b) Figure 3.36 Magnitude of the frequency response for the lowpass moving- average filter of eq. {3.162): {a) M = N = 16; {b) M = N = 32. Nonrecursive filters can also be used to perform highpass filtering operations. To illustrate this, again with a simple example, consider the difference equation y [ n ] _ x[n] - x[n - 1] - 2 . (3.163) For input signals that are approximately constant, the value of y[n] is close to zero. For input signals that vary greatly from sample to sample, the values of y[n] can be ex- Sec. 3.12 Summary 249 pected to have larger amplitude. Thus, the system described by eq. (3.163) approximates a highpass filtering operation, attenuating slowly varying low-frequency components and passing rapidly varying higher frequency components with little attenuation. To see this more precisely we need to look at the system's frequency response. In this case, h[n] ~{S[n]- o[n- 1]}, so that direct application of eq. (3.122) yields (3.164) In Figure 3.37 we have plotted the magnitude of H(ejw), showing that this simple system approximates a highpass filter, albeit one with a very gradual transition from pass- band to stopband. By considering more general nonrecursive filters, we can achieve far sharper transitions in lowpass, highpass, and other frequency-selective filters. w Figure 3.37 Frequency response of a simple highpass filter. Note that, since the impulse response of any FIR system is of finite length (i.e., from eq. (3.157), h[n] = bn for -N ::::: n ::::: M and 0 otherwise), it is always absolutely summable for any choices of the bn. Hence, all such filters are stable. Also, if N > 0 in eq. (3.157), the system is noncausal, since y[n] then depends on future values of the input. In some applications, such as those involving the processing of previously recorded signals, causality is not a necessary constraint, and thus, we are free to use filters with N > 0. In others, such as many involving real-time processing, causality is essential, and in such cases we must take N ::::: 0. 3.12 SUMMARY In this chapter, we have introduced and developed Fourier series representations for both continuous-time and discrete-time systems and have used these representations to take a first look at one of the very important applications of the methods of signal and system analysis, namely, filtering. In particular, as we discussed in Section 3.2, one of the primary motivations for the use of Fourier series is the fact that complex exponential signals are eigenfunctions of LTI systems. We have also seen, in Sections 3.3-3.7, that any periodic signal of practical interest can be represented in a Fourier series-i.e., as a weighted sum"
3.12 Summary,"Sec. 3.12 Summary 249 pected to have larger amplitude. Thus, the system described by eq. (3.163) approximates a highpass filtering operation, attenuating slowly varying low-frequency components and passing rapidly varying higher frequency components with little attenuation. To see this more precisely we need to look at the system's frequency response. In this case, h[n] ~{S[n]- o[n- 1]}, so that direct application of eq. (3.122) yields (3.164) In Figure 3.37 we have plotted the magnitude of H(ejw), showing that this simple system approximates a highpass filter, albeit one with a very gradual transition from pass- band to stopband. By considering more general nonrecursive filters, we can achieve far sharper transitions in lowpass, highpass, and other frequency-selective filters. w Figure 3.37 Frequency response of a simple highpass filter. Note that, since the impulse response of any FIR system is of finite length (i.e., from eq. (3.157), h[n] = bn for -N ::::: n ::::: M and 0 otherwise), it is always absolutely summable for any choices of the bn. Hence, all such filters are stable. Also, if N > 0 in eq. (3.157), the system is noncausal, since y[n] then depends on future values of the input. In some applications, such as those involving the processing of previously recorded signals, causality is not a necessary constraint, and thus, we are free to use filters with N > 0. In others, such as many involving real-time processing, causality is essential, and in such cases we must take N ::::: 0. 3.12 SUMMARY In this chapter, we have introduced and developed Fourier series representations for both continuous-time and discrete-time systems and have used these representations to take a first look at one of the very important applications of the methods of signal and system analysis, namely, filtering. In particular, as we discussed in Section 3.2, one of the primary motivations for the use of Fourier series is the fact that complex exponential signals are eigenfunctions of LTI systems. We have also seen, in Sections 3.3-3.7, that any periodic signal of practical interest can be represented in a Fourier series-i.e., as a weighted sum 250 Fourier Series Representation of Periodic Signals Chap. 3 of harmonically related complex exponentials that share a common period with the signal being represented. In addition, we have seen that the Fourier series representation has a number of important properties which describe how different characteristics of signals are reflected in their Fourier series coefficients. One of the most important properties of Fourier series is a direct consequence of the eigenfunction property of complex exponentials. Specifically, if a periodic signal is ap- plied to an LTI system, then the output will be periodic with the same period, and each of the Fourier coefficients of the output is the corresponding Fourier coefficient of the input multiplied by a complex number whose value is a function of the frequency corre- sponding to that Fourier coefficient. This function of frequency is characteristic of the LTI system and is referred to as the frequency response of the system. By examining the fre- quency response, we were led directly to the idea of filtering of signals using LTI systems, a concept that has numerous applications, including several that we have described. One important class of applications involves the notion of frequency-selective filtering-i.e., the idea of using an LTI system to pass certain specified bands of frequencies and stop or significantly attentuate others. We introduced the concept of ideal frequency-selective filters and also gave several examples of frequency-selective filters described by linear constant-coefficient differential or difference equations. The purpose of this chapter has been to begin the process of developing both the tools of Fourier analysis and an appreciation for the utility of these tools in applications. In the chapters that follow, we continue with this agenda by developing the Fourier transform representations for aperiodic signals in continuous and discrete time and by taking a deeper look not only at filtering, but also at other important applications of Fourier methods. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 3.1. A continuous-time periodic signal x(t) is real valued and has a fundamental period T = 8. The nonzero Fourier series coefficients for x(t) are a1 = a_ 1 = 2,a3 = a*_ 3 = 4j. Express x(t) in the form x(t) = L Ak cos(wkt + cPk). k=O 3.2. A discrete-time periodic signal x[n] is real valued and has a fundamental period N = 5. The nonzero Fourier series coefficients for x[n] are"
Problems,"250 Fourier Series Representation of Periodic Signals Chap. 3 of harmonically related complex exponentials that share a common period with the signal being represented. In addition, we have seen that the Fourier series representation has a number of important properties which describe how different characteristics of signals are reflected in their Fourier series coefficients. One of the most important properties of Fourier series is a direct consequence of the eigenfunction property of complex exponentials. Specifically, if a periodic signal is ap- plied to an LTI system, then the output will be periodic with the same period, and each of the Fourier coefficients of the output is the corresponding Fourier coefficient of the input multiplied by a complex number whose value is a function of the frequency corre- sponding to that Fourier coefficient. This function of frequency is characteristic of the LTI system and is referred to as the frequency response of the system. By examining the fre- quency response, we were led directly to the idea of filtering of signals using LTI systems, a concept that has numerous applications, including several that we have described. One important class of applications involves the notion of frequency-selective filtering-i.e., the idea of using an LTI system to pass certain specified bands of frequencies and stop or significantly attentuate others. We introduced the concept of ideal frequency-selective filters and also gave several examples of frequency-selective filters described by linear constant-coefficient differential or difference equations. The purpose of this chapter has been to begin the process of developing both the tools of Fourier analysis and an appreciation for the utility of these tools in applications. In the chapters that follow, we continue with this agenda by developing the Fourier transform representations for aperiodic signals in continuous and discrete time and by taking a deeper look not only at filtering, but also at other important applications of Fourier methods. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 3.1. A continuous-time periodic signal x(t) is real valued and has a fundamental period T = 8. The nonzero Fourier series coefficients for x(t) are a1 = a_ 1 = 2,a3 = a*_ 3 = 4j. Express x(t) in the form x(t) = L Ak cos(wkt + cPk). k=O 3.2. A discrete-time periodic signal x[n] is real valued and has a fundamental period N = 5. The nonzero Fourier series coefficients for x[n] are Chap. 3 Problems 251 Express x[n] in the form x[n] = Ao + ~ Ak sin(wkn + cfJk). k~l 3.3. For the continuous-time periodic signal x(t) = 2 +cos (327T t ) + 4sm. (537T t ), determine the fundamental frequency w0 and the Fourier series coefficients ak such that x(t) = ~ akeJkwot k~-x 3.4. Use the Fourier series analysis equation (3.39) to calculate the coefficients ak for the continuous-time periodic signal x(t) = { 1.5, Ost<l -1.5, l:St<2 with fundamental frequency w0 = 1T. 3.5. Let x1 (t) be a continuous-time periodic signal with fundamental frequency w 1 and Fourier coefficients ak. Given that X2(t) = X1(1- t) + X1(t- 1), how is the fundamental frequency w2 of x 2(t) related tow 1? Also, find a relationship between the Fourier series coefficients bk of x 2(t) and the coefficients ak. You may use the properties listed in Table 3.1. 3.6. Consider three continuous-time periodic signals whose Fourier series representa- tions are as follows: Use Fourier series properties to help answer the following questions: (a) Which of the three signals is/are real valued? (b) Which of the three signals is/are even? 3.7. Suppose the periodic signal x(t) has fundamental period T and Fourier coefficients ak. In a variety of situations, it is easier to calculate the Fourier series coefficients 252 Fourier Series Representation of Periodic Signals Chap.3 bk for g(t) = dx(t)/dt, as opposed to calculating ak directly. Given that I:J T x(t)dt = 2, find an expression for ak in terms of bk and T. You may use any of the properties listed in Table 3.1 to help find the expression. 3.8. Suppose we are given the following information about a signal x(t): 1. x(t) is real and odd. 2. x(t) is periodic with period T = 2 and has Fourier coefficients ak. 3. ak = 0 for lkl > I. 4. Ho2 ix(t)j2 dt = 1. Specify two different signals that satisfy these conditions. 3.9. Use the analysis equation (3.95) to evaluate the numerical values of one period of the Fourier series coefficients of the periodic signal x[n] = L {48[n- 4m] + 88[n- 1 - 4m]}. In= -X 3.10. Let x[n] be areal and odd periodic signal with period N = 7 and Fourier coefficients ak· Given that a,s = j, at6 = 2}, a17 = 3}, determine the values of ao, a_,, a-2, and a-3· 3.11. Suppose we are given the following information about a signal x[n]: 1. x[ n] is a real and even signal. 2. x[n] has period N = 10 and Fourier coefficients ak. 3. a11 = 5. 9 4. Yo 2:: jx[nJI2 = 50. n=O Show that x[ n] = A cos(Bn + C), and specify numerical values for the constants A, B, and C. 3.12. Each of the two sequences Xt [n] and x2[n] has a period N = 4, and the correspond- ing Fourier series coefficients are specified as Xt [n] ~ ah x2[n] ~ bh where 1 ao = a3 = -a, 2 Using the multiplication property in Table 3.1, determine the Fourier series coeffi- cients ck for the signal g[n] = Xt [n]x2[n]. Chap. 3 Problems 253 3.13. Consider a continuous-time LTI system whose frequency response is Ix sin(4w) H(jw) = -x h(t)e-jwtdt = w If the input to this system is a periodic signal ~ O:s:t<4 x(t) = { l, 4:St<8 with period T = 8, determine the corresponding system output y(t). 3.14. When the impulse train x[n] = L o[n - 4k] k=-X is the input to a particular LTI system with frequency response H(eiw), the output of the system is found to be y[n] = cos (257T n + 47T ) . Determine the values of H(eikrr/2 ) fork = 0, 1, 2, and 3. 3.15. Consider a continuous-time ideallowpass filterS whose frequency response is H(jw) 1, lwl :s: 100 = { 0, lwl > 100 · When the input to this filter is a signal x(t) with fundamental period T = 1r/6 and Fourier series coefficients ak. it is found that s x(t) ~ y(t) = x(t). For what values of k is it guaranteed that ak = 0? 3.16. Determine the output of the filter shown in Figure P3.16 for the following periodic inputs: (a) x 1[n] = (-1)"" (b) x2[n] = 1 + sin(3; n + 'i-) (c) x3[n] = ~~=-oc(~r-4ku[n- 4k] ""IT 57r ""IT 3 12 Figure P3.16 254 Fourier Series Representation of Periodic Signals Chap.3 3.17. Consider three continuous-time systems S1, S2, and S3 whose responses to a complex exponential input ei51 are specified as sl : ej5t --7 tej5t, S2 : ej5t ----7 ejS(t-1), S3 : ei51 ----7 cos(St). For each system, determine whether the given information is sufficient to conclude that the system is definitely not LTI. 3.18. Consider three discrete-time systems S1, S2, and S3 whose respective responses to a complex exponential input ei""""12 are specified as sl : ej7rnl2 ----7 ejmz/2u[n], s2 : ej7rn/2 ----7 ej37rnl2, s3 : ej7rn/2 --7 2ej57rn/2. For each system, determine whether the given information is sufficient to conclude that the system is definitely not LTI. 3.19. Consider a causal LTI system implemented as the RL circuit shown in Figure P3.19. A current source produces an input current x(t), and the system output is considered to be the current y(t) flowing through the inductor. 1f1 Figure P3. 19 (a) Find the differential equation relating x(t) and y(t). (b) Determine the frequency response of this system by considering the output of the system to inputs of the form x(t) = eiwt. (c) Determine the output y(t) if x(t) = cos(t). 3.20. Consider a causal LTI system implemented as the RLC circuit shown in Figure P3.20. In this circuit, x(t) is the input voltage. The voltage y(t) across the capac- itor is considered the system output. R=1H L=1H + x(t) Figure P3.20 Chap. 3 Problems 255 (a) Find the differential equation relating x(t) and y(t). (b) Determine the frequency response of this system by considering the output of the system to inputs of the form x(t) = ejwr. (c) Determine the output y(t) if x(t) = sin(t). BASIC PROBLEMS 3.21. A continuous-time periodic signal x(t) is real valued and has a fundamental period T = 8. The nonzero Fourier series coefficients for x(t) are specified as a,= a*_ 1 = j,as =a-s= 2. Express x(t) in the form x(t) = L Ak cos(wkt + cf>k). k=O 3.22. Determine the Fourier series representations for the following signals: (a) Each x(t) illustrated in Figure P3.22(a)-(f). (b) x(t) periodic with period 2 and x(t) = e -r for - 1 < t < 1 x(t) x(t) ~ I / , I ~ / -5 -4 -3 -2 -1 2 3 4 5 (b) x(t) ~ (c) Figure P3.22 256 Fourier Series Representation of Periodic Signals Chap.3 x(t) t ~3 t ~1 1 t 3 t 5 r t ~4 l ~2 l l~ 2 2 l 4 l 6 (d) x(t) l I ~7 ~6 u ~~ ~ 2n ~ 1 ~ 1' t u I n I 3 4 5 6 u (e) x(t) ~-QJ,:~ r--, r 3 4 5 6 7 (f) Figure P3.22 Continued (c) x(t) periodic with period 4 and x(t) = { sin 7rt, 0, 3.23. In each of the following, we specify the Fourier series coefficients of a continuous- time signal that is periodic with period 4. Determine the signal x(t) in each case. { 0 k = 0 (a) ak = ( '·)k sin hr/4 otherwise J k1r ' (b) ak = (-l)ksi;~;/8, ao = /6 ( ) - { jk, lkl < 3 c ak - 0, otherwise (d) ak = { 1' k even 2, k odd 3.24. Let x(t) = { i-t, be a periodic signal with fundamental period T = 2 and Fourier coefficients ak. (a) Determine the value of ao. (b) Determine the Fourier series representation of dx(t)ldt. (c) Use the result of part (b) and the differentiation property of the continuous-tim{ Fourier series to help determine the Fourier series coefficients of x(t). Chap. 3 Problems 257 3.25. Consider the following three continuous-time signals with a fundamental period of T = 112: x(t) = cos( 41Tt), y(t) = sin(47Tt), z(t) = x(t)y(t). (a) Determine the Fourier series coefficients of x(t). (b) Determine the Fourier series coefficients of y(t). (c) Use the results of parts (a) and (b), along with the multiplication property of the continuous-time Fourier series, to determine the Fourier series coefficients of z(t) = x(t)y(t). (d) Determine the Fourier series coefficients of z(t) through direct expansion of z(t) in trigonometric form, and compare your result with that of part (c). 3.26. Let x(t) be a periodic signal whose Fourier series coefficients are k = 0 otherwise· Use Fourier series properties to answer the following questions: (a) Is x(t) real? (b) Is x(t) even? (c) Is dx(t)ldt even? 3.27. A discrete-time periodic signal x[n] is real valued and has a fundamental period N = 5. The nonzero Fourier series coefficients for x[n] are Express x[n] in the form x[n] = Ao + L Ak sin(wkn + cflk). k=l 3.28. Determine the Fourier series coefficients for each of the following discrete-time periodic signals. Plot the magnitude and phase of each set of coefficients ak· (a) Each x[n] depicted in Figure P3.28(a)-(c) (b) x[n] = sin(27Tn/3)cos(7Tn/2) (c) x[n] periodic with period 4 and x[n] = 1 - sin :n for 0 :5 n :5 3 (d) x[n] periodic with period 12 and x [ n ] = 1 - sm· 41r n f or 0 :5 n :5 11 258 Fourier Series Representation of Periodic Signals Chap.3 x[n] 111 .. 11111 .. 11111.~lllll .. lllll .. lllll .. ll -14 -7 0 7 14 21 n (a) x[n] .llll .. llll .. llll.~llll .. 1111 .. 1111 .. 1111. -18 -12 -6 0 6 12 18 n (b) x[n] 2 n (c) Figure P3.28 3.29. In each of the following, we specify the Fourier series coefficients of a signal that is periodic with period 8. Determine the signal x[n] in each case. (a) ak = cos(k;) + sin(3!7T) (b) ak = { ~~n(k:;7), ~:; ~ 6 (c) ak as in Figure P3.29(a) (d) ak as in Figure P3.29(b) 1 . 111 . 111 . 111 . 111 . 111 . 111 .111 . 11 -8 0 8 16 k (a) ak 2 I 1,il 1I11 ... 11li1:.i11111 ... 11 I -8 0 8 16 k (b) Figure P3.29 3.30. Consider the following three discrete-time signals with a fundamental period of 6: x[n] = 1 +cos (627T n ), y[n] = s.m (267T n + 4'TT ) , z[n] = x[n]y[n]. Chap.3 Problems 259 (a) Determine the Fourier series coefficients of x[n]. (b) Determine the Fourier series coefficients of y[n]. (c) Use the results of parts (a) and (b), along with the multiplication property of the discrete-time Fourier series, to determine the Fourier series coefficients of z[n] = x[n]y[n]. (d) Determine the Fourier series coefficients of z[n] through direct evaluation, and compare your result with that of part (c). 3.31. Let x[n] = { ~: be a periodic signal with fundamental period N = 10 and Fourier series coefficients ak. Also, let g[n] = x[n] - x[n - 1]. (a) Show that g[n] has a fundamental period of 10. (b) Determine the Fourier series coefficients of g[n]. (c) Using the Fourier series coefficients of g[n] and the First-Difference property in Table 3.2, determine ak for k -:/= 0. 3.32. Consider the signal x[ n] depicted in Figure P3.32. This signal is periodic with period N = 4. The signal can be expressed in terms of a discrete-time Fourier series as 3 x[n] = L akeik(27T/4)n. (P3.32-1) k=O x[n] f12 1-8 1-4 _Jo 14 18 112 116 n Figure P3.32 As mentioned in the text, one way to determine the Fourier series coefficients is to treat eq. (P3.32-1) as a set of four linear equations (for n = 0, 1, 2, 3) in four unknowns (ao, a1, a2, and a3). (a) Write out these four equations explicitly, and solve them directly using any stan- dard technique for solving four equations in four unknowns. (Be sure first to reduce the foregoing complex exponentials to the simplest form.) (b) Check your answer by calculating the ak directly, using the discrete-time Fourier series analysis equation 3 ak = ~ L x[n]e- jk(21T/4)n. n=O 260 Fourier Series Representation of Periodic Signals Chap.3 3.33. Consider a causal continuous-time LTI system whose input x(t) and output y(t) are related by the following differential equation: d dty(t) + 4y(t) = x(t). Find the Fourier series representation of the output y(t) for each of the following inputs: (a) x(t) = cos 27rt (b) x(t) = sin 47rt + cos( 67rt + 7r/4) 3.34. Consider a continuous-time LTI system with impulse response h(t) = e-4lrl. Find the Fourier series representation of the output y(t) for each of the following inputs: (a) x(t) = L::_xo(t - n) (b) x(t) = L:: _x(- l)no(t - n) (c) x(t) is the periodic wave depicted in Figure P3.34. x(t) 1 H ··l n n n ~ n n n n -3 -2 -1 0 2 3 4 Figure P3.34 3.35. Consider a continuous-time LTI system S whose frequency response is H(jw) = { I, lwl 2: 250 0, otherwise · When the input to this system is a signal x(t) with fundamental period T = 7r/7 and Fourier series coefficients ah it is found that the output y(t) is identical to x(t). For what values of k is it guaranteed that ak = O? 3.36. Consider a causal discrete-time LTI system whose input x[n] and output y[n] are related by the following difference equation: I y[n] - 4y[n - I] = x[nl Find the Fourier series representation of the output y[n] for each of the following inputs: (a) x[n] = sinc3; n) (b) x[n] = cos(*n) + 2cos(¥n) 3.37. Consider a discrete-time LTI system with impulse response l ~nl h[n] = (2) Chap.3 Problems 261 Find the Fourier series representation of the output y[n] for each of the following inputs: (a) x[n] = L~= -xD[n - 4k] (b) x[ n] is periodic with period 6 and b: n = 0, :tl x[n] = { n = :±::2, :±::3 3.38. Consider a discrete-time LTI system with impulse response 1, 0:Sn:52 h[n] = { -1, -2:5n:5-l. 0, otherwise Given that the input to this system is +oc x[n] = 2= o[n - 4k], k=-X determine the Fourier series coefficients of the output y[n]. 3.39. Consider a discrete-time LTI system S whose frequency response is H(elw. ) = [ I ' lwl :5 ~ 0, ¥ < lwl <'1T'' Show that if the input x[n] to this system has a period N = 3, the output y[n] has only one nonzero Fourier series coefficient per period. ADVANCED PROBLEMS 3.40. Let x(t) be a periodic signal with fundamental period T and Fourier series coeffi- cients ak. Derive the Fourier series coefficients of each of the following signals in terms of ak: (a) x(t - to) + x(t + to) (b) &v{x(t)} (c) CRc{x(t)} (d) d 2 x(t) dt2 (e) x(3t - 1) [for this part, first determine the period of x(3t - I)] 3.41. Suppose we are given the following information about a continuous-time periodic signal with period 3 and Fourier coefficients ak: 1. ak = ak+2· 2. ak = a-k· 3. J~ a55 x(t)dt = 1. 4. f x(t)dt = 2. Determine x(t). 262 Fourier Series Representation of Periodic Signals Chap.3 3.42. Let x(t) be a real-valued signal with fundamental period T and Fourier series coef- ficients ak· (a) Show that ak = a*_k and a0 must be real. (b) Show that if x(t) is even, then its Fourier series coefficients must be real and even. (c) Show that if x(t) is odd, then its Fourier series coefficients are imaginary and odd and ao = 0. (d) Show that the Fourier coefficients of the even part of x(t) are equal to ffi-e{ak}. (e) Show that the Fourier coefficients of the odd part of x(t) are equal to jdm{ak}· 3.43. (a) A continuous-time periodic signal x(t) with period Tis said to be odd hannonic if, in its Fourier series representation +x x(t) = L akejk(27TIT)t, (P3.43-1) k= -CG ak = 0 for every non -zero even integer k. (i) Show that if x(t) is odd harmonic, then x(t) = -x~ + ~). (P3.43-2) (ii) Show that if x(t) satisfies eq. (P3.43-2), then it is odd harmonic. (b) Suppose that x(t) is an odd-harmonic periodic signal with period 2 such that x(t) = t for 0 < t < 1. Sketch x(t) and find its Fourier series coefficients. (c) Analogously, to an odd-harmonic signal, we could define an even-harmonic signal as a signal for which ak = 0 fork odd in the representation in eq. (P3.43- 1). Could T be the fundamental period for such a signal? Explain your answer. (d) More generally, show that Tis the fundamental period of x(t) in eq. (P3.43-1) if one of two things happens: (1) Either a 1 or a-1 is nonzero; or (2) There are two integers k and l that have no common factors and are such that both ak and a, are nonzero. 3.44. Suppose we are given the following information about a signal x(t): 1. x(t) is a real signal. 2. x(t) is periodic with period T = 6 and has Fourier coefficients ak. 3. ak = 0 for k = 0 and k > 2. 4. x(t) = - x(t - 3). s. H!3 lx<t)l2 dt = 4. 6. a1 is a positive real number. Show that x(t) = A cos(Bt + C), and determine the values of the constants A, B, and C. Chap.3 Problems 263 3.45. Let x(t) be a real periodic signal with Fourier series representation given in the sine-cosine form of eq. (3.32); i.e., x(t) = ao + 2 L[Bk cos kwot - ck sin kwot]. (P3.45-1) k=I (a) Find the exponential Fourier series representation of the even and odd parts of x(t); that is, find the coefficients ak and f3k in terms of the coefficients in eq. (P3.45-1) so that +co Sv{x(t)} = L akeJkwot, k= -00 +co Od{x(t)} = L f3kejkwot. k= -00 (b) What is the relationship between a k and a-kin part (a)? What is the relationship between f3 k and f3- k? (c) Suppose that the signals x(t) and z(t) shown in Figure P3.45 have the sine-cosine series representations x(t) = a0 + 2 t~i[ Bk cos (2- 1T-kt) - Ck sm. (-21Tkt)~ 3 3- r z(t) = do+ 2 t~i[ Ek cos (2- 1T-kt) - Fk sm. (-21Tk-t)~ 3 3 'J' x(t) \ t -5 -3 -2 0 1 3 4 6 7 9 z(t) 2 -6 -4 -3 -1 2 34 5 67 8 9 -2 Figure P3.45 264 Fourier Series Representation of Periodic Signals Chap. 3 Sketch the signal y(t) = 4(a0 + d0 ) + 2 t~:Jr rB k+ )l. Ek 1c os (2-7T-kt) + F, sm. (2-7Tk-t)11j° 3 3 3.46 In this problem, we derive two important properties of the continuous-time Fourier series: the multiplication property and Parseval's relation. Let x(t) and y(t) both be continuous-time periodic signals having period T0 and with Fourier series represen- tations given by +x +x x(r) = 2= akejkw""', y(t) = 2= bkejkw""'. (P3.46-l) k=-x k= -x cos 20nt {a) X2{t) z(t) cos 20nt, / where z(t) is as in Figure P3.22(f) (b) -3 -1 (c) Figure P3.46 Chap.3 Problems 265 (a) Show that the Fourier series coefficients of the signal +oc z(t) = x(t)y(t) = L ckejkwot k= -00 are given by the discrete convolution +oo ck = 2:: anbk-n· n= -oo (b) Use the result of part (a) to compute the Fourier series coefficients of the signals x1 (t), x2(t), and x3(t) depicted in Figure P3.46. (c) Suppose that y(t) in eq. (P3.46-1) equals x*(t). Express the bk in the equation in terms of ak. and use the result of part (a) to prove Parseval's relation for periodic signals-that is, 3.47 Consider the signal x(t) = cos 27T't. Since x(t) is periodic with a fundamental period of 1, it is also periodic with a period of N, where N is any positive integer. What are the Fourier series coefficients of x(t) if we regard it as a periodic signal with period 3? 3.48. Let x[n] be a periodic sequence with period N and Fourier series representation x[n] = L akejk(27r!N)n_ (P3.48-1) k=<N> The Fourier series coefficients for each of the following signals can be expressed in terms of akin eq. (P3.48-l). Derive the expressions. (a) x[n - no] (b) x[n] - x[n - 1] (c) x[n] - x[n - ~] (assume that N is even) (d) x[n] + x[n + ~] (assume that N is even; note that this signal is periodic with period N/2) (e) x*[-n] (f) (- l)n x[n] (assume that N is even) (g) (- l)n x[n] (assume that N is odd; note that this signal is periodic with period 2N) (h) y[n] = { x[n], n even 0, n odd 3.49. Let x[n] be a periodic sequence with period N and Fourier series representation x[n] = L akejk(27TIN)n. (P3.49-1) k=<N> 266 Fourier Series Representation of Periodic Signals Chap. 3 (a) Suppose that N is even and that x[n] in eq. (P3.49-1) satisfies x[n] = - x [ n + ~] for all n. Show that ak = 0 for all even integers k. (b) Suppose that N is divisible by 4. Show that if x[n] = -x[n +~]for all n, then ak = 0 for every value of k that is a multiple of 4. (c) More generally, suppose that N is divisible by an integer M. Show that if (N/M)-1 [ N] ~ x n + r M = 0 for all n, then ak = 0 for every value of k that is a multiple of M. 3.50. Suppose we are given the following information about a periodic signal x[n] with period 8 and Fourier coefficients ak: 1. ak = -ak-4· 2. x[2n + 1] = (-l)ll. Sketch one period of x[n]. 3.51. Let x[n] be a periodic signal with period N = 8 and Fourier series coefficients ak = -ak-4· A signal y[n] = ( 1+(-l)ll) 2 x[n - 1] with period N = 8 is generated. Denoting the Fourier series coefficients of y[n] by bk. find a function J[k] such that 3.52. x[n] is a real periodic signal with period N and complex Fourier series coefficients ak. Let the Cartesian form for ak be denoted by where bk and ck are both real. (a) Show that a-k = a'k. What is the relation between bk and b-k? What is the relation between ck and c - k? (b) Suppose that N is even. Show that aN12 is real. Chap.3 Problems 267 (c) Show that x[n] can also be expressed as a trigonometric Fourier series of the form + (N~-l) / 2 (2 k ) (2 k ) x[n] = ao 2 bk cos : n - ck sin : n if N is odd or as (N-2l12 + + 2 ~ (2 k ) (2 k ) x[n] = (ao aN12(-lt) bk cos : n - ck sin : n if N is even. (d) Show that ifthe polar form of ak is Akejek, then the Fourier series representation for x[n] can also be written as (N-1)/2 ( 2 k ) x[n] = ao + 2 L Akcos _!!._!!_+Ok k=l N if N is odd or as if N is even. (e) Suppose that x[n] and z[n], as depicted in Figure P3.52, have the sine-cosine series representations x[n] 3 2 1 l -7 l 1 14 l n z[n] 3 2 n Figure P3.52 268 Fourier Series Representation of Periodic Signals Chap.3 x[n] = ao + 2 t~i1 bk cos (2- 77T-kn) - ck sm. (2-77T-kn)j , z[n] =do+ 2 t~i1 dkcos (2-77T-kn) - fksm. (2-77T-kn)j . Sketch the signal 3 y[n] = a0 - do+ 2 t;I d kcos (2- 77T-kn) +Uk - ck)sm. (2-7T7 -kn)j . 3.53. Let x[n] be a real periodic signal with period N and Fourier coefficients ak. (a) Show that if N is even, at least two of the Fourier coefficients within one period of ak are real. (b) Show that if N is odd, at least one of the Fourier coefficients within one period of a k is real. 3.54. Consider the function N-1 a[k] = L ej(21TIN)kn. n=O (a) Show that a[k] = N for k = 0, ±N, ±2N, ±3N, .... (b) Show that a[k] = 0 whenever k is not an integer multiple of N. (Hint: Use the finite sum formula.) (c) Repeat parts (a) and (b) if a[k] = L ej(21TIN)kn. n=<N> 3.55. Let x[n] be a periodic signal with fundamental period N and Fourier series coeffi- cients ak. In this problem, we derive the time-scaling property [ n ] -_ { x[ .!!. ], n = 0, ±m, ±2m, · · · X(m) m 0, elsewhere listed in Table 3.2. (a) Show that X(m)[n] has period of mN. (b) Show that if x[n] = v[n] + w[n], then Chap.3 Problems 269 (c) Assuming that x[n] = ej27rkon!N for some integer k0, verify that l m-1 X(m)[n] = - L ej27r(ko+lN)nl(mN)_ m l=O That is, one complex exponential in x[n] becomes a linear combination of m complex exponentials in X(m)[n]. (d) Using the results of parts (a), (b), and (c), show that if x[n] has the Fourier coefficients ak> then X(m)[n] must have the Fourier coefficients ~ak. 3.56. Let x[n] be a periodic signal with period N and Fourier coefficients ak. (a) Express the Fourier coefficients bk of Jx[nJl2 in terms of ak. (b) If the coefficients ak are real, is it guaranteed that the coefficients bk are also real? 3.57. (a) Let N-1 x[n] = L akejk(27r!N)n (P3.57-1) k=O and NL-1 y[n] = bkejk(27r/N)n k=O be periodic signals. Show that N-1 x[n]y[n] = L ckejk(2TrlN)n, k=O where N-1 N-1 ck = L azbk-l = L ak-1b1. l=O l=O (b) Generalize the result of part (a) by showing that ck = L azbk-l = L ak-1b1. l=<N> l=<N> (c ) Use the result of part (b) to find the Fourier series representation of the following signals, where x[n] is given by eq. (P3.57-l). (i) x[n]cos(6~n) (ii) x[n]L:::,:'_ 00 8[n - rN] (iii) x[n] (L:::,:°_ 008 [n - '~])(assume thatNis divisible by 3) (d) Find the Fourier series representation for the signal x[n]y[n], where x[n] = cos(rrn/3) 270 Fourier Series Representation of Periodic Signals Chap.3 and { o1: lnl :5 3 y[n] = 4 :5 lnl :5 6 is periodic with period 12. (e ) Use the result of part (b) to show that L x[n]y[n] = N L a1b-1, n=<N> l=<N> and from this expression, derive Parseval's relation for discrete-time periodic signals. 3.58. Let x[n] and y[n] be periodic signals with common period N, and let z[n] = L x[r]y[n - r] r=<N> be their periodic convolution. (a) Show that z[n] is also periodic with period N. (b) Verify that if ah bh and ck are the Fourier coefficients of x[n], y[n], and z[n], respectively, then (c) Let x[n] = sin (3~n) and [n] = { 1, 0 :5 n :5 3 y 0, 4 :5 n :5 7 be two signals that are periodic with period 8. Find the Fourier series represen- tation for the periodic convolution of these signals. (d) Repeat part (c) for the following two periodic signals that also have period 8: x[n] [ s.m (34'l Tn) = ' 0 <- n <- 3 ' 0, 4 :5 n :5 7 y[n] = Gro :5 n :5 7. 3.59. (a) Suppose x[n] is a periodic signal with period N. Show that the Fourier series coefficients of the periodic signal 00 g(t) = L x[k] 8(t - kT) k= -00 are periodic with period N. Chap.3 Problems 271 (b) Suppose that x(t) is a periodic signal with period T and Fourier series coeffi- cients ak with period N. Show that there must exist a periodic sequence g[n] such that 00 x(t) = L g[k] 5(t - kTIN). k= -00 (c) Can a continuous periodic signal have periodic Fourier coefficients? 3.60. Consider the following pairs of signals x[n] and y[n]. For each pair, determine whether there is a discrete-time LTI system for which y[n] is the output when the corresponding x[n] is the input. If such a system exists, determine whether the sys- tem is unique (i.e., whether there is more than one LTI system with the given input- output pair). Also, determine the frequency response of an LTI system with the desired behavior. If no such LTI system exists for a given x[n], y[n] pair, explain why. In In (a) x[n] = (2 ), y[n] = (4 ) (b) x[n] = <! n)u[n], y[n] = (~)nu[n] (c) x[n] = <! n)u[n], y[n] = 4nu[-n] (d) x[n] = ein!S, y[n] = 2ein!S (e) x[n] = ein18 u[n], y[n] = 2ein18 u[n] (t) x[n] = r. y[n] = 2jll(l - j) (g) x[n] = cos(1Tn/3),y[n] = cos(7Tn/3) + J3 sin(7Tn/3) (h) x[n] and YI [n] as in Figure P3.60 (i) x[n] and y2 [n] as in Figure P3.60 x[n] ••• 111 •••••••• ~llt ••••••••• tlt ••••••••• IJJ -12 0 12 24 n 111 ••• 111 ••• 111 ••• TTT ••• llt ••• TTT ••• Tll ••• -15 -9 -3 0 3 9 15 21 n Y2[n) •••••• Tlt •••••• JJT •••••• TJJ •••••• 111 •••••• -9 0 9 18 n Figure P3.60 3.61. As we have seen, the techniques of Fourier analysis are of value in examining continuous-time LTI systems because periodic complex exponentials are eigenfunc- tions for LTI systems. In this problem, we wish to substantiate the following state- ment: Although some LTI systems may have additional eigenfunctions, the complex exponentials are the only signals that are eigenfunctions of every LTI system. 272 Fourier Series Representation of Periodic Signals Chap.3 (a) What are the eigenfunctions of the LTI system with unit impulse response h(t) = B(t)? What are the associated eigenvalues? (b) Consider the LTI system with unit impulse response h(t) = B(t - T). Find a signal that is not of the form est, but that is an eigenfunction of the system with eigenvalue 1. Similarly, find the eigenfunctions with eigenvalues 1/2 and 2 that are not complex exponentials. (Hint: You can find impulse trains that meet these requirements.) (c) Consider a stable LTI system with impulse response h(t) that is real and even. Show that cos w t and sin w t are eigenfunctions of this system. (d) ConsidertheLTI system with impulse response h(t) = u(t). Suppose that cf>(t) is an eigenfunction of this system with eigenvalue A. Find the differential equation that cf>(t) must satisfy, and solve the equation. This result, together with those of parts (a) through (c), should prove the validity of the statement made at the beginning of the problem. 3.62. One technique for building a de power supply is to take an ac signal and full-wave rectify it. That is, we put the ac signal x(t) through a system that produces y(t) = lx(t)l as its output. (a) Sketch the input and output waveforms if x(t) = cost. What are the fundamen- tal periods of the input and output? (b) If x(t) = cost, determine the coefficients of the Fourier series for the output y(t). (c) What is the amplitude of the de component of the input signal? What is the amplitude of the de component of the output signal? 3.63. Suppose that a continuous-time periodic signal is the input to an LTI system. The signal has a Fourier series representation 00 x(t) = L alklejk(1TI4)r, k= -00 where a is a real number between 0 and 1, and the frequency response of the system is lwl $ W H(jw) = { 1' 0, lwi>W. How large must W be in order for the output of the system to have at least 90% of the average energy per period of x(t)? 3.64. As we have seen in this chapter, the concept of an eigenfunction is an extremely important tool in the study ofLTI systems. The same can be said for linear, but time- varying, systems. Specifically, consider such a system with input x(t) and output i y(t). We say that a signal t/J(t) is an eigenfunction of the system if l j cf>(t) ---+ Acf>(t). 'j 1 That is, if x(t) = cf>(t), then y(t) = At/J(t), where the complex constant A is called J the eigenvalue associated with t/J(.t). l Chap.3 Problems 273 (a) Suppose that we can represent the input x(t) to our system as a linear combina- tion of eigenfunctions cfJk(t), each of which has a corresponding eigenvalue Ak; that is, x(t) = L ckcfJk(t). k= -00 Express the output y(t) of the system in terms of {ck}, {c/Jk(t)}, and {Ak}. (b) Consider the system characterized by the differential equation - 2 d 2 () x(t) dx(t) y t - t ----;_jfl + t--;[(. Is this system linear? Is it time invariant? (c) Show that the functions are eigenfunctions of the system in part (b). For each cfJk(t), determine the cor- responding eigenvalue Ak. (d) Determine the output of the system if 1 x(t) = lOt- 10 + 3t + 2t4 + 7T. EXTENSION PROBLEMS 3.65. Two functions u(t) and v(t) are sraid to be orthogonal over the interval (a,b) if u(t)v*(t) dt = 0. (P3.65-1) If, in addition, rlu (t)l2 dt = 1 = rlv (t)l2 dt, the functions are said to be normalized and hence are called orthonormal. A set of functions {cfJk(t)} is called an orthogonal (orthonormal) set if each pair of functions in the set is orthogonal (orthonormal). (a) Consider the pairs of signals u(t) and v(t) depicted in Figure P3.65. Determine whether each pair is orthogonal over the interval (0, 4). (b) Are the functions sin mwot and sin nwot orthogonal over the interval (0, T), where T = 27Tiwo? Are they alsd orthonormal? (c) Repeat part (b) for the functions c/Jm(t) and c/Jn(t), where cfJk(t) = Jr [cos kwot + sin kw0t]. 274 Fourier Series Representation of Periodic Signals Chap. 3 u(t) v(t) 2 3 4 1 2 3 41 -1 -1 (a) Exponentials with Exponentials with time constant = 1 time constant = 1 l j 4 -3 (b) u(t) v(t) 4 -1 (c) u(t) v(t) 7T 1-----, 7T - - 2 3 4 1 2 3 4 (d) Figure P3.65 (d) Show that the functions cf>t(t) = eikwot are orthogonal over any interval of length T = 27rlwo. Are they orthonormal? (e) Let x(t) be an arbitrary signal, and let X0 (t) and Xe(t) be, respectively, the odd and even parts of x(t). Show that X0 (t) and Xe(t) are orthogonal over the interval ( -T, T) for any T. Chap. 3 Problems 275 (f) Show that if {<f>k(t)} is a set of orthogonal signals over the interval (a, b), then the set {(1/ jA~)<f>k(t)}, where Ak = f h l<f>k(t)l2 dt, a is orthonormal. (g) Let {</>;(t)} be a set of orthonormal signals on the interval (a, b), and consider a signal of the form x(t) = ~ a;<f>;(t), where the a; are complex constants. Show that rl x(t)l2 dt = ~ la;l2. l (h) Suppose that <f> 1( t), ... , <f>N(t) are nonzero only in the time interval 0 ::::: t ::::: T and that they are orthonormal over this time interval. Let L; denote the LTI system with impulse response hJt) = </>;(T - t). (P3.65-2) Show that if <f>J(t) is applied to this system, then the output at time T is 1 if i = j and 0 if i =P j. The system with impulse response given by eq. (P3.65-2) was referred to in Problems 2.66 and 2.67 as the matched filter for the signal </>;(t). 3.66. The purpose of this problem is to show that the representation of an arbitrary pe- riodic signal by a Fourier series or, more generally, as a linear combination of any set of orthogonal functions is computationally efficient and in fact very useful for obtaining good approximations of signals. 12 Specifically, let {<f>Jt)}, i = 0, ± 1, ±2, ... be a set of orthonormal functions on the interval a ::::: t ::::: b, and let x(t) be a given signal. Consider the follow- ing approximation of x(t) over the interval a ::::: t ::::: b: +N Xn(t) = ~ a;</>;(t). (P3.66-1) i~-N Here, the a; are (in general, complex) constants. To measure the deviation between x(t) and the series approximation xN(t), we consider the error eN(t) defined as (P3.66-2) A reasonable and widely used criterion for measuring the quality of the approxima- tion is the energy in the error signal over the interval of interest-that is, the integral 12See Problem 3.65 for the definitions of orthogonal and orthonormal functions. 276 Fourier Series Representation of Periodic Signals Chap. 3 of the square of the magnitude of the error over the interval a ::5 t ::5 b: (P3.66-3) (a) Show that E is minimized by choosing a; = rx (t)c/J;(t)dt. (P3.66-4) [Hint: Use eqs. (P3.66-1)-(P3.66-3) to express E in terms of a;, cl>i(t), and x(t). Then express a; in rectangular coordinates as a; = b; + jc;, and show that the equations aE aE . -ab = 0 and -ac; = 0, z = 0, :±: 1, :±:2, ... , N i are satisfied by the a; as given by eq. (P3.66-4).] (b) How does the result of part (a) change if and the {cP;(t)} are orthogonal but not orthonormal? (c) Let cPn(t) = ejnwot, and choose any interval of length To = 2nlw0 . Show that the a; that minimize E are as given in eq. (3.50). (d) The set of Walsh functions is an often-used set of orthonormal functions. (See Problem 2.66.) The set of five Walsh functions, cPo(t), cP1 (t), ... , cP4(t), is illus- trated in Figure P3.66, where we have scaled time so that the cP;(t) are nonzero and orthonormal over the interval 0 ::::; t ::5 1. Let x(t) = sin 7Tt. Find the ap- proximation of x(t) of the form 4 x(t) = ,L a;cP;(t) i=O such that is minimized. (e) Show that XN(t) in eq. (P3.66-1) and eN(t) in eq. (P3.66-2) are orthogonal if the ai are chosen as in eq. (P3.66-4). The results of parts (a) and (b) are extremely important in that they show that each coefficient ai is independent of all the other aj's, i :;i: j. Thtis, if we add more terms to the approximation [e.g., if we compute the approxi- mation .XN+t(t)], the coefficients of cPi(t), i = 1, .. . ,N, that were previously deterinined will not change. In contrast to ·this, consider another type of se- Chap. 3 Problems 277 <Po(t) (a) 1 2 -1 (b) 1 1 4 2 -1 (c) -1 (d) -1 (e) Figure P3.66 ries expansion, the polynomial Taylor series. The infinite Taylor series for et is et = 1 + t + t2/2! + ... , but as we shall show, when we consider a finite polynomial series and the error criterion of eq. (P3.66-3), we get a very different result. Specifically, let cf>o(t) = 1, </11( t) = t, </12(t) = t2, and so on. (f) Are the </Ji(t) orthogonal over the interval 0 s t s 1? 278 Fourier Series Representation of Periodic Signals Chap.3 (g) Consider an approximation of x(t) = e1 over the interval 0 s t s 1 of the form io(t) = aocf>o(t). Find the value of a0 that minimizes the energy in the error signal over the in- terval. (h) We now wish to approximate e1 by a Taylor series using two terms-i.e., i 1 (t) = a0 + a1t . Find the optimum values for ao and a1. [Hint: Compute E in terms of a0 and a 1, and then solve the simultaneous equations aE = 0 and aao Note that your answer for a0 has changed from its value in part (g), where there was only one term in the series. Further, as you increase the number of terms in the series, that coefficient and all others will continue to change. We can thus see the advantage to be gained in expanding a function using orthogonal terms.] 3.67 As we discussed in the text, the origins of Fourier analysis can be found in problems of mathematical physics. In particular, the work of Fourier was motivated by his investigation of heat diffusion. In this problem, we illustrate how the Fourier series enter into the investigation. 13 Consider the problem of determining the temperature at a given depth beneath the surface of the earth as a function of time, where we assume that the temperature at the surface is a given function of time T(t) that is periodic with period I. (The unit of time is one year.) Let T(x, t) denote the temperature at a depth x below the surface at time t. This function obeys the heat diffusion equation aT(x, t) ~k2 a2 T(x, t) at (P3.67-l) 2 ax2 with auxiliary condition T(O, t) = T(t). (P3.67-2) Here, k is the heat diffusion constant for the earth (k > 0). Suppose that we expand T(t) in a Fourier series: +x T(t) = L anejnZm. (P3.67-3) n= -:o Similarly, let us expand T(x, t) at any given depth x in a Fourier series in t. We obtain +oo T(x, t) = L bn(x)ejnZm, (P3.67- 4) n= -oc where the Fourier coefficients bn(x} depend upon the depth x. 13The problem has been adapted from A. Sommerfeld, Partial Differential Equations in Physics (Nt York: Academic Press, 1949), pp 68-71. Chap. 3 Problems 279 (a) Use eqs. (P3.67-1)-(P3.67-4) to show that h11 (x) satisfies the differential equa- tion 47T""jn ---yzr-b""(x) (P3.67-5a) with auxiliary condition (P3.67-5b) Since eq. (P3.67-5a) is a second-order equation, we need a second auxiliary condition. We argue on physical grounds that, far below the earth's surface, the variations in temperature due to surface fluctuations should disappear. That is, lim T(x, t) = a constant. (P3.67-5c) x~x (b) Show that the solution of eqs. (P3.67-5) is bn(x) = [ an exp[- J27Tinl(l + j)xl k], n 2:: 0 an exp[- J27Tinl(l - j)xlk], n s 0 (c) Thus, the temperature oscillations at depth x are damped and phase-shifted ver- sions of the temperature oscillations at the surface. To see this more clearly, let T(t) = ao + a, sin 2m (so that a0 represents the mean yearly temperature). Sketch T(t) and T(x, t) over a one-year period for a0 = 2, and a 1 = 1. Note that at this depth not only are the temperature os- cillations significantly damped, but the phase shift is such that it is warmest in winter and coldest in summer. This is exactly the reason why vegetable cellars are constructed! 3.68. Consider the closed contour shown in Figure P3.68. As illustrated, we can view this curve as being traced out by the tip of a rotating vector of varying length. Let r(O) denote the length ofthe vector as a function of the angle (J. Then r( (J) is periodic in (J with period 27T and thus has a Fourier series representation. Let {ad denote the Fourier coefficients of r(O). (a) Consider now the projection x(0 ) of the vector r(0 ) onto the x-axis, as indicated in the figure. Determine the Fourier coefficients for x(O) in terms of the ak's. (b) Consider the sequence of coefficients bk = akejk7rt4. Sketch the figure in the plane that corresponds to this set of coefficients. 280 Fourier Series Representation of Periodic Signals Chap. 3 -1 -1 Figure P3.68 (c) Repeat part (b) with (d) Sketch figures in the plane such that r(O) is not constant, but does have each of the following properties: (i) r(O) is even. (ii) The fundamental period of r(O) is 7T. (iii) The fundamental period of r(O) is 7T/2. 3.69. In this problem, we consider the discrete-time counterpart of the concepts introduced in Problems 3.65 and 3.66. In analogy with the continuous-time case, two discrete- time signals <Pdn] and <Pm[n] are said to be orthogonal over the interval (N1, N 2) if k = m (P3.69-1) k# m · If the value of the constants Ak and Am are both 1, then the signals are said to be orthonormal. (a) Consider the signals ¢k[n] = o[n - k], k = 0, :':: l, :'::2, ... ' ±N. Show that these signals are orthonormal over the interval (-N, N). (b) Show that the signals cPk[n] = eik(21TIN)n, k = 0, 1, ... , N- 1, are orthogonal over ~y interval of length N. (c) Showthatif · M x[n] = ,La;cf>;[n], i=l Chap. 3 Problems 281 where the cf>i[n] are orthogonal over the interval (N1, N 2 ), then N? M ::;2 lx[n]i2 = L lail2 Ai. i= I (d) Let cf>i[n], i = 0, 1, ... , M, be a set of orthogonal functions over the interval (N1, N2), and let x[n] be a given signal. Suppose that we wish to approximate x[n] as a linear combination of the cf>i[n]; that is, M i[n] = L aicf>i[n], i=O where the ai are constant coefficients. Let e[n] = x[n] - i[n], and show that if we wish to minimize N, E = ::;2 le[n]i2, n=N1 then the ai are given by (P3.69-2) [Hint: As in Problem 3.66, express E in terms of ai, cf>i[n], Ai, and x[n], write ai = bi + }ci, and show that the equations aE - = 0 and abi are satisfied by the ai given by eq. (P3.69-2). Note that applying this result when the cf>i[n] are as in part (b) yields eq. (3.95) for ak.] (e) Apply the result of part (d) when the cf>i[n] are as in part (a) to determine the coefficients ai in terms of x[n]. 3.70. (a) In this problem, we consider the definition of the two-dimensional Fourier se- ries for periodic signals with two independent variables. Specifically, consider a signal x(t1, t2) that satisfies the equation This signal is periodic with period T1 in the t 1 direction and with period T2 in the t2 direction. Such a signal has a series representation of the form +oo +oo x(t], t2) = L, L, amnej<I1UJ),t, +1Ut.lztz), n= -oo m= -oo 282 Fourier Series Representation of Periodic Signals Chap.3 where Wt = 2w1Tt, w2 = 2w!T2. Find an expression for amn in terms of x(tt, t2). (b) Determine the Fourier series coefficients amn for the following signals: (i) cos(27ftt + 2t2) (ii) the signal illustrated in Figure P3.70 x(t1 t2) = 1 in shaded areas and · 0 elsewhere [] [··· 3T2/2 r-~-' T2 f---...., D [··· T2/2 f---""'-' -3T,f2 - T,J2 T,/2 3T1/2 --~r----.~~~----1-~~~----~--~-----.-- t1 J D [ Figure P3.70 3.71. Consider the mechanical system shown in Figure P3.71. The differential equation relating velocity v(t) and the input force f(t) is given by Bv(t) + K I v(t) dt = j(t). v(t) ~ f(t) B Figure P3.71 Chap. 3 Problems 283 (a) Assuming that the output is f,(t), the compressive force acting on the spring, write the differential equation relating f,(t) and f(t). Obtain the frequency re- sponse of the system, and argue that it approximates that of a lowpass filter. (b) Assuming that the output is /J(t), the compressive force acting on the dash- pot, write the differential equation relating /J(t) and f(t). Obtain the frequency response of the system, and argue that it approximates that of a highpass filter. 4 THE CONTINUOUS-TIME FOURIER TRANSFORM 4.0 INTRODUCTION In Chapter 3, we developed a representation of periodic signals as linear combinations of complex exponentials. We also saw how this representation can be used in describing the effect of LTI systems on signals. In this and the following chapter, we extend these concepts to apply to signals that are not periodic. As we will see, a rather large class of signals, including all signals with finite energy, can also be represented through a linear combination of complex exponentials. Whereas for periodic signals the complex exponential building blocks are harmonically related, for aperiodic signals they are infinitesimally close in frequency, and the represen- tation in terms of a linear combination takes the form of an integral rather than a sum. The resulting spectrum of coefficients in this representation is called the Fourier transform, and the synthesis integral itself, which uses these coefficients to represent the signal as a linear combination of complex exponentials, is called the inverse Fourier transform. The development of this representation for aperiodic signals in continuous time is one of Fourier's most important contributions, and our development of the Fourier trans- form follows very closely the approach he used in his original work. In particular, Fourier reasoned that an aperiodic signal can be viewed as a periodic signal with an infinite pe- riod. More precisely, in the Fourier series representation of a periodic signal, as the period increases the fundamentfll frequency decreases and the harmonically related components become closer in frequency. As the period becomes infinite, the frequency components form a continuum and the Fourier series sum becomes an integral. In the next section we develop the Fourier series representation for continuous-time periodic signals, and in the sections that follow we build on this foundation as we explore many of the important 284"
4 The Continuous-Time Fourier Transform,"4 THE CONTINUOUS-TIME FOURIER TRANSFORM 4.0 INTRODUCTION In Chapter 3, we developed a representation of periodic signals as linear combinations of complex exponentials. We also saw how this representation can be used in describing the effect of LTI systems on signals. In this and the following chapter, we extend these concepts to apply to signals that are not periodic. As we will see, a rather large class of signals, including all signals with finite energy, can also be represented through a linear combination of complex exponentials. Whereas for periodic signals the complex exponential building blocks are harmonically related, for aperiodic signals they are infinitesimally close in frequency, and the represen- tation in terms of a linear combination takes the form of an integral rather than a sum. The resulting spectrum of coefficients in this representation is called the Fourier transform, and the synthesis integral itself, which uses these coefficients to represent the signal as a linear combination of complex exponentials, is called the inverse Fourier transform. The development of this representation for aperiodic signals in continuous time is one of Fourier's most important contributions, and our development of the Fourier trans- form follows very closely the approach he used in his original work. In particular, Fourier reasoned that an aperiodic signal can be viewed as a periodic signal with an infinite pe- riod. More precisely, in the Fourier series representation of a periodic signal, as the period increases the fundamentfll frequency decreases and the harmonically related components become closer in frequency. As the period becomes infinite, the frequency components form a continuum and the Fourier series sum becomes an integral. In the next section we develop the Fourier series representation for continuous-time periodic signals, and in the sections that follow we build on this foundation as we explore many of the important 284"
4.0 Introduction,"4 THE CONTINUOUS-TIME FOURIER TRANSFORM 4.0 INTRODUCTION In Chapter 3, we developed a representation of periodic signals as linear combinations of complex exponentials. We also saw how this representation can be used in describing the effect of LTI systems on signals. In this and the following chapter, we extend these concepts to apply to signals that are not periodic. As we will see, a rather large class of signals, including all signals with finite energy, can also be represented through a linear combination of complex exponentials. Whereas for periodic signals the complex exponential building blocks are harmonically related, for aperiodic signals they are infinitesimally close in frequency, and the represen- tation in terms of a linear combination takes the form of an integral rather than a sum. The resulting spectrum of coefficients in this representation is called the Fourier transform, and the synthesis integral itself, which uses these coefficients to represent the signal as a linear combination of complex exponentials, is called the inverse Fourier transform. The development of this representation for aperiodic signals in continuous time is one of Fourier's most important contributions, and our development of the Fourier trans- form follows very closely the approach he used in his original work. In particular, Fourier reasoned that an aperiodic signal can be viewed as a periodic signal with an infinite pe- riod. More precisely, in the Fourier series representation of a periodic signal, as the period increases the fundamentfll frequency decreases and the harmonically related components become closer in frequency. As the period becomes infinite, the frequency components form a continuum and the Fourier series sum becomes an integral. In the next section we develop the Fourier series representation for continuous-time periodic signals, and in the sections that follow we build on this foundation as we explore many of the important 284 Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 285 properties of the continuous-time Fourier transform that form the foundation of frequency- domain methods for continuous-time signals and systems. In Chapter 5, we parallel this development for discrete-time signals. 4. 1 REPRESENTATION OF APERIODIC SIGNALS: THE CONTINUOUS-TIME FOURIER TRANSFORM 4. 1 . 1 Development of the Fourier Transform Representation of an Aperiodic Signal To gain some insight into the nature of the Fourier transform representation, we begin by revisiting the Fourier series representation for the continuous-time periodic square wave examined in Example 3.5. Specifically, over one period, 6: JtJ < T1 x(t) = { T1 < JtJ < T/2 and periodically repeats with period T, as shown in Figure 4.1. As determined in Example 3.5, the Fourier series coefficients ak for this square wave are 2 sin(kwoTI) [eq. (3.44)] (4.1) kwoT where w 0 = 211""/T. In Figure 3.7, bar graphs ofthese coefficients were shown for a fixed value of T 1 and several different values ofT. An alternative way of interpreting eq. (4.1) is as samples of an envelope function, specifically, T ak -_ 2sinwT11 . (4.2) W w=kw0 That is, with w thought of as a continuous variable, the function (2 sin wT1 )lw represents the envelope of Tab and the coefficients ak are simply equally spaced samples of this envelope. Also, for fixed T 1, the envelope of Tak is independent ofT. In Figure 4.2, we again show the Fourier series coefficients for the periodic square wave, but this time as samples of the envelope of Tab as specified in eq. (4.2). From the figure, we see that as x(t) ... J 1.1 1.1 ~ 1.1 1.1 [ ... I I -2T -T _.I -T1 T1 T T 2T 2 2 Figure 4. 1 A continuous-time periodic square wave."
4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform,"Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 285 properties of the continuous-time Fourier transform that form the foundation of frequency- domain methods for continuous-time signals and systems. In Chapter 5, we parallel this development for discrete-time signals. 4. 1 REPRESENTATION OF APERIODIC SIGNALS: THE CONTINUOUS-TIME FOURIER TRANSFORM 4. 1 . 1 Development of the Fourier Transform Representation of an Aperiodic Signal To gain some insight into the nature of the Fourier transform representation, we begin by revisiting the Fourier series representation for the continuous-time periodic square wave examined in Example 3.5. Specifically, over one period, 6: JtJ < T1 x(t) = { T1 < JtJ < T/2 and periodically repeats with period T, as shown in Figure 4.1. As determined in Example 3.5, the Fourier series coefficients ak for this square wave are 2 sin(kwoTI) [eq. (3.44)] (4.1) kwoT where w 0 = 211""/T. In Figure 3.7, bar graphs ofthese coefficients were shown for a fixed value of T 1 and several different values ofT. An alternative way of interpreting eq. (4.1) is as samples of an envelope function, specifically, T ak -_ 2sinwT11 . (4.2) W w=kw0 That is, with w thought of as a continuous variable, the function (2 sin wT1 )lw represents the envelope of Tab and the coefficients ak are simply equally spaced samples of this envelope. Also, for fixed T 1, the envelope of Tak is independent ofT. In Figure 4.2, we again show the Fourier series coefficients for the periodic square wave, but this time as samples of the envelope of Tab as specified in eq. (4.2). From the figure, we see that as x(t) ... J 1.1 1.1 ~ 1.1 1.1 [ ... I I -2T -T _.I -T1 T1 T T 2T 2 2 Figure 4. 1 A continuous-time periodic square wave. 286 The Continuous-Time Fourier Transform Chap.4 (a) (b) Figure 4.2 The Fourier series co- efficients and their envelope for the periodic square wave in Figure 4.1 for several values of T (with T; fixed): (a) T = 47;; (b) T = 8T1: (c) T = 16T1. T increases, or equivalently, as the fundamental frequency w 0 = 27r/T decreases, the envelope is sampled with a closer and closer spacing. As T becomes arbitrarily large, the original periodic square wave approaches a rectangular pulse (i.e., all that remains in the time domain is an aperiodic signal corresponding to one period of the square wave). Also, the Fourier series coefficients, multiplied by T, become more and more closely spaced samples of the envelope, so that in some sense (which we will specify shortly) the set of Fourier series coefficients approaches the envelope function as T --7 oo. This example illustrates the basic idea behind Fourier's development of a represen- tation for aperiodic signals. Specifically, we think of an aperiodic signal as the limit of a periodic signal as the period becomes arbitrarily large, and we examine the limiting be- . havior of the Fourier series representation for this signal. In particular, consider a signal x(t) that is of finite duration. That is, for some number Tt. x(t) = 0 if ltl > T1, as illus- trated in Figure 4.3(a). From this aperiodic signal, we can construct a periodic signal i(t) for which x(t) is one period, as indicated in Figure 4.3(b). As we choose the period T to"" be larger, i(t) is identical to x(t) over a longer interval, and as T ---+ oo, i(t) is equal to x(t) for any finite value oft. Let us now examine the effect of this on the Fourier series representation of i(t). Rewriting eqs. (3.38) and (3.39) here for convenience, with the integral in eq. (3.39) Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 287 x(t) 0b -T1 T1 (a) -2T -T -T1 0 T1 T 2T (b) Figure 4.3 (a) Aperiodic signal x(t); (b) periodic signal x(t), constructed to be equal to x(t) over one period. carried out over the interval - T 12 :5 t :5 T 12, we have +oo x<r) = :L akejkwot, (4.3) k= -oc ak = -1 IT/2 . i(t)e-Jkwotdt, (4.4) T -T/2 where w 0 = 27TIT. Since i(t) = x(t) for ltl < T/2, and also, since x(t) = 0 outside this interval, eq. (4.4) can be rewritten as 1 IT /2 . } I+ oo . ak = - x(t)e-Jkwotdt = - x(t)e-Jkwotdt. T -T/2 T -00 Therefore, defining the envelope X(jw) of Tak as X(jw) = L+oooo x(t)e-jwtdt, (4.5) we have, for the coefficients ako ak = ~X(j kwo). (4.6) Combining eqs. (4.6) and (4.3), we can express i(t) in terms of X(jw) as +co 1 . i(t) = L TX(jkwo)elkcoot, k=-co or equivalently, since 27TIT = w0, 1 +co . i(t) = 27T L X(jkw0 )eJkcootw0• (4.7) k=-CO 288 The Continuous-Time Fourier Transform Chap.4 As T ----? oo, i(t) approaches x(t), and consequently, in the limit eq. (4.7) becomes a rep- resentation of x(t). Furthermore, w 0 ----? 0 as T ----? oo, and the right-hand side of eq. (4.7) passes to an integral. This can be seen by considering the graphical interpretation of the equation, illustrated in Figure 4.4. Each term in the summation on the right-hand side is the area of a rectangle of height X(j kw0 )eikwot and width w0 . (Here, tis regarded as fixed.) As w0 ----? 0, the summation converges to the integral of X(jw )eiwt. Therefore, using the fact that i(t) ----? x(t) as T ----7 oo, we see that eqs. (4.7) and (4.5) respectively become 1 f+x . x(t) = 27T -oo X(jw )e1w1d w (4.8) and + X(jw) = I- oc x x(t)e-Jwtdt. (4.9) X(jw)eiwt (k + 1)w0 Figure 4.4 Graphical interpretation w of eq. (4.7). Equations (4.8) and (4.9) are referred to as the Fourier transform pair, with the func- tion X(jw) referred to as the Fourier Transform or Fourier integral of x(t) and eq. (4.8) as the inverse Fourier transform equation. The synthesis equation (4.8) plays a role for aperiodic signals similar to that of eq. (3.38) for periodic signals, since both represent a signal as a linear combination of complex exponentials. For periodic signals, these com- plex exponentials have amplitudes {ak}, as given by eq. (3.39), and occur at a discrete set of harmonically related frequencies kw0, k = 0, .± 1, ±2, .... For aperiodic signals, the complex exponentials occur at a continuum of frequencies and, according to the synthesis equation (4.8), have ""amplitude"" X(jw )(dw/27T). In analogy with the terminology used for the Fourier series coefficients of a periodic signal, the transform X(jw) of an aperiodic signal x(t) is commonly referred to as the spectrum of x(t), as it provides us with the in- formation needed for describing x(t) as a linear combination (specifically, an integral) of sinusoidal signals at different frequencies. Based on the above development, or equivalently on a comparison of eq. (4.9) and eq. (3.39), we also note that the Fourier coefficients ak of a periodic signal i(t) can be expressed in terms of equally spaced samples of the Fourier transform of one period of i(t). Specifically, suppose that i(t) is a periodic signal with period T and Fourier coefficients Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 289 ak. Let x(t) be a finite-duration signal that is equal to i(t) over exactly one period-say, for s ::s: t ::s: s + T for some value of s-and that is zero otherwise. Then, since eq. (3.39) allows us to compute the Fourier coefficients of i(t) by integrating over any period, we can write Since x(t) is zero outside the range s ::s: t ::s: s + T we can equivalently write Comparing with eq. (4.9) we conclude that ak = ~X(jw)l , (4.10) w=kwo where X(jw) is the Fourier transform of x(t). Equation 4.10 states that the Fourier coef- ficients of i(t) are proportional to samples of the Fourier transform of one period of i(t). This fact, which is often of use in practice, is examined further in Problem 4.37. 4.1.2 Convergence of Fourier Transforms Although the argument we used in deriving the Fourier transform pair assumed that x(t) was of arbitrary but finite duration, eqs. (4.8) and (4.9) remain valid for an extremely broad class of signals of infinite duration. In fact, our derivation of the Fourier transform suggests that a set of conditions like those required for the convergence of Fourier series should also apply here, and indeed, that can be shown to be the case. 1 Specifically, consider X(jw) evaluated according to eq. (4.9), and let x(t) denote the signal obtained by using X(jw) in the right-hand side of eq. (4.8). That is, = -12 I+ oc . x(t) X(jw )eJWt dw. 7T -ac What we would like to know is when eq. (4.8) is valid [i.e., when is x(t) a valid represen- tation of the original signal x(t)?]. If x(t) has finite energy, i.e., if it is square integrable, so that (4.11) then we are guaranteed that X(jw) is finite [i.e., eq. (4.9) converges] and that, with e(t) denoting the error between x(t) and x(t) [i.e., e(t) = x(t) - x(t)], 1F or a mathematically rigorous discussion of the Fourier transform and its properties and applications, seeR. Bracewell, The Fourier Transform and Its Applications, 2nd ed. (New York: McGraw-Hill Book Com- pany, 1986); A. Papoulis, The Fourier Integral and Its Applications (New York: McGraw-Hill Book Company, 1987); E. C. Titchmarsh, Introduction to the Theory ofF ourier Integrals (Oxford: Clarendon Press, 1948); and the book by Dym and McKean referenced in footnote 2 of Chapter 3. · 290 The Continuous-Time Fourier Transform Chap. 4 (4.12) Equations (4.11) and (4.12) are the aperiodic counterparts of eqs. (3.51) and (3.54) for periodic signals. Thus, in a manner similar to that for periodic signals, if x(t) has finite energy, then, although x(t) and its Fourier representation x(t) may differ significantly at individual values oft, there is no energy in their difference. Just as with periodic signals, there is an alternative set of conditions which are suffi- cient to ensure that i(t) is equal to x(t) for any t except at a discontinuity, where it is equal to the average of the values on either side of the discontinuity. These conditions, again referred to as the Dirichlet conditions, require that: 1. x(t) be absolutely integrable; that is, J_+xx lx(t)ldt < rx. (4 .13) 2. x(t) have a finite number of maxima and minima within any finite interval. 3. x(t) have a finite number of discontinuities within any finite interval. Futhermore, each of these discontinuities must be finite. Therefore, absolutely integrable signals that are continuous or that have a finite number of discontinuities have Fourier transforms. Although the two alternative sets of conditions that we have given are sufficient to guarantee that a signal has a Fourier transform, we will see in the next section that peri- odic signals, which are neither absolutely integrable nor square integrable over an infinite interval, can be considered to have Fourier transforms if impulse functions are permitted in the transform. This has the advantage that the Fourier series and Fourier transform can be incorporated into a common framework, which we will find to be very convenient in subsequent chapters. Before examining the point further in Section 4.2, however, let us consider several examples of the Fourier transform. 4. 1 .3 Examples of Continuous-Time Fourier Transforms Example 4.1 Consider the signal x(t) = e-""1 u(t) a> 0. From eq. (4.9), X(jw) = {""' e-ate-i""''dt = ___1_ _ -e-<a+jw)tl""'. Jo a+ JW o That is, X(jw) = - 1-.-, a > 0. a+ JW Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 291 Since this Fourier transform is complex valued, to plot it as a function of w, we express X(jw) in terms of its magnitude and phase: IX(jw)i= 1 , <r.X(jw)=-tan~ 1 (~a)· Ja2 + w2 Each of these components is sketched in Figure 4.5. Note that if a is complex rather than real, then x(t) is absolutely integrable as long as (Jl.e{a} > 0, and in this case the preceding calculation yields the same form for X(jw ). That is, X(jw) = - 1-.-, <Re{a} > 0. a+ JW IX(jw)l 1/a -a a w (a) <J:X(jw) TI/2 w (b) Figure 4.5 Fourier transform of the signal x(t) = e~at u(t), a> 0, consid- ered in Example 4.1. Example 4.2 Let x(t) = e-aftl, a> 0. 292 The Continuous-Time Fourier Transform Chap.4 This signal is sketched in Figure 4.6. The Fourier transform of the signal is 1 1 ---+--- a- jw a+ jw 2a a2 + w2"" In this case X(jw) is real, and it is illustrated in Figure 4.7. x(t) Figure 4.6 Signal x(t) = e-altl of Example 4.2. XGw) 2/a Figure 4.7 Fourier transform of the signal considered in Example 4.2 and depicted in Figure 4.6. Example 4.3 Now let us determine the Fourier transform of the unit impulse x(t) = 8(t). Substituting into eq. (4.9) yields +oo X(jw) = f-oo 8(t)e-i<»tdt = 1. That is, the unit impulse bas a Fourier transform consisting of equal contributions at frequencies. Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 293 Example 4.4 Consider the rectangular pulse signal { o1: ltl < Tt x(t) = ltl > T1 ' (4.16) as shown in Figure 4.8(a). Applying eq. (4 .9), we find that the Fourier transform of this signal is TI . T X( J.W ) -- f e - jwt d t-- 2 -Sil-l W- ,l (4.17) -TI W as sketched in Figure 4.8(b). x(t) I II X(jw) Figure 4.8 (a) The rectangular pulse signal of Example 4.4 and (b) its Fourier transform. As we discussed at the beginning of this section, the signal given by eq. (4.16) can be thought of as the limiting form of a periodic square wave as the period becomes arbitrarily large. Therefore, we might expect that the convergence of the synthesis equation for this signal would behave in a manner similar to that observed in Example 3.5 for the square wave. This is, in fact, the case. Specifically, consider the inverse Fourier transform for the rectangular pulse signal: xA (t ) -_ -1 I+ co 2 sinwT, e Jwtd w. 21T -co W Then, since x(t) is square integrable, 294 The Continuous-Time Fourier Transform Chap.4 +oc 2 J-o o lx(t) - .X(t)l dt = 0. Furthermore, because x(t) satisfies the Dirichlet conditions, x(t) = x(t), except at the points of discontinuity, t = ±T1, where x(t) converges to 112, which is the average of the values of x(t) on both sides of the discontinuity. In addition, the convergence of x(t) to x(t) exhibits the Gibbs phenomenon, much as was illustrated for the periodic square wave in Figure 3.9. Specifically, in analogy with the finite Fourier series approximation, eq. (3.47), consider the following integral over a finite-length interval of frequencies: _I_ Jw 2sinwTt eiwt dw. 271' -w w As W ~ oo, this signal converges to x(t) everywhere, except at the discontinuities. More- over, the signal exhibits ripples near the discontinuities. The peak amplitude of these rip- ples does not decrease as W increases, although the ripples do become compressed toward the discontinuity, and the energy in the ripples converges to zero. Example 4.5 Consider the signal x(t) whose Fourier transform is X(1' w)={l, iwi<W. 0, lwl (4.18) > W This transform is illustrated in Figure 4.9(a). Using the synthesis equation (4.8), we can X(jw) I ,11 -w w w (a) x(t) --rr/W -rr/W (b) Figure 4. 9 Fourier transform pair of Example 4.5: (a) Fourier transform for Example 4.5 and (b) the corresponding time function. Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 295 then determine x(t ) -_ -1 f w e Jwtd w_ -si-n W- t (4.19) 27T -w 7Tt ' which is depicted in Figure 4.9(b). Comparing Figures 4.8 and 4.9 or, equivalently, eqs. (4.16) and (4.17) with eqs. (4 .18) and (4 .19), we see an interesting relationship. In each case, the Fourier transform pair consists of a function of the form (sin a8)/b8 and a rectangular pulse. However, in Example 4.4, it is the signal x(t) that is a pulse, while in Example 4.5, it is the transform X(jw ). The special relationship that is apparent here is a direct consequence of the duality property for Fourier transforms, which we discuss in detail in Section 4.3.6. Functions of the form given in eqs. (4.17) and (4.19) arise frequently in Fourier analysis and in the study of LTI systems and are referred to as sine functions. A commonly used precise form for the sine function is . (O) smc = -s-i-n- -1;;ro8 . (4.20) The sine function is plotted in Figure 4.1 0. Both of the signals in eqs. (4 .17) and (4 .19) can be expressed in terms of the sine function: 2 sinwT1 = 2T1 sm. c (w-T-1) w 7T sin Wt = W . (Wt) 7T smc ---;: . 7Tt sinc(O) Figure 4. 1 0 The sine function. Finally, we can gain insight into one other property of the Fourier transform by examining Figure 4.9, which we have redrawn as Figure 4.11 for several different values of W. From this figure, we see that as W increases, X(jw) becomes broader, while the main peak of x(t) at t = 0 becomes higher and the width of the first lobe of this sig- nal (i.e., the part of the signal for ltl < 1riW) becomes narrower. In fact, in the limit as W ~ oo, X(jw) = 1 for all w, and consequently, from Example 4.3, we see that x(t) in eq. (4.19) converges to an impulse as W ~ oo. The behavior depicted in Figure 4.11 is an example of the inverse relationship that exists between the time and frequency domains, 296 The Continuous-Time Fourier Transform Chap.4 X1(jw) X2(jw) 11 11 I I -w1 w1 w -w2 w2 w (a) (b) X3(jw) rn -w3 w3 w (c) Figure 4. 11 Fourier transform pair of Figure 4.9 for several different values of W. and we can see a similar effect in Figure 4.8, where an increase in T1 broadens x(t) but makes X(jw) narrower. In Section 4.3.5, we provide an explanation of this behavior in the context of the scaling property of the Fourier transform. 4.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS In the preceding section, we introduced the Fourier transform representation and gave several examples. While our attention in that section was focused on aperiodic signals, we can also develop Fourier transform representations for periodic signals, thus allowing us to"
4.2 The Fourier Transform for Periodic Signals,"296 The Continuous-Time Fourier Transform Chap.4 X1(jw) X2(jw) 11 11 I I -w1 w1 w -w2 w2 w (a) (b) X3(jw) rn -w3 w3 w (c) Figure 4. 11 Fourier transform pair of Figure 4.9 for several different values of W. and we can see a similar effect in Figure 4.8, where an increase in T1 broadens x(t) but makes X(jw) narrower. In Section 4.3.5, we provide an explanation of this behavior in the context of the scaling property of the Fourier transform. 4.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS In the preceding section, we introduced the Fourier transform representation and gave several examples. While our attention in that section was focused on aperiodic signals, we can also develop Fourier transform representations for periodic signals, thus allowing us to Sec. 4.2 The Fourier Transform for Periodic Signals 297 consider both periodic and aperiodic signals within a unified context. In fact, as we will see, we can construct the Fourier transform of a periodic signal directly from its Fourier series representation. The resulting transform consists of a train of impulses in the frequency domain, with the areas of the impulses proportional to the Fourier series coefficients. This will turn out to be a very useful representation. To suggest the general result, let us consider a signal x(t) with Fourier transform X(jw) that is a single impulse of area 27T at w = w0 ; that is, X(jw) = 27T8(w - wo). (4.21) To determine the signal x(t) for which this is the Fourier transform, we can apply the inverse transform relation, eq. (4.8), to obtain 1 f+x x(t) = - 27T8(w - wo)eJwt dw 2 7T -X More generally, if X (jw) is of the form of a linear combination of impulses equally spaced in frequency, that is, +x X(jw) = ~ 27Tak8(w - kwo), (4.22) k=-010 then the application of eq. (4.8) yields +x x(t) = ~ akejkwot (4.23) k= -00 We see that eq. (4.23) corresponds exactly to the Fourier series representation of a periodic signal, as specified by eq. (3.38). Thus, the Fouri~f transform of a periodic signal with Fourier series coefficients {ak} can be interpr~ted as a train of impulses occurring at the harmonically related frequencies and for which the area of the impulse at the kth harmonic frequency kw 0 is 27T times the kth Fourier series coefficient ak. Example 4.6 Consider again the square wave illustrated in Figure 4.1. The Fourier series coefficients for this signal are and the Fourier transform of the signal is X(jw) = f 2 sin ~woTI 8(w - kwo), k=-X 298 The Continuous-Time Fourier Transform Chap.4 which is sketched in Figure 4.12 forT = 4T1• In comparison with Figure 3.7(a), the only differences are a proportionality factor of 27T and the use of impulses rather than a bar graph. XOw) ,.,1T / \ I \ I \ I I I I 2 I I 2 Figure 4.12 Fourier transform of a symmetric periodic square wave. Example 4.7 Let x(t) = sin wot. The Fourier series coefficients for this signal are 2j' k # 1 or -1. Thus, the Fourier transform is as shown in Figure 4.13(a). Similarly, for x(t) = cos wot, the Fourier series coefficients are k # 1 or -1. The Fourier transform of this signal is depicted in Figure 4.13(b). These two transforms will be of considerable importance when we analyze sinusoidal modulation systems in Chapter 8. Sec. 4.2 The Fourier Transform for Periodic Signals 299 XO w) 1Tij 0 w (a) XO w) 1T 1T t I t 0 w (b) Figure 4.13 Fourier transforms of (a) x(t) = sin wat; (b) x(t) = cos wat. Example 4.8 A signal that we will find extremely useful in our analysis of sampling systems in Chap- ter 7 is the impulse train +oc x(t) = L 8(t - kT), k=-00 which is periodic with period T, as indicated in Figure 4.14(a). The Fourier series coef- ficients for this signal were computed in Example 3.8 and are given by 1 I+ T/2 . 1 ak = - 8(t)e- Jkwot dt = -. T -T/2 T That is, every Fourier coefficient of the periodic impulse train has the same value, liT. Substituting this value for akin eq. (4.22) yields X(jw) ~ 2; k~oo I+- 2;k). Thus, the Fourier transform of a periodic impulse train in the time domain with pe- riod Tis a periodic impulse train in the frequency domain with period 27T/T, as sketched in Figure 4.14(b ). Here again, we see an illustration of the inverse relationship between the time and the frequency domains. As the spacing between the impulses in the time domain (i.e., the period) gets longer, the spacing between the impulses in the frequency domain (namely, the fundamental frequency) gets smaller. 300 The Continuous-Time Fourier Transform Chap.4 x(t) ... 1 1 1! 1 1 -2T -T 0 T 2T (a) X(jw) 2 ... t t T I t t 41T 21T 0 21T 41T w T T T T (b) Figure 4.14 (a) Periodic impulse train; (b) its Fourier transform. 4.3 PROPERTIES OF THE CONTINUOUS-TIME FOURIER TRANSFORM In this and the following two sections, we consider a number of properties of the Fourier transform. A detailed listing of these properties is given in Table 4.1 in Section 4.6. As was the case for the Fourier series representation of periodic signals, these properties provide us with a significant amount of insight into the transform and into the relationship between the time-domain and frequency-domain descriptions of a signal. In addition, many of the properties are often useful in reducing the complexity of the evaluation of Fourier trans- forms or inverse transforms. Furthermore, as described in the preceding section, there is a close relationship between the Fourier series and Fourier transform representations of a periodic signal, and using this relationship, we can translate many of the Fourier transform properties into corresponding Fourier series properties, which we discussed independently in Chapter 3. (See, in particular, Section 3.5 and Table 3.1.) Throughout the discussion in this section, we will be referring frequently to functions of time and their Fourier transforms, and we will find it convenient to use a shorthand notation to indicate the pairing of a signal and its transform. As developed in Section 4.1, a signal x(t) and its Fourier transform X(jw) are related by the Fourier transform synthesis and analysis equations, 1 J+ % . [eq. (4.8)] x(t) = 7T -% X(jw )e1wt dw (4.24) 2 and [eq. (4.9)] (4.25)"
4.3 Properties of the Continuous-Time Fourier Transform,"300 The Continuous-Time Fourier Transform Chap.4 x(t) ... 1 1 1! 1 1 -2T -T 0 T 2T (a) X(jw) 2 ... t t T I t t 41T 21T 0 21T 41T w T T T T (b) Figure 4.14 (a) Periodic impulse train; (b) its Fourier transform. 4.3 PROPERTIES OF THE CONTINUOUS-TIME FOURIER TRANSFORM In this and the following two sections, we consider a number of properties of the Fourier transform. A detailed listing of these properties is given in Table 4.1 in Section 4.6. As was the case for the Fourier series representation of periodic signals, these properties provide us with a significant amount of insight into the transform and into the relationship between the time-domain and frequency-domain descriptions of a signal. In addition, many of the properties are often useful in reducing the complexity of the evaluation of Fourier trans- forms or inverse transforms. Furthermore, as described in the preceding section, there is a close relationship between the Fourier series and Fourier transform representations of a periodic signal, and using this relationship, we can translate many of the Fourier transform properties into corresponding Fourier series properties, which we discussed independently in Chapter 3. (See, in particular, Section 3.5 and Table 3.1.) Throughout the discussion in this section, we will be referring frequently to functions of time and their Fourier transforms, and we will find it convenient to use a shorthand notation to indicate the pairing of a signal and its transform. As developed in Section 4.1, a signal x(t) and its Fourier transform X(jw) are related by the Fourier transform synthesis and analysis equations, 1 J+ % . [eq. (4.8)] x(t) = 7T -% X(jw )e1wt dw (4.24) 2 and [eq. (4.9)] (4.25) Sec. 4.3 Properties of the Continuous-Time Fourier Transform 301 We will sometimes find it convenient to refer to X(jw) with the notation g:{x(t)} and to x(t) with the notation g:- 1{X(jw)}. We will also refer to x(t) and X(jw) as a Fourier transform pair with the notation ~ x(t) ~ X(jw ). Thus, with reference to Example 4.I, -- = g:{e-at u(t)}, a+ jw e-at u(t) = g:-I { I . } ' a+ JW and ~ I e-atu(t) ~ .. a+ JW 4.3. 1 Linearity If ~ x(t) ~ X(jw) and ~ y(t) ~ Y(jw ), then ~ ax(t) + by(t) ~ aX(jw) + bY(jw ). (4.26) The proof of eq. (4.26) follows directly by application of the analysis eq. (4.25) to ax(t) + by(t). The linearity property is easily extended to a linear combination of an arbitrary number of signals. 4.3.2 Time Shifting If ~ x(t) ~ X(jw), then (4.27) 302 The Continuous-Time Fourier Transform Chap.4 To establish this property, consider eq. (4.24): x(t) = -1 Jx X(jw)e 1.w tdw. 27T -X Replacing t by t - to in this equation, we obtain - 1 J+ x x(t- to) = X(jw)e1.w U-to)dw 27T -X - 1 J+ x . . (e-JwtoX(jw))e1wtdw. 27T -X Recognizing this as the synthesis equation for x(t- t0), we conclude that ~{x(t- to)} = e- jwto X(jw ). One consequence of the time-shift property is that a signal which is shifted in time does not have the magnitude of its Fourier transform altered. That is, if we express X(jw) in polar form as ~{x(t)} = X(jw) = IX(}w )iej<tX(jw), then ~{x(t- to)}= e-jwtoX(jw) = IX(jw)lej[<l:X(jw)-wtol. Thus, the effect of a time shift on a signal is to introduce into its transform a phase shift, namely, -wt0 , which is a linear function of w. Example 4.9 To illustrate the usefulness of the Fourier transform linearity and time-shift proper- ties, let us consider the evaluation of the Fourier transform of the signal x(t) shown in Figure 4.15(a). First, we observe that x(t) can be expressed as the linear combination 1 x(t) = 2X 1 (t - 2.5) + x 2(t - 2.5), where the signals x 1 (t) and x 2(t) are the rectangular pulse signals shown in Figure 4.15(b) and (c). Then, using the result from Example 4.4, we obtain X ( . ) _ 2sin(w/2) d X (. ) _ 2sin(3w/2) I }W- ~ 2]W- . w w Finally, using the linearity and time-shift properties of the Fourier transform yields X(jw) = e- jSw/2 { sin(w/2) +~ sin(3w/2)} . Sec. 4.3 Properties of the Continuous-Time Fourier Transform 303 ~(t) 1.~ 1 2 3 4 (a) _1 1 2 2 (b) I I I -1. 1. 2 2 (c) Figure 4. 1 5 Decomposing a signal into the linear combination of two sim- pler signals. (a) The signal x(t) for Example 4.9; (b) and (c) the two compo- nent signals used to represent x(t). 4.3.3 Conjugation and Conjugate Symmetry The conjugation property states that if ~ x(t) ~ X(jw ), then x * (t) ~~ X * (- jw ). (4.28) This property follows from the evaluation of the complex conjugate of eq. (4.25): X'(jw) = [r: x(t)e-iwt dtr +oo = f- oo x*(t)ejwt dt. Replacing w by -w, we see that +oo X*(- jw) = f- oo x*(t)e- jwt dt. (4.29) 304 The Continuous-Time Fourier Transform Chap.4 Recognizing that the right-hand side of eq. (4 .29) is the Fourier transform analysis equation for x*(t), we obtain the relation given in eq. (4.28). The conjugation property allows us to show that if x(t) is real, then X(jw) has con- jugate symmetry; that is, I X(- jw) = X'(jw) [x(t) real]. I (4.30) Specifically, if x(t) is real so that x*(t) = x(t), we have, from eq. (4.29), +oo X*(- jw) = J-o o x(t)ejwt dt = X(jw ), and eq. (4.30) follows by replacing w with -w. From Example 4.1, with x(t) = e-atu(t), 1 X(jw) = . a+ JW and X(- jw) = = X*(jw). a- jw As one consequence of eq. (4.30), if we express X(jw) in rectangular form as X(jw) = ffi-e{X(jw )} + jdm{X(jw )}, then if x(t) is real, ffi-e{X(jw )} = ffi-e{X(- jw )} and dm{X(jw)} = -dm{X(- jw )}. That is, the real part of the Fourier transform is an even function of frequency, and the imaginary part is an odd function of frequency. Similarly, if we express X(jw) in polar form as X(jw) = IX(jw )jej<tX(jw), then it follows from eq. (4.30) that IX(jw )I is an even function of w and 4:.X(jw) is an odd function of w. Thus, when computing or displaying the Fourier transform of a real- valued signal, the real and imaginary parts or magnitude and phase of the transform need only be specified for positive frequencies, as the values for negative frequencies can be determined directly from the values for w > 0 using the relationships just derived. As a further consequence of eq. (4.30), if x(t) is both real and even, then X(jw) will also be real and even. To see this, we write +oo X(- jw) = J- oo x(t)ejwt dt, Sec. 4.3 Properties of the Continuous-Time Fourier Transform 305 or, with the substitution T = -t, +oo X(- jw) = I-o o x( -T)e- jwT dT. Since x( -T) = x(T), we have +oo X(- jw) = I-o o x(T)e-JwtdT = X(jw). Thus, X(jw) is an even function. This, together with eq. (4.30), also requires that X*(jw) = X(jw) [i.e., that X(jw) is real]. Example 4.2 illustrates this property for the real, even signal e-altl. In a similar manner, it can be shown that if x(t) is a real and odd function of time, so that x(t) = - x( -t), then X(jw) is purely imaginary and odd. Finally, as was discussed in Chapter 1, a real function x(t) can always be expressed in terms of the sum of an even function Xe(t) = Sv{x(t)} and an odd function x 0 (t) = <9d{x(t)}; that is, x(t) = Xe(t) + X0 (t). From the linearity of the Fourier transform, ~{x(t)} = ~{xe(t)} + ~{x0 (t)}, and from the preceding discussion, ~{xe(t)} is a real function and ~{x0 (t)} is purely imag- inary. Thus, we can conclude that, with x(t) real, ~ x(t) ~ X(jw ), ~ Sv{x(t)} ~ (Re{X(jw )}, ~ <9d{x(t)} ~ j!Jm{X(jw)}. One use of these symmetry properties is illustrated in the following example. Example 4. 1 0 Consider again the Fourier transform evaluation of Example 4.2 for the signal x(t) = e-alrl, where a > 0. This time we will utilize the symmetry properties of the Fourier transform to aid the evaluation process. From Example 4.1, we have ~ 1 e-atu(t) ~ --.-. a+ JW Note that for t > 0, x(t) equals e-ar u(t), while for t < 0, x(t) takes on mirror image values. That is, 306 The Continuous-Time Fourier Transform Chap.4 x(t) = e -altl = e-at u(t) + eat u(- t) = 2 [ e-atu(t) ~ eatu(-t)] = 28v{e-at u(t)}. Since e-at u(t) is real valued, the symmetry properties of the Fourier transform lead us to conclude that 1 Bv{e-atu(t)} ~ ffi-e{ - -.-}. a+ JW It follows that 1 2 X(jw) = 2ffi-e{ - -.-} = 2 +a 2 , a+ JW a w which is the same as the answer found in Example 4.2. 4.3.4 Differentiation and Integration Let x(t) be a signal with Fourier transform X(jw ). Then, by differentiating both sides of the Fourier transform synthesis equation (4 .24) , we obtain dx(t) 1 J+ oo 1· = - jwX(jw )e wt dw. dt 27T -00 Therefore, ddx(tf) ~ . X( . ) ~JW JW. (4.31) This is a particularly important property, as it replaces the operation of differentiation in the time domain with that of multiplication by jw in the frequency domain. We will find the substitution to be extremely useful in our discussion in Section 4. 7 on the use of Fourier transforms for the analysis of LTI systems described by differential equations. Since differentiation in the time domain corresponds to multiplication by jw in the frequency domain, one might conclude that integration should involve division by jw in the frequency domain. This is indeed the case, but it is only one part of the picture. The precise relationship is t ~ 1 J x(r)dr ~ -.X (jw) + 7TX(O)o(w). (4.32) -oo JW The impulse term on the right-hand side of eq. (4.32) reflects the de or average value that can result from integration. The use of eqs. (4.31) and (4.32) is illustrated in the next two examples. Sec. 4.3 Properties of the Continuous-Time Fourier Transform 307 Example 4. 11 Let us determine the Fourier transform X(jw) of the unit step x(t) = u(t), making use of eq. (4.32) and the knowledge that ~ g(t) = S(t) ~ G(jw) = 1. Noting that x(t) = Loo g( r)dr and taking the Fourier transform of both sides, we obtain X(jw) = G(_Jw) + 7TG(O)S(w), )W where we have used the integration property listed in Table 4.1. Since G(jw) = 1, we conclude that X(jw) = ~ + 7TS(w). (4.33) JW Observe that we can apply the differentiation property of eq. (4.31) to recover the transform of the impulse. That is, S(t) = du(t) ~ jw [~ + 7TS(w )] = 1, dt JW where the last equality follows from the fact that wS(w) = 0. Example 4. 1 2 Suppose that we wish to calculate the Fourier transform X(jw) for the signal x(t) dis- played in Figure 4.16(a). Rather than applying the Fourier integral directly to x(t), we instead consider the signal d g(t) = dt x(t). x(t) 1 -1 +A ~ (a) g(t) = d~it) (b) -1 -1 Figure 4. 16 (a) A signal x(t) for which the Fourier transform is to be eval- uated; (b) representation of the derivative of x(t) as the sum of two components. 308 The Continuous-Time Fourier Transform Chap.4 As illustrated in Figure 4.16(b ), g(t) is the sum of a rectangular pulse and two impulses. The Fourier transforms of each of these component signals may be determined from Table 4.2: G(jw) = -2swinw- ) - . . ( e.Jw- e-Jw. Note that G(O) = 0. Using the integration property, we obtain . G(jw) X(;w) = -.- + 7TG(0)8(w). )W With G(O) = 0 this becomes . 2sinw 2cosw X(JW) = -.2- - -.-. ;w JW The expression for X(jw) is purely imaginary and odd, which is consistent with the fact that x(t) is real and odd. 4.3.5 Time and Frequency Scaling If ~ x(t) ~ X(jw ), then ~ 1 (jw) x(at) ~ rarx a ' (4.34) where a is a nonzero real number. This property follows directly from the definition of the Fourier transform-specifically, +(X) 5""{x(at)} = f-(X) x(at)e- fwtdt. Using the substitution T = at, we obtain a>O a<O which corresponds to eq. (4.34). Thus, aside from the amplitude factor llial, a linear scal- ing in time by a factor of a corresponds to a linear scaling in frequency by a factor of 11 a, and vice versa. Also, letting a = -1, we see from eq. (4.34) that Sec. 4.3 Properties of the Continuous-Time Fourier Transform 309 (4.35) That is, reversing a signal in time also reverses its Fourier transform. A common illustration of eq. (4.34) is the effect on frequency content that results when an audiotape is recorded at one speed and played back at a different speed. If the playback speed is higher than the recording speed, corresponding to compression in time (i.e., a > 1) , then the spectrum is expanded in frequency (i.e., the audible effect is that the playback frequencies are higher). Conversely, the signal played back will be scaled down in frequency if the playback speed is slower than the recording speed (0 < a < 1). For example, if a recording of the sound of a small bell ringing is played back at a reduced speed, the result will sound like the chiming of a larger and deeper sounding bell. The scaling property is another example of the inverse relationship between time and frequency that we have already encountered on several occasions. For example, we have seen that as we increase the period of a sinusoidal signal, we decrease its frequency. Also, as we saw in Example 4.5 (see Figure 4.11 ), if we consider the transform 1 X(jw) = { ' \w\<W 0, \w\>W' then as we increase W, the inverse transform of X(jw) becomes narrower and taller and approaches an impulse as W ~ oo. Finally, in Example 4.8, we saw that the spacing in the frequency domain between impulses in the Fourier transform of a periodic impulse train is inversely proportional to the spacing in the time domain. The inverse relationship between the time and frequency domains is of great im- portance in a variety of signal and systems contexts, including filtering and filter design, and we will encounter its consequences on numerous occasions in the remainder of the book. In addition, the reader may very well come across the implications of this property in studying a wide variety of other topics in science and engineering. One example is the uncertainty principle in physics; another is illustrated in Problem 4.49. 4.3.6 Duality By comparing the transform and inverse transform relations given in eqs. (4.24) and (4.25), we observe that these equations are similar, but not quite identical, in form. This symmetry leads to a property of the Fourier transform referred to as duality. In Example 4.5, we alluded to duality when we noted the relationship that exists between the Fourier transform pairs of Examples 4.4 and 4.5. In the former example we derived the Fourier transform pair \t\ < T 1 g: 2 sinwT1 XJ(I) = { 6: \t\ ~ Xt(jW) = (4.36) > Tt w while in the latter we considered the pair sin Wt g: • { 1 x2(t) = --~ X2C1w) = ' \w\<W (4.37) ~t 0' \w\>W"" 310 The Continuous-Time Fourier Transform Chap.4 x1 (t) ~ ~ -T1 T1 x2(t) X2(jw) /Whr 11 I -w w w Figure 4.17 Relationship between the Fourier transform pairs of eqs. (4.36) and (4.37). The two Fourier transform pairs and the relationship between them are depicted in Figure 4.17. The symmetry exhibited by these two examples extends to Fourier transforms in general. Specifically, because of the symmetry between eqs. (4 .24) and (4 .25), for any transform pair, there is a dual pair with the time and frequency variables interchanged. This is best illustrated through an example. Example 4. 1 3 Let us consider using duality to find the Fourier transform G(jw) of the signal 2 g(t) = -1-2. +t In Example 4.2 we encountered a Fourier transform pair in which the Fourier transform, as a function of w, had a form similar to that of the signal x(t). Specifically, suppose we consider a signal x(t) whose Fourier transform is X(jw) = -+21w 2' Then, from Example 4.2, x(t) = e-ltl ~ X(jw) = .; w . 1 2 Sec. 4.3 Properties of the Continuous-Time Fourier Transform 311 The synthesis equation for this Fourier transform pair is e-iri = - 1 IX (- -2 2 ) e;.w tdw. 21T -X 1 + w Multiplying this equation by 21T and replacing t by -t, we obtain 27Te-lt1 = Ix (~)e- Jwtdw. -X 1 + w Now, interchanging the names of the variables t and w, we find that (4.38) The right-hand side of eq. (4.38) is the Fourier transform analysis equation for 2/(1 + t2 ), and thus, we conclude that ~{ _2_} = 21Te-lwi. 1 + t2 The duality property can also be used to determine or to suggest other properties of Fourier transforms. Specifically, if there are characteristics of a function of time that have implications with regard to the Fourier transform, then the same characteristics associated with a function of frequency will have dual implications in the time domain. For example, in Section 4.3.4, we saw that differentiation in the time domain corresponds to multiplica- tion by jw in the frequency domain. From the preceding discussion, we might then suspect that multiplication by jt in the time domain corresponds roughly to differentiation in the frequency domain. To determine the precise form of this dual property, we can proceed in a fashion exactly analogous to that used in Section 4.3.4. Thus, if we differentiate the analysis equation (4 .25) with respect to w, we obtain dX(jw) f +oo dw = -oo -jtx(t)e-Jwtdt. (4.39) That is, . () ~ dX(jw) -}tXt ~ dw . (4.40) Similarly, we can derive the dual properties of eqs. (4.27) and (4.32): . ~ elwot x(t) ~ X(j(w - wo)) (4 .41) and 1 ~ f(J) - -:-X(t) + 7TX(0)8(t) ~ X(T])dTJ. (4.42) }t -00 312 The Continuous-Time Fourier Transform Chap.4 4.3.7 Parseval's Relation If x(t) and X(jw) are a Fourier transform pair, then (4.43) This expression, referred to as Parseval's relation, follows from direct application of the Fourier transform. Specifically, Reversing the order of integration gives The bracketed term is simply the Fourier transform of x(t); thus, The term on the left-hand side of eq. (4.43) is the total energy in the signal x(t). Parse val's relation says that this total energy may be determined either by computing the energy per unit time (jx(t)j2) and integrating over all time or by computing the energy per unit frequency (jX(jw )j2/27T) and integrating over all frequencies. For this reason, jX(jw )1 2 is often referred to as the energy-density spectrum of the signal x(t). (See also Problem 4.45.) Note that Parseval's relation for finite-energy signals is the direct counterpart of Parseval's relation for periodic signals (eq. 3.67), which states that the average power of a periodic signal equals the sum of the average powers of its individual harmonic compo- nents, which in tum are equal to the squared magnitudes of the Fourier series coefficients. Parseval's relation and other Fourier transform properties are often useful in deter- mining some time domain characteristics of a signal directly from the Fourier transform. The next example is a simple illustration of this. Example 4. 14 For each of the Fourier transforms shown in Figure 4.18, we wish to evaluate the follow- ing time-domain expressions: E f""YO /x(t)/2 = dt D = !!_x(t)l dt t=O Sec. 4.3 Properties of the Continuous-Time Fourier Transform 313 X(jw) -1 -0.5 0 0.5 w (a) X(jw) jfiT -1 0 w ....__ ___ , -j fiT (b) Figure 4. 18 The Fourier transforms considered in Example 4.14. To evaluate E in the frequency domain, we may use Parseval's relation. That is, E 1 Joe = 27T _""' IX(jw)i1dw (4.44) which evaluates to ~ for Figure 4.18( a) and to 1 for Figure 4.18(b ). To evaluate D in the frequency domain, we first use the differentiation property to observe that g(t) = :t x(t) ~ jwX(jw) = G(jw ). Noting that D = g(O) = -1 fx G(jw)dw (4.45) 27T -X we conclude: D = f_""'x jwX(jw) dw (4.46) which evaluates to zero for figure 4.18( a) and to - ~ for Figure 4.18(b ). (2 ~'Tr ) There are many other properties of the Fourier transform in addition to those we have already discussed. In the next two sections, we present two specific properties that play 314 The Continuous-Time Fourier Transform Chap.4 particularly central roles in the study of LTI systems and their applications. The first of these, discussed in Section 4.4, is referred to as the convolution property, which is central to many signals and systems applications, including filtering. The second, discussed in Section 4.5, is referred to as the multiplication property, and it provides the foundation for our discussion of sampling in Chapter 7 and amplitude modulation in Chapter 8. In Section 4.6, we summarize the properties of the Fourier transform. 4.4 THE CONVOLUTION PROPERTY As we saw in Chapter 3, if a periodic signal is represented in a Fourier series-i.e., as a linear combination of harmonically related complex exponentials, as in eq. (3.38)- then the response of an LTI system to this input can also be represented by a Fourier series. Because complex exponentials are eigenfunctions of LTI systems, the Fourier series coefficients of the output are those of the input multiplied by the frequency response of the system evaluated at the corresponding harmonic frequencies. In this section, we extend this result to the situation in which the signals are aperiodic. We first derive the property somewhat informally, to build on the intuition we developed for periodic signals in Chapter 3, and then provide a brief, formal derivation starting directly from the convolution integral. Recall our interpretation of the Fourier transform synthesis equation as an expression for x(t) as a linear combination of complex exponentials. Specifically, referring back to eq. (4.7), x(t) is expressed as the limit of a sum~ that is, 1 I+ :rc . 1 +00 . x(t) = - X(jw)e 1w1dw = lim - L X(jkw )eJkw01 0 w 0. (4.47) 2 7f -00 W() --?() 2 7f k =-X As developed in Sections 3.2 and 3.8, the response of a linear system with impulse response h(t) to a complex exponential e.ikwol is H(j kw 0 )e.ikwol, where +% H(j kwo) = I-00 h(t)e -jkwol dt. (4.48) We can recognize the frequency response H(jw), as defined in eq. (3.121), as the Fourier transform of the system impulse response. In other words, the Fourier transform of the impulse response (evaluated at w = kw0 ) is the complex scaling factor that the LTI system applies to the eigenfunction e.i kwo1 • From superposition [see eq. (3 .124)], we then have 1 +x 1 +x 7T L"""" '""' X (J"" k wo ) e jkw 0 I wo ~ 7T L"""" '""' X (J"" k w0 )H( J"" k w ) e jkw 0 0 I w0, 2 2 k=-X k=-X and thus, from eq. (4.47), the response of the linear system to x(t) is 1 y(t) = lim - f X(jkwo)H(jkwo)ejkwo1w0 W()--7{) 27f k= -X (4.49) = -1 I +00 X(jw)H(jw)e1. w1dw. 27f -X"
4.4 The Convolution Property,"314 The Continuous-Time Fourier Transform Chap.4 particularly central roles in the study of LTI systems and their applications. The first of these, discussed in Section 4.4, is referred to as the convolution property, which is central to many signals and systems applications, including filtering. The second, discussed in Section 4.5, is referred to as the multiplication property, and it provides the foundation for our discussion of sampling in Chapter 7 and amplitude modulation in Chapter 8. In Section 4.6, we summarize the properties of the Fourier transform. 4.4 THE CONVOLUTION PROPERTY As we saw in Chapter 3, if a periodic signal is represented in a Fourier series-i.e., as a linear combination of harmonically related complex exponentials, as in eq. (3.38)- then the response of an LTI system to this input can also be represented by a Fourier series. Because complex exponentials are eigenfunctions of LTI systems, the Fourier series coefficients of the output are those of the input multiplied by the frequency response of the system evaluated at the corresponding harmonic frequencies. In this section, we extend this result to the situation in which the signals are aperiodic. We first derive the property somewhat informally, to build on the intuition we developed for periodic signals in Chapter 3, and then provide a brief, formal derivation starting directly from the convolution integral. Recall our interpretation of the Fourier transform synthesis equation as an expression for x(t) as a linear combination of complex exponentials. Specifically, referring back to eq. (4.7), x(t) is expressed as the limit of a sum~ that is, 1 I+ :rc . 1 +00 . x(t) = - X(jw)e 1w1dw = lim - L X(jkw )eJkw01 0 w 0. (4.47) 2 7f -00 W() --?() 2 7f k =-X As developed in Sections 3.2 and 3.8, the response of a linear system with impulse response h(t) to a complex exponential e.ikwol is H(j kw 0 )e.ikwol, where +% H(j kwo) = I-00 h(t)e -jkwol dt. (4.48) We can recognize the frequency response H(jw), as defined in eq. (3.121), as the Fourier transform of the system impulse response. In other words, the Fourier transform of the impulse response (evaluated at w = kw0 ) is the complex scaling factor that the LTI system applies to the eigenfunction e.i kwo1 • From superposition [see eq. (3 .124)], we then have 1 +x 1 +x 7T L"""" '""' X (J"" k wo ) e jkw 0 I wo ~ 7T L"""" '""' X (J"" k w0 )H( J"" k w ) e jkw 0 0 I w0, 2 2 k=-X k=-X and thus, from eq. (4.47), the response of the linear system to x(t) is 1 y(t) = lim - f X(jkwo)H(jkwo)ejkwo1w0 W()--7{) 27f k= -X (4.49) = -1 I +00 X(jw)H(jw)e1. w1dw. 27f -X Sec. 4.4 The Convolution Property 315 Since y(t) and its Fourier transform Y(jw) are related by y(t) = -1 J+x Y(jw )e1.w t dw, (4.50) 27 T -cc we can identify Y(jw) from eq. (4.49), yielding Y(jw) = X(jw)H(jw). (4.51) As a more formal derivation, we consider the convolution integral +x y(t) = f-cc X( T)h(t - T)dT. (4.52) We desire Y(jw ), which is (4.53) Interchanging the order of integration and noting that x( r) does not depend on t, we have (4.54) By the time-shift property, eq. (4.27), the bracketed term is e- jwr H(jw ). Substituting this into eq. (4.54) yields +x f +oo Y(jw) = f- cc x(r)e-jwrH(jw)dT = H(jw) -cc x(r)e-jwrdT. (4.55) The integral is X(jw ), and hence, Y(jw) = H(jw)X(jw). That is, g: y(t) = h(t) * x(t) ~ Y(jw) = H(jw )X(jw ). (4.56) Equation (4 .56) is of major importance in signal and system analysis. As expressed in this equation, the Fourier transform maps the convolution of two signals into the product of their Fourier transforms. H(jw ), the Fourier transform of the impulse response, is the frequency response as defined in eq. (3.121) and captures the change in complex amplitude of the Fourier transform of the input at each frequency w. For example, in frequency- selective filtering we may want to have H (jw) = 1 over one range of frequencies, so that the frequency components in this band experience little or no attenuation or change due to the system, while over another range of frequencies we may want to have H(jw) = 0, so that components in this range are eliminated or significantly attenuated. 316 The Continuous-Time Fourier Transform Chap.4 The frequency response H(jw) plays as important a role in the analysis of LTI sys- tems as does its inverse transform, the unit impulse response. For one thing, since h(t) completely characterizes an LTI system, then so must H(jw ). In addition, many of the properties of LTI systems can be conveniently interpreted in terms of H(jw ). For exam- ple, in Section 2.3, we saw that the impulse response of the cascade of two LTI systems is the convolution of the impulse responses of the individual systems and that the over- all impulse response does not depend on the order in which the systems are cascaded. Using eq. (4.56), we can rephrase this in terms of frequency responses. As illustrated in Figure 4.19, since the impulse response of the cascade of two LTI systems is the con- volution of the individual impulse responses, the convolution property then implies that the overall frequency response of the cascade of two systems is simply the product of the individual frequency responses. From this observation, it is then clear that the overall frequency response does not depend on the order of the cascade. x(t) H1(jw) H2(jw) y(t) (a) x(t) •I H1(jw)H2(jw) I • y(t) (b) x(t) y(t) Figure 4. 19 Three equivalent LTI systems. Here, each block represents an LTI system with the indicated (c) frequency response. As discussed in Section 4.1.2, convergence of the Fourier transform is guaranteed only under certain conditions, and consequently, the frequency response cannot be defined for every LTI system. If, however, an LTI system is stable, then, as we saw in Section 2.3.7 and Problem 2.49, its impulse response is absolutely integrable; that is, r: ,h(t),dt < oo. (4.57) Equation (4 .57) is one of the three Dirichlet conditions that together guarantee the exis- tence of the Fourier transform H(jw) of h(t). Thus, assuming that h(t) satisfies the other two conditions, as essentially all signals of physical or practical significance do, we see that a stable LTI system has a frequency response H(jw ). In using Fourier analysis to study LTI systems, we will be restricting ourselves to systems whose impulse responses possess Fourier transforms. In order to use trans- form techniques to examine unstable LTI systems we will develop a generalization of Sec. 4.4 The Convolution Property 317 the continuous-time Fourier transform, the Laplace transform. We defer this discussion to Chapter 9, and until then we will consider the many problems and practical applications that we can analyze using the Fourier transform. 4.4. 1 Examples To illustrate the convolution property and its applications further, let us consider several examples. Example 4. 1 5 Consider a continuous-time LTI system with impulse response h(t) = o(t - to). (4.58) The frequency response of this system is the Fourier transform of h(t) and is given by H(jw) = e-Jwto. (4.59) Thus, for any input x(t) with Fourier transform X(jw ), the Fourier transform of the output is Y(jw) = H(jw)X(jw) (4.60) = e-Jwtox(jw). This result, in fact, is consistent with the time-shift property of Section 4.3.2. Specifi- cally, a system for which the impulse response is o(t- to) applies a time shift of to to the input-that is, y(t) = x(t - to). Thus, the shifting property given in eq. (4.27) also yields eq. (4.60). Note that, either from our discussion in Section 4.3.2 or directly from eq. (4.59), the frequency response of a system that is a pure time shift has unity magnitude at all frequencies (i.e., ie-Jwro I = 1) and has a phase characteristic -wt0 that is a linear function of w. Example 4. 1 6 As a second example, let us examine a differentiator-that is, an LTI system for which the input x(t) and the output y(t) are related by y(t) = d~;t). From the differentiation property of Section 4.3.4, Y(jw) = jwX(jw). (4.61) Consequently, from eq. (4.56), it follows that the frequency response of a differentiator is H(jw) = jw. (4.62) 318 The Continuous-Time Fourier Transform Chap.4 Example 4. 1 7 Consider an integrator-that is, an LTI system specified by the equation y(t) = fx X(T)dT. The impulse response for this system is the unit step u(t), and therefore, from Exam- ple 4.11 and eq. (4.33), the frequency response of the system is H(jw) = _;_ + 7TO(w). JW Then using eq. (4.56), we have Y(jw) = H(jw)X(jw) _;_ X(jw) + 1T X(jw );(w) JW _;_X(jw) + 1TX(O)o(w), JW which is consistent with the integration property of eq. (4.32). Example 4. 18 As we discussed in Section 3.9.2, frequency-selective filtering is accomplished with an LTI system whose frequency response H (jw) passes the desired range of frequencies and significantly attenuates frequencies outside that range. For example, consider the ideal lowpass filter introduced in Section 3.9.2, which has the frequency reponse illustrated in Figure 4.20 and given by H(jw) = { 1 /w/ <We. (4.63) 0 /w/ >we Now that we have developed the Fourier transform representation, we know that the impulse response h(t) of this ideal filter is the inverse transform of eq. (4 .63). Using the result in Example 4.5, we then have h(t) = sin Wet (4.64) 1Tf which is plotted in Figure 4.21. 0 -stopband+ Passband --I--stopband-- Figure 4.20 Frequency response of an ideal lowpass filter. Sec. 4.4 The Convolution Property 319 h(t) Figure 4.21 Impulse response of an ideal lowpass filter. From Example 4.18, we can begin to see some of the issues that arise in filter design that involve looking in both the time and frequency domains. In particular, while the ideal lowpass filter does have perfect frequency selectivity, its impulse response has some char- acteristics that may not be desirable. First, note that h(t) is not zero fort < 0. Consequently, the ideal lowpass filter is not causal, and thus, in applications requiring causal systems, the ideal filter is not an option. Moreover, as we discuss in Chapter 6, even if causality is not an essential constraint, the ideal filter is not easy to approximate closely, and non- ideal filters that are more easily implemented are typically preferred. Furthermore, in some applications (such as the automobile suspension system discussed in Section 6. 7.1 ), oscil- latory behavior in the impulse response of a lowpass filter may be undesirable. In such applications the time domain characteristics of the ideal lowpass filter, as shown in Fig- ure 4.21, may be unacceptable, implying that we may need to trade off frequency-domain characteristics such as ideal frequency selectivity with time-domain properties. For example, consider the LTI system with impulse response h(t) = e -r u(t). (4.65) The frequency response of this system is H(jw) = + (4.66) jw 1· Comparing eqs. (3.145) and (4.66), we see that this system can be implemented with the simple RC circuit discussed in Section 3.10. The impulse response and the magnitude of the frequency response are shown in Figure 4.22. While the system does not have the strong frequency selectivity of the ideal lowpass filter, it is causal and has an impulse response that decays monotonically, i.e., without oscillations. This filter or somewhat more complex ones corresponding to higher order differential equations are quite frequently preferred to ideal filters because of their causality, ease of implementation, and flexibility in allowing trade-offs, among other design considerations such as frequency selectivity and oscillatory behavior in the time domain. Many of these issues will be discussed in more detail in Chapter 6. The convolution property is often useful in evaluating the convolution integral-i.e., in computing the response of LTI systems. This is illustrated in the next example. 320 The Continuous-Time Fourier Transform Chap.4 h(t) e (a) IH(jw)l -1 (b) Figure 4.22 (a) Impulse response of the LTI system in eq. (4.65); (b) magnitude of the frequency response of the system. Example 4. 19 Consider the response of an LTI system with impulse response h(t) = e-at u(t), a > 0, to the input signal x(t) = e-bt u(t), b > 0. Rather than computing y(t) = x(t) * h(t) directly, let us transform the problem into the frequency domain. From Example 4.1, the Fourier transforms of x(t) and h(t) are 1 X(jw) = - -.- b+ JW and 1 H(jw) = - -.-. a+ JW Therefore, Y(jw) = ( (4.67) a+ . )l(b + . ) JW )W To determine the output y(t), we wish to obtain the inverse transform of Y(jw ). This is most simply done by expanding Y(jw) in a partial-fraction expansion. Such expansions are extremely useful in evaluating inverse transforms, and the general method for performing a partial-fraction expansion is developed in the appendix. For this Sec. 4.4 The Convolution Property 321 example, assuming that b =P a, the partial fraction expansion for Y(jw) takes the form Y(jw) = _A_._+ -Bb. ' (4.68) a+ )W + JW where A and B are constants to be determined. One way to find A and B is to equate the right-hand sides of eqs. (4.67) and (4.68), multiply both sides by (a+ jw )(b + jw ), and solve for A and B. Alternatively, in the appendix we present a more general and efficient method for computing the coefficients in partial-fraction expansions such as eq. (4.68). Using either of these approaches, we find that 1 A= b- a = -B, and therefore, Y(jw) = _1_ [-1- __1 _]. (4.69) b - a a + jw b + jw The inverse transform for each of the two terms in eq. (4.69) can be recognized by inspection. Using the linearity property of Section 4.3.1, we have 1 y(t) = --[e-at u(t)- e-bt u(t)]. b-a When b = a, the partial fraction expansion of eq. (4.69) is not valid. However, with b = a, eq. (4.67) becomes Y(jw) = (a + jw )2. Recognizing this as . d [ 1 ] (a+jw)2 =Jdw a+jw' we can use the dual of the differentiation property, as given in eq. (4.40). Thus, ~ 1 e-at u(t) ~ --.- a+ JW ~ j_!}__ [-1 te-atu(t) -.-] = dw a+ JW (a+ jw)2' and consequently, y(t) = te-at u(t). Example 4.20 As another illustration of the usefulness of the convolution property, let us consider the problem of determining the response of an ideallowpass filter to an input signal x(t) that has the form of a sine function. That is, sinwit x(t ) = --. 7Tt Of course, the impulse response of the ideallowpass filter is of a similar form, namely, 322 The Continuous-Time Fourier Transform Chap.4 h(t) = sin wet. 1T't The filter output y(t) will therefore be the convolution of two sine functions, which, as we now show, also turns out to be a sine function. A particularly convenient way of deriving this result is to first observe that Y(jw) = X(jw )H(jw ), where X(jw) = { 01 lwl ::::;; w; elsewhere and H(jw) = { 1 lwl ::::;; We 0 elsewhere Therefore, Y(jw) = { 1 lwl ::::;; wo , 0 elsewhere where w0 is the smaller of the two numbers w; and we. Finally, the inverse Fourier trans- form of Y(jw) is given by sinwet 'f l· -- 1 We::::;; W; y(t) = . 1T't smw;t 'f -- 1 W;::::;; We 1T't That is, depending upon which of we and w; is smaller, the output is equal to either x(t) or h(t). 4.5 THE MULTIPLICATION PROPERTY The convolution property states that convolution in the time domain corresponds to mul- tiplication in the frequency domain. Because of duality between the time and frequency domains, we would expect a dual property also to hold (i.e., that multiplication in the time domain corresponds to convolution in the frequency domain). Specifically, = = -1 f+x r(t) s(t)p(t) ~ R(jw) S(j8)P(j(w- 8))d() (4.70) 27T -X This can be shown by exploiting duality as discussed in Section 4.3.6, together with the convolution property, or by directly using the Fourier transform relations in a manner anal- ogous to the procedure used in deriving the convolution property. Multiplication of one signal by another can be thought of as using one signal to scale or modulate the amplitude of the other, and consequently, the multiplication of two sig- nals is often referred to as amplitude modulation. For this reason, eq. (4.70) is sometimes"
4.5 The Multiplication Property,"322 The Continuous-Time Fourier Transform Chap.4 h(t) = sin wet. 1T't The filter output y(t) will therefore be the convolution of two sine functions, which, as we now show, also turns out to be a sine function. A particularly convenient way of deriving this result is to first observe that Y(jw) = X(jw )H(jw ), where X(jw) = { 01 lwl ::::;; w; elsewhere and H(jw) = { 1 lwl ::::;; We 0 elsewhere Therefore, Y(jw) = { 1 lwl ::::;; wo , 0 elsewhere where w0 is the smaller of the two numbers w; and we. Finally, the inverse Fourier trans- form of Y(jw) is given by sinwet 'f l· -- 1 We::::;; W; y(t) = . 1T't smw;t 'f -- 1 W;::::;; We 1T't That is, depending upon which of we and w; is smaller, the output is equal to either x(t) or h(t). 4.5 THE MULTIPLICATION PROPERTY The convolution property states that convolution in the time domain corresponds to mul- tiplication in the frequency domain. Because of duality between the time and frequency domains, we would expect a dual property also to hold (i.e., that multiplication in the time domain corresponds to convolution in the frequency domain). Specifically, = = -1 f+x r(t) s(t)p(t) ~ R(jw) S(j8)P(j(w- 8))d() (4.70) 27T -X This can be shown by exploiting duality as discussed in Section 4.3.6, together with the convolution property, or by directly using the Fourier transform relations in a manner anal- ogous to the procedure used in deriving the convolution property. Multiplication of one signal by another can be thought of as using one signal to scale or modulate the amplitude of the other, and consequently, the multiplication of two sig- nals is often referred to as amplitude modulation. For this reason, eq. (4.70) is sometimes Sec. 4.5 The Multiplication Property 323 referred to as the modulation property. As we shall see in Chapters 7 and 8, this property has several very important applications. To illustrate eq. (4.70), and to suggest one of the applications that we will discuss in subsequent chapters, let us consider several examples. Example 4.21 Let s(t) be a signal whose spectrum S(jw) is depicted in Figure 4.23(a). Also, consider the signal p(t) = cos wot. Then P(jw) = 1r8(w - wo) + 1r8(w + wo), as sketched in Figure 4.23(b), and the spectrum R(jw) of r(t) = s(t)p(t) is obtained by w (a) PGw) 1T t I t w (b) RGw) = _1_ [SGw) * PGw)] M ~:i (c) Figure 4.23 Use of the multiplication property in Example 4.21: (a) the Fourier transform of a signal s(t); (b) the Fourier transform of p(t) = cos wot; (c) the Fourier transform of r(t) = s(t)p(t). 324 The Continuous-Time Fourier Transform Chap.4 an application of eq. (4.70), yielding +oo 1 R(jw) = 27T f-oosue)PU(w - ()))d() = ~S(j(w - wo)) + ~S(j(w + wo)), (4.71) which is sketched in Figure 4.23(c). Here we have assumed that w0 > w 1, so that the two nonzero portions of R(jw) do not overlap. Clearly, the spectrum of r(t) consists of the sum of two shifted and scaled versions of S(jw ). From eq. (4.71) and from Figure 4.23, we see that all of the information in the signal s(t) is preserved when we multiply this signal by a sinusoidal signal, although the information has been shifted to higher frequencies. This fact forms the basis for sinu- soidal amplitude modulation systems for communications. In the next example, we learn how we can recover the original signal s(t) from the amplitude-modulated signal r(t). Example 4.22 Let us now consider r(t) as obtained in Example 4.21, and let g(t) = r(t)p(t), where, again, p(t) = cos w0t. Then, R(jw ), P(jw ), and G(jw) are as shown in Figure 4.24. From Figure 4.24(c) and the linearity of the Fourier transform, we see that g(t) is the sum of (l/2)s(t) and a signal with a spectrum that is nonzero only at higher frequen- R(jw) & tN2 & -wo wo w (a) 1T P(jw) 1T t I 1 -wo wo w (b) G(jw) A/4 ,;k A/4 ~ ~ -2w0 -w1 w1 w (c) Figure 4.24 Spectra of signals considered in Example 4.22: (a) R(jw ); (b) P(jw ); (c) G(jw ). Sec. 4.5 The Multiplication Property 325 cies (centered around ±2w0 ). Suppose then that we apply the signal g(t) as the input to a frequency-selective lowpass filter with frequency response H(jw) that is constant at low frequencies (say, for lwl < w1) and zero at high frequencies (for lwl > wt). Then the output of this system will have as its spectrum H(jw )G(jw ), which, because of the particular choice of H(jw ), will be a scaled replica of S(jw ). Therefore, the output itself will be a scaled version of s(t). In Chapter 8, we expand significantly on this idea as we develop in detail the fundamentals of amplitude modulation. Example 4.23 Another illustration of the usefulness of the Fourier transform multiplication property is provided by the problem of determining the Fourier transform of the signal ( ) _ sin(t) sin(t/2) X t - 1T""t2 • The key here is to recognize x(t) as the product of two sine functions: x(t) = ,. ( si:~) )('in~;2)). Applying the multiplication property of the Fourier transform, we obtain X(jw) = !~ { sin(t)} * ~ { sin(t/2)} . 2 'TT""t 'TT""t Noting that the Fourier transform of each sine function is a rectangular pulse, we can proceed to convolve those pulses to obtain the function X(jw) displayed in Figure 4.25. ~1~:w) v<::, ~ -3 _1 1 3 w 2 2 2 2 Figure 4.25 The Fourier transform of x(t) in Example 4.23. 4.5.1 Frequency-Selective Filtering with Variable Center Frequency As suggested in Examples 4.21 and 4.22 and developed more fully in Chapter 8, one of the important applications of the multiplication property is amplitude modulation in commu- nication systems. Another important application is in the implementation of frequency- selective bandpass filters with tunable center frequencies that can be adjusted by the simple tum of a dial. In a frequency-selective bandpass filter built with elements such as resistors, operational amplifiers, and capacitors, the center frequency depends on a number of element values, all of which must be varied simultaneously in the correct way if the center frequency is to be adjusted directly. This is generally difficult and cumber- some in comparison with building a filter whose characteristics are fixed. An alternative to directly varying the filter characteristics is to use a fixed frequency-selective filter and 326 The Continuous-Time Fourier Transform Chap.4 shift the spectrum of the signal appropriately, using the principles of sinusoidal amplitude modulation. For example, consider the system shown in Figure 4.26. Here, an input signal x(t) is multiplied by the complex exponential signal ejwct. The resulting signal is then passed through a lowpass filter with cutoff frequency w0 , and the output is multiplied by e- jwct. The spectra of the signals x(t), y(t), w(t), and f(t) are illustrated in Figure 4.27. e iwct Ideal lowpass filter .~ H{jw) y(t) w(t) x(t) • 11 1-----+-t X J-------1~ f(t) -wo wo w Figure 4.26 Implementation of a bandpass filter using amplitude modula- tion with a complex exponential carrier. X(jw) ~ w Y(jw) Frequency response of 1-- ideal lowpass filter ~ I I W(jw) A1 w F(jw) w Figure 4.27 Spectra of the signals in the system of Figure 4.26. Sec. 4.5 The Multiplication Property 327 Specifically, from either the multiplication property or the frequency-shifting property it follows that the Fourier transform of y(t) = eiwct x(t) is Y(jw) = r: ll(O- wc)X(w- O)dO so that Y(jw) equals X(jw) shifted to the right by We and frequencies in X(jw) near w = we have been shifted into the passband of the lowpass filter. Similarly, the Fourier transform ofj(t) = e-Jwc1w(t) is F(jw) = W(j(w + wo)), so that the Fourier transform ofF Uw) is W Uw) shifted to the left by We. From Figure 4.27, we observe that the overall system of Figure 4.26 is equivalent to an ideal bandpass fil- ter with center frequency -we and bandwidth 2w0 , as illustrated in Figure 4.28. As the frequency We of the complex exponential oscillator is varied, the center frequency of the bandpass filter varies. I I I (1) Figure 4.28 Bandpass filter equiva- lent of Figure 4.26. In the system of Figure 4.26 with x(t) real, the signals y(t), w(t), and f(t) are all complex. If we retain only the real part of f(t), the resulting spectrum is that shown in Figure 4.29, and the equivalent bandpass filter passes bands of frequencies centered around We and -we, as indicated in Figure 4.30. Under certain conditions, it is also possi- ble to use sinusoidal rather than complex exponential modulation to implement the system of the latter figure. This is explored further in Problem 4.46. (1) Figure 4.29 Spectrum of ffi-e{f(t)} associated with Figure 4.26. H(jw) I I I tf I I (1) Figure 4.30 Equivalent bandpass filter for ffi-e{f(t)} in Figure 4.29. 328 The Continuous-Time Fourier Transform Chap.4 4.6 TABLES OF FOURIER PROPERTIES AND OF BASIC FOURIER TRANSFORM PAIRS In the preceding sections and in the problems at the end of the chapter, we have consid- ered some of the important properties of the Fourier transform. These are summarized in Table 4.1, in which we have also indicated the section of this chapter in which each prop- erty has been discussed. In Table 4.2, we have assembled a list of many of the basic and important Fourier transform pairs. We will encounter many of these repeatedly as we apply the tools of TABLE 4. 1 PROPERTIES OF THE FOURIER TRANSFORM Section Property Aperiodic signal Fourier transform x(t) X(jw) y(t) Y(jw) ----------------------------------------------------------- 4.3.1 Linearity ax(t) + by(t) aX(jw) + bY(jw) 4.3.2 Time Shifting x(t- to) e-Jwto X(jw) 4.3.6 Frequency Shifting eiwof x(t) X(j(w- wo)) 4.3.3 Conjugation x*(t) X*(-jw) 4.3.5 Time Reversal x( -t) X(- jw) 4.3.5 Time and Frequency x(at) _!_xCW) Scaling lal a 4.4 Convolution x(t) * y(t) X(jw)Y(jw) 4.5 Multiplication x(t)y(t) ~ J:!ue);(j(w- 8))de d 4.3.4 Differentiation in Time dt x(t) jwX(jw) 1 . 4.3.4 Integration f>' x(t)dt -J:W- X(]w) + 7TX(O)o(w) 4.3.6 Differentiation in tx(t) jd~X(jw) Frequency r(jw) ~X'(- jw) <Re{X(jw )} = (fk{X(- jw )} 4.3.3 Conjugate Symmetry x(t) real 9m{X(jw )} = -9m{X(- jw)} for Real Signals /X(jw)/ = /X(-jw)/ <r.X(jw) = -<r.X(- jw) 4.3.3 Symmetry for Real and x(t) real and even X(jw) real and even Even Signals 4.3.3 Symmetry for Real and x(t) real and odd X(jw) purely imaginary and odd Odd Signals Xe(t) = 8v{x(t)} [x(t) real] <Re{X(jw)} 4.3.3 Even-Odd Decompo- sition for Real Sig- X0 (t) = 0d{x(t)} [x(t) real] j9m{X(jw)} nals 4.3.7 Parseval's Relation for IA periodic Signals I+ oo 1 +oo -x /x(t)i1dt = 27T -x /X(jw)!Zdw"
4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs,"328 The Continuous-Time Fourier Transform Chap.4 4.6 TABLES OF FOURIER PROPERTIES AND OF BASIC FOURIER TRANSFORM PAIRS In the preceding sections and in the problems at the end of the chapter, we have consid- ered some of the important properties of the Fourier transform. These are summarized in Table 4.1, in which we have also indicated the section of this chapter in which each prop- erty has been discussed. In Table 4.2, we have assembled a list of many of the basic and important Fourier transform pairs. We will encounter many of these repeatedly as we apply the tools of TABLE 4. 1 PROPERTIES OF THE FOURIER TRANSFORM Section Property Aperiodic signal Fourier transform x(t) X(jw) y(t) Y(jw) ----------------------------------------------------------- 4.3.1 Linearity ax(t) + by(t) aX(jw) + bY(jw) 4.3.2 Time Shifting x(t- to) e-Jwto X(jw) 4.3.6 Frequency Shifting eiwof x(t) X(j(w- wo)) 4.3.3 Conjugation x*(t) X*(-jw) 4.3.5 Time Reversal x( -t) X(- jw) 4.3.5 Time and Frequency x(at) _!_xCW) Scaling lal a 4.4 Convolution x(t) * y(t) X(jw)Y(jw) 4.5 Multiplication x(t)y(t) ~ J:!ue);(j(w- 8))de d 4.3.4 Differentiation in Time dt x(t) jwX(jw) 1 . 4.3.4 Integration f>' x(t)dt -J:W- X(]w) + 7TX(O)o(w) 4.3.6 Differentiation in tx(t) jd~X(jw) Frequency r(jw) ~X'(- jw) <Re{X(jw )} = (fk{X(- jw )} 4.3.3 Conjugate Symmetry x(t) real 9m{X(jw )} = -9m{X(- jw)} for Real Signals /X(jw)/ = /X(-jw)/ <r.X(jw) = -<r.X(- jw) 4.3.3 Symmetry for Real and x(t) real and even X(jw) real and even Even Signals 4.3.3 Symmetry for Real and x(t) real and odd X(jw) purely imaginary and odd Odd Signals Xe(t) = 8v{x(t)} [x(t) real] <Re{X(jw)} 4.3.3 Even-Odd Decompo- sition for Real Sig- X0 (t) = 0d{x(t)} [x(t) real] j9m{X(jw)} nals 4.3.7 Parseval's Relation for IA periodic Signals I+ oo 1 +oo -x /x(t)i1dt = 27T -x /X(jw)!Zdw Sec. 4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs 329 TABLE 4.2 BASIC FOURIER TRANSFORM PAIRS Fourier series coefficients Signal Fourier transform (if periodic) L+oo +oo akejkwot 27T L akB(w - kw0 ) k=-00 k=-00 a1 = 1 ak = 0, otherwise a1 = a-1 = 4 coswot 1r[8(w - wo) + B(w + wo)] ak = 0, otherwise I ~ a1 = -a-1 = 2] sinwot [B(w - wo) - B(w + wo)] 1 <lk = 0, otherwise ao = 1, ak = 0, k =F 0 x(t) = 1 27T8(w) this is the Fourier series representation for) (any choice of T > 0 Periodic square wave !1, ltl < T1 x(t) = 0, T1 < ltl ~ ~ ~ 2 sin kwoT1 !:.'( _ k ) woT1 . (kwoT1 ) _ sin kwoT1 L u w w 0 -- smc -- - k1r and k=-00 k 7T 7T x(t + T) = x(t) +oo L B(t- nT) 27T f 8 (w- 27Tk) ak = T1 for all k T k=-00 T 1 ltl < T1 2sinwTJ x(t) { ' 0, ltl > T1 w sin Wt X(jw)= 1' lwl < W 7Tt { 0, lwi>W B(t) 1 u(t) ~ + 1r8(w) JW B(t- to) a+ jw te-at u(t), CRe{a} > 0 (a+jw) 2 tn-1 -at () (n-l)!e u t, CRe{a} > 0 (a+jw)n 330 The Continuous-Time Fourier Transform Chap.4 Fourier analysis in our examination of signals and systems. All of the transform pairs, except for the last one in the table, have been considered in examples in the preceding sections. The last pair is considered in Problem 4.40. In addition, note that several of the signals in Table 4.2 are periodic, and for these we have also listed the corresponding Fourier series coefficients. 4.7 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENTIAL EQUATIONS As we have discussed on several occasions, a particularly important and useful class of continuous-time LTI systems is those for which the input and output satisfy a linear constant-coefficient differential equation of the form ±a k dky;t) = ±bk dkx;o. (4.72) k=O dt k=O dt In this section, we consider the question of determining the frequency response of such an LTI system. Throughout the discussion we will always assume that the frequency response of the system exists, i.e., that eq. (3.121) converges. There are two closely related ways in which to determine the frequency response H(jw) for an LTI system described by the differential equation (4.72). The first of these, which relies on the fact that complex exponential signals are eigenfunctions of LTI systems, was used in Section 3.10 in our analysis of several simple, nonideal filters. Specifically, if x(t) = eJwt, then the output must be y(t) = H (jw )eJwt. Substituting these expressions into the differential equation (4.72) and performing some algebra, we can then solve for H(jw ). In this section we use an alternative approach to arrive at the same answer, making use of the differentiation property, eq. (4 . 31 ), of Fourier transforms. Consider an LTI system characterized by eq. (4.72). From the convolution property, Y(jw) = H(jw)X(jw), or equivalently, H( 1' w) = Y(jw) (4.73) X(jw)' where X(jw ), Y(jw ), and H(jw) are the Fourier transforms of the input x(t), output y(t), and impulse response h(t), respectively. Next, consider applying the Fourier transform to both sides of eq. (4.72) to obtain (4.74) From the linearity property, eq. (4.26), this becomes ~ ~{ dky(t)} = ~ b ~{ dkx(t)} L ak dtk L k dtk ' (4.75) k=O k=O"
4.7 Systems Characterized by Linear Constant-Coefficient Differential Equations,"330 The Continuous-Time Fourier Transform Chap.4 Fourier analysis in our examination of signals and systems. All of the transform pairs, except for the last one in the table, have been considered in examples in the preceding sections. The last pair is considered in Problem 4.40. In addition, note that several of the signals in Table 4.2 are periodic, and for these we have also listed the corresponding Fourier series coefficients. 4.7 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENTIAL EQUATIONS As we have discussed on several occasions, a particularly important and useful class of continuous-time LTI systems is those for which the input and output satisfy a linear constant-coefficient differential equation of the form ±a k dky;t) = ±bk dkx;o. (4.72) k=O dt k=O dt In this section, we consider the question of determining the frequency response of such an LTI system. Throughout the discussion we will always assume that the frequency response of the system exists, i.e., that eq. (3.121) converges. There are two closely related ways in which to determine the frequency response H(jw) for an LTI system described by the differential equation (4.72). The first of these, which relies on the fact that complex exponential signals are eigenfunctions of LTI systems, was used in Section 3.10 in our analysis of several simple, nonideal filters. Specifically, if x(t) = eJwt, then the output must be y(t) = H (jw )eJwt. Substituting these expressions into the differential equation (4.72) and performing some algebra, we can then solve for H(jw ). In this section we use an alternative approach to arrive at the same answer, making use of the differentiation property, eq. (4 . 31 ), of Fourier transforms. Consider an LTI system characterized by eq. (4.72). From the convolution property, Y(jw) = H(jw)X(jw), or equivalently, H( 1' w) = Y(jw) (4.73) X(jw)' where X(jw ), Y(jw ), and H(jw) are the Fourier transforms of the input x(t), output y(t), and impulse response h(t), respectively. Next, consider applying the Fourier transform to both sides of eq. (4.72) to obtain (4.74) From the linearity property, eq. (4.26), this becomes ~ ~{ dky(t)} = ~ b ~{ dkx(t)} L ak dtk L k dtk ' (4.75) k=O k=O Sec. 4.7 Systems Characterized by Linear Constant-Coefficient Differential Equations 331 and from the differentiation property, eq. (4.31), N M L ak(jw)kY(jw) = L bk(jw)kX(jw), k=O k=O or equivalently, Thus, from eq. (4.73), (4.76) Observe that H(jw) is thus a rational function; that is, it is a ratio of polynomials in (jw ). The coefficients of the numerator polynomial are the same coefficients as those that appear on the right-hand side of eq. (4.72), and the coefficients of the denominator polynomial are the same coefficients as appear on the left side of eq. (4.72). Hence, the frequency response given in eq. (4.76) for the LTI system characterized by eq. (4.72) can be written down directly by inspection. The differential equation (4 . 72) is commonly referred to as an Nth-order differen- tial equation, as the equation involves derivatives of the output y(t) up through the Nth derivative. Also, the denominator of H(jw) in eq. (4.76) is an Nth-order polynomial in (jw). Example 4.24 Consider a stable LTI system characterized by the differential equation (dy[((t) + ay(t) = x(t), (4.77) with a> 0. From eq. (4.76), the frequency response is 1 H(jw) = -.- -. (4.78) JW +a Comparing this with the result of Example 4.1, we see that eq. (4.78) is the Fourier transform of e-at u(t). The impulse response of the system is then recognized as h(t) = e-at u(t). Example 4.25 Consider a stable LTI system that is characterized by the differential equation 332 The Continuous-Time Fourier Transform Chap. 4 From eq. (4.76), the frequency response is H( . ) _ (jw) + 2 (4.79) JW - (jw)2 + 4(jw) + 3 · To determine the corresponding impulse response, we require the inverse Fourier trans- form of H(jw ). This can be found using the technique of partial-fraction expansion em- ployed in Example 4.19 and discussed in detail in the appendix. (In particular, see Ex- ample A.1, in which the details of the calculations for the partial-fraction expansion of eq. (4.79) are worked out.) As a first step, we factor the denominator of the right-hand side of eq. (4.79) into a product of lower order terms: H(. ) jw + 2 (4.80) JW = (jw + 1)(jw + 3) Then, using the method of partial-fraction expansion, we find that I I H( . ) - 2 )W - -.-+-1 + -.-2- 3. )W )W + The inverse transform of each term can be recognized from Example 4.24, with the result that The procedure used in Example 4.25 to obtain the inverse Fourier transform is gen- erally useful in inverting transforms that are ratios of polynomials in jw. In particular, we can use eq. (4.76) to determine the frequency response of any LTI system described by a linear constant-coefficient differential equation and then can calculate the impulse response by performing a partial-fraction expansion that puts the frequency response into a form in which the inverse transform of each term can be recognized by inspection. In addition, if the Fourier transform X(jw) of the input to such a system is also a ratio of polynomials in jw, then so is Y(jw) = H(jw )X(jw ). In this case we can use the same technique to solve the differential equation-that is, to find the response y(t) to the input x(t). This is illustrated in the next example. Example 4.26 Consider the system of Example 4.25, and suppose that the input is x(t) = e~ 1 u(t). Then, using eq. (4.80), we have 1 Y(jw) = H(jw)X(jw) = [(jw !~)~~ + 3)] [jw + 1] jw + 2 (jw + 1)2 (jw + (4.81) 3) · Sec. 4.8 Summary 333 As discussed in the appendix, in this case the partial-fraction expansion takes the form (4.82) where A 11 , A 12 , and A21 are constants to be determined. In Example A.2 in the appendix, the technique of partial-fraction expansion is used to determine these constants. The values obtained are 4' so that I I I Y( · ) 4 2 4 JW = jw + 1 + (jw + 1) 2 - jw + 3 · (4.83) Again, the inverse Fourier transform for each term in eq. (4.83) can be obtained by in- spection. The first and third terms are of the same type that we have encountered in the preceding two examples, while the inverse transform of the second term can be obtained from Table 4.2 or, as was done in Example 4.19, by applying the dual of the differenti- ation property, as given in eq. (4.40), to ll(jw + 1). The inverse transform of eq. (4.83) is then found to be From the preceding examples, we see how the techniques of Fourier analysis allow us to reduce problems concerning LTI systems characterized by differential equations to straightforward algebraic problems. This important fact is illustrated further in a number of the problems at the end of the chapter. In addition (see Chapter 6), the algebraic structure of the rational transforms encountered in dealing with LTI systems described by differen- tial equations greatly facilitate the analysis of their frequency-domain properties and the development of insights into both the time-domain and frequency-domain characteristics of this important class of systems. 4.8 SUMMARY In this chapter, we have developed the Fourier transform representation for continous-time signals and have examined many of the properties that make this transform so useful. In particular, by viewing an aperiodic signal as the limit of a periodic signal as the period becomes arbitrarily large, we derived the Fourier transform representation for aperiodic signals from the Fourier series representation for periodic signals developed in Chapter 3. In addition, periodic signals themselves can be represented using Fourier transforms con- sisting of trains of impulses located at the harmonic frequencies of the periodic signal and with areas proportional to the corresponding Fourier series coefficients. The Fourier transform possesses a wide variety of important properties that de- scribe how different characteristics of signals are reflected in their transforms, and in"
4.8 Summary,"Sec. 4.8 Summary 333 As discussed in the appendix, in this case the partial-fraction expansion takes the form (4.82) where A 11 , A 12 , and A21 are constants to be determined. In Example A.2 in the appendix, the technique of partial-fraction expansion is used to determine these constants. The values obtained are 4' so that I I I Y( · ) 4 2 4 JW = jw + 1 + (jw + 1) 2 - jw + 3 · (4.83) Again, the inverse Fourier transform for each term in eq. (4.83) can be obtained by in- spection. The first and third terms are of the same type that we have encountered in the preceding two examples, while the inverse transform of the second term can be obtained from Table 4.2 or, as was done in Example 4.19, by applying the dual of the differenti- ation property, as given in eq. (4.40), to ll(jw + 1). The inverse transform of eq. (4.83) is then found to be From the preceding examples, we see how the techniques of Fourier analysis allow us to reduce problems concerning LTI systems characterized by differential equations to straightforward algebraic problems. This important fact is illustrated further in a number of the problems at the end of the chapter. In addition (see Chapter 6), the algebraic structure of the rational transforms encountered in dealing with LTI systems described by differen- tial equations greatly facilitate the analysis of their frequency-domain properties and the development of insights into both the time-domain and frequency-domain characteristics of this important class of systems. 4.8 SUMMARY In this chapter, we have developed the Fourier transform representation for continous-time signals and have examined many of the properties that make this transform so useful. In particular, by viewing an aperiodic signal as the limit of a periodic signal as the period becomes arbitrarily large, we derived the Fourier transform representation for aperiodic signals from the Fourier series representation for periodic signals developed in Chapter 3. In addition, periodic signals themselves can be represented using Fourier transforms con- sisting of trains of impulses located at the harmonic frequencies of the periodic signal and with areas proportional to the corresponding Fourier series coefficients. The Fourier transform possesses a wide variety of important properties that de- scribe how different characteristics of signals are reflected in their transforms, and in 334 The Continuous-Time Fourier Transform Chap.4 this chapter we have derived and examined many of these properties. Among them are two that have particular significance for our study of signals and systems. The first is the convolution property, which is a direct consequence of the eigenfunction property of com- plex exponential signals and which leads to the description of an LTI system in terms of its frequency response. This description plays a fundamental role in the frequency- domain approach to the analysis of LTI systems, which we will continue to explore in subsequent chapters. The second property of the Fourier transform that has extremely important implications is the multiplication property, which provides the basis for the frequency-domain analysis of sampling and modulation systems. We examine these sys- tems further in Chapters 7 and 8. We have also seen that the tools of Fourier analysis are particularly well suited to the examination of LTI systems characterized by linear constant- coefficient differential equations. Specifically, we have found that the frequency response for such a system can be determined by inspection and that the technique of partial-fraction expansion can then be used to facilitate the calculation of the impulse response of the system. In subsequent chapters, we will find that the convenient algebraic structure of the frequency responses of these systems allows us to gain considerable insight into their characteristics in both the time and frequency domains. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 4.1. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) e-2U-l)u(t- 1) (b) e-2lt-ll Sketch and label the magnitude of each Fourier transform. 4.2. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) B(t + 1) + B(t- 1) (b) fr{u( -2- t) + u(t- 2)} Sketch and label the magnitude of each Fourier transform. 4.3. Determine the F*o)u rier transform of each of the following periodic signals: (a) sin(21Tt + (b) 1 + cos(61rt + ¥) 4.4. Use the Fourier transform synthesis equation (4.8) to determine the inverse Fourier transforms of: (a) X1( jw) = 21T B(w) + 1r B(w - 41T) + 1r B(w + 41T)"
Problems,"334 The Continuous-Time Fourier Transform Chap.4 this chapter we have derived and examined many of these properties. Among them are two that have particular significance for our study of signals and systems. The first is the convolution property, which is a direct consequence of the eigenfunction property of com- plex exponential signals and which leads to the description of an LTI system in terms of its frequency response. This description plays a fundamental role in the frequency- domain approach to the analysis of LTI systems, which we will continue to explore in subsequent chapters. The second property of the Fourier transform that has extremely important implications is the multiplication property, which provides the basis for the frequency-domain analysis of sampling and modulation systems. We examine these sys- tems further in Chapters 7 and 8. We have also seen that the tools of Fourier analysis are particularly well suited to the examination of LTI systems characterized by linear constant- coefficient differential equations. Specifically, we have found that the frequency response for such a system can be determined by inspection and that the technique of partial-fraction expansion can then be used to facilitate the calculation of the impulse response of the system. In subsequent chapters, we will find that the convenient algebraic structure of the frequency responses of these systems allows us to gain considerable insight into their characteristics in both the time and frequency domains. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 4.1. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) e-2U-l)u(t- 1) (b) e-2lt-ll Sketch and label the magnitude of each Fourier transform. 4.2. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) B(t + 1) + B(t- 1) (b) fr{u( -2- t) + u(t- 2)} Sketch and label the magnitude of each Fourier transform. 4.3. Determine the F*o)u rier transform of each of the following periodic signals: (a) sin(21Tt + (b) 1 + cos(61rt + ¥) 4.4. Use the Fourier transform synthesis equation (4.8) to determine the inverse Fourier transforms of: (a) X1( jw) = 21T B(w) + 1r B(w - 41T) + 1r B(w + 41T) Chap. 4 Problems 335 2, O:s;w:s;2 (b) X2(jw) = -2, -2 ::; w < 0 { 0, lwl >2 4.5. Use the Fourier transform synthesis equation (4.8) to determine the inverse Fourier transform of X(jw) = IX(jw )lei<t:X(Jw), where IX(jw )I = 2{u(w + 3) - u(w - 3)}, <.X(jw) = -~w + 7T. Use your answer to determine the values oft for which x(t) = 0. 4.6. Given that x(t) has the Fourier transform X(jw ), express the Fourier transforms of the signals listed below in terms of X(jw ). You may find useful the Fourier transform properties listed in Table 4.1. (a) x 1 (t) = x(1 - t) + x( -1 - t) (b) x2(t) = x(3t- 6) 2 (c) x3(t) = ;r2 x(t - 1) 4. 7. For each of the following Fourier transforms, use Fourier transform properties (Table 4.1) to determine whether the corresponding time-domain signal is (i) real, imaginary, or neither and (ii) even, odd, or neither. Do this without evaluating the inverse of any of the given transforms. (a) X1(jw) = u(w)- u(w- 2) (b) X2(jw) = cos(2w) sin(~) (c) X3(jw) = A(w)eiB(w), where A(w) = (sin2w)/w and B(w) = 2w + ~ (d) X(jw) = L~= _00 (~)1kl 5(w - k;) 4.8. Consider the signal 0, t < -.!. 2 x(t) = !t + ~' -.!2. <- t <- .!2."" 1, t >.!. 2 (a) Use the differentiation and integration properties in Table 4.1 and the Fourier transform pair for the rectangular pulse in Table 4.2 to find a closed-form ex- pression for X(jw ). (b) What is the Fourier transform of g(t) = x(t)- ~? 4.9. Consider the signal ltl > 1 x(t) = { ~; + 1)/2, -1 ::; t::; r (a) With the help of Tables 4.1 and 4.2, determine the closed-form expression for X(jw). (b) Take the real part of your answer to part (a), and verify that it is the Fourier transform of the even part of x(t). (c) What is the Fourier transform of the odd part of x(t)? 336 The Continuous-Time Fourier Transform Chap.4 4.10. (a) Use Tables 4.1 and 4.2 to help determine the Fourier transform of the following signal: 2 x(t) = t ( ·~n (b) Use Parse val's relation and the result of the previous part to determine the nu- merical value of A = +oo t2 ( s•m f t )4 dt -00 7Tt 4.11. Given the relationships y(t) = x(t) * h(t) and g(t) = x(3t) * h(3t), and given that x(t) has Fourier transform X(jw) and h(t) has Fourier transform H(jw ), use Fourier transform properties to show that g(t) has the form g(t) = Ay(Bt). Determine the values of A and B. 4.12. Consider the Fourier transform pair -ltl ~ 2 e ~ 1 +w2"" (a) Use the appropriate Fourier transform properties to find the Fourier transform of te-ltl. (b) Use the result from part (a), along with the duality property, to determine the Fourier transform of 4t Hint: See Example 4.13. 4.13. Let x(t) be a signal whose Fourier transform is X(jw) = 5(w) + 5(w- 7T) + 5(w- 5), and let h(t) = u(t) - u(t - 2). (a) Is x(t) periodic? (b) Is x(t) * h(t) periodic? (c) Can the convolution of two aperiodic signals be periodic? Chap. 4 Problems 337 4.14. Consider a signal x(t) with Fourier transform X(jw ). Suppose we are given the following facts: 1. x(t) is real and nonnegative. 2. ~- 1 {(1 + jw )X(jw )} = Ae-2 t u(t), where A is independent oft. 00 3. J_ iX(jw )1 2 dw = 21T. 00 Determine a closed-form expression for x(t). 4.15. Let x(t) be a signal with Fourier transform X(jw ). Suppose we are given the fol- lowing facts: 1. x(t) is real. 2. x(t) = 0 fort ~ 0. 3. 2~ J_ :ooo ffie{X(jw )}eiwt dw = ltle-ltl. Determine a closed-form expression for x(t). 4.16. Consider the signal oo sin(kE:) 1T x(t) = k~oo (k*) D(t- k4 ). (a) Determine g(t) such that x(t) = sin t) ( 1Tt g(t). (b) Use the multiplication property of the Fourier transform to argue that X(jw) is periodic. Specify X(jw) over one period. 4.17. Determine whether each of the following statements is true or false. Justify your answers. (a) An odd and imaginary signal always has an odd and imaginary Fourier trans- form. (b) The convolution of an odd Fourier transform with an even Fourier transform is always odd. 4.18. Find the impulse response of a system with the frequency response 2 H(jw) = (sin (3w )) cos w w2 4.19. Consider a causal LTI system with frequency response H(jw) = . 1 3 JW + For a particular input x(t) this system is observed to produce the output y(t) = e-3tu(t)-e-4tu(t). Determine x(t). 4.20. Find the impulse response of the causal LTI system represented by the RLC circuit considered in Problem 3.20. Do this by taking the inverse Fourier transform of the circuit's frequency response. You may use Tables 4.1 and 4.2 to help evaluate the inverse Fourier transform. 338 The Continuous-Time Fourier Transform Chap. 4 BASIC PROBLEMS 4.21. Compute the Fourier transform of each of the following signals: (a) [e-at cos wot]u(t), a > 0 (b) e-31tl sin 2t (c) x(t) = { 1 +cos 7T't, lltll :::; 1 (d) 2:~-o ak o(t- kT), Ia I < 1 0, t > 1 - (e) [te-2tsin4t]u(t) (f) [sin7Tt][sin27T(t-l)] 1Tt 1T(t-l) (g) x(t) as shown in Figure P4.21(a) (h) x(t) as shown in Figure P4.21(b) 2 (i) x(t) = { 1 - t , 0 < t ~ 1 (j) """"""""+oo -lt-2nl 0, otherwise Ln=-ao e X (t) X (t) ... t t t 2 t t t !f t t -6 -5 -4 -3 -2 -1 0 1 2 3 (a) (b) Figure P4.21 4.22. Determine the continuous-time signal corresponding to each of the following transforms. <t X Ow) IXOw)l -1 w (a) [7 1 2 3 w -1 (b) Figure P4.22 Chap. 4 Problems 339 (a) X(jw) = 2sin[3(w-27T)] (w-27T) (b) X(jw) = cos(4w + 7r/3) (c) X(jw) as given by the magnitude and phase plots of Figure P4.22(a) (d) X(jw) = 2[o(w - 1) - o(w + 1)] + 3[o(w - 27r) + o(w + 27r)] (e) X(jw) as in Figure P4.22(b) 4.23. Consider the signal -t 0 < t < 1 x0 (t) = e ' - - { 0, elsewhere · Determine the Fourier transform of each of the signals shown in Figure P4.23. You should be able to do this by explicitly evaluating only the transform of x0(t) and then using properties of the Fourier transform. x0(t) -1 0 1 (a) (b) -1 0 1 0 1 (c) (d) Figure P4.23 4.24. (a) Determine which, if any, of the real signals depicted in Figure P4.24 have Fourier transforms that satisfy each of the following conditions: (1) CR-e{X(jw)} = 0 (2) dm{X(jw )} = 0 (3) There exists a real a such that ejaw X(jw) is real (4) J_::'ooX(jw)dw = 0 (5) J_::'oowX(jw)dw = 0 (6) X(jw) is periodic (b) Construct a signal that has properties (1), (4), and (5) and does not have the others. 340 The Continuous-Time Fourier Transform Chap.4 X (t) (a) (b) X (t) I 2ij3ij vv v v (c) X t) (d) x(t) = e -t2/2 (e) (f) Figure P4.24 Chap. 4 Problems 341 4.25. Let X(jw) denote the Fourier transform of the signal x(t) depicted in Figure P4.25. (a) Find 1:X(jw ). (b) Find X(jO). (c) Findf:oox(jw)dw. (d) Evaluate J: oo X(jw ) 2s:w ejlw dw. (e) Evaluate J: oo iX(Jw )1 2 dw. (f) Sketch the inverse Fourier transform of CRe{X(jw )}. Note: You should perform all these calculations without explicitly evaluating X(jw ). X (t) -1 0 2 3 Figure P4.25 4.26. (a) Compute the convolution of each of the following pairs of signals x(t) and h(t) by calculating X(jw) and H(jw ), using the convolution property, and inverse transforming. (i) x(t) = te-lt u(t), h(t) = e-4t u(t) (ii) x(t) = te-lt u(t), h(t) = te-4t u(t) (iii) x(t) = e-tu(t), h(t) = etu(-t) (b) Suppose that x(t) = e-(t-l)u(t- 2) and h(t) is as depicted in Figure P4.26. Ver- ify the convolution property for this pair of signals by showing that the Fourier transform of y(t) = x(t) * h(t) equals H(jw )X(jw ). -1 3 Figure P4.26 4.27. Consider the signals x(t) = u(t - 1) - 2u(t- 2) + u(t - 3) and 00 i(t) = L, x(t- kT), k= -00 342 The Continuous-Time Fourier Transform Chap.4 where T > 0. Let ak denote the Fourier series coefficients of i(t), and let X(jw) denote the Fourier transform of x(t). (a) Determine a closed-form expression for X(jw ). (b) Determine an expression for the Fourier coefficients ak and verify that ak = tx(i2;k ). 4.28. (a) Let x(t) have the Fourier transform X(jw ), and let p(t) be periodic with funda- mental frequency wo and Fourier series representation +oo p(t) = 2.: anejnwot. n=-oo Determine an expression for the Fourier transform of y(t) = x(t)p(t). (P4.28-1) (b) Suppose that X(jw) is as depicted in Figure P4.28(a). Sketch the spectrum of y(t) in eq. (P4.28-1) for each of the following choices of p(t): (i) p(t) = cos(t/2) (ii) p(t) = cost (iii) p(t) = cos 2t (iv) p(t) = (sin t)(sin 2t) (v) p(t) = cos 2t- cost (vi) p(t) = 2:;: _00 8(t- 1Tn) (vii) p(t) = 2:;: _00 8(t - 27Tn) (viii) p(t) = 2:;: _00 8(t - 41Tn) (ix) p(t) = 2:;: -oo 8(t- 27Tn) - i 2:;: _00 8(t- 1Tn) (x) p(t) = the periodic square wave shown in Figure P4.28(b ). X(jw) -1 w (a) p (t) J D D D rn D D D D D... (b) Figure P4.28 Chap. 4 Problems 343 4.29. A real-valued continuous-time function x(t) has a Fourier transform X(jw) whose magnitude and phase are as illustrated in Figure P4.29(a). The functions Xa(t), xb(t), Xc(t), and xd(t) have Fourier transforms whose magnitudes are identical to X(jw ), but whose phase functions differ, as shown in Figures P4.29(b)-(e). The phase functions 1:.Xa(jw) and 1:.Xb(jw) are formed by adding a linear phase to 1:.X(jw ). The function 1:Xc(jw) is formed by reflecting 1:X(jw) about w = 0, and 1:Xd(jw) is obtained by a combination of a reflection and an addition of a linear phase. Using the properties of Fourier transforms, deter- mine the expressions for xa(t), xb(t), Xc(t), and xd(t) in terms of x(t). IXGw)l 1: X Ow) (a) ..................... w ........................................... .............. ....... Slope =-a (b) Figure P4.29 344 The Continuous-Time Fourier Transform Chap.4 ...... .............................. Slope =b ............ .................. (c) w ----------- -TI/2 (d) ____ ---Slope =d ---- w ------ ---- ---- --- (e) Figure P4.29 Continued 4.30. Suppose g(t) = x(t) cost and the Fourier transform of the g(t) is G(jw) = { l, lwl ~ 2 0, otherwise· (a) Determine x(t). (b) Specify the Fourier transform X1( jw) of a signal x1( t) such that g(t) = Xt (I) COS (~1). Chap. 4 Problems 345 4.31. (a) Show that the three LTI systems with impulse responses h1 (t) = u(t), h2(t) = -28(t) + 5e-2tu(t), and all have the same response to x(t) = cost. (b) Find the impulse response of another LTI system with the same response to cost. This problem illustrates the fact that the response to cos t cannot be used to specify an LTI system uniquely. 4.32. Consider an LTI system S with impulse response h(t) = sin(4(t - 1)). 7T(t - 1) Determine the output of S for each of the following inputs: (a) x1( t) = cos(6t + ~) (b) x2(t) = 2:;= 0(~)k sin(3kt) (c) X (t) = sin(4(t+ I)) 3 7T(t+ 1) (d) X4(t) = cin2t)2 7Tt 4.33. The input and the output of a stable and causal L TI system are related by the dif- ferential equation -d2-y(t+) 6d-y-(t) 2 + 8 y( t) -- 2 x(t ) dt dt (a) Find the impulse response of this system. (b) What is the response of this system if x(t) = te-2t u(t)? (c) Repeat part (a) for the stable and causal LTI system described by the equation d 2y(t) r;:;. dy(t) ( ) = d 2 x(t) _ ( ) dt2 + .y 2L dt + y t 2 dt2 2 X t 4.34. A causal and stable LTI system S has the frequency response . jw +4 H(jw)=6 -w 2 + sJW·. 346 The Continuous-Time Fourier Transform Chap.4 (a) Determine a differential equation relating the input x(t) and output y(t) of S. (b) Determine the impulse response h(t) of S. (c) What is the output of S when the input is x(t) = e-4tu(t)-te-4tu(t)? 4.35. In this problem, we provide examples of the effects of nonlinear changes in phase. (a) Consider the continuous-time LTI system with frequency response H( ·w) = a- jw J a + J.W ' where a> 0. What is the magnitude of H(jw )? What is <r:..H(jw )? What is the impulse response of this system? (b) Determine the output of the system of part (a) with a = 1 when the input is cos(t/ J3) + cost + cos J3t. Roughly sketch both the input and the output. 4.36. Consider an LTI system whose response to the input x(t) = [e-t + e- 3t]u(t) is y(t) = [2e-t- 2e-4t]u(t). (a) Find the frequency response of this system. (b) Determine the system's impulse response. (c) Find the differential equation relating the input and the output of this system. ADVANCED PROBLEMS 4.37. Consider the signal x(t) in Figure P4.37. (a) Find the Fourier transform X(jw) of x(t). (b) Sketch the signal 00 x(t) = x(t) * L o(t - 4k). k= -00 (c) Find another signal g(t) such that g(t) is not the same as x(t) and 00 x(t) = g(t) * L o(t- 4k). k= -00 Chap. 4 Problems 347 (d) Argue that, although G(jw) is different from X(jw), G(jn;k) = X(jn;k) for all integers k. You should not explicitly evaluate G(jw) to answer this question. X (t) Figure P4.37 4.38. Let x(t) be any signal with Fourier transform X(jw ). The frequency-shift property of the Fourier transform may be stated as . ~ e1wot x(t) ~ X(j(w - wo)). (a) Prove the frequency-shift property by applying the frequency shift to the anal- ysis equation X(jw) = r~ x(t)e- Jwt dt. (b) Prove the frequency-shift property by utilizing the Fourier transform of eiwot in conjunction with the multiplication property of the Fourier transform. 4.39. Suppose that a signal x(t) has Fourier transform X(jw ). Now consider another signal g(t) whose shape is the same as the shape of X(jw ); that is, g(t) = X(jt). (a) Show that the Fourier transform G(jw) of g(t) has the same shape as 21Tx( -t); that is, show that G(jw) = 21Tx(-w). (b) Using the fact that g:{o(t + B)} = efBw in conjunction with the result from part (a), show that g:{ejBt} = 21T o(w - B). 4.40. Use properties of the Fourier transform to show by induction that the Fourier trans- form of tn-1 x(t) = (n _ I)! e-at u(t), a > 0, 348 The Continuous-Time Fourier Transform Chap.4 is 1 (a+jw)n' 4.41. In this problem, we derive the multiplication property ofthe continuous-time Fourier transform. Let x(t) and y(t) be two continuous-time signals with Fourier transforms X(jw) and Y(jc.q ), respectively. Also, let g(t) denote the inverse Fourier transform of 2~ {X(jw) * Y(jw )}. (a) Show that g(t) = 1 f +oo [ 1 f +oo . ] 7T -oo X(j8) 7T -oo Y(j(w - O))eJwt dw dO. 2 2 (b) Show that 1 f +oo . . - Y(j(w - 8))e1wt dw = el8t y(t). 27T -00 (c) Combine the results of parts (a) and (b) to conclude that g(t) = x(t)y(t). 4.42. Let g1 (t) = {[cos(wot)]x(t)} * h(t) and g2(t) = {[sin(wot)]x(t)} * h(t), where 00 x(t) = L akejkiOOt k= -DO is a real-valued periodic signal and h(t) is the impulse response of a stable LTI system. (a) Specify a value for w 0 and any necessary constraints on H(jw) to ensure that g1 (t) = (Jl.e{as} and (b) Give an example of h(t) such that H(jw) satisfies the constraints you specified in part (a). 4.43. Let sint g(t) = x(t) cos 2 t * -. 7Tt Assuming that x(t) is real and X(jw) = 0 for lwl 2: 1, show that there exists an LTI system S such that s x(t) ~ g(t). Chap. 4 Problems 349 4.44. The output y(t) of a causal LTI system is related to the input x(t) by the equation dy(t) + J +oc -d- lOy(t) = x( r)z(t- r) dr - x(t), f -oc where z(t) = e-t u(t) + 3 o(t). (a) Find the frequency response H(jw) = Y(jw )IX(jw) of this system. (b) Determine the impulse response of the system. 4.45. In the discussion in Section 4.3.7 ofParseval's relation for continuous-time signals, we saw that +oc 2 1 J +oc 2 J- oc lx(t)l dt = 21T -oo IX(jw )1 dw. This says that the total energy of the signal can be obtained by integrating IX(jw )1 2 over all frequencies. Now consider a real-valued signal x(t) processed by the ideal bandpass filter H(jw) shown in Figure P4.45. Express the energy in the output sig- nal y(t) as an integration over frequency of IX(Jw )1 2 • For~ sufficiently small so that IX(Jw )I is approximately constant over a frequency interval of width~. show that the energy in the output y(t) of the bandpass filter is approximately proportional to ~IX(Jwo)l2 • On the basis of the foregoing result, ~IX(jw0 )1 2 is proportional to the energy in the signal in a bandwidth~ around the frequency w0• For this reason, IX(jw )12 is often referred to as the energy-density spectrum of the signal x(t). ---1 a 1- 0 w Figure P4.45 4.46. In Section 4.5 .1, we discussed the use of amplitude modulation with a complex exponential carrier to implement a bandpass filter. The specific system was shown in Figure 4.26, and if only the real part of f(t) is retained, the equivalent bandpass filter is that shown in Figure 4.30. In Figure P4.46, we indicate an implementation of a bandpass filter using sinusoidal modulation and lowpass filters. Show that the output y(t) of the sys- tem is identical to that which would be obtained by retaining only CRe{f(t)} in Fig- ure 4.26. 350 The Continuous-Time Fourier Transform Chap.4 x(t) y(t) t EJ---<r-1 I t w Figure P4.46 4.47. An important property of the frequency response H(jw) of a continuous-time LTI system with a real, causal impulse response h(t) is that H(jw) is completely spec- ified by its real part, (Jl.e{H(jw )}. The current problem is concerned with deriving and examining some of the implications of this property, which is generally referred to as real-part sufficiency. (a) Prove the property of real-part sufficiency by examining the signal he(t), which is the even part of h(t). What is the Fourier transform of he(t)? Indicate how h(t) can be recovered from he(t). (b) If the real part of the frequency response of a causal system is (Jl.e{H(jw )} = cos w, what is h(t)? (c) Show that h(t) can be recovered from h 0 (t), the odd part of h(t), for every value of t except t = 0. Note that if h(t) does not contain any singularities [o(t), u1( t), u2(t), etc.] at t = 0, then the frequency response +oc H(jw) = J-o c h(t)e- jwt dt will not change if h(t) is set to some arbitrary finite value at the single point t = 0. Thus, in this case, show that H(jw) is also completely specified by its imaginary part. Chap. 4 Problems 351 EXTENSION PROBLEMS 4.48. Let us consider a system with a real and causal impulse response h(t) that does not have any singularities at t = 0. In Problem 4.47, we saw that either the real or the imaginary part of H(jw) completely determines H(jw ). In this problem we derive an explicit relationship between HR(jw) and H1(jw ), the real and imaginary parts of H(jw). (a) To begin, note that since h(t) is causal, h(t) = h(t)u(t), (P4.48-l) except perhaps at t = 0. Now, since h(t) contains no singularities at t = 0, the Fourier transforms of both sides of eq. (P4.48-l) must be identical. Use this fact, together with the multiplication property, to show that H( J.W ) -- -1.- J+x -H(-j'Y-J)d 'YJ. (P4.48-2) j7T -X w- 'YJ Use eq. (P4.48-2) to determine an expression for HR(jw) in terms of H 1(jw) and one for H1(jw) in terms of HR(jw ). (b) The operation y( )t _- -1 J +x X( T) d --T (P4.48-3) 7T -X t- T is called the Hilbert transform. We have just seen that the real and imaginary parts of the transform of a real, causal impulse response h(t) can be determined from one another using the Hilbert transform. Now considereq. (P4.48-3), and regard y(t) as the output of an LTI system with input x(t). Show that the frequency response of this system is H( . ) = { - j, w > 0 JW ]., w < o· (c) What is the Hilbert transform of the signal x(t) = cos 3t? 4.49. Let H(jw) be the frequency response of a continuous-time LTI system, and suppose that H(jw) is real, even, and positive. Also, assume that max{H(jw )} = H(O). w (a) Show that: (i) The impulse response, h(t), is real. (ii) max{ih(t)i} = h(O). Hint: If f(t. w) is a complex function of two variables, then 352 The Continuous-Time Fourier Transform Chap.4 (b) One important concept in system analysis is the bandwidth of an LTI system. There are many different mathematical ways in which to define bandwidth, but they are related to the qualitative and intuitive idea that a system with fre- quency response G(jw) essentially ""stops"" signals of the form eiwt for values of w where G(jw) vanishes or is small and ""passes"" those complex exponentials in the band of frequency where G(jw) is not small. The width of this band is the bandwidth. These ideas will be made much clearer in Chapter 6, but for now we will consider a special definition of bandwidth for those systems with frequency responses that have the properties specified previously for H(jw ). Specifically, one definition of the bandwidth Bw of such a system is the width of the rect- angle of height H(jO) that has an area equal to the area under H(jw ). This is illustrated in Figure P4.49(a). Note that since H(jO) = maxw H(jw ), the fre- quencies within the band indicated in the figure are those for which H (jw) is largest. The exact choice of the width in the figure is, of course, a bit arbitrary, but we have chosen one definition that allows us to compare different systems and to make precise a very important relationship between time and frequency. What is the bandwidth of the system with frequency response . = { 1, lwl < w? H(jw) 0, lwi>W. H(jw) H(O) ---: Area of rectangle = 1- area under H (jw) w (a) Figure P4.49a (c) Find an expression for the bandwidth Bw in terms of H(jw ). (d) Let s(t) denote the step response of the system set out in part (a). An important measure of the speed of response of a system is the rise time, which, like the bandwidth, has a qualitative definition, leading to many possible mathematical definitions, one of which we will use. Intuitively, the rise time of a system is a measure of how fast the step response rises from zero to its final value, s(oo) = lim s(t). (-H:IJ Thus, the smaller the rise time, the faster is the response of the system. For the system under consideration in this problem, we will define the rise time as s(oo) tr = h(O) Chap. 4 Problems 353 Since s' (t) = h(t), and also because of the property that h(O) = maxt h(t), tr is the time it would take to go from zero to s( oo) while maintaining the maximum rate of change of s(t). This is illustrated in Figure P4.49(b ). Find an expression for tr in terms of H(jw ). s(t) (b) Figure P4.49b (e) Combine the results of parts (c) and (d) to show that Bwtr = 27r. (P4.49-l) Thus, we cannot independently specify both the rise time and the bandwidth of our system. For example, eq. (P4.49-l) implies that, if we want a fast system (tr small), the system must have a large bandwidth. This is a fundamental trade-off that is of central importance in many problems of system design. 4.50. In Problems 1.45 and 2.67, we defined and examined several of the properties and uses of correlation functions. In the current problem, we examine the properties of such functions in the frequency domain. Let x(t) and y(t) be two real signals. Then the cross-correlation function of x(t) and y(t) is defined as c/>xy(t) = r~~ X(t + T)y( T) dT. Similarly, we can define cf>_vx(t), cf>xx(t), and c/>yy(t). [The last two of these are called the autocorrelation functions of the signals x(t) and y(t), respectively.] Let <I> xy(jw ), <l>yx(jw ), <l>xxUw ), and <l>yy(jw) denote the Fourier transforms of cf>xy(t), cf>.vx(t), cf>xx(t), and cf>.v_v(t), respectively. (a) What is the relationship between <l>xy(jw) and <l>yx(}w )? (b) Find an expression for <l>xy(jw) in terms of X(jw) and Y(jw ). (c) Show that <l>_u(jw) is real and nonnegative for every w. (d) Suppose now that x(t) is the input to an LTI system with a real-valued impulse response and with frequency response H(jw) and that y(t) is the output. Find expressions for <l>xy(jw) and <l>yy(}w) in terms of <l>xxUw) and H(jw ). 354 The Continuous-Time Fourier Transform Chap.4 (e) Let x(t) be as is illustrated in Figure P4.50, and let the LTI system impulse response be h(t) = e-at u(t), a > 0. Compute <I> xxCjw ), <I> xy(jw ), and <l>yy(jw) using the results of parts (a)-( d). (f) Suppose that we are given the following Fourier transform of a function cf>(t): . w 2 + 100 <l>(jw) = w 2 + 25 · Find the impulse responses of two causal, stable LTI systems that have autocor- relation functions equal to cp(t). Which one of these has a causal, stable inverse? x(t) Figure P4.50 4.51. (a) Consider two LTI systems with impulse responses h(t) and g(t), respectively, and suppose that these systems are inverses of one another. Suppose also that the systems have frequency responses denoted by H(jw) and G(jw ), respectively. What is the relationship between H(jw) and G(jw )? (b) Consider the continuous-time LTI system with frequency response 1 2 < iwl < 3 H(jw) = { 0,· otherwise (i) Is it possible to find an input x(t) to this system such that the output is as depicted in Figure P4.50? If so, find x(t). If not, explain why not. (ii) Is this system invertible? Explain your answer. (c) Consider an auditorium with an echo problem. As discussed in Problem 2.64, we can model the acoustics of the auditorium as an LTI system with an im- pulse response consisting of an impulse train, with the kth impulse in the train corresponding to the kth echo. Suppose that in this particular case the impulse response is h(t) = L e-kT 8(t- kT), k=O where the factor e-kT represents the attenuation of the kth echo. In order to make a high-quality recording from the stage, the effect of the echoes must be removed by performing some processing of the sounds sensed by the recording equipment. In Problem 2.64, we used convolutional techniques to consider one example of the design of such a processor (for a different acous- tic model). In the current problem, we will use frequency-domain techniques. Specifically, let G(jw) denote the frequency response of the LTI system to be Chap. 4 Problems 355 used to process the sensed acoustic signal. Choose G(jw) so that the echoes are completely removed and the resulting signal is a faithful reproduction of the original stage sounds. (d) Find the differential equation for the inverse of the system with impulse re- sponse h(t) = 2 O(t) + UJ (t). (e) Consider the LTI system initially at rest and described by the differential equa- tion d 2y(t) dy(t) ( ) _ d 2 x(t) dx(t) ( ) d t2 + 6 d t + 9 y t - ~ + 3 ----;[( + 2 X t . The inverse of this system is also initially at rest and described by a differen- tial equation. Find the differential equation describing the inverse, and find the impulse responses h(t) and g(t) of the original system and its inverse. 4.52. Inverse systems frequently find application in problems involving imperfect mea- suring devices. For example, consider a device for measuring the temperature of a liquid. It is often reasonable to model such a device as an LTI system that, because of the response characteristics of the measuring element (e.g., the mercury in ather- mometer), does not respond instantaneously to temperature changes. In particular, assume that the response of this device to a unit step in temperature is s(t) = (1 - e -r12 )u(t). (P4.52-1) (a) Design a compensatory system that, when provided with the output of the mea- suring device, produces an output equal to the instantaneous temperature of the liquid. (b) One of the problems that often arises in using inverse systems as compensators for measuring devices is that gross inaccuracies in the indicated temperature may occur if the actual output of the measuring device produces errors due to small, erratic phenomena in the device. Since there always are such sources of error in real systems, one must take them into account. To illustrate this, consider a measuring device whose overall output can be modeled as the sum of the response of the measuring device characterized by eq. (P4.52-1) and an interfering ""noise"" signal n(t). Such a model is depicted in Figure P4.52(a), where we have also included the inverse system of part (a), which now has as its input the overall output of the measuring device. Suppose that n(t) = sin wt. What is the contribution of n(t) to the output of the inverse system, and how does this output change as w is increased? (c) The issue raised in part (b) is an important one in many applications of LTI system analysis. Specifically, we are confronted with the fundamental trade- off between the speed of response of the system and the ability of the system to attenuate high-frequency interference. In part (b) we saw that this trade- off implied that, by attempting to speed up the response of a measuring device (by means of an inverse system), we produced a system that would also amplify 356 The Continuous-Time Fourier Transform Chap.4 ~--------------------1 I Actual measuring device n(t) I I I I I I I LTI model of Inverse system ....I. ... I measuring device + _l to LTI model I s(t) = (1- e-t/2 u(t) I of measuring ) I I device l ____________________ j (a) r--------------------1 I n(t) I I Perfect measuring I ~ device ...,.._1--...,.~l Compensating ~ I s (t) = u (t) 1 system IL ___________________ JI (b) Figure P4.52 corrupting sinusoidal signals. To illustrate this concept further, consider a mea- suring device that responds instantaneously to changes in temperature, but that also is corrupted by noise. The response of such a system can be modeled, as depicted in Figure P4.52(b ), as the sum of the response of a perfect measuring qevice and a corrupting signal n(t). Suppose that we wish to design a compen- satory system that will slow down the response to actual temperature variations, but also will attenuate the noise n(t). Let the impulse response of this system be h(t) = ae -at u(t). Choose a so that the overall system of Figure P4.52(b) responds as quickly as possible to a step change in temperature, subject to the constraint that the amplitude of the portion of the output due to the noise n(t) = sin 6t is no larger than 114. 4.53. As mentioned in the text, the techniques of Fourier analysis can be extended to signals having two independent variables. As their one-dimensional counterparts do in some applications, these techniques play an important role in other applications, such as image processing. In this problem, we introduce some of the elementary ideas of two-dimensional Fourier analysis. Let x(t1, t2) be a signal that depends upon two independent variables t1 and t2 . The two-dimensional Fourier transform of x(t1, t2 ) is defined as X(jw,, jw2) = L+: L+xx x(t,, t2)e- j(w,t, +w,t,) dt, dt2. (a) Show that this double integral can be performed as two successive one- dimensional Fourier transforms, first in t 1 with t2 regarded as fixed and then in t2. Chap. 4 Problems 357 (b) Use the result of part (a) to determine the inverse transform-that is, an expres- sion for x(tJ, t2) in terms of X(jw1, jw2). (c) Determine the two-dimensional Fourier transforms of the following signals: (i) x(t1, t2) = e-t1 +2t2 u(t1 - 1)u(2 - t2) •• { e-1t~l-lt2l, if -1 < t1 ::; 1 and -1 ::; t ::; 1 (n) x(t1, t2) = O, 2 otherwise (iii) x(t , t2) = { e-lt1Ht21, ifhO ::; .t1 ::; 1 or 0 ::; t2 ::; 1 (or both) 1 0, ot erw1se (iv) x(t1, t2) as depicted in Figure P4.53. (v) e-ltl +t2l-lt1-t2l x(t1, t2) = 1 in shaded area and 0 outside -1 -1 Figure P4.53 (d) Determine the signal x(t1, t2) whose two-dimensional Fourier transform is 2 X(jw1, jw2) = ~ l>(w2- 2wJ). 4 + )WJ (e) Let x(t1, t2) and h(t1, t2) be two signals with two-dimensional Fourier trans- forms X(jw 1, jw2) and H(jw 1, jw2), respectively. Determine the transforms of the following signals in terms of X(jw 1, jw2) and H(jw 1, jw2): (i) x(t1 - T1, t2 - T2) (ii) x(at1, bt2) 00 00 (iii) y(tJ, t2) = J_+ J_+ X(T], T2)h(t1 - T], f2- T2)dT1 d T2 00 00 5 THE DISCRETE-TIME FOURIER TRANSFORM 5.0 INTRODUCTION In Chapter 4, we introduced the continuous-time Fourier transform and developed the many characteristics of that transform which make the methods of Fourier analysis of such great value in analyzing and understanding the properties of continuous-time signals and systems. In the current chapter, we complete our development of the basic tools of Fourier analysis by introducing and examining the discrete-time Fourier transform. In our discussion of Fourier series in Chapter 3, we saw that there are many similari- ties and strong parallels in analyzing continuous-time and discrete-time signals. However, there are also important differences. For example, as we saw in Section 3.6, the Fourier series representation of a discrete-time periodic signal is a .finite series, as opposed to the infinite series representation required for continuous-time periodic signals. As we will see in this chapter, there are corresponding differences between continuous-time and discrete- time Fourier transforms. In the remainder of the chapter, we take advantage of the similarities between continuous-time and discrete-time Fourier analysis by following a strategy essentially identical to that used in Chapter 4. In particular, we begin by extending the Fourier se- ries description of periodic signals in order to develop a Fourier transform representation for discrete-time aperiodic signals, and we follow with an analysis of the properties and characteristics of the discrete-time Fourier transform that parallels that given in Chap- ter 4. By doing this, we not only will enhance our understanding of the basic concepts of Fourier analysis that are common to both continuous and discrete time, but also will con- trast their differences in order to deepen our understanding of the distinct characteristics of each. 358"
5 The Discrete-Time Fourier Transform,"5 THE DISCRETE-TIME FOURIER TRANSFORM 5.0 INTRODUCTION In Chapter 4, we introduced the continuous-time Fourier transform and developed the many characteristics of that transform which make the methods of Fourier analysis of such great value in analyzing and understanding the properties of continuous-time signals and systems. In the current chapter, we complete our development of the basic tools of Fourier analysis by introducing and examining the discrete-time Fourier transform. In our discussion of Fourier series in Chapter 3, we saw that there are many similari- ties and strong parallels in analyzing continuous-time and discrete-time signals. However, there are also important differences. For example, as we saw in Section 3.6, the Fourier series representation of a discrete-time periodic signal is a .finite series, as opposed to the infinite series representation required for continuous-time periodic signals. As we will see in this chapter, there are corresponding differences between continuous-time and discrete- time Fourier transforms. In the remainder of the chapter, we take advantage of the similarities between continuous-time and discrete-time Fourier analysis by following a strategy essentially identical to that used in Chapter 4. In particular, we begin by extending the Fourier se- ries description of periodic signals in order to develop a Fourier transform representation for discrete-time aperiodic signals, and we follow with an analysis of the properties and characteristics of the discrete-time Fourier transform that parallels that given in Chap- ter 4. By doing this, we not only will enhance our understanding of the basic concepts of Fourier analysis that are common to both continuous and discrete time, but also will con- trast their differences in order to deepen our understanding of the distinct characteristics of each. 358"
5.0 Introduction,"5 THE DISCRETE-TIME FOURIER TRANSFORM 5.0 INTRODUCTION In Chapter 4, we introduced the continuous-time Fourier transform and developed the many characteristics of that transform which make the methods of Fourier analysis of such great value in analyzing and understanding the properties of continuous-time signals and systems. In the current chapter, we complete our development of the basic tools of Fourier analysis by introducing and examining the discrete-time Fourier transform. In our discussion of Fourier series in Chapter 3, we saw that there are many similari- ties and strong parallels in analyzing continuous-time and discrete-time signals. However, there are also important differences. For example, as we saw in Section 3.6, the Fourier series representation of a discrete-time periodic signal is a .finite series, as opposed to the infinite series representation required for continuous-time periodic signals. As we will see in this chapter, there are corresponding differences between continuous-time and discrete- time Fourier transforms. In the remainder of the chapter, we take advantage of the similarities between continuous-time and discrete-time Fourier analysis by following a strategy essentially identical to that used in Chapter 4. In particular, we begin by extending the Fourier se- ries description of periodic signals in order to develop a Fourier transform representation for discrete-time aperiodic signals, and we follow with an analysis of the properties and characteristics of the discrete-time Fourier transform that parallels that given in Chap- ter 4. By doing this, we not only will enhance our understanding of the basic concepts of Fourier analysis that are common to both continuous and discrete time, but also will con- trast their differences in order to deepen our understanding of the distinct characteristics of each. 358 Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 359 5.1 REPRESENTATION OF APERIODIC SIGNALS: THE DISCRETE-TIME FOURIER TRANSFORM 5. 1. 1 Development of the Discrete-Time Fourier Transform In Section 4.1 [eq. (4.2) and Figure 4.2], we saw that the Fourier series coefficients for a continuous-time periodic square wave can be viewed as samples of an envelope function and that, as the period of the square wave increases, these samples become more and more finely spaced. This property suggested representing an aperiodic signal x(t) by first constructing a periodic signal x(t) that equaled x(t) over one period. Then, as this period approached infinity, x(t) was equal to x(t) over larger and larger intervals of time, and the Fourier series representation for x(t) converged to the Fourier transform representation for x(t). In this section, we apply an analogous procedure to discrete-time signals in order to develop the Fourier transform representation for discrete-time aperiodic sequences. Consider a general sequence x[n] that is of finite duration. That is, for some integers N 1 andN2,x[n] = Ooutsidetherange -N1 ::; n::; N2.Asignalofthistypeisillustrated in Figure 5.l(a). From this aperiodic signal, we can construct a periodic sequence x[n] for which x[n] is one period, as illustrated in Figure 5.l(b). As we choose the period N to be larger, x[n] is identical to x[n] over a longer interval, and as N ~ oo, x[n] = x[n] for any finite value of n. Let us now examine the Fourier series representation of x[n]. Specifically, from eqs. (3.94) and (3.95), we have x[n] = L akejk(27r!N)n, (5.1) k=(N) x[n] ......... riiirriillflir ......... n (a) x[n] ...I IIIIriilfffir ...... IIIIrriilffJir ......r lllrrtilffJir ... . (b) Figure 5.1 (a) Finite-duration signal x[n]; (b) periodic signal x[n] con- structed to be equal to x[n] over one period."
5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform,"Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 359 5.1 REPRESENTATION OF APERIODIC SIGNALS: THE DISCRETE-TIME FOURIER TRANSFORM 5. 1. 1 Development of the Discrete-Time Fourier Transform In Section 4.1 [eq. (4.2) and Figure 4.2], we saw that the Fourier series coefficients for a continuous-time periodic square wave can be viewed as samples of an envelope function and that, as the period of the square wave increases, these samples become more and more finely spaced. This property suggested representing an aperiodic signal x(t) by first constructing a periodic signal x(t) that equaled x(t) over one period. Then, as this period approached infinity, x(t) was equal to x(t) over larger and larger intervals of time, and the Fourier series representation for x(t) converged to the Fourier transform representation for x(t). In this section, we apply an analogous procedure to discrete-time signals in order to develop the Fourier transform representation for discrete-time aperiodic sequences. Consider a general sequence x[n] that is of finite duration. That is, for some integers N 1 andN2,x[n] = Ooutsidetherange -N1 ::; n::; N2.Asignalofthistypeisillustrated in Figure 5.l(a). From this aperiodic signal, we can construct a periodic sequence x[n] for which x[n] is one period, as illustrated in Figure 5.l(b). As we choose the period N to be larger, x[n] is identical to x[n] over a longer interval, and as N ~ oo, x[n] = x[n] for any finite value of n. Let us now examine the Fourier series representation of x[n]. Specifically, from eqs. (3.94) and (3.95), we have x[n] = L akejk(27r!N)n, (5.1) k=(N) x[n] ......... riiirriillflir ......... n (a) x[n] ...I IIIIriilfffir ...... IIIIrriilffJir ......r lllrrtilffJir ... . (b) Figure 5.1 (a) Finite-duration signal x[n]; (b) periodic signal x[n] con- structed to be equal to x[n] over one period. 360 The Discrete-Time Fourier Transform Chap.5 ak = ~ ~ i[n]e- jk(27T!N)n. (5.2) n=(N) Since x[n] = i[n] over a period that includes the interval -N1 ~ n ~ N2, it is convenient to choose the interval of summation in eq. (5.2) to include this interval, so that i[n] can be replaced by x[n] in the summation. Therefore, 1 N2 . 1 +cc . ak = N ~ x[n]e- Jk(27TIN)n = N ~ x[n]e- Jk(27TIN)n, (5.3) n=-N, n=-cc where in the second equality in eq. (5.3) we have used the fact that x[n] is zero outside the interval - N 1 ~ n ~ N2. Defining the function +cc X(ejw) = ~ x[n]e- jwn, (5.4) 11= -cc we see that the coefficients ak are proportional to samples of X(ejw), i.e., (5.5) where w0 = 21TIN is the spacing of the samples in the frequency domain. Combining eqs. (5.1) and (5.5) yields i[n] = ~ _!__X(ejkw0 )ejkwon. (5.6) k=(N) N Since w0 = 21TIN, or equivalently, liN = w 0121T, eq. (5.6) can be rewritten as (5.7) As with eq. (4.7), as N increases w 0 decreases, and as N ~ oo eq. (5.7) passes to an integral. To see this more clearly, consider X(ejw)ejwn as sketched in Figure 5.2. From Figure 5.2 Graphical interpretation of eq. (5.7). Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 361 eq. (5.4), X(eiw) is seen to be periodic in w with period 21T, and so is eiwn. Thus, the product X(eiw)eiwn will also be periodic. As depicted in the figure, each term in the summation in eq. (5.7) represents the area of a rectangle of height X(efkwo)eiwon and width w 0 . As w 0 ~ 0, the summation becomes an integral. Furthermore, since the summation is carried out over N consecutive intervals of width w 0 = 21TIN, the total interval of integration will always have a width of 21T. Therefore, as N ~ oo, x[n] = x[n], and eq. (5.7) becomes x[n] = - 1 J X(e1.w )eJw. n dw, 21T 27T where, since X(eiw)efwn is periodic with period 21T, the interval of integration can be taken as any interval of length 21T. Thus, we have the following pair of equations: x[n] = -1 J X(e1.w )e1.w n dw, (5.8) 21T 27T +oc X(eiw) = L x[n]e-Jwn. (5.9) n= -oc Equations (5.8) and (5.9) are the discrete-time counterparts of eqs. (4.8) and (4.9). The function X(eiw) is referred to as the discrete-time Fourier transform and the pair of equations as the discrete-time Fourier transform pair. Equation (5.8) is the synthesis equa- tion, eq. (5.9) the analysis equation. Our derivation of these equations indicates how an aperiodic sequence can be thought of as a linear combination of complex exponentials. In particular, the synthesis equation is in effect a representation of x[n] as a linear com- bination of complex exponentials infinitesimally close in frequency and with amplitudes X(eiw)(dwi21T). For this reason, as in continuous time, the Fourier transform X(eiw) will often be referred to as the spectrum of x[n], because it provides us with the information on how x[n] is composed of complex exponentials at different frequencies. Note also that, as in continuous time, our derivation of the discrete-time Fourier transform provides us with an important relationship between discrete-time Fourier series and transforms. In particular, the Fourier coefficients ak of a periodic signal x[n] can be expressed in terms of equally spaced samples of the Fourier transform of a finite-duration, aperiodic signal x[n] that is equal to x[n] over one period and is zero otherwise. This fact is of considerable importance in practical signal processing and Fourier analysis, and we look at it further in Problem 5.41. As our derivation indicates, the discrete-time Fourier transform shares many sim- ilarities with the continuous-time case. The major differences between the two are the periodicity of the discrete-time transform X(eiw) and the finite interval of integration in the synthesis equation. Both of these stem from a fact that we have noted several times be- fore: Discrete-time complex exponentials that differ in frequency by a multiple of 21T are identical. In Section 3.6 we saw that, for periodic discrete-time signals, the implications of this statement are that the Fourier series coefficients are periodic and that the Fourier series representation is a finite sum. For aperiodic signals, the analogous implications are that X(eiw) is periodic (with period 27T) and that the synthesis equation involves an inte- gration only over a frequency interval that produces distinct complex exponentials (i.e., any interval of length 27T). In Section 1.3.3, we noted one further consequence of the pe- 362 The Discrete-Time Fourier Transform Chap.5 riodicity of eiwn as a function of w: w = 0 and w = 27T yield the same signal. Signals at frequencies near these values or any other even multiple of 1T are slowly varying and therefore are all appropriately thought of as low-frequency signals. Similarly, the high frequencies in discrete time are the values of w near odd multiples of 1T. Thus, the signal x1 [n] shown in Figure 5.3(a) with Fourier transform depicted in Figure 5.3(b) varies more slowly than the signal x2 [n] in Figure 5.3(c) whose transform is shown in Figure 5.3(d). 0 n (a) (b) j j ~ I ~ I \ I J I \ J I \ I I I \ I n w (c) (d) Figure 5.3 (a) Discrete-time signal x1 [n]. (b) Fourier transform of x1 [n]. Note that X1 (ejw) is concentrated near w = 0, ±2?T, ±4?T, .... (c) Discrete- time signal x2[n]. (d) Fourier transform of x2[n]. Note that X2(ejw) is concen- trated near w = ± 1r, ±3?T, .... 5.1.2 Examples of Discrete-Time Fourier Transforms To illustrate the discrete-time Fourier transform, let us consider several examples. Example 5.1 Consider the signal x[n] = a""u[n], lal < 1. Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 363 In this case, +co X(ejw) = L anu[n]e- jwn n=-oo GO = """"'(ae-jwt = ----:-- L 1- ae-jw' n=O The magnitude and phase of X(ejw) are shown in Figure 5.4(a) for a> 0 and in Fig- ure 5.4(b) for a< 0. Note that all of these functions are periodic in w with period 27T. w (a) <l:X(Eh tan- 1 (lal//1 - a2 )-......... (b~ -tan-1 (lal/!1 - a2 ) Figure 5.4 Magnitude and phase of the Fourier transform of Example 5.1 for (a) a> 0 and (b) a< 0. 364 The Discrete-Time Fourier Transform Chap.5 Example 5.2 Let x[n] = alnl, iai < 1. This signal is sketched for 0 < a < 1 in Figure 5.5(a). Its Fourier transform is obtained from eq. (5.9): +x X(ejw) = L alnle- jwn n=-'X; X -1 = Lane-jwn + L a-""e-Jwn. n=O n=-x x[n] 0 n (a) X(eiw) (Ha)/(1-a) (b) Figure 5.5 (a) Signal x[n] = alnl of Example 5.2 and (b) its Fourier trans- form (0 < a < 1) . Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 365 Making the substitution of variables m = - n in the second summation, we obtain X 00 X(eiw) = '2_(ae-Jwt + '2_(aeiw)m. n=O m=I Both of these summations are infinite geometric series that we can evaluate in closed form, yielding aeiw X ( eiw) = + ----,---- 1- ae-Jw 1- aeiw 1 - a2 1 - 2a cos w + a2 • In this case, X(eiw) is real and is illustrated in Figure 5.5(b), again for 0 <a < 1. Example 5.3 Consider the rectangular pulse x[n] = { ~: (5.10) which is illustrated in Figure 5.6(a) for N1 = 2. In this case, Nl X(eiw) = 2_ e- Jwn. (5.11) n=-Nl x[n] .................. II]II ................. . -N1 0 N1 n (a) (b) Figure 5.6 (a) Rectangular pulse signal of Example 5.3 for N1 = 2 and (b) its Fourier transform. 366 The Discrete-Time Fourier Transform Chap.5 Using calculations similar to those employed in obtaining eq. (3.104) in Example 3.12, we can write . sinw (N1 + ~) X(eJw) = sin(w/2) · (5.12) This Fourier transform is sketched in Figure 5.6(b) for N 1 = 2. The function in eq. (5.12) is the discrete-time counterpart of the sine function, which appears in the Fourier trans- form of the continuous-time rectangular pulse (see Example 4.4). An important differ- ence between these two functions is that the function in eq. (5.12) is periodic with period 27T, whereas the sine function is aperiodic. 5.1.3 Convergence Issues Associated with the Discrete-Time Fourier Transform Although the argument we used to derive the discrete-time Fourier transform in Sec- tion 5.1.1 was constructed assuming that x[n] was of arbitrary but finite duration, eqs. (5.8) and (5.9) remain valid for an extremely broad class of signals with infinite duration (such as the signals in Examples 5.1 and 5.2). In this case, however, we again must con- sider the question of convergence of the infinite summation in the analysis equation (5.9). The conditions on x[n] that guarantee the convergence of this sum are direct counterparts of the convergence conditions for the continuous-time Fourier transform.' Specifically, eq. (5.9) will converge either if x[n] is absolutely summable, that is, +oc L ix[n]i < oo, (5.13) n= -oo or if the sequence has finite energy, that is, (5.14) n= -oo In contrast to the situation for the analysis equation (5.9), there are generally no convergence issues associated with the synthesis equation (5.8), since the integral in this equation is over a finite interval of integration. This is very much the same situation as for the discrete-time Fourier series synthesis equation (3.94), which involves a finite sum and consequently has no issues of convergence associated with it either. In particular, if we approximate an aperiodic signal x[n] by an integral of complex exponentials with frequencies taken over the intervallwl :s W, i.e., x[n] = -1 Jw X(e1.w )eJW. n dw, (5.15) 27T -w 1F or discussions of the convergence issues associated with the discrete-time Fourier transform, see A. V. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1989), and L. R. Rabiner and B. Gold, Theory and Application ofD igital Signal Processing (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1975). Sec. 5.2 The Fourier Transform for Periodic Signals 367 then x[n] = x[n] for W = 1r. Thus, much as in Figure 3.18, we would expect not to see any behavior like the Gibbs phenomenon in evaluating the discrete-time Fourier transform synthesis equation. This is illustrated in the following example. Example 5.4 Let x[ n] be the unit impulse; that is, x[n] = 8[n]. In this case the analysis equation (5.9) is easily evaluated, yielding In other words, just as in continuous time, the unit impulse has a Fourier transform repre- sentation consisting of equal contributions at all frequencies. If we then apply eq. (5.15) to this example, we obtain x""[ n] _ --1 Iw e jwn d w_- -sin- W n. (5.16) 27T -w 7Tn This is plotted in Figure 5. 7 for several values of W. As can be seen, the frequency of the oscillations in the approximation increases as W is increased, which is similar to what we observed in the continuous-time case. On the other hand, in contrast to the continuous- time case, the amplitude of these oscillations decreases relative to the magnitude of .X[O] as W is increased, and the oscillations disappear entirely for W = 7T. 5.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS As in the continuous-time case, discrete-time periodic signals can be incorporated within the framework of the discrete-time Fourier transform by interpreting the transform of a periodic signal as an impulse train in the frequency domain. To derive the form of this representation, consider the signal (5.17) In continuous time, we saw that the Fourier transform of eiwot can be interpreted as an impulse at w = w 0 . Therefore, we might expect the same type of transform to result for the discrete-time signal of eq. (5.17). However, the discrete-time Fourier transform must be periodic in w with period 21r. This then suggests that the Fourier transform of x[n] in eq. (5.17) should have impulses at w 0, w 0 ± 21T, w 0 ± 41T, and so on. In fact, the Fourier transform of x[n] is the impulse train +oo X(eiw) = ~ 21T o(w - Wo - 21Tl), (5.18) l= -00"
5.2 The Fourier Transform for Periodic Signals,"Sec. 5.2 The Fourier Transform for Periodic Signals 367 then x[n] = x[n] for W = 1r. Thus, much as in Figure 3.18, we would expect not to see any behavior like the Gibbs phenomenon in evaluating the discrete-time Fourier transform synthesis equation. This is illustrated in the following example. Example 5.4 Let x[ n] be the unit impulse; that is, x[n] = 8[n]. In this case the analysis equation (5.9) is easily evaluated, yielding In other words, just as in continuous time, the unit impulse has a Fourier transform repre- sentation consisting of equal contributions at all frequencies. If we then apply eq. (5.15) to this example, we obtain x""[ n] _ --1 Iw e jwn d w_- -sin- W n. (5.16) 27T -w 7Tn This is plotted in Figure 5. 7 for several values of W. As can be seen, the frequency of the oscillations in the approximation increases as W is increased, which is similar to what we observed in the continuous-time case. On the other hand, in contrast to the continuous- time case, the amplitude of these oscillations decreases relative to the magnitude of .X[O] as W is increased, and the oscillations disappear entirely for W = 7T. 5.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS As in the continuous-time case, discrete-time periodic signals can be incorporated within the framework of the discrete-time Fourier transform by interpreting the transform of a periodic signal as an impulse train in the frequency domain. To derive the form of this representation, consider the signal (5.17) In continuous time, we saw that the Fourier transform of eiwot can be interpreted as an impulse at w = w 0 . Therefore, we might expect the same type of transform to result for the discrete-time signal of eq. (5.17). However, the discrete-time Fourier transform must be periodic in w with period 21r. This then suggests that the Fourier transform of x[n] in eq. (5.17) should have impulses at w 0, w 0 ± 21T, w 0 ± 41T, and so on. In fact, the Fourier transform of x[n] is the impulse train +oo X(eiw) = ~ 21T o(w - Wo - 21Tl), (5.18) l= -00 x[n] x[n] w = 11""/4 w = 311""/8 -~ 1 t ...... ...... .:rr_rr,. . .......... . 0 00 • ••• 0 ••• • n u •n•• ••••••.,)i!Ir.,.•••••• h 000 ••h 0 n (a) (b) w = 11""/2 3 1 4 2 n (c) (d) x[n] 7 8 0 n (e) (f) Figure s. 7 Approximation to the unit sample obtained as in eq. (5.16) using complex exponentials with frequencies lwl ~ W: (a) W = 'TT'/4; (b) W = 37r/8; (c) W = 1r!2; (d) W = 31T'/4; (e) W = ?1r/8; (f) W = 1r. Note that for W = 1r, x[n] = S[n]. Sec. 5.2 The Fourier Transform for Periodic Signals 369 which is illustrated in Figure 5.8. In order to check the validity of this expression, we must evaluate its inverse transform. Substituting eq. (5.18) into the synthesis equation (5.8), we find that w Figure 5.8 Fourier transform of x[n] = eiwan. Note that any interval of length 27T includes exactly one impulse in the summation given in eq. (5.18). Therefore, if the interval of integration chosen includes the impulse located at w 0 + 21Tr, then Now consider a periodic sequence x[n] with period Nand with the Fourier series representation x[n] = ~ akejk(2TTIN)n. (5.19) k=(N) In this case, the Fourier transform is (5.20) so that the Fourier transform of a periodic signal can be directly constructed from its Fourier coefficients. To verify that eq. (5.20) is in fact correct, note that x[n] in eq. (5.19) is a linear combination of signals of the form in eq. (5.17), and thus the Fourier transform of x[n] must be a linear combination of transforms of the form of eq. (5.18). In particular, suppose that we choose the interval of summation in eq. (5.19) ask = 0, 1, ... , N- 1, so that x[n] = ao + al ej(2TT/N)n + a2ej2(2TT!N)n (5.21) + ... + aN-lej(N-l)(2TTIN)n. 370 The Discrete-Time Fourier Transform Chap.5 Thus, x[n] is a linear combination of signals, as in eq. (5.17), with w 0 = 0, 27T/N, 47T/N, ... , (N - 1)27r/N. The resulting Fourier transform is illustrated in Figure 5.9. In Figure 5.9(a), we have depicted the Fourier transform of the first term on the right-hand side of eq. (5.21): The Fourier transform of the constant signal a 0 = a 0 ejO·n is a periodic impulse train, as in eq. (5.18), with w 0 = 0 and a scaling of27Tao on each of the impulses. Moreover, from Chapter 4 we know that the Fourier series coefficients a k are periodic with period N, so that 27Ta0 = 27TaN = 27Ta-N. In Figure 5.9(b) we have illustrated the Fourier transform of the second term in eq. (5.21), where we have again used eq. (5.18), 21ra0 = 21Ta_N 21ra0 21ra0 = 21raN 1 1 1 21T 0 21T w (a) 21ra1 = 21ra_N + 1 21ra1 21ra1 = 21raN + 1 t t t w ( (-N + 1) 2~) (2~) 2 ((N + 1) ~) (b) 2 ( (-N- 1) ~ ) 2 (- 2~) ( (N- 1) ~ ) l l l w 21TaN_1 = 21Ta_N _ 1 21TaN_1 = 21Ta_1 21TaN- 1 (c) 21Ta_N 21ra0 21raN 21Ta_N+1 21ra1 21TaN+1 (d) Figure 5. 9 Fourier transform of a discrete-time periodic signal: (a) Fourier transform of the first term on the right-hand side of eq. (5.21 ); (b) Fourier transform of the sec- ond term in eq. (5.21 ); (c) Fourier transform of the last term in eq. (5.21 ); (d) Fourier transform of x[n] in eq (5.21 ). Sec. 5.2 The Fourier Transform for Periodic Signals 371 in this case for a1ei(27TIN)n, and the fact that 21Ta1 = 21TaN+I = 21Ta-N+I· Similarly, Figure 5.9(c) depicts the final term. Finally, Figure 5.9(d) depicts the entire expression for X(eiw). Note that because of the periodicity of the ah X(eiw) can be interpreted as a train of impulses occurring at multiples of the fundamental frequency 21T/N, with the area of the impulse located at w = 21TkiN being 21Tab which is exactly what is stated in eq. (5.20). Example 5.5 Consider the periodic signal 27T with w 0 = 5 . (5.22) From eq. (5.18), we can immediately write (5.23) That is, X( e1.w ) = 7T 8 (w - 27T) + 7T 8 (w + 27T) 5 5 , (5.24) and X(ejw) repeats periodically with a period of 27T, as illustrated in Figure 5.10. X(eiw) 1T r t t I t I -21T t -wo 0 wo r 21T r w (-21T-w0) (-21T+w0) (21r-w0) (21T+w0) Figure 5. 1 0 Discrete-time Fourier transform of x[n] = cos won. Example 5.6 The discrete-time counterpart of the periodic impulse train of Example 4.8 is the se- quence +oo x[n] = L 8[n- kN], (5.25) k=-X as sketched in Figure 5.11(a). The Fourier series coefficients for this signal can be cal- culated directly from eq. (3.95): ak = ~ L x[n]e- jk(27TIN)n. n=(N) 372 The Discrete-Time Fourier Transform Chap.5 Choosing the interval of summation as 0 :::; n :::; N- 1, we have (5.26) Using eqs. (5.26) and (5.20), we can then represent the Fourier transform of the signal as (5.27) which is illustrated in Figure 5.1l(b). x[n] ... J 1J J J ... •••• ••••••••• ••••••••• ••••••••• ••••••• -N 0 N 2N n (a) X(eij 21r1N w (b) Figure 5.11 (a) Discrete-time periodic impulse train; (b) its Fourier transform. 5.3 PROPERTIES OF THE DISCRETE-TIME FOURIER TRANSFORM As with the continuous-time Fourier transform, a variety of properties of the discrete-time Fourier transform provide further insight into the transform and, in addition, are often useful in reducing the complexity in the evaluation of transforms and inverse transforms. In this and the following two sections we consider these properties, and in Table 5.1 we present a concise summary of them. By comparing this table with Table 4.1, we can get a clear picture of some of the similarities and differences between continuous-time and discrete-time Fourier transform properties. When the derivation or interpretation of a discrete-time Fourier transform property is essentially identical to its continuous-time counterpart, we will simply state the property. Also, because of the close relationship between the Fourier series and the Fourier transform, many of the transform properties"
5.3 Properties of the Discrete-Time Fourier Transform,"372 The Discrete-Time Fourier Transform Chap.5 Choosing the interval of summation as 0 :::; n :::; N- 1, we have (5.26) Using eqs. (5.26) and (5.20), we can then represent the Fourier transform of the signal as (5.27) which is illustrated in Figure 5.1l(b). x[n] ... J 1J J J ... •••• ••••••••• ••••••••• ••••••••• ••••••• -N 0 N 2N n (a) X(eij 21r1N w (b) Figure 5.11 (a) Discrete-time periodic impulse train; (b) its Fourier transform. 5.3 PROPERTIES OF THE DISCRETE-TIME FOURIER TRANSFORM As with the continuous-time Fourier transform, a variety of properties of the discrete-time Fourier transform provide further insight into the transform and, in addition, are often useful in reducing the complexity in the evaluation of transforms and inverse transforms. In this and the following two sections we consider these properties, and in Table 5.1 we present a concise summary of them. By comparing this table with Table 4.1, we can get a clear picture of some of the similarities and differences between continuous-time and discrete-time Fourier transform properties. When the derivation or interpretation of a discrete-time Fourier transform property is essentially identical to its continuous-time counterpart, we will simply state the property. Also, because of the close relationship between the Fourier series and the Fourier transform, many of the transform properties Sec. 5.3 Properties of the Discrete-Time Fourier Transform 373 translate directly into corresponding properties for the discrete-time Fourier series, which we summarized in Table 3.2 and briefly discussed in Section 3.7. In the following discussions, it will be convenient to adopt notation similar to that used in Section 4.3 to indicate the pairing of a signal and its transform. That is, X(ejw) = ~{x[n]}, x[n] = ~- 1 {X(ejw)}, ~ . x[n] ~ X(e 1w). 5.3.1 Periodicity of the Discrete-Time Fourier Transform As we discussed in Section 5.1, the discrete-time Fourier transform is always periodic in w with period 27T; i.e., (5.28) This is in contrast to the continuous-time Fourier transform, which in general is not peri- odic. 5.3.2 Linearity of the Fourier Transform If and then (5.29) 5.3.3 Time Shifting and Frequency Shifting If ~ . x[n] ~ X(e1w), then (5.30) 374 The Discrete-Time Fourier Transform Chap.5 and (5.31) Equation (5.30) can be obtained by direct substitution of x[n- n0 ] into the analysis equa- tion (5.9), while eq. (5.31) is derived by substituting X(ej(w-wo>) into the synthesis equa- tion (5.8). As a consequence ofthe periodicity and frequency-shifting properties of the discrete- time Fourier transform, there exists a special relationship between ideallowpass and ideal highpass discrete-time filters. This is illustrated in the next example. Example 5.7 In Figure 5.12(a) we have depicted the frequency response H1p(eiw) of a lowpass filter with cutoff frequency We. while in Figure 5.12(b) we have displayed H1p(ei<w-7T))-that is, the frequency response H1p(eiw) shifted by one-half period, i.e., by 7T. Since high frequencies in discrete time are concentrated near 7T (and other odd multiples of 7T), the filter in Figure 5 .12(b) is an ideal high pass filter with cutoff frequency 7T - w c. That is, (5.32) As we can see from eq. (3.122), and as we will discuss again in Section 5.4, the frequency response of an LTI system is the Fourier transform of the impulse response of the system. Thus, if h1p[n] and hhp[n] respectively denote the impulse responses of ~ H,p(<,iw) Q I I Q -27T -7T -we We 7T 27T w (a) Hlp(ei(w-1T)) q ±1 I -27T -7T \ p I 27T w -(7T-we) (7T-we) (b) Figure 5.12 (a) Frequency response of a lowpass filter; (b) frequency re- sponse of a highpass filter obtained by shifting the frequency response in (a) by w = 1r corresponding to one-half period. Sec. 5.3 Properties of the Discrete-Time Fourier Transform 375 Figure 5.12, eq. (5.32) and the frequency-shifting property imply that the lowpass and highpass filters in hhp[n] = eJ7Tn hip[n] (5.33) = ( -lY hip[n]. (5.34) 5.3.4 Conjugation and Conjugate Symmetry If then x * [n] ~~ X * (e- 1.w ). (5.35) Also, if x[n] is real valued, its transform X(ejw) is conjugate symmetric. That is, (5.36) From this, it follows that ffie{X(ejw)} is an even function of w and 9m{X(ejw)} is an odd function of w. Similarly, the magnitude of X(ejw) is an even function and the phase angle is an odd function. Furthermore, ~ . 8v{x[n]} ~ ffie{X(elw)} and where 8v and fJd denote the even and odd parts, respectively, of x[n]. For example, if x[n] is real and even, its Fourier transform is also real and even. Example 5.2 illustrates this symmetry for x[n] = alnl. 5.3.5 Differencing and Accumulation In this subsection, we consider the discrete-time counterpart of integration-that is, accumulation-and its inverse, first differencing. Let x[n] be a signal with Fourier trans- form X(ejw). Then, from the linearity and time-shifting properties, the Fourier transform pair for the first-difference signal x[n] - x[n - 1] is given by ~ . . x[n] - x[n- 1] ~ (1 - e-JW)X(elw). (5.37) 376 The Discrete-Time Fourier Transform Chap.5 Next, consider the signal n y[n] = ~ x[m]. (5.38) m= -oo Since y[n] - y[n - 1] = x[n], we might conclude that the transform of y[n] should be related to the transform of x[n] by division by (1 - e-Jw). This is partly correct, but as with the continuous-time integration property given by eq. (4.32), there is more involved. The precise relationship is (5.39) The impulse train on the right-hand side of eq. (5.39) reflects the de or average value that can result from summation. Example 5.8 Let us derive the Fourier transform X(ejw) of the unit step x[n] = u[n] by making use of the accumulation property and the knowledge that ~ . g[n] = 8[n] ~ G(e1w) = 1. From Section 1.4.1 we know that the unit step is the running sum of the unit impulse. That is, n x[n] = L g[m]. m=-x Taking the Fourier transform of both sides and using accumulation yields X(ejw) = (1 _ ~- jw) G(ejw) + 7TG(ej0 ) i 8(w - 27Tk) k=-OC 1 X 1 _ e-jw + 7T L 8(w- 27Tk). k=-X 5.3.6 Time Reversal Let x[n] be a signal with spectrum X(eiw), and consider the transform Y(eiw) of y[n] = x[ -n]. From eq. (5.9), +oo +oo Y(eiw) = ~ y[n]e- jwn = ~ x[-n]e-Jwn. (5.40) n= -oo n= -oo Substituting m = -n into eq. (5.40), we obtain +oo Y(eiw) = ~ x[m]e- j(-w)m = X(e- jw). (5.41) m= -oo Sec. 5.3 Properties of the Discrete-Time Fourier Transform 377 That is, (5.42) 5. 3. 7 Time Expansion Because of the discrete nature of the time index for discrete-time signals, the relation be- tween time and frequency scaling in discrete time takes on a somewhat different form from its continuous-time counterpart. Specifically, in Section 4.3.5 we derived the continuous- time property g: r1 (jw) x(at) ~ arx ---;;- . (5.43) However, if we try to define the signal x[an], we run into difficulties if a is not an integer. Therefore, we cannot slow down the signal by choosing a < 1. On the other hand, if we let a be an integer other than± 1-for example, if we consider x[2n]-we do not merely speed up the original signal. That is, since n can take on only integer values, the signal x[2n] consists of the even samples of x[n] alone. There is a result that does closely parallel eq. (5.43), however. Let k be a positive integer, and define the signal _ { x[nl k], if n is a multiple of k x(k) [ n ] - 0, (5.44) if n is not a multiple of k. As illustrated in Figure 5.13 fork = 3, X(k)[n] is obtained from x[n] by placing k- 1 zeros between successive values of the original signal. Intuitively, we can think of x(k)[n] as a slowed-down version of x[n]. Since x(k)[n] equals 0 unless n is a multiple of k, i.e., unless n = rk~ we see that the Fourier transform of x(k)[n] is given by +oo +oo x(k)(e1(J)) = L X(k)[n]e- jwn = L X(k)[rk]e- jwrk. n= -oo r= -oo x[n] Ill!!llriii n -1 1 X~~ I .l .... I. . I l. . I. Figure 5.13 The signal x(3)[n] ob- I r• • I. ... r• • I. ... tained from x[n] by inserting two zeros between successive values of the -6 -3 0 3 6 n original signal. 378 The Discrete-Time Fourier Transform Chap.5 Furthermore, since x<k>[rk] = x[r], we find that +x x(k)(ejw) = L x[r]e- j(kw)r = X(e.ikw). r=-x That is, (5.45) Note that as the signal is spread out and slowed down in time by taking k > 1, its Fourier transform is compressed. For example, since X ( e.iw) is periodic with period 27T, X(e.ikw) is periodic with period 27T/ k. This property is illustrated in Figure 5.14 for a rectangular pulse. x[n] __.. _______ :::-_ 0 n 0 n w JL 7T 2 -------- -- -- 0 -- -- -------- n w Figure 5.14 Inverse relationship between the time and frequency domains: As k in- creases, x(k) [n] spreads out while its transform is compressed. Sec. 5.3 Properties of the Discrete-Time Fourier Transform 379 Example 5.9 As an illustration of the usefulness of the time-expansion property in determining Fourier transforms, let us consider the sequence x[n] displayed in Figure 5.15(a). This sequence can be related to the simpler sequence y[n] depicted in Figure 5.15(b). In particular x[n] = Y(2)[n] + 2y(2)[n- 1], where _ { y[n/2], if n is even Y(2) [ n ] - 0 , 1"" f n I·S 0 dd and y(2)[n - 1] represents y(2)[n] shifted one unit to the right. The signals y(2)[n] and 2y(2)[n- 1] are depicted in Figures 5.15(c) and (d), respectively. Next, note that y[n] = g[n - 2], where g[n] is a rectangular pulse as considered in Example 5.3 (with N 1 = 2) and as depicted in Figure 5.6(a). Consequently, from Example 5.3 and the time-shifting property, we see that Y( jw) = _ j 2w sin(5w/2) e e sin(w/2) · x[n] 11 21 • I I I I I 1 I I 0 2 3 4 5 6 7 8 9 • • n (a) y[n] 11 • I I I I 0 2 3 4 •5 •6 •7 •8 •9 • • n (b) Y(2)[n] 11 • • I I I I 0 1 2 3• 4 •5 6 •7 8 9• • • n (c) 2y(2)[n-1] 2I • •0 • I • I • I I 2 3 4 5 6 7 8• 9 • • n (d) Figure 5.15 (a) The signal x[n] in Example 5.9; (b) the signal y[n]; (c) the signal Y(2)[n] obtained by inserting one zero between successive values of y[n]; and (d) the signal 2y(2)[n- 1] . 380 The Discrete-Time Fourier Transform Chap.5 Using the time-expansion property, we then obtain ~ _ J4w sin(5w) y(2)[n] ~ e -.- (-) , smw and using the linearity and time-shifting properties, we get 2 , [ _ 1] ~ 2 _ JSw sin(5w) )( 2J n e sin(w) · Combining these two results, we have X(eiw) = e-J4w(l + 2e-Jw)(si~(5w))· sm(w) 5.3.8 Differentiation in Frequency Again, let If we use the definition of X(ejw) in the analysis equation (5.9) and differentiate both sides, we obtain L+x - jnx[n]e- jwn. n= -x The right-hand side of this equation is the Fourier transform of- jnx[n]. Therefore, mul- tiplying both sides by j, we see that g: dX(ejw) nx[n] ~ j dw . (5.46) The usefulness of this property will be illustrated in Example 5.13 in Section 5 .4. 5.3.9 Parseval's Relation If x[n] and X(ejw) are a Fourier transform pair, then (5.47) We note that this is similar to eq. (4.43), and the derivation proceeds in a similar man- ner. The quantity on the left-hand side of eq. (5.47) is the total energy in the signal x[n], and Sec. 5.3 Properties of the Discrete-Time Fourier Transform 381 Parse val's relation states that this energy can also be determined by integrating the energy per unit frequency, iX(eiw)i 2127T, over a full27T interval of distinct discrete-time frequen- cies. In analogy with the continuous-time case, iX(eiw)i 2 is referred to as the energy-density spectrum of the signal x[n]. Note also that eq. (5.47) is the counterpart for aperiodic sig- nals of Parseval's relation, eq. (3.110), for periodic signals, which equates the average power in a periodic signal with the sum of the average powers of its individual harmonic components. Given the Fourier transform of a sequence, it is possible to use Fourier transform properties to determine whether a particular sequence has a number of different properties. To illustrate this idea, we present the following example. Example 5. 1 0 Consider the sequence x[n] whose Fourier transform X(eiw) is depicted for -7r ::::; w ::::; 7r in Figure 5.16. We wish to determine whether or not, in the time domain, x[n] is periodic, real, even, and/or of finite energy. IX(eiw)l _....____L~:l""'""------+-~----""--- w _1I. 1I. 2 (a) 2 (b) Figure 5. 16 Magnitude and phase of the Fourier transform for Exam- ple5.10. 382 The Discrete-Time Fourier Transform Chap.5 Accordingly, we note first that periodicity in the time domain implies that the Fourier transform is zero, except possibly for impulses located at various integer multi- ples of the fundamental frequency. This is not true for X(eiw). We conclude, then, that x[n] is not periodic. Next, from the symmetry properties for Fourier transforms, we know that a real- valued sequence must have a Fourier transform of even magnitude and a phase function that is odd. This is true for the given IX(eiw)l and 1-:X(eiw). We thus conclude that x[n] is real. Third, if x[n] is an even function, then, by the symmetry properties forreal signals, X(eiw) must be real and even. However, since X(eiw) = IX(eiw)le- i 2w, X(eiw) is not a real-valued function. Consequently, x[n] is not even. Finally, to test for the finite-energy property, we may use Parseval's relation, It is clear from Figure 5.16 that integrating IX(eiw)l2 from -w to 1r will yield a finite quantity. We conclude that x[n] has finite energy. In the next few sections, we consider several additional properties. The first two of these are the convolution and multiplication properties, similar to those discussed in Sections 4.4 and 4.5. The third is the property of duality, which is examined in Section 5.7, where we consider not only duality in the discrete-time domain, but also the duality that exists between the continuous-time and discrete-time domains. 5.4 THE CONVOLUTION PROPERTY In Section 4.4, we discussed the importance of the continuous-time Fourier transform with regard to its effect on the operation of convolution and its use in dealing with continuous- time LTI systems. An identical relation applies in discrete time, and this is one of the principal reasons that the discrete-time Fourier transform is of such great value in repre- senting and analyzing discrete-time LTI systems. Specifically, if x[n], h[n], and y[n] are the input, impulse response, and output, respectively, of an LTI system, so that y[n] = x[n] * h[n], then (5.48) where X(ejw), H(ejw), and Y(ejw) are the Fourier transforms of x[n], h[n], and y[n], re- spectively. Furthermore, comparing eqs. (3.122) and (5.9), we see that the frequency re- sponse of a discrete-time LTI system, as first defined in Section 3.8, is the Fourier transform of the impulse response of the system. The derivation of eq. (5.48) exactly parallels that carried out in Section 4.4. In par- ticular, as in continuous time, the Fourier synthesis equation (5.8) for x[n] can be inter-"
5.4 The Convolution Property,"382 The Discrete-Time Fourier Transform Chap.5 Accordingly, we note first that periodicity in the time domain implies that the Fourier transform is zero, except possibly for impulses located at various integer multi- ples of the fundamental frequency. This is not true for X(eiw). We conclude, then, that x[n] is not periodic. Next, from the symmetry properties for Fourier transforms, we know that a real- valued sequence must have a Fourier transform of even magnitude and a phase function that is odd. This is true for the given IX(eiw)l and 1-:X(eiw). We thus conclude that x[n] is real. Third, if x[n] is an even function, then, by the symmetry properties forreal signals, X(eiw) must be real and even. However, since X(eiw) = IX(eiw)le- i 2w, X(eiw) is not a real-valued function. Consequently, x[n] is not even. Finally, to test for the finite-energy property, we may use Parseval's relation, It is clear from Figure 5.16 that integrating IX(eiw)l2 from -w to 1r will yield a finite quantity. We conclude that x[n] has finite energy. In the next few sections, we consider several additional properties. The first two of these are the convolution and multiplication properties, similar to those discussed in Sections 4.4 and 4.5. The third is the property of duality, which is examined in Section 5.7, where we consider not only duality in the discrete-time domain, but also the duality that exists between the continuous-time and discrete-time domains. 5.4 THE CONVOLUTION PROPERTY In Section 4.4, we discussed the importance of the continuous-time Fourier transform with regard to its effect on the operation of convolution and its use in dealing with continuous- time LTI systems. An identical relation applies in discrete time, and this is one of the principal reasons that the discrete-time Fourier transform is of such great value in repre- senting and analyzing discrete-time LTI systems. Specifically, if x[n], h[n], and y[n] are the input, impulse response, and output, respectively, of an LTI system, so that y[n] = x[n] * h[n], then (5.48) where X(ejw), H(ejw), and Y(ejw) are the Fourier transforms of x[n], h[n], and y[n], re- spectively. Furthermore, comparing eqs. (3.122) and (5.9), we see that the frequency re- sponse of a discrete-time LTI system, as first defined in Section 3.8, is the Fourier transform of the impulse response of the system. The derivation of eq. (5.48) exactly parallels that carried out in Section 4.4. In par- ticular, as in continuous time, the Fourier synthesis equation (5.8) for x[n] can be inter- Sec. 5.4 The Convolution Property 383 preted as a decomposition of x[n] into a linear combination of complex exponentials with infinitesimal amplitudes proportional to X(eiw). Each of these exponentials is an eigen- function of the system. In Chapter 3, we used this fact to show that the Fourier series coefficients of the response of an LTI system to a periodic input are simply the Fourier coefficients of the input multiplied by the system's frequency response evaluated at the corresponding harmonic frequencies. The convolution property (5.48) represents the ex- tension of this result to aperiodic inputs and outputs by using the Fourier transform rather than the Fourier series. As in continuous time, eq. (5.48) maps the convolution of two signals to the simple algebraic operation of multiplying their Fourier transforms, a fact that both facilitates the analysis of signals and systems and adds significantly to our understanding of the way in which an LTI system responds to the input signals that are applied to it. In particular, from eq. (5.48), we see that the frequency response H(eiw) captures the change in complex amplitude of the Fourier transform of the input at each frequency w. Thus, in frequency- selective filtering, for example, we want H(eiw) = 1 over the range of frequencies cor- responding to the desired passband and H(eiw) = 0 over the band of frequencies to be eliminated or significantly attenuated. 5.4. 1 Examples To illustrate the convolution property, along with a number of other properties, we consider several examples in this section. Example 5. 11 Consider an LTI system with impulse response h[n] = 8[n- no]. The frequency response is +""' H(eiw) = L 8[n- no]e- jwn = e- jwno. n=-x Thus, for any input x[n] with Fourier transform X(eiw), the Fourier transform of the output is (5.49) We note that, for this example, y[n] = x[n - n0 ] and eq. (5.49) is consistent with the time-shifting property. Note also that the frequency response H(eiw) = e-Jwno of a pure time shift has unity magnitude at all frequencies and a phase characteristic -wn0 that is linear with frequency. Example 5. 12 Consider the discrete-time ideal lowpass filter introduced in Section 3.9.2. This sys- tem has the frequency response H(eiw) illustrated in Figure 5.17(a). Since the impulse 384 The Discrete-Time Fourier Transform Chap.5 response and frequency response of an LTI system are a Fourier transform pair, we can determine the impulse response of the ideallowpass filter from the frequency response using the Fourier transform synthesis equation (5.8). In particular, using -7r ~ w ::::; 1r as the interval of integration in that equation, we see from Figure 5.17(a) that (5.50) which is shown in Figure 5.17(b). H(eiw) ,I , I II -2'IT -'IT -wo 0 wo 'IT 2'IT w (a) h[n] •••••••••••••••.,,Ttltt,,.•••••••••••••••n (b) Figure 5. 17 (a) Frequency response of a discrete-time ideal lowpass filter; (b) impulse response of the ideal lowpass filter. In Figure 5.17, we come across many of the same issues that surfaced with the continuous-time ideallowpass filter in Example 4.18. First, since h[n] is not zero for n < 0, the ideallowpass filter is not causal. Second, even if causality is not an important is- sue, there are other reasons, including ease of implementation and preferable time domain characteristics, that nonideal filters are generally used to perform frequency-selective fil- tering. In particular, the impulse response of the ideallowpass filter in Figure 5.17 (b) is oscillatory, a characteristic that is undesirable in some applications. In such cases, a trade- off between frequency-domain objectives such as frequency selectivity and time-domain properties such as nonoscillatory behavior must be made. In Chapter 6, we will discuss these and related ideas in more detail. As the following example illustrates, the convolution property can also be of value in facilitating the calculation of convolution sums. Sec. 5.4 The Convolution Property 385 Example 5. 1 3 Consider an LTI system with impulse response with Ia I < 1, and suppose that the input to this system is x[n] = f3nu[n], with l/31 < 1. Evaluating the Fourier transforms of h[n] and x[n], we have H(ejw) = 1 . (5.51) 1-ae-Jw and (5.52) so that (5.53) As with Example 4.19, determining the inverse transform of Y(ejw) is most easily done by expanding Y(ejw) by the method of partial fractions. Specifically, Y(ejw) is a ratio of polynomials in powers of e- jw, and we would like to express this as a sum of simpler terms of this type so that we can find the inverse transform of each term by inspection (together, perhaps, with the use of the frequency differentiation property of Section 5.3.8). The general algebraic procedure for rational transforms is described in the appendix. For this example, if a ¥- f3, the partial fraction expansion of Y ( ejw) is of the form A Y(ejw) = + B .. 1- (5.54) ae-jw 1- f3e-1w Equating the right-hand sides of eqs (5.53) and (5.54), we find that A= _a_ B = __/3 _. a- f3' a-{3 Therefore, from Example 5.1 and the linearity property, we can obtain the inverse trans- form of eq. (5.54) by inspection: y[n] = _a_anu[n]- _f3_f3 11 u[n] a-{3 a-{3 (5.55) 1 = --[an+I u[n]- f3n+I u[n]]. a-{3 For a = f3, the partial-fraction expansion in eq. ( 5.54) is not valid. However, in this case, Y ( ejw) = ( 1 1 - . )2 ' - ae JW 386 The Discrete-Time Fourier Transform Chap.5 which can be expressed as Y(eiw) = j jw d ( 1 ) ~e 1 - (5.56) dw ae-Jw · As in Example 4.19, we can use the frequency differentiation property, eq. (5.46), together with the Fourier transform pair ~ anu[n] ~ ---,---- 1-ae-Jw' to conclude that nanu[n]~ J-.d( 1) w - ae _ .. 1 JW To account for the factor eiw, we use the time-shifting property to obtain (n + 1)an+ 1 u[n + 1] ~~ je.lwd_( 1.) , dw 1- ae-Jw and finally, accounting for the factor 1/a, in eq. (5.56), we obtain y[n] = (n + l)anu[n + 1]. (5.57) It is worth noting that, although the right-hand side is multiplied by a step that begins at n = -1, the sequence (n + 1)anu[n + 1] is still zero prior ton = 0, since the factor n + 1 is zero at n = -1. Thus, we can alternatively express y[n] as y[n] = (n + 1)anu[n]. (5.58) As illustrated in the next example, the convolution property, along with other Fourier transform properties, is often useful in analyzing system interconnections. Example 5. 14 Consider the system shown in Figure 5.18(a) with input x[n] and output y[n]. The LTI systems with frequency response H 1p(eiw) are ideallowpass filters with cutoff frequency TT/4 and unity gain in the passband. Let us first consider the top path in Figure 5.18(a). The Fourier transform of the signal w 1 [n] can be obtained by noting that ( -l)n = eJ7Tn so that w 1 [n] = eJ7Tn x[n]. Using the frequency-shifting property, we then obtain WI (eiw) = X(ei(w-7T)). The convolution property yields Wz(eiw) = Hzp(eiw)X(ei(w-7T)). Since w3 [n] = ei7Tnw2 [n], we can again apply the frequency-shifting property to obtain W3(e1w) = Wz(eJ(w-7T)) = Hzp(ei(w-7T))X(ei(w-21T)). Sec. 5.4 The Convolution Property 387 x[n] y[n] w4[n] (a) H(eiw) 1 I I I 1 I I I -'IT _371"" 71"" 2!. 3TI 'IT w 4 4 4 4 (b) Figure 5. 18 (a) System interconnection for Example 5.14; (b) the overall frequency response for this system. Since discrete-time Fourier transforms are always periodic with period 27T, Applying the convolution property to the lower path, we get W4(ejw) = H,p(eiw)X(eiw). From the linearity property of the Fourier transform, we obtain Y(eiw) = W3(ejw) + W4(ejw) = [HJp(ei(w-TT)) + Htp(eiw)]X(eiw). Consequently, the overall system in Figure 5.18(a) has the frequency response H(eiw) = [HJp(ei(w-TT)) + H,p(eiw)] which is shown in Figure 5.18(b). As we saw in Example 5.7, H 1p(ei<w-'TT)) is the frequency response of an ideal highpass filter. Thus, the overall system passes both low and high frequencies and stops frequencies between these two pass bands. That is, the filter has what is often referred to as an ideal bandstop characteristic, where the stopband is the region 7T/4 < lwl < 37T/4. It is important to note that, as in continuous time, not every discrete-time LTI system has a frequency response. For example, the LTI system with impulse response h[n] = 2nu[n] does not have a finite response to sinusoidal inputs, which is reflected in the fact 388 The Discrete-Time Fourier Transform Chap.5 that the Fourier transform analysis equation for h[n] diverges. However, if an LTI system is stable, then, from Section 2.3.7, its impulse response is absolutely summable; that is, +oo L, lh[n]l < oo. (5.59) n=-oo Therefore, the frequency response always converges for stable systems. In using Fourier methods, we will be restricting ourselves to systems with impulse responses that have well- defined Fourier transforms. In Chapter 10, we will introduce an extension of the Fourier transform referred to as the z-transform that will allow us to use transform techniques for LTI systems for which the frequency response does not converge. 5.5 THE MULTIPLICATION PROPERTY In Section 4.5, we introduced the multiplication property for continuous-time signals and indicated some of its applications through several examples. An analogous property exists for discrete-time signals and plays a similar role in applications. In this section, we derive this result directly and give an example of its use. In Chapters 7 and 8, we will use the multiplication property in the context of our discussions of sampling and communications. Consider y[n] equal to the product of x1 [n] and x2[n], with Y(eiw), X1 (eiw), and X2(eiw) denoting the corresponding Fourier transforms. Then +oo +oo Y(eiw) = L y[n]e- Jwn = L XI [n]x2[n]e- jwn, n= -oo n= -oo or since (5.60) it follows that (5.61) Interchanging the order of summation and integration, we obtain Y(ej""') = 2~ L, X, (ej9) [n~oo X2[n]e- j(w-9)n ]dll. (5.62) The bracketed summation is X2(ei(w-O)), and consequently, eq. (5.62) becomes (5.63)"
5.5 The Multiplication Property,"388 The Discrete-Time Fourier Transform Chap.5 that the Fourier transform analysis equation for h[n] diverges. However, if an LTI system is stable, then, from Section 2.3.7, its impulse response is absolutely summable; that is, +oo L, lh[n]l < oo. (5.59) n=-oo Therefore, the frequency response always converges for stable systems. In using Fourier methods, we will be restricting ourselves to systems with impulse responses that have well- defined Fourier transforms. In Chapter 10, we will introduce an extension of the Fourier transform referred to as the z-transform that will allow us to use transform techniques for LTI systems for which the frequency response does not converge. 5.5 THE MULTIPLICATION PROPERTY In Section 4.5, we introduced the multiplication property for continuous-time signals and indicated some of its applications through several examples. An analogous property exists for discrete-time signals and plays a similar role in applications. In this section, we derive this result directly and give an example of its use. In Chapters 7 and 8, we will use the multiplication property in the context of our discussions of sampling and communications. Consider y[n] equal to the product of x1 [n] and x2[n], with Y(eiw), X1 (eiw), and X2(eiw) denoting the corresponding Fourier transforms. Then +oo +oo Y(eiw) = L y[n]e- Jwn = L XI [n]x2[n]e- jwn, n= -oo n= -oo or since (5.60) it follows that (5.61) Interchanging the order of summation and integration, we obtain Y(ej""') = 2~ L, X, (ej9) [n~oo X2[n]e- j(w-9)n ]dll. (5.62) The bracketed summation is X2(ei(w-O)), and consequently, eq. (5.62) becomes (5.63) Sec. 5.5 The Multiplication Property 389 Equation (5.63) corresponds to a periodic convolution of X1( eiw) and X2(eiw), and the integral in this equation can be evaluated over any interval of length 27T. The usual form of convolution (in which the integral ranges from -oo to +oo) is often referred to as ape- riodic convolution to distinguish it from periodic convolution. The mechanics of periodic convolution are most easily illustrated through an example. Example 5.1 5 Consider the problem of finding the Fourier transform X(eiw) of a signal x[n] which is the product of two other signals; that is, x[n] = x1 [n]x2[n], where sin(37Tn/4) x 1[n] = ---- and x [n] = sin(7Tn/2)_ 2 1Tn From the multiplication property given in eq. (5.63), we know that X(eiw) is the periodic convolution of X1 (eiw) and X2(eiw), where the integral in eq. (5.63) can be taken over any interval of length 27T. Choosing the interval -7r < (}::::; 7T, we obtain (5.64) Equation (5.64) resembles aperiodic convolution, except for the fact that the inte- gration is limited to the interval -7r < (} ::::; 7T. However, we can convert the equation into an ordinary convolution by defining for -7r < w ::::; 7T otherwise Then, replacing X1 (ei0 ) in eq. (5.64) by X1( ei0 ), and using the fact that X1(ei0 ) is zero for I 8 I> 7T, we see that 1 X(eiw) = - - f7T Xl(el0 )X2(ei<w-IJ))d8 27T -7T = - 1- foo X1 (el0 )X2(ei<w-IJ))d8. 27T -00 Thus, X(eiw) is 1/27T times the aperiodic convolution of the rectangular pulse X1 (eiw) and the periodic square wave X2(eiw), both of which are shown in Figure 5.19. The result of this convolution is the Fourier transform X(eiw) shown in Figure 5.20. 390 The Discrete-Time Fourier Transform Chap.5 I -21T 21T w -21T w Figure 5.19 X1 (ei""') representing one period of X1(ei""'), and X2(ei""'). The linear convolution of X1 ( ei""') and X2 ( ei""') corresponds to the periodic convolu- tion of X1 (ei""') and X2(ei""'). .1L .1L 31T 1T w 4 2 4 Figure 5.20 Result of the periodic convolution in Example 5.15. 5.6 TABLES OF FOURIER TRANSFORM PROPERTIES AND BASIC FOURIER TRANSFORM PAIRS In Table 5 .l, we summarize a number of important properties of the discrete-time Fourier transform and indicate the section of the text in which each is discussed. In Table 5.2, we summarize some of the basic and most important discrete-time Fourier transform pairs. Many of these have been derived in examples in the chapter. 5.7 DUALITY In considering the continuous-time Fourier transform, we observed a symmetry or duality between the analysis equation (4.9) and the synthesis equation (4.8). No corresponding duality exists between the analysis equation (5.9) and the synthesis equation (5.8) for the discrete-time Fourier transform. However, there is a duality in the discrete-time Fourier series equations (3.94) and (3.95), which we develop in Section 5.7.1. In addition, there is"
5.6 Tables of Fourier Transform Properties and Basic Fourier Transform Pairs,"390 The Discrete-Time Fourier Transform Chap.5 I -21T 21T w -21T w Figure 5.19 X1 (ei""') representing one period of X1(ei""'), and X2(ei""'). The linear convolution of X1 ( ei""') and X2 ( ei""') corresponds to the periodic convolu- tion of X1 (ei""') and X2(ei""'). .1L .1L 31T 1T w 4 2 4 Figure 5.20 Result of the periodic convolution in Example 5.15. 5.6 TABLES OF FOURIER TRANSFORM PROPERTIES AND BASIC FOURIER TRANSFORM PAIRS In Table 5 .l, we summarize a number of important properties of the discrete-time Fourier transform and indicate the section of the text in which each is discussed. In Table 5.2, we summarize some of the basic and most important discrete-time Fourier transform pairs. Many of these have been derived in examples in the chapter. 5.7 DUALITY In considering the continuous-time Fourier transform, we observed a symmetry or duality between the analysis equation (4.9) and the synthesis equation (4.8). No corresponding duality exists between the analysis equation (5.9) and the synthesis equation (5.8) for the discrete-time Fourier transform. However, there is a duality in the discrete-time Fourier series equations (3.94) and (3.95), which we develop in Section 5.7.1. In addition, there is"
5.7 Duality,"390 The Discrete-Time Fourier Transform Chap.5 I -21T 21T w -21T w Figure 5.19 X1 (ei""') representing one period of X1(ei""'), and X2(ei""'). The linear convolution of X1 ( ei""') and X2 ( ei""') corresponds to the periodic convolu- tion of X1 (ei""') and X2(ei""'). .1L .1L 31T 1T w 4 2 4 Figure 5.20 Result of the periodic convolution in Example 5.15. 5.6 TABLES OF FOURIER TRANSFORM PROPERTIES AND BASIC FOURIER TRANSFORM PAIRS In Table 5 .l, we summarize a number of important properties of the discrete-time Fourier transform and indicate the section of the text in which each is discussed. In Table 5.2, we summarize some of the basic and most important discrete-time Fourier transform pairs. Many of these have been derived in examples in the chapter. 5.7 DUALITY In considering the continuous-time Fourier transform, we observed a symmetry or duality between the analysis equation (4.9) and the synthesis equation (4.8). No corresponding duality exists between the analysis equation (5.9) and the synthesis equation (5.8) for the discrete-time Fourier transform. However, there is a duality in the discrete-time Fourier series equations (3.94) and (3.95), which we develop in Section 5.7.1. In addition, there is Sec. 5.7 Duality 391 TABLE 5.1 PROPERTIES OF THE DISCRETE-TIME FOURIER TRANSFORM Section Property Aperiodic Signal Fourier Transform x[n] X(elw)} periodic with y[n] Y(elw) period 277' 5.3.2 Linearity ax[n] + by[n] aX(eiw) + bY(elw) 5.3.3 Time Shifting x[n- no] e- Jw""o X(eiw) 5.3.3 Frequency Shifting e;wo"" x[n] X(ej(w-wol) 5.3.4 Conjugation x*[n] X*(e- ;w) 5.3.6 Time Reversal x[-n] X(e jw) { x[nlk], if n = multiple of k 5.3.7 Time Expansion X(k [n] = X(eikw) 1 0, if n =I= multiple of k 5.4 Convolution x[n] * y[n] X(e1w)Y(e1w) 5.5 Multiplication x[n]y[n] __!___I X(eili)Y(ej(w-IIJ)d(J 277' 27T 5.3.5 Differencing in Time x[n] - x[n- 1] ( 1 - e- ;w)X(e1w) II 5.3.5 Accumulation ~ x[k] __1 _. X(eiw) 1 - e- ;w k = -cc +7TX(ei0 ) ~ o(w- 27Tk) k= -X .dX(eiw) 5.3.8 Differentiation in Frequency nx[n] 1 ------;[;;;- X(eiw) = X*(e- Jw) lffi,e{X(eiw)} = ffi,e{X(e- Jw)} 5.3.4 Conjugate Symmetry for x[n] real 9m{X(eiw)} = -9m{X. (e-Jw)} Real Signals [X (e1w)[ = [X(e- jw)[ <tX(eiw) = -<tX(e-;w) 5.3.4 Symmetry for Real, Even x[n] real an even X ( eiw) real and even Signals 5.3.4 Symmetry for Real, Odd x[n] real and odd X(elw) purely imaginary and Signals odd 5.3.4 Even -odd Decomposition Xe[n] = 8v{x[n]} [x[n] real] ffi,e{X(eiw)} of Real Signals Xo[n] = 0d{x[n]} [x[n] real] j9n-t{X(eiw)} 5.3.9 Parseval's Relation for Aperiodic Signals ,""'!?;"" 2 2 [x[n][ = 2~ Lr [X(eiw)[ dw a duality relationship between the discrete-time Fourier transform and the continuous-time Fourier series. This relation is discussed in Section 5.7.2. 5. 7. 1 Duality in the Discrete-Time Fourier Series Since the Fourier series coefficients ak of a periodic signal x[n] are themselves a periodic sequence, we can expand the sequence ak in a Fourier series. The duality property for discrete-time Fourier series implies that the Fourier series coefficients for the periodic se- quence ak are the values of (1/N)x[- n] (i.e., are proportional to the values of the original TABLE 5.2 BASIC DISCRETE-TIME FOURIER TRANSFORM PAIRS Signal Fourier Transform Fourier Series Coefficients (if periodic) L akejk(2n/N)n k~+eoc ( 2 k) 271' x- aJj w - ~ ak k=(N) (a) _ 2Trm wo -/II +x ejwon L k = m, m ::':: N, m ::':: 2N, ... 271' l>(w - w0 - 271'/) ak { I. 1=-00 = 0, otherwise (b) wo 277 irrational ::? The signal is aperiodic (a) wo = 21Tln /II L+""' l: k = ::'::m, ::':: m ::':: N, ::':: m ::':: 2N, ... cosw0 n 1T {l>(w - wo - 211'1) + l>(w + w0 - 27T/)} ak I= = { -'X- otherwise (b) ~ 2-n irrational ::? The signal is aperiodic (a) wo _ 21Tr -N k = r, r ::':: N, r ::':: 2N, ... sinw0 n ~ f {l>(w - wo - 271'1) - l>(w + w0 - 271'/)} ak = {_ ...t!_ 2j. k = - r, - r ::':: N, - r ::':: 2N, ... j I= -oc 0. otherwise (b) wu 211 irrational ::? The signal is aperiodic +oo L { I, k = 0, ±N, ±2N. ... x[n] =I 271' l>(w - 271'/) (/k = I= 0, otherwise -X Periodic square wave { I, lnl s N, sin[(27Tk/ N)(N 1 + ~ )] x[n] = 2 k) ak = . k =P 0, ±N, ±2N, ... 0, N, < lnl s N/2 +x ( N sin[27Tk/2N] 271' k~x akl> w - ~ and 2N1 +I ak = -N--, k = 0, ±N, ±2N. ... x[n + N] = x[n] +<X L I l>[n- kN] 2: kr;oc l>(w- 2~k) (/k = N for all k k= -oc I anu[n], ial <I - I - ae- jw x[n] c0· lnl s N, sin[w(N1 + ~)] - , lnl >N, sin(w/2) 0 :S lwl :S W sin Wn = ~sine (Wn) X(w) = { I, TTn 1T 1T 0, W < lwl s 1T - O<W<1T X(w) periodic with period 271' l>[n] I - I +""' u[n] - -- e--;w. + L 1Ti>(w - 21Tk) - 1 k= -X l>[n- no] e- jwno - I (n + l)anu[n], ial <I - (I - ae- jw)2 (n+r-1)' I n!(r- 1)!. anu[n], lal <I - (I- ar jwy 392 Sec. 5.7 Duality 393 signal, reversed in time). To see this in more detail, consider two periodic sequences with period N, related through the summation f[m] = ~ L g[r]e- jr(27T!N)m. (5.65) r=(N) If we let m = k and r = n, eq. (5.65) becomes f[k] = ~ L g[n]e- jk(27TIN)n. n=(N) Comparing this with eq. (3.95), we see that the sequence f[k] corresponds to the Fourier series coefficients of the signal g[n]. That is, if we adopt the notation introduced in Chapter 3 for a periodic discrete-time signal and its set of Fourier coeffi- cients, the two periodic sequences related through eq. (5.65) satisfy ~s g[n] ~ f[k]. (5.66) Alternatively, if we let m = nand r = - k, eq. (5.65) becomes f[n] = L _!._ g[- k]ejk(27T!N)n. k=<N>N Comparing this with eq. (3.94), we find that (1/N)g[- k] corresponds to the sequence of Fourier series coefficients of f [n ]. That is, ~s 1 f[n] ~ Ng[ -k]. (5.67) As in continuous time, this duality implies that every property of the discrete-time Fourier series has a dual. For example, referring to Table 3.2, we see that the pair of prop- erties (5.68) and ejm(27TIN)n x[n] ~ ak-m (5.69) are dual. Similarly, from the same table, we can extract another pair of dual properties: L ~s x[r]y[n- r] ~ Nakbk (5.70) r=(N) and ~s x[n]y[n] ~ L azbk-l· (5.71) l=(N) 394 The Discrete-Time Fourier Transform Chap.5 In addition to its consequences for the properties of discrete-time Fourier series, du- ality can often be useful in reducing the complexity of the calculations involved in deter- mining Fourier series representations. This is illustrated in the following example. Example 5.16 Consider the following periodic signal with a period of N = 9: 1 sin(57Tn/9) n =1=- multiple of 9 x[n] ~ !~ sin(>Tn/9) ' (5.72) n = multiple of 9 9' In Chapter 3, we found that a rectangular square wave has Fourier coefficients in a form much as in eq. (5.72). Duality, then, suggests that the coefficients for x[n] must be in the form of a rectangular square wave. To see this more precisely, let g[n] be a rectangular square wave with period N = 9 such that g[n] = { ~: lnl ~ 2 2 < lnl ~ 4. The Fourier series coefficients bk for g[n] can be determined from Example 3.12 as ! sin(57Tk/9) l k =1=- multiple of 9 _ 9 sin( 7Tk/9) ' b k - 5 9' k = multiple of 9 The Fourier series analysis equation (3.95) for g[n] can now be written as 2 bk = ~ ~ (l)e- J27Tnk19. n= -2 Interchanging the names of the variables k and nand noting that x[n] = b11 , we find that 1 2 . x[n] = _ ~ (l)e- 127Tnkt9. 9 k=-2 Letting k' = - k in the sum on the right side, we obtain 2 x[n] = ~ ~ e+ J27Tnk'l9. k'=-2 Finally, moving the factor 119 inside the summation, we see that the right side of this equation has the form of the synthesis equation (3.94) for x[n]. We thus conclude that the Fourier coefficients of x[n] are given by 119 lkl ~ 2 ak = { ' 0, 2 < lkl ~ 4, and, of course, are periodic with period N = 9. Sec. 5.7 Duality 395 5. 7. 2 Duality between the Discrete-Time Fourier Transform and the Continuous-Time Fourier Series In addition to the duality for the discrete Fourier series, there is a duality between the discrete-time Fourier transform and the continuous-time Fourier series. Specifically, let us compare the continuous-time Fourier series equations (3.38) and (3.39) with the discrete- time Fourier transform equations (5.8) and (5.9). We repeat these equations here for con- venience: [eq. (5.8)] x[n] = _1_ f X(eiw)ejwndw, (5.73) 27T 27T +oo [eq. (5.9)] X(eiw) = L x[n]e- Jwn, (5.74) n=-oo +oo [eq. (3.38)] x(t) = L akejkwot, (5.75) k=-00 [eq. (3.39)] ak = _!__ J x(t)e- Jkwotdt. (5.76) T T Note that eqs. (5.73) and (5.76) are very similar, as are eqs. (5.74) and (5.75), and in fact, we can interpret eqs. (5.73) and (5.74) as a Fourier series representation of the periodic frequency response X(eiw). In particular, since X(eiw) is a periodic function of w with period 27T, it has a Fourier series representation as a weighted sum of harmonically related periodic exponential functions of w, all of which have the common period of 27T. That is, X ( eiw) can be represented in a Fourier series as a weighted sum of the signals eiwn, n = 0, ± 1, ±2, .... From eq. (5.74), we see that the nth Fourier coefficient in this expansion-i.e., the coefficient multiplying eJwn_is x[- n]. Furthermore, since the period of X(eiw) is 27T, eq. (5.73) can be interpreted as the Fourier series analysis equation for the Fourier series coefficient x[n]-i.e., for the coefficient multiplying e-Jwn in the expression for X(eiw) in eq. (5.74). The use of this duality relationship is best illustrated with an example. Example 5. 1 7 The duality between the discrete-time Fourier transform synthesis equation and the continuous-time Fourier series analysis equation may be exploited to determine the discrete-time Fourier transform of the sequence x[n] = sin( 7Tn/2). 1Tn To use duality, we first must identify a continuous-time signal g(t) with period T = 21T and Fourier coefficients ak = x[k]. From Example 3.5, we know that if g(t) is a periodic square wave with period 21T (or, equivalently, with fundamental frequency w 0 = 1) and with ~: It! :::; TI g(t) = { TI <It! :::; 1T' then the Fourier series coefficients of g(t) are 396 The Discrete-Time Fourier Transform Chap.5 sin(kTt) k7T Consequently, if we take T1 = 7T/2, we will have ak = x[k]. In this case the analysis equation for g(t) is sin(7rk/2) 1 I7T _ ·kr 1 I7TI2 --- = - g(t)e 1 dt = - (1)e-Jktdt. 7Tk 27T -TT 27T -TT/2 Renaming k as n and t as w, we have sin (7rnl2) 1 I7 T12 . ---- = - (1)e-;nwdw. (5.77) 7Tn 27T -TT/2 Replacing n by-non both sides of eq. (5.77) and noting that the sine function is even, we obtain sin (7rn/2) 1 I7 T12 = - . (l)e;nwdw. 7Tn 27T -TT/2 The right-hand side of this equation has the form of the Fourier transform synthesis equation for x[n], where . { 01 lwl ~ 7T/2 X(elw) = 7T/2 < lwl ~ 7T. In Table 5.3, we present a compact summary of the Fourier series and Fourier trans- form expressions for both continuous-time and discrete-time signals, and we also indicate the duality relationships that apply in each case. TABLE 5.3 SUMMARY OF FOURIER SERIES AND TRANSFORM EXPRESSIONS Continuous time Discrete time Time domain Frequency domain Time domain Frequency domain I x(t) = ak = x[n] = I ak = L.t:-oo I akejkwot fo JT o x(t)e- jkwot Lk~(N) akejk(2TTIN)n ~ Lk~(N) x[n]e- jk(2Tr/N)n I Fourier I Series continuous time di""rete frequency ~ di""<Orele time discrete frequency periodic in time aperiodic in frequency %7/-· periodic in time ~ periodic in frequency I ""'~ I x(t) = X(jw) = x[n) = I X(ejw) = ~ J_ 00 + X(jw )ejwt dw 00 [_+..,"""" x(t)e- jw'dt I -1w 2.;: _ x[n]e- jwn b1T X( eiw )ejwn 00 I Fourier I Transform continuous time ~ I continuous frequency discrete time continuous frequency I aperiodic in time . aperiodic in frequency aperiodic in time I periodic in frequency I I 5.8 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENCE EQUATIONS A general linear constant-coefficient difference equation for an LTI system with input x[n] and output y[n] is of the form"
5.8 Systems Characterized by Linear Constant-Coefficient Difference Equations,"396 The Discrete-Time Fourier Transform Chap.5 sin(kTt) k7T Consequently, if we take T1 = 7T/2, we will have ak = x[k]. In this case the analysis equation for g(t) is sin(7rk/2) 1 I7T _ ·kr 1 I7TI2 --- = - g(t)e 1 dt = - (1)e-Jktdt. 7Tk 27T -TT 27T -TT/2 Renaming k as n and t as w, we have sin (7rnl2) 1 I7 T12 . ---- = - (1)e-;nwdw. (5.77) 7Tn 27T -TT/2 Replacing n by-non both sides of eq. (5.77) and noting that the sine function is even, we obtain sin (7rn/2) 1 I7 T12 = - . (l)e;nwdw. 7Tn 27T -TT/2 The right-hand side of this equation has the form of the Fourier transform synthesis equation for x[n], where . { 01 lwl ~ 7T/2 X(elw) = 7T/2 < lwl ~ 7T. In Table 5.3, we present a compact summary of the Fourier series and Fourier trans- form expressions for both continuous-time and discrete-time signals, and we also indicate the duality relationships that apply in each case. TABLE 5.3 SUMMARY OF FOURIER SERIES AND TRANSFORM EXPRESSIONS Continuous time Discrete time Time domain Frequency domain Time domain Frequency domain I x(t) = ak = x[n] = I ak = L.t:-oo I akejkwot fo JT o x(t)e- jkwot Lk~(N) akejk(2TTIN)n ~ Lk~(N) x[n]e- jk(2Tr/N)n I Fourier I Series continuous time di""rete frequency ~ di""<Orele time discrete frequency periodic in time aperiodic in frequency %7/-· periodic in time ~ periodic in frequency I ""'~ I x(t) = X(jw) = x[n) = I X(ejw) = ~ J_ 00 + X(jw )ejwt dw 00 [_+..,"""" x(t)e- jw'dt I -1w 2.;: _ x[n]e- jwn b1T X( eiw )ejwn 00 I Fourier I Transform continuous time ~ I continuous frequency discrete time continuous frequency I aperiodic in time . aperiodic in frequency aperiodic in time I periodic in frequency I I 5.8 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENCE EQUATIONS A general linear constant-coefficient difference equation for an LTI system with input x[n] and output y[n] is of the form Sec. 5.8 Systems Characterized by Linear Constant-Coefficient Difference Equations 397 N M .:Z aky[n - k] = .:Z bkx[n - k]. (5.78) k=O k=O The class of systems described by such difference equations is quite an important and useful one. In this section, we take advantage of several of the properties of the discrete- time Fourier transform to determine the frequency response H ( ejw) for an LTI system described by such an equation. The approach we follow closely parallels the discussion in Section 4.7 for continuous-time LTI systems described by linear constant-coefficient differential equations. There are two related ways in which to determine H(ejw). The first of these, which we illustrated in Section 3.11 for several simple difference equations, explicitly uses the fact that complex exponentials are eigenfunctions of LTI systems. Specifically, if x[n] = ejwn is the input to an LTI system, then the output must be of the form H(ejw)ejwn. Substi- tuting these expressions into eq. (5.78) and performing some algebra allows us to solve for H(ejw). In this section, we follow a second approach making use of the convolution, linear- ity, and time-shifting properties of the discrete-time Fourier transform. Let X ( ejw ), Y ( ejw ), and H(ejw) denote the Fourier transforms of the input x[n], output y[n], and impulse re- sponse h[n], respectively. The convolution property, eq. (5.48), of the discrete-time Fourier transform then implies that H(ejw) = Y(el:w). (5.79) X(eJw) Applying the Fourier transform to both sides of eq. (5.78) and using the linearity and time-shifting properties, we obtain the expression .:NZ ake~ jkw Y(ejw) = .:MZ bke~ jkw X(ejw), k=O k=O or equivalently, ""'M b e~ jkw Lk=O k (5.80) Comparing eq. (5.80) with eq. (4.76), we see that, as in the case of continuous time, H(ejw) is a ratio of polynomials, but in discrete time the polynomials are in the variable e~ jw. The coefficients of the numerator polynomial are the same coefficients as appear on the right side of eq. (5.78), and the coefficients of the denominator polynomial are the same as appear on the left side of that equation. Therefore, the frequency response of the LTI system specified by eq. (5.78) can be written down by inspection. The difference equation (5.78) is generally referred to as an Nth-order difference equation, as it involves delays in the output y[n] of uptoN time steps. Also, the denomi- nator of H(ejw) in eq. (5.80) is an Nth-order polynomial in e~ jw. Example 5. 18 Consider the causal LTI system that is characterized by the difference equation y[n] - ay[n- 1] = x[n], (5.81) 398 The Discrete-Time Fourier Transform Chap.5 with \a\ < 1. From eq. (5.80), the frequency response of this system is 1 H(elw) = ---- (5.82) 1-ae-Jw Comparing this with Example 5.1, we recognize it as the Fourier transform of the se- quence a""u[n]. Thus, the impulse response of the system is (5.83) Example 5. 1 9 Consider a causal LTI system that is characterized by the difference equation 3 1 y[n] - 4 y[n- 1] + Sy [n- 2] = 2x[n]. (5.84) From eq. (5.80), the frequency response is (5.85) As a first step in obtaining the impulse response, we factor the denominator of eq. (5.85): jw - 2 H(e ) - I . I .. (5.86) (1 - 2e- Jw)(l - 4e- Jw) H(eiw) can be expanded by the method of partial fractions, as in Example A.3 in the appendix. The result of this expansion is (5.87) The inverse transform of each term can be recognized by inspection, with the result that h[n] = 4 (21: )"" u[n] - 2 (14) "" u[n]. (5.88) The procedure followed in Example 5.19 is identical in style to that used in contin- uous time. Specifically, after expanding H(e.iw) by the method of partial fractions, we can find the inverse transform of each term by inspection. The same approach can be applied to the frequency response of any LTI system described by a linear constant-coefficient dif- ference equation in order to determine the system impulse response. Also, as illustrated in the next example, if the Fourier transform X ( e.iw) of the input to such a system is a ratio of polynomials in e- .iw, then Y(e.iw) is as well. In this case, we can use the same technique to find the response y[n] to the input x[n]. Example 5.20 Consider the LTI system of Example 5.19, and let the input to this system be 11 x[n] = (~ ) u[n]. Sec. 5.9 Summary 399 Then, using eq. (5.80) and Example 5.1 or 5.18, we obtain (5.89) (1 - ~e- jw)(1 - ~e-ew)2 · As described in the appendix, the form of the partial-fraction expansion in this case is Y(ejw) = B11 + BI2 + ~21 ' (5.90) 1- ~e-jw (1- ~e-jw)2 1- 2e-jw where the constants Btt. B12, and B2 1 can be determined using the techniques described in the appendix. This particular expansion is worked out in detail in Example A.4, and the values obtained are B11 = -4, B12 = -2, B21 = 8, so that . 4 2 8 Y(elw) = - - + . (5.91) 1- ~e-jw (1- ~e-jw)2 1- ~e-jw The first and third terms are of the same type as those encountered in Example 5.19, while the second term is of the same form as one seen in Example 5.13. Either from these examples or from Table 5.2, we can invert each of the terms in eq. (5.91) to obtain the inverse transform y[n] ~ { -4(U- 2(n + l)(U + s(U} u[n]. (5.92) 5.9 SUMMARY In this chapter, we have paralleled Chapter 4 as we developed the Fourier transform for discrete-time signals and examined many of its important properties. Throughout the chap- ter, we have seen a great many similarities between continuous-time and discrete-time Fourier analysis, and we have also seen some important differences. For example, the re- lationship between Fourier series and Fourier transforms in discrete time is exactly anal- ogous to that in continuous time. In particular, our derivation of the discrete-time Fourier transform for aperiodic signals from the discrete-time Fourier series representations is very much the same as the corresponding continuous-time derivation. Furthermore, many of the properties of continuous-time transforms have exact discrete-time counterparts. On the other hand, in contrast to the continuous-time case, the discrete-time Fourier transform of an aperiodic signal is always periodic with period 27T. In addition to similarities and differences such as these, we have described the duality relationships among the Fourier representations of continuous-time and discrete-time signals. The most important similiarities between continuous- and discrete-time Fourier anal- ysis are in their uses in analyzing and representing signals and LTI systems. Specifically, the convolution property provides us with the basis for the frequency-domain analysis of LTI systems. We have already seen some of the utility of this approach in our discussion of"
5.9 Summary,"Sec. 5.9 Summary 399 Then, using eq. (5.80) and Example 5.1 or 5.18, we obtain (5.89) (1 - ~e- jw)(1 - ~e-ew)2 · As described in the appendix, the form of the partial-fraction expansion in this case is Y(ejw) = B11 + BI2 + ~21 ' (5.90) 1- ~e-jw (1- ~e-jw)2 1- 2e-jw where the constants Btt. B12, and B2 1 can be determined using the techniques described in the appendix. This particular expansion is worked out in detail in Example A.4, and the values obtained are B11 = -4, B12 = -2, B21 = 8, so that . 4 2 8 Y(elw) = - - + . (5.91) 1- ~e-jw (1- ~e-jw)2 1- ~e-jw The first and third terms are of the same type as those encountered in Example 5.19, while the second term is of the same form as one seen in Example 5.13. Either from these examples or from Table 5.2, we can invert each of the terms in eq. (5.91) to obtain the inverse transform y[n] ~ { -4(U- 2(n + l)(U + s(U} u[n]. (5.92) 5.9 SUMMARY In this chapter, we have paralleled Chapter 4 as we developed the Fourier transform for discrete-time signals and examined many of its important properties. Throughout the chap- ter, we have seen a great many similarities between continuous-time and discrete-time Fourier analysis, and we have also seen some important differences. For example, the re- lationship between Fourier series and Fourier transforms in discrete time is exactly anal- ogous to that in continuous time. In particular, our derivation of the discrete-time Fourier transform for aperiodic signals from the discrete-time Fourier series representations is very much the same as the corresponding continuous-time derivation. Furthermore, many of the properties of continuous-time transforms have exact discrete-time counterparts. On the other hand, in contrast to the continuous-time case, the discrete-time Fourier transform of an aperiodic signal is always periodic with period 27T. In addition to similarities and differences such as these, we have described the duality relationships among the Fourier representations of continuous-time and discrete-time signals. The most important similiarities between continuous- and discrete-time Fourier anal- ysis are in their uses in analyzing and representing signals and LTI systems. Specifically, the convolution property provides us with the basis for the frequency-domain analysis of LTI systems. We have already seen some of the utility of this approach in our discussion of 400 The Discrete-Time Fourier Transform Chap.5 filtering in Chapters 3-5 and in our examination of systems described by linear constant- coefficient differential or difference equations, and we will gain a further appreciation for its utility in Chapter 6, in which we examine filtering and time-versus-frequency issues in more detail. In addition, the multiplication properties in continuous and discrete time are essential to our development of sampling in Chapter 7 and communications in Chapter 8. Chapter 5 Problems The first section of problems belongs to the basic category and the answers are provided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 5.1. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) (~) 11 - 1 u[n- 1] (b) (~)ln-II Sketch and label one period of the magnitude of each Fourier transform. 5.2. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) o [ n - 1] + o [ n + 1] (b) o [ n + 2] - o [ n - 2] Sketch and label one period of the magnitude of each Fourier transform. 5. 3. Determine the Fourier transform for -1T :::; w < 1T in the case of each of the fol- lowing periodi*c )s ignals: (a) sin( }n + (b) 2 +cos( *n + i) 5.4. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transforms of: (a) X1(ejw) = L~=-oo{21To(w- 21Tk) + 1TO(w- ~- 21Tk) + 1TO(w + ~- 27Tk)} (b) X2(ejw) = { 2), . 0 < w :::; 1T -2], -1T < w :::; 0 5.5. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transform of X(ejw) = iX(ejw)iej-tX(ejw>, where 0*::::: ;: ; lwl < * 3w lwl :::; 1T T· Use your answer to determine the values of n for which x[n] = 0. 5.6. Given that x[n] has Fourier transform X(ejw), express the Fourier transforms of the following signals in terms of X(ejw). You may use the Fourier transform properties listed in Table 5 .1. (a) x 1[ n] = x[l - n] + x[ -1 - n] (b) X2[n] = x*[-ni+x[nj (c) x3[n] = (n - 1)2 x[n]"
Problems,"400 The Discrete-Time Fourier Transform Chap.5 filtering in Chapters 3-5 and in our examination of systems described by linear constant- coefficient differential or difference equations, and we will gain a further appreciation for its utility in Chapter 6, in which we examine filtering and time-versus-frequency issues in more detail. In addition, the multiplication properties in continuous and discrete time are essential to our development of sampling in Chapter 7 and communications in Chapter 8. Chapter 5 Problems The first section of problems belongs to the basic category and the answers are provided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 5.1. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) (~) 11 - 1 u[n- 1] (b) (~)ln-II Sketch and label one period of the magnitude of each Fourier transform. 5.2. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) o [ n - 1] + o [ n + 1] (b) o [ n + 2] - o [ n - 2] Sketch and label one period of the magnitude of each Fourier transform. 5. 3. Determine the Fourier transform for -1T :::; w < 1T in the case of each of the fol- lowing periodi*c )s ignals: (a) sin( }n + (b) 2 +cos( *n + i) 5.4. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transforms of: (a) X1(ejw) = L~=-oo{21To(w- 21Tk) + 1TO(w- ~- 21Tk) + 1TO(w + ~- 27Tk)} (b) X2(ejw) = { 2), . 0 < w :::; 1T -2], -1T < w :::; 0 5.5. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transform of X(ejw) = iX(ejw)iej-tX(ejw>, where 0*::::: ;: ; lwl < * 3w lwl :::; 1T T· Use your answer to determine the values of n for which x[n] = 0. 5.6. Given that x[n] has Fourier transform X(ejw), express the Fourier transforms of the following signals in terms of X(ejw). You may use the Fourier transform properties listed in Table 5 .1. (a) x 1[ n] = x[l - n] + x[ -1 - n] (b) X2[n] = x*[-ni+x[nj (c) x3[n] = (n - 1)2 x[n] Chap. 5 Problems 401 5.7. For each of the following Fourier transforms, use Fourier transform properties (Table 5.1) to determine whether the corresponding time-domain signal is (i) real, imagi- nary, or neither and (ii) even, odd, or neither. Do this without evaluating the inverse of any of the given transforms. (a) X1 (eiw) = e- jw) l~ 1 (sin kw) (b) X2(ei<v) = j sin(w) cos(5w) (c) X3(eiw) = A(w) + eJB(w) where 1, 0 :::; lwl :::; i 3 A(w) = { O, i lwl :::; and B(w) = - ; + 'TT. < 7T 5.8. Use Tables 5.1 and 5.2 to help determine x[n] when its Fourier transform is . 1 s.m ;3, w ) X(e 1w) = . -.- ~- + 57TO(w), 1- e-.1w ( sm ~ 2 5.9. The following four facts are given about a real signal x[n] with Fourier transform X(ej111 ): 1. x[n] = 0 for n > 0. 2. x[O] > 0. 3. 9n~{X(eiw)} = sinw- sin2w. 4. 2~ J_7T7T IX(eiw)l2 dw = 3. Determine x[n]. 5.10. Use Tables 5.1 and 5.2 in conjunction with the fact that n=-x to determine the numerical value of A= in(~)n· n=O 5.11. Consider a signal g[n] with Fourier transform G(eiw). Suppose g[n] = X(2)[n], where the signal x[n] has a Fourier transform X(eiw). Determine a real number a such that 0 < a < 27T and G(eiw) = G(eJ<w-cd). 5.12. Let 2 y[n] = -sin *-n) ( * (sin wen) ' 7Tn 7Tn where * denotes convolution and lwei :::; 7T. Determine a stricter constraint on We 402 The Discrete-Time Fourier Transform Chap.5 which ensures that s.m 7-T n )2 y[n] = ( 7T~ . 5.13. An LTI system with impulse response h 1[n] = (*)nu[n] is connected in parallel with another causal LTI system with impulse response h2 [n]. The resulting parallel interconnection has the frequency response H jw _ -12 + 5e- jw (e)- 12-?e-jw+e-j2w' Determine h2 [n]. 5.14. Suppose we are given the following facts about an LTI systemS with impulse re- sponse h[n] and frequency response H(ejw): 1. (~)nu[n] ~ g[n], where g[n] = 0 for n 2: 2 and n < 0. 2. H(ej7TI2) = 1. 3. H(ejw) = H(ej(w-7T)). Determine h[n]. 5.15. Let the inverse Fourier transform of Y(ejw) be y[n] = ('i::,n r where 0 <We< 7T. Determine the value of We which ensures that 5.16. The Fourier transform of a particular signal is X(ejw) = ~ (1/2)k . L 1 - .!e- j(w-7T!2k) k=O 4 It can be shown that x[n] = g[n]q[n], where g[n] is of the form anu[n] and q[n] is a periodic signal with period N. (a) Determine the value of a. (b) Determine the value of N. (c) Is x[n] real? 5.17. The signal x[ n] = (- 1) n has a fundamental period of 2 and corresponding Fourier series coefficients ak. Use duality to determine the Fourier series coefficients bk of the signal g[ n] = an with a fundamental period of 2. 5.18. Given the fact that ~ 1- a2 alnl ~ 1- 2acosw + a2' lal < 1, Chap. 5 Problems 403 use duality to determine the Fourier series coefficients of the following continuous- time signal with period T = 1: x(t) = 5 - 4 cos(27Tt) · 5.19. Consider a causal and stable LTI systemS whose input x[n] and output y[n] are related through the second-order difference equation 1 1 y[n]- (;y[n- 1]- (;y[n- 2] = x[n]. (a) Determine the frequency response H(eiw) for the systemS. (b) Determine the impulse response h[n] for the systemS. 5.20. A causal and stable LTI system S has the property that (a) Determine the frequency response H(eiw) for the systemS. (b) Determine a difference equation relating any input x[n] and the corresponding output y[n]. BASIC PROBLEMS 5.21. Compute the Fourier transform of each of the following signals: (a) x[n] = u[n - 2] - u[n - 6] (b) x [ n] = ( ~) -n u [- n - 1] (c) x[n] = (l)l 111u[-n- 2] 3 (d) x[n] = 211 sin(*n)u[ -n] (e) x[n] = (~)lnl cos(-~(n- 1)) (f) x[n] = { n, -3 ~ _n ~ 3 0, otherwise (g) x[n] = sin(~n) + cos(n) (h) x[n] = sinCS; n) + cosC; n) (i) x[n] = x[n - 6], and x[n] = u[n] - u[n - 5] for 0 ~ n ~ 5 (j) x[n] = (n - 1)( l )lnl 3 (k) x[nl = cinC:,~15>) cosC; n) 5.22. The following are the Fourier transforms of discrete-time signals. Determine the signal corresponding to each transform. 1 7!. < lw I < 37T (a) X(eiw) = ' 4 - - 4 { 0, 3 ; ~ lw I ~ 7T, 0 ~ lw I < * (b) X(eiw) = 1 + 3e-.iw + 2e- i 2w - 4e- i 3w + e-JIOw (c) X(eiw) = e-Jw/2 for -7r ~ w ~ 7T (d) X(eiw) = cos2 w + sin2 3w 404 The Discrete-Time Fourier Transform Chap.5 (e) X(ejw) = L~= _00 ( -1)kB(w - ¥k ) -jw I (f) X(ejw) = ~ 1-~rJw (g) X(ejw) = 1- ~e- jw 1-±e-jw_~e-2jw (h) X(ejw) = 1-(~)6e-J6w 1-~e-Jw 5.23. Let X(ejw) denote the Fourier transform of the signal x[n] depicted in Figure P5.23. Perform the following calculations without explicitly evaluating X(ejw): (a) Evaluate X(ej0 ). (b) Find <t.X(ejw). (c) Evaluate J~ 7TX(ejw)dw. (d) Find X(ej7T). (e) Determine and sketch the signal whose Fourier transform is CR..e{x(w )}. (f) Evaluate: (i) J~ 7TIX(ejw)l 2dw (ii) J_ 7T7T Id Xd~w) 12 dw x[n] n Fig P5.23 5.24. Determine which, if any, of the following signals have Fourier transforms that sat- isfy each of the following conditions: 1. ffi..e{X(ejw)} = 0. 2. dm{X(ejw)} = 0. 3. There exists a real a such that ejaw X ( ejw) is real. 4. J ~7TX(ejw)dw = 0. 5. X(ejw) periodic. 6. X(ej0 ) = 0. (a) x[n] as in Figure P5.24(a) (b) x[n] as in Figure P5.24(b) (c) x[n] = (~)nu[n] (d) x[n] = (~)lnl (e) x[n] = B[n- 1] + B[n + 2] (f) x[n] = B[n- 1] + B[n + 3] (g) x[n] as in Figure P5.24(c) (h) x[n] as in Figure P5.24(d) (i) x[n] = B[n- 1] - B[n + 1] Chap. 5 Problems 405 x[n] 1 • • • • .;I t(l II I -1 0 1 2 3 4 5 •6 • • • • • n (a) x[n] .I. .I. . 1I . 1 • I. . I . . I. 2 I I-2-1 ~J I I I n (b) x[n] 2 n (c) x[n] 2 n (d) Fig P5.24 5.25. Consider the signal depicted in Figure P5 .25. Let the Fourier transform of this signal be written in rectangular form as X(ejw) = A(w) + jB(w ). Sketch the function of time corresponding to the transform 406 The Discrete-Time Fourier Transform Chap.5 x[n] 3 2 n -2 Fig P5.25 5.26. Let x1 [n] be the discrete-time signal whose Fourier transform X1( eiw) is depicted in Figure P5.26(a). (a) Consider the signal x2[n] with Fourier transform X2(eiw), as illustrated in Fig- ure P5.26(b). Express x2[n] in terms of x 1 [n]. [Hint: First express X2(eiw) in terms of X1( eiw), and then use properties of the Fourier transform.] (b) Repeat part (a) for x3 [n] with Fourier transform X3(eiw), as shown in Figure P5.26(c). (c) Let n=-x a=---- X L Xt[n] n=-'XJ This quantity, which is the center of gravity of the signal x 1 [n], is usually re- ferred to as the delay time of x 1 [n]. Find a. (You can do this without first deter- mining x 1 [n] explicitly.) 1T 1T w 3 (a) Fig P5.26a Chap. 5 Problems 407 w (b) w (c) Fig P5.26b,c (d) Consider the signal x4 [n] = x 1 [n] * h[n], where sin( rrn/6) h[n] = --- rrn Sketch X4(ejw). 5.27. (a) Let x[n] be a discrete-time signal with Fourier transform X(ejw), which is il- lustrated in Figure P5 .27. Sketch the Fourier transform of w[n] = x[n]p[n] for each of the following signals p[n]: (i) p[n] = cos rrn (ii) p[n] = cos( rrn/2) (iii) p[n] = sin( rrn/2) 00 (iv) p[n] = L: o[n- 2k] k= -00 00 (v) p[n] = L: o[n- 4k] k= -00 '1T '1T w 2 Fig P5.27 408 The Discrete-Time Fourier Transform Chap.5 (b) Suppose that the signal w[n] of part (a) is applied as the input to an LTI system with unit sample response h[n] = sin( '7Tn/2). 1Tn Determine the output y[n] for each of the choices of p[n] in part (a). 5.28. The signals x[n] and g[n] are known to have Fourier transforms X(ejw) and G(ejw), respectively. Furthermore, X(ejw) and G(ejw) are related as follows: - 1 J +1r X(ej8 )G(ej(w-O))d() = 1 + e- jw (P5.28-1) 21T -7T (a) If x[n] = ( -l)n, determine a sequence g[n] such that its Fourier transform G(ejw) satisfies eq. (P5.28-1). Are there other possible solutions for g[n]? (b) Repeat the previous part for x[n] = (~)nu[n]. 5.29. (a) Consider a discrete-time LTI system with impulse response h[n] = G)"" u[n]. Use Fourier transforms to determine the response to each of the following input signals: (i) x[n] = (~)nu[n] (ii) x[n] = (n + l)(~)nu[n] (iii) x[n] = ( -l)n (b) Suppose that h[n] = [(H cos(~n)]u[n]. Use Fourier transforms to determine the response to each of the following in- puts: (i) x[n] = (~ )nu[n] (ii) x[n] = cos( '7Tn/2) (c) Let x[n] and h[n] be signals with the following Fourier transforms: X(ejw) = 3ejw + 1 - e- jw + 2e-j3w, H(ejw) = -ejw + 2e-2jw + ej4w. Determine y[n] = x[n] * h[n]. 5.30. In Chapter 4, we indicated that the continuous-time LTI system with impulse re- sponse h(t) = -Ws m. c (-Wt) = -sin W-t 1T 1T 1Tt plays a very important role in LTI system analysis. The same is true of the discrete- time LTI system with impulse response h[n] = -Ws m. c (W- n) = -sin- Wn. 1T 1T 1Tn Chap. 5 Problems 409 (a) Determine and sketch the frequency response for the system with impulse re- sponse h[n]. (b) Consider the signal x[n] = sm. (7ST n) - 2 cos (74T n) . Suppose that this signal is the input to LTI systems with the following impulse responses. Determine the output in each case. (i) h[n] sin(1711/6) 1711 (ii) h[n] = sin(1711/6) + sin(171112) 1711 1711 (iii) h[n] = sin(1711/6)sin(1711/3) 172112 (iv) h[n] = sin(17n/6)sin(17n/3) 1711 (c) Consider an LTI system with unit sample response h[n] = sin( 7Tn/3). 7Tn Determine the output for each of the following inputs: (i) x[n] = the square wave depicted in Figure P5.30 00 (ii) x[n] = L o[n- 8k] k= -00 (iii) x[n] = ( -1)11 times the square wave depicted in Figure P5.30 (iv) x[n] = o[n + 1] + o[n- 1] x[n] III ... IIIII .. ~IIIII ... IIIII ... IIIII ... II -8 0 8 16 n Fig P5.30 5.31. An LTI system S with impulse response h[n] and frequency response H(ejw) is known to have the property that, when -7T ::::; w0 ::::; 7T, cos won ~ wo cos won. (a) Determine H(ejw). (b) Determine h[n]. 5.32. Let h1 [n] and h2[n] be the impulse responses of causal LTI systems, and let H 1 (ejw) and H 2(ejw) be the corresponding frequency responses. Under these conditions, is the following equation true in general or not? Justify your answer. 410 The Discrete-Time Fourier Transform Chap.S 5.33. Consider a causal LTI system described by the difference equation 1 y[n] + 2y [n - 1] = x[n]. (a) Determine the frequency response H(ejw) of this system. (b) What is the response of the system to the following inputs? (i) x[n] = <!)nu[n] (ii) x[n] = (-!)nu[n] (iii) x[n] = o[n] + !o[n - 1] (iv) x[n] = o[n] - !o[n - 1] (c) Find the response to the inputs with the following Fourier transforms: (i) X(ejw) = 1- ±e-JW I+ ~e-.1w ("""") X( e jw) -- l+.!e-jw II I - 4I e-.Jw. (iii) X(ejw) = 1 ( 1- £r jw)(l + ~e- jw) (iv) X(ejw) = 1 + 2e- 3jw 5.34. Consider a system consisting of the cascade of two LTI systems with frequency responses and (a) Find the difference equation describing the overall system. (b) Determine the impulse response of the overall system. 5.35. A causal LTI system is described by the difference equation y[n] - ay[n - 1] = bx[n] + x[n - 1], where a is real and less than 1 in magnitude. (a) Find a value of b such that the frequency response of the system satisfies IH(ejw)l = 1, for all w. This kind of system is called an all-pass system, as it does not attenuate the input ejwn for any value of w. Use the value of b that you have found in the rest of the problem. (b) Roughly sketch <r:.H(ejw), 0 :::; w :::; 7T, when a = !· (c) Roughly sketch <r:.H(ejw), 0 :::; w :::; 7T, when a = -!. Chap. 5 Problems 411 (d) Find and plot the output of this system with a = - ~ when the input is x[n] = (~ ru [n]. From this example, we see that a nonlinear change in phase can have a signif- icantly different effect on a signal than the time shift that results from a linear phase. 5.36. (a) Let h[n] and g[n] be the impulse responses of two stable discrete-time LTI sys- tems that are inverses of each other. What is the relationship between the fre- quency responses of these two systems? (b) Consider causal LTI systems described by the following difference equations. In each case, determine the impulse response of the inverse system and the difference equation that characterizes the inverse. (i) y[n] = x[n] - *x[n- 1] (ii) y[n] + ~y[n- 1] = x[n] (iii) y[n] + ~y[n- 1] = x[n] - *x[n- 1] (iv) y[n] + ~y[n- 1]- ~y[n- 2] = x[n]- *x[n- 1]- ~x[n- 2] (v) y[n] + ~y[n- 1]- ~y[n- 2] = x[n]- ~x[n- 1] (vi) y[n] + ~y[n- 1]- ~y[n- 2] = x[n] (c) Consider the causal, discrete-time LTI system described by the difference equa- tion 1 1 y[n] + y[n- 1] + 4y[n- 2] = x[n- 1] - 2x[n- 2]. (P5.36-1) What is the inverse of this system? Show that the inverse is not causal. Find an- other causal LTI system that is an ""inverse with delay"" of the system described by eq. (P5.36-1). Specifically, find a causal LTI system such that the output w[n] in Figure P5.36 equals x[n - 1]. LTI system y[n] Causal x[n] described by LTI w[n] eq. (P5.36-1) system Fig P5.36 ADVANCED PROBLEMS 5.37. Let X(efw) be the Fourier transform of x[n]. Derive expressions in terms of X(efw) for the Fourier transforms of the following signals. (Do not assume that x[n] is real.) (a) CRe{x[n]} (b) x*[ -n] (c) 8v{x[n]} 412 The Discrete-Time Fourier Transform Chap.5 5.38. Let X(eiw) be the Fourier transform of a real signal x[n]. Show that x[n] can be written as x[n] = r{B(w)cosw + C(w)sinw}dw by finding expressions for B(w) and C(w) in terms of X(eiw). 5.39. Derive the convolution property ~ . . x[n] * h[n] ~ X(elw)H(elw). 5.40. Let x[n] and h[n] be two signals, and let y[n] = x[n] * h[n]. Write two expressions for y[O], one (using the convolution sum directly) in terms of x[n] and h[n], and one (using the convolution property of Fourier transforms) in terms of X(eiw) and H(eiw). Then, by a judicious choice of h[n], use these two expressions to derive Parsev al's relation-that is, In a similar fashion, derive the following generalization of Parseval's relation: +oo 1 J7T . . ~ x[n]z*[n] = 7T _ X(elw)Z*(elw)dw. n-oo 2 7T 5.41 Let i[n] be a periodic signal with period N. A finite-duration signal x[n] is related to i[n] through x[n] = { i[n], no :::sn:::sno+N-1 0, otherwise for some integer no. That is, x[n] is equal to i[n] over one period and zero elsewhere. (a) If i[n] has Fourier series coefficients ak and x[n] has Fourier transform X(eiw), show that regardless of the value of no. (b) Consider the following two signals: x[n] = u[n] - u[n - 5] 00 i[n] = ~ x[n- kN] k= -00 where N is a positive integer. Let ak denote the Fourier coefficients of i[n] and let X(eiw) denote the Fourier transform of x[n]. (i) Determine a closed-form expression for X(eiw). Chap. 5 Problems 413 (ii) Using the result of part (i), determine an expression for the Fourier coeffi- cients ak. 5.42. In this problem, we derive the frequency-shift property of the discrete-time Fourier transform as a special case of the multiplication property. Let x[n] be any discrete- time signal with Fourier transform X(eiw), and let g[n] = eiwon x[n]. (a) Determine and sketch the Fourier transform of (b) The multiplication property of the Fourier transform tells us that, since g[n] = p[n]x[n], G(eiw) = _l_ f X(eifJ)P(ei<w-{}))dO. 27T <27T> Evaluate this integral to show that G(eiw) = X(ej(w-wo)). 5.43. Let x[n] be a signal with Fourier transform X(eiw), and let g[n] = x[2n] be a signal whose Fourier transform is G(eiw). In this problem, we derive the rela- tionship between G(eiw) and X(eiw). (a) Let (e- Jmz x[n]) + x[n] v[n] = . 2 Express the Fourier transform V(eiw) of v[n] in terms of X(eiw). (b) Noting that v[n] = 0 for n odd, show that the Fourier transform of v[2n] is equal to V(ei~). (c) Show that x[2n] = v[2n]. It follows that G(eiw) = V(ejw/2). Now use the result of part (a) to express G(eiw) in terms of X(eiw). 5.44. (a) Let x 1 [n] = cos (7Tn) + s.m (7TT n) 3 414 The Discrete-Time Fourier Transform Chap.5 be a signal, and let X1( ejw) denote the Fourier transform of x 1 [n]. Sketch x 1 [n], together with the signals with the following Fourier transforms: (i) X2(ejw) = X1(ejw)ejw, lwl < 1T (ii) X3(ejw) = XI (ejw)e- j3wl2, lwl < 1T (b) Let w(t) = cos (1TT t) + s.m (1TTt ) 3 2 be a continuous-time signal. Note that x 1 [n] can be regarded as a sequence of evenly spaced samples of w(t); that is, x 1 [n] = w(nT). Show that x2[n] = w(nT- a) and x3[n] = w(nT- /3) and specify the values of a and f3. From this result we can conclude that x 2 [n] and x3 [n] are also evenly spaced samples of w(t). 5.45. Consider a discrete-time signal x[n] with Fourier transform as illustrated in Figure P5.45. Provide dimensioned sketches of the following continuous-time signals: (a) XJ (t) = L~ = _ X[n]ej(27TIIO)nt 70 (b) X2(t) = L~=-xx[ -n]ej(27T!IO)nt CRe { X(eiw)} w -21T 1T 1T 21T w 2 Fig P5.45 Chap. 5 Problems 415 (c) X3(t) = 2.:= _0d{x[n]}e.i(21TI&)nt (d) x4(t) = 2.: 00 = _,/Re{x[n]}e.i(21TI6)nt 5.46. In Example 5.1, we showed that for Ia I < 1, ~ 1 a 11 u[n] ~ .. 1- ae- JW (a) Use properties of the Fourier transform to show that ~ 1 (n + 1)a 11 u[n] ~ ( 1 - ae _. ) . JW 2 (b) Show by induction that the inverse Fourier transform of X(e.iw) = 1 (1- ae-Jwy is (n + r- 1)! x[n] = 1 _ )' a 11 u[n]. n.(r 1 . 5.47. Determine whether each of the following statements is true or false. Justify your answers. In each statement, the Fourier transform of x[n] is denoted by X(e.iw). (a) If X(e.iw) = X(e.i(w-l)), then x[n] = 0 for lnl > 0. (b) If X(e.iw) = X(e.i(w-1T)), then x[n] = 0 for lnl > 0. (c) If X(eiw) = X(eiw12 ), then x[n] = 0 for lnl > 0. (d) If X(e.iw) = X(e.i2w), then x[n] = 0 for lnl > 0. 5.48. We are given a discrete-time, linear, time-invariant, causal system with input de- noted by x[n] and output denoted by y[n]. This system is specified by the following pair of difference equations, involving an intermediate signal w[n]: 1 y[n] + 4y[n- 1] + 1 2 w[n] + 2w[n- 1] = 3x[n], 5 5 y[n]- 4y[n- 1] + 2w[n]- 2w[n- 1] = - 3x[n]. (a) Find the frequency response and unit sample response of the system. (b) Find a single difference equation relating x[n] and y[n] for the system. 5.49. (a) A particular discrete-time system has input x[n] and output y[n]. The Fourier transforms of these signals are related by the equation Y(ejw) = 2X(ejw) + e- jw X(ejw)- dX~~w). (i) Is the system linear? Clearly justify your answer. (ii) Is the system time invariant? Clearly justify your answer. (iii) What is y[n] if x[n] = 8[n]? 416 The Discrete-Time Fourier Transform Chap.5 (b) Consider a discrete-time system for which the transform Y(eiw) of the output is related to the transform of the input through the relation Find an expression for y[n] in terms of x[n]. 5.50. (a) Suppose we want to design a discrete-time LTI system which has the property that if the input is x[n] = (l)n u[n] - 1 (l)n-1 2 4 2 u[n - 1], then the output is y[n] ~ (~ )"" u[n]. (i) Find the impulse response and frequency response of a discrete-time LTI system that has the foregoing property. (ii) Find a difference equation relating x[n] and y[n] that characterizes the system. (b) Suppose that a system has the response (114)nu[n] to the input (n + 2)(112)nu[n]. If the output of this system is 8[n] - ( -112)nu[n], what is the input? 5.51. (a) Consider a discrete-time system with unit sample response h[n] = 1 )n u[n] + 1 (1 )n (2 2 4 u[n]. Determine a linear constant-coefficient difference equation relating the input and output of the system. (b) Figure P5.51 depicts a block diagram implementation of a causal LTI system. (i) Find a difference equation relating x[n] and y[n] for this system. (ii) What is the frequency response of the system? (iii) Determine the system's impulse response. Fig PS.S1 Chap. 5 Problems 417 5.52. (a) Let h[n] be the impulse response of a real, causal, discrete-time LTI system. Show that the system is completely specified by the real part of its frequency response. (Hint: Show how h[n] can be recovered from Sv{h[n]}. What is the Fourier transform of Sv{ h[ n]} ?) This is the discrete-time counterpart of the real- part sufficiency property of causal LTI systems considered in Problem 4.47 for continuous-time systems. (b) Let h[n] be real and causal. If ffi-e{H(eiw)} = 1 +a cos 2w(a real), determine h[n] and H(eiw). (c) Show that h[n] can be completely recovered from knowledge of 9m{H(eiw)} and h[O]. (d) Find two real, causal LTI systems whose frequency responses have imaginary parts equal to sin w. EXTENSION PROBLEMS 5.53. One ofthe reasons for the tremendous growth in the use of discrete-time methods for the analysis and synthesis of signals and systems was the development of exceed- ingly efficient tools for performing Fourier analysis of discrete-time sequences. At the heart of these methods is a technique that is very closely allied with discrete-time Fourier analysis and that is ideally suited for use on a digital computer or for im- plementation in digital hardware. This technique is the discrete Fourier transform (DFT) for finite-duration signals. Let x[n] be a signal of finite duration; that is, there is an integer N1 so that x[n] = 0, outside the interval 0 :::::; n :::::; N1 - 1 Furthermore, let X(eiw) denote the Fourier transform of x[n]. We can construct a periodic signal i[n] that is equal to x[n] over one period. Specifically, let N 2:: N1 be a given integer, and let i[n] be periodic with period Nand such that i[n] = x[n], The Fourier series coefficients for i[n] are given by ak = _!_ L i[n]e- jk(27TIN)n N (N) Choosing the interval of summation to be that over which i[n] = x[n], we obtain 1 N-1 . ak = N L x[n]e- Jk(27TIN)n (P5.53-l) n=O The set of coefficients defined by eq. (P5.53-l) comprise the DFT of x[n]. Specifi- cally, the DFT of x[n] is usually denoted by X[k], and is defined as 418 The Discrete-Time Fourier Transform Chap.5 1 N-1 X[k] = ak = N L x[n]e- jk(27TIN)n, k = 0, 1, ... , N - 1 (P5.53-2) n=O The importance of the DFf stems from several facts. First note that the original finite duration signal can be recovered from its DFf. Specifically, we have N-1 x[n] = L X[k]ejk(27r!N)n, n = 0, 1, ... , N - 1 (P5.53-3) k=O Thus, the finite-duration signal can either be thought of as being specified by the finite set of nonzero values it assumes or by the finite set of values of X[ k] in its DFf. A second important feature of the DFf is that there is an extremely fast algorithm, called the fast Fourier transform (FFT), for its calculation (see Problem 5.54 for an introduction to this extremely important technique). Also, because of its close relationship to the discrete-time Fourier series and transform, the DFf inherits some of their important properties. (a) Assume that N 2: N1• Show that X[k] = ~x(ej<27TkJN)) where X[k] is the DFf of x[n]. That is, the DFf corresponds to samples of X(ejw) taken every 2TTIN. Equation (P5.53-3) leads us to conclude that x[n] can be uniquely represented by these samples of X(ejw). (b) Let us consider samples of X(ejw) taken every 27r/M, where M < N1• These samples correspond to more than one sequence of duration N1• To illustrate this, consider the two signals Xt [n] and x2 [n] depicted in Figure P5.53. Show that if we choose M = 4, we have Xt (ej(27Tkl4)) = X2 (ej(27Tk!4)) for all values of k. x1 [n] x2 [n] ...... :r .I: ..... . • ••••2 .Iri:~r ... 0 12 3 n _111112341 7 n -1 Fig P5.53 5.54. As indicated in Problem 5.53, there are many problems of practical importance in which one wishes to calculate the discrete Fourier transform (DFf) of discrete-time signals. Often, these signals are of quite long duration, and in such cases it is very Chap. 5 Problems 419 important to use computationally efficient procedures. One of the reasons for the significant increase in the use of computerized techniques for the analysis of signals was the development of a very efficient technique known as the fast Fourier trans- form (FFf) algorithm for the calculation of the OFf of finite-duration sequences. In this problem, we develop the principle on which the FFf is based. Let x[n] be a signal that is 0 outside the interval 0 ~ n ~ N1 - 1. For N ~ N1, theN-point OFf of x[n] is given by 1 N-1 X[k] = - ~ x[n]e- Jk(21TIN)n, k = 0, 1, ... , N- 1. (P5.54-1) N k=O It is convenient to write eq. (P5.54-1) as 1 N-1 X[k] = N ~ x[n]WNk, (P5.54-2) k=O where (a) One method for calculating X[k] is by direct evaluation of eq. (P5.54-2). A useful measure of the complexity of such a computation is the total number of complex multiplications required. Show that the number of complex multipli- cations required to evaluate eq. (P5.54-2) directly, fork = 0, 1, ... , N- 1, is N 2. Assume that x[ n] is complex and that the required values of WNk have been precomputed and stored in a table. For simplicity, do not exploit the fact that, for certain values of n and k, WNk is equal to ± 1 or ± j and hence does not, strictly speaking, require a full complex multiplication. (b) Suppose that N is even. Let f[n] = x[2n] represent the even-indexed samples of x[n], and let g[n] = x[2n + 1] represent the odd-indexed samples. (i) Show that f[n] and g[n] are zero outside the interval 0 ~ n ~ (N/2)- 1. (ii) Show that theN-point OFf X[k] of x[n] can be expressed as (N/2)-1 (N/2)-1 1 1 X[k] = N ~ f[n]WN72 + N w~ ~ g[n]WN72 n=O n=O 21 - 1 k - F [k] + 2W NG[k], k = 0, 1, ... , N - 1, (P5.54-3) where 2 (N/2)-1 F[k] = N ~ f[n]WN~2 , n=O 2 (N/2)-1 G[k] = N ~ g[n]WN72. n=O 420 The Discrete-Time Fourier Transform Chap.5 (iii) Show that, for all k, F- [k + 2N l = F-[k], G- [k + Nl = G-2 ~k]. Note that F[k], k = 0, 1, ... , (N/2) - 1, and G[k], k = 0, 1, ... , (N/2)- 1, are the (N/2)-point DFTs of f[n] and g[n], respectively. Thus, eq. (P5.54-3) indicates that the length-N DFT of x[n] can be calculated in terms of two DFTs of length N/2. (iv) Determine the number of complex multiplications required to compute X[k], k = 0, 1, 2, ... , N - 1, from eq. (P5.54-3) by first computing F[k] and G[k]. [Make the same assumptions about multiplications as in part (a), and ignore the multiplications by the quantity 1/2 in eq. (P5.54-3).] (c) If, likeN, N/2 is even, then f[n] and g[n] can each be decomposed into se- quences of even- and odd-indexed samples, and therefore, their DFTs can be computed using the same process as in eq. (P5.54-3). Furthermore, if N is an integer power of 2, we can continue to iterate the process, thus achieving sig- nificant savings in computation time. With this procedure, approximately how many complex multiplications are required for N = 32, 256, 1,024, and 4,096? Compare this to the direct method of calculation in part (a). 5.55. In this problem we introduce the concept of windowing, which is of great importance both in the design of LTI systems and in the spectral analysis of signals. Windowing is the operation of taking a signal x[ n] and multiplying it by a finite-duration window signal w[n]. That is, p[n] = x[n]w[n]. Note that p[n] is also of finite duration. The importance of windowing in spectral analysis stems from the fact that in numerous applications one wishes to compute the Fourier transform of a signal that has been measured. Since in practice we can measure a signal x[n] only over a finite time interval (the time window), the actual signal available for spectral analysis is p[n] = { x[n], -M:::; n:::; M 0, otherwise where - M :::; n :::; M is the time window. Thus, p[n] = x[n]w[n], where w[n] is the rectangular window; that is, -M:::; n ::SM w[n] = { 6: (P5.55-1) otherwise Windowing also plays a role in LTI system design. Specifically, for a variety of reasons (such as the potential utility of the FFT algorithm; see Problem P5.54), it is Chap. 5 Problems 421 often advantageous to design a system that has an impulse response of finite duration to achieve some desired signal-processing objective. That is, we often begin with a desired frequency response H(ejw) whose inverse transform h[n] is an impulse response of infinite (or at least excessively long) duration. What is required then is the construction of an impulse response g[ n] of finite duration whose transform G(ejw) adequately approximates H(ejw). One general approach to choosing g[n] is to find a window function w[n] such that the transform of h[n]w[n] meets the desired specifications for G(ejw). Clearly, the windowing of a signal has an effect on the resulting spectrum. In this problem, we illustrate that effect. (a) To gain some understanding of the effect of windowing, consider windowing the signal x[n] = L 8[n- k] k= -00 using the rectangular window signal given in eq. (P5.55-1). (i) What is X(ejw)? (ii) Sketch the transform of p[n] = x[n]w[n] when M = 1. (iii) Do the same forM = 10. (b) Next, consider a signal x[n] whose Fourier transform is specified by iwi < 7r/4 7r/4 < iwi ::::; 7T'. Let p[n] = x[n]w[n], where w[n] is the rectangular window of eq. (P5.55-1). Roughly sketch P(ejw) forM = 4, 8, and 16. (c) One of the problems with the use of a rectangular window is that it introduces ripples in the transform P(ejw). (This is in fact directly related to the Gibbs phenomenon.) For that reason, a variety of other window signals have been developed. These signals are tapered; that is, they go from 0 to 1 more gradually than the abrupt transition of the rectangular window. The result is a reduction in the amplitude of the ripples in P( ejw) at the expense of adding a bit of distortion in terms of further smoothing of X(ejw). To illustrate the points just made, consider the signal x[ n] described in part (b), and let p[n] = x[n]w[n], where w[n] is the triangular or Bartlett window; that is, w[n] = { 1 - J~1' -M::::; n::::; M 0, otherwise Roughly sketch the Fourier transform of p[n] = x[n]w[n] forM = 4, 8, and 16. [Hint: Note that the triangular signal can be obtained as a convolution of a rectangular signal with itself. This fact leads to a convenient expression for W(ejw).] 422 The Discrete-Time Fourier Transform Chap.5 (d) Let p[n] = x[n]w[n], where w[n] is a raised cosine signal known as the Han- ning window; i.e., w[n] = { 4[1 + COS(7Tn/M)], -M ~ n ~ M 0, otherwise Roughly sketch P( eiw) for M = 4, 8, and 16. 5.56. Let x[m, n] be a signal that is a function of the two independent, discrete variables m and n. In analogy with one dimension and with the continuous-time case treated in Problem 4.53, we can define the two-dimensional Fourier transform of x[m, n] as (P5.56-1) n= -'Xffl= -X (a) Show that eq. (P5.56-1) can be calculated as two successive one-dimensional Fourier transforms, first in m, with n regarded as fixed, and then inn. Use this result to determine an expression for x[m, n] in terms of X(eiw 1, eiw 2 ). (b) Suppose that x[m, n] = a[m]b[n], where a[m] and b[n] are each functions of only one independent variable. Let A(eiw) and B(eiw) denote the Fourier transforms of a[m] and b[n], respectively. Express X(eiw 1, eiw 2 ) in terms of A(eiw) and B(eiw). (c) Determine the two-dimensional Fourier transforms of the following signals: (i) x[m, n] = B[m- 1]B[n + 4] (ii) x[m, n] = <4Yz-mu[n- 2]u[ -m] (iii) x[m, n] = (4)n cos(27Tm/3)u[n] (. ) [ ] _ { 1, -2 < m < 2 and -4 < n < 4 IV x m, n - 0, otherwise 1 (v) x[m, n] = { o' -2 + n < m < 2 + nand -4 < n < 4 h . , ot erw1se (vi) x[m, n] = sin ( ~n + 2~m) (d) Determine the signal x[m, n] whose Fourier transform is 0 < lw1l ~ 7T/4 and 0 < lw2l ~ 7T/2 7T/4 < lw1l < 7T or 7T/2 < lw2l < 7T · (e) Let x[m, n] and h[m, n] be two signals whose two-dimensional Fourier trans- forms are denoted by X(eiw 1, eiw 2 ) and H(eiw 1, eiw 2 ), respectively. Deter- mine the transforms of the following signals in terms of X(eiw 1, eiw2 ) and H(eiw1, eiwz): (i) x[m, n]eiWJmeJWzn c·) [ ] { x[k, r], if m = 2k and n = 2r 11 Y m, n = 0, if m is not a multiple of 2 or n is not a multiple of 3 (iii) y[m, n] = x[m, n]h[m, n] 6 TIME AND FREQUENCY CHARACTERIZATION OFSIGNALSANDSYSTEMS 6.0 INTRODUCTION The frequency-domain characterization of an LTI system in terms of its frequency re- sponse represents an alternative to the time-domain characterization through convolution. In analyzing LTI systems, it is often particularly convenient to utilize the frequency do- main because differential and difference equations and convolution operations in the time domain become algebraic operations in the frequency domain. Moreover, concepts such as frequency-selective filtering are readily and simply visualized in the frequency domain. However, in system design, there are typically both time-domain and frequency-domain considerations. For example, as we briefly discussed in Examples 4.18 and 5 .12, and as we will illustrate in more detail in this chapter, significant oscillatory behavior in the impulse response of a frequency-selective filter may be undesirable, and consequently, we may wish to sacrifice the level of frequency selectivity in a filter in order to meet the required tolerances on impulse response behavior. Situations such as this are the rule rather than the exception in practice, as in most applications we would like to specify or constrain certain characteristics of a system in both the time domain and the frequency domain, frequently resulting in conflicting requirements. Hence, in system design and analysis, it is important to relate time-domain and frequency-domain characteristics and trade-offs. Introducing these issues and relationships is the primary focus of the chapter. 6. 1 THE MAGNITUDE-PHASE REPRESENTATION OF THE FOURIER TRANSFORM The Fourier transform is in general complex valued and, as we discussed, can be repre- sented in terms of its real and imaginary components or in terms of magnitude and phase. 423"
6 Time and Frequency Characterization of Signals and Systems,"6 TIME AND FREQUENCY CHARACTERIZATION OFSIGNALSANDSYSTEMS 6.0 INTRODUCTION The frequency-domain characterization of an LTI system in terms of its frequency re- sponse represents an alternative to the time-domain characterization through convolution. In analyzing LTI systems, it is often particularly convenient to utilize the frequency do- main because differential and difference equations and convolution operations in the time domain become algebraic operations in the frequency domain. Moreover, concepts such as frequency-selective filtering are readily and simply visualized in the frequency domain. However, in system design, there are typically both time-domain and frequency-domain considerations. For example, as we briefly discussed in Examples 4.18 and 5 .12, and as we will illustrate in more detail in this chapter, significant oscillatory behavior in the impulse response of a frequency-selective filter may be undesirable, and consequently, we may wish to sacrifice the level of frequency selectivity in a filter in order to meet the required tolerances on impulse response behavior. Situations such as this are the rule rather than the exception in practice, as in most applications we would like to specify or constrain certain characteristics of a system in both the time domain and the frequency domain, frequently resulting in conflicting requirements. Hence, in system design and analysis, it is important to relate time-domain and frequency-domain characteristics and trade-offs. Introducing these issues and relationships is the primary focus of the chapter. 6. 1 THE MAGNITUDE-PHASE REPRESENTATION OF THE FOURIER TRANSFORM The Fourier transform is in general complex valued and, as we discussed, can be repre- sented in terms of its real and imaginary components or in terms of magnitude and phase. 423"
6.0 Introduction,"6 TIME AND FREQUENCY CHARACTERIZATION OFSIGNALSANDSYSTEMS 6.0 INTRODUCTION The frequency-domain characterization of an LTI system in terms of its frequency re- sponse represents an alternative to the time-domain characterization through convolution. In analyzing LTI systems, it is often particularly convenient to utilize the frequency do- main because differential and difference equations and convolution operations in the time domain become algebraic operations in the frequency domain. Moreover, concepts such as frequency-selective filtering are readily and simply visualized in the frequency domain. However, in system design, there are typically both time-domain and frequency-domain considerations. For example, as we briefly discussed in Examples 4.18 and 5 .12, and as we will illustrate in more detail in this chapter, significant oscillatory behavior in the impulse response of a frequency-selective filter may be undesirable, and consequently, we may wish to sacrifice the level of frequency selectivity in a filter in order to meet the required tolerances on impulse response behavior. Situations such as this are the rule rather than the exception in practice, as in most applications we would like to specify or constrain certain characteristics of a system in both the time domain and the frequency domain, frequently resulting in conflicting requirements. Hence, in system design and analysis, it is important to relate time-domain and frequency-domain characteristics and trade-offs. Introducing these issues and relationships is the primary focus of the chapter. 6. 1 THE MAGNITUDE-PHASE REPRESENTATION OF THE FOURIER TRANSFORM The Fourier transform is in general complex valued and, as we discussed, can be repre- sented in terms of its real and imaginary components or in terms of magnitude and phase. 423"
6.1 The Magnitude-Phase Representation of the Fourier Transform,"6 TIME AND FREQUENCY CHARACTERIZATION OFSIGNALSANDSYSTEMS 6.0 INTRODUCTION The frequency-domain characterization of an LTI system in terms of its frequency re- sponse represents an alternative to the time-domain characterization through convolution. In analyzing LTI systems, it is often particularly convenient to utilize the frequency do- main because differential and difference equations and convolution operations in the time domain become algebraic operations in the frequency domain. Moreover, concepts such as frequency-selective filtering are readily and simply visualized in the frequency domain. However, in system design, there are typically both time-domain and frequency-domain considerations. For example, as we briefly discussed in Examples 4.18 and 5 .12, and as we will illustrate in more detail in this chapter, significant oscillatory behavior in the impulse response of a frequency-selective filter may be undesirable, and consequently, we may wish to sacrifice the level of frequency selectivity in a filter in order to meet the required tolerances on impulse response behavior. Situations such as this are the rule rather than the exception in practice, as in most applications we would like to specify or constrain certain characteristics of a system in both the time domain and the frequency domain, frequently resulting in conflicting requirements. Hence, in system design and analysis, it is important to relate time-domain and frequency-domain characteristics and trade-offs. Introducing these issues and relationships is the primary focus of the chapter. 6. 1 THE MAGNITUDE-PHASE REPRESENTATION OF THE FOURIER TRANSFORM The Fourier transform is in general complex valued and, as we discussed, can be repre- sented in terms of its real and imaginary components or in terms of magnitude and phase. 423 424 Time and Frequency Characterization of Signals and Systems Chap.6 The magnitude-phase representation of the continuous-time Fourier transform X(jw) is X(jw) = IX(jw )lef1:X(Jw). (6.1) Similarly the magnitude-phase representation of the discrete-time Fourier transform X(efw) is (6.2) In the following discussion, we concentrate for the most part on the continuous-time case in describing and illustrating several points related to magnitude-phase representations. The essential points apply equally to the discrete-time case. From the Fourier transform synthesis equation (4.8), we can think of X(jw) as pro- viding us with a decomposition of the signal x(t) into a ""sum"" of complex exponentials at different frequencies. In fact, as discussed in Section 4.3.7, IX(jw)l2 may be interpreted as the energy-density spectrum of x(t). That is, IX(jw )l2dw/27T can be thought of as the amount of energy in the signal x(t) that lies in the infinitesimal frequency band between w and w + dw. Thus, the magnitude IX(jw )I describes the basic frequency content of a signal-i.e., IX(jw )I provides us with the information about the relative magnitudes of the complex exponentials that make up x(t). For example, if IX(jw )I = 0 outside of a small band of frequencies centered at zero, then x(t) will display only relatively low-frequency oscillations. The phase angle <r.X(jw ), on the other hand, does not affect the amplitudes of the individual frequency components, but instead provides us with information concerning the relative phases of these exponentials. The phase relationships captured by <r.X(jw) have a significant effect on the nature of the signal x(t) and thus typically contain a subs tan- tial amount of information about the signal. In particular, depending upon what this phase function is, we can obtain very different-looking signals, even if the magnitude function remains unchanged. For example, consider again the example illustrated in Figure 3.3. In this case, a ship encounters the superposition of three wave trains, each of which can be modeled as a sinusoidal signal. With fixed magnitudes for these sinusoids, the amplitude of their sum may be quite small or very large, depending on the relative phases. The im- plications of phase for the ship, therefore, are quite significant. As another illustration of the effect of phase, consider the signal + 1 2 x(t) = 1 2 cos(27Tt + </>1) + cos(47Tt + </>2) + 3 cos(67Tt + 4>3). (6.3) In Figure 3.4, we depicted x(t) for the case when </> 1 = </>2 = 4>3 = 0. In Figure 6.1, we illustrate x(t) for this case also and for several other choices for the phase of the individual components. As this figure demonstrates, the resulting signals can differ significantly for different relative phases. In general, changes in the phase function of X(jw) lead to changes in the time- domain characteristics of the signal x(t). In some instances phase distortion may be important, whereas in others it is not. For example, a well-known property of the auditory system is a relative insensitivity to phase. Specifically, if the Fourier transform of a spoken sound (e.g., a vowel) is subjected to a distortion such that the phase is changed but the magnitude is unchanged, the effect can be perceptually negligible, although the waveform in the time domain may look considerably different. While mild phase distortions such as those affecting individual sounds do not lead to a loss of intelligibility, more severe phase Sec. 6.1 The Magnitude-Phase Representation of The Fourier Transform 425 (a) (b) At\Af'J~~~ (c) Figure 6. 1 The signal x(t) given in eq. (6.3) for several different choices of the phase angles </>1, <1>2, and 4>3: (a) </>1 = <1>2 = 4>3 = 0; (b) </>1 = 4 rad, <1>2 = 8 rad, 4>3 = 12 rad; (c) </>1 = 6 rad, <1>2 = -2.7 rad, 4>3 = 0.93 rad; (d) 4>1 = 1.2 rad, <1>2 = 4.1 (d) rad, 4>3 = -7.02 rad. distortions of speech certainly do. As an extreme illustration, if x(t) is a tape recording of a sentence, then the signal x( -t) represents the sentence played backward. From Table 4.1, assuming x(t) is real valued, the corresponding effect in the frequency domain is to replace the Fourier transform phase by its negative: ~{x(-t)} =X(- jw) = IX(jw)le-J4:X(jw). That is, the spectrum of a sentence played in reverse has the same magnitude function as the spectrum of the original sentence and differs only in phase. Clearly, this phase change has a significant impact on the intelligibility of the recording. A second example illustrating the effect and importance of phase is found in examin- ing images. As we briefly discussed in Chapter 3, a black-and-white picture can be thought of as a signal x(t1, t2), with t 1 denoting the horizontal coordinate of a point on the picture, t2 the vertical coordinate, and x(t1, t2) the brightness of the image at the point (t1, t2). The Fourier transform X(jw 1, jw 2) of the image represents a decomposition of the image into complex exponential components of the form eJw,t, eiw 2t2 that capture the spatial varia- tions of x(t1, t2) at different frequencies in each of the two coordinate directions. Several elementary aspects of two-dimensional Fourier analysis are addressed in Problems 4.53 and 5.56. In viewing a picture, some of the most important visual information is contained in the edges and regions of high contrast. Intuitively, regions of maximum and minimum 426 Time and Frequency Characterization of Signals and Systems Chap.6 (a) (b) (c) (d) intensity in a picture are places at which complex exponentials at different frequencies are in phase. Therefore, it seems plausible to expect the phase of the Fourier transform of a picture to contain much of the information in the picture, and in pat1icular, the phase should capture the information about the edges. To substantiate this expectation, in Figure 6.2(a) we have repeated the picture shown in Figure 1.4. In Figure 6.2(b) we have depicted the magnitude of the two-dimensional Fourier transform of the image in Figure 6.2(a), where in this image the horizontal axis is w 1, the vertical is w 2, and the brightness of the image at the point (w 1, w 2 ) is proportional to the magnitude of the transform X(jw 1, jw 2 ) of the image in Figure 6.2(a). Similarly, the phase of this transform is depicted in Figure 6.2(c). Figure 6.2(d) is the result of setting the phase [Figure 6.2(c)] of X(jw 1, jw 2 ) to zero (with- out changing its magnitude) and inverse transforming. In Figure 6.2(e) the magnitude of X(jw 1, jw2) was set equal to 1, but the phase was kept unchanged from what it was in Figure 6.2(c). Finally, in Figure 6.2(f) we have depicted the image obtained by inverse transforming the function obtained by using the phase in Figure 6.2( c) and the magnitude of the transform of a completely different image-the picture shown in Figure 6.2(g)! These figures clearly illustrate the importance of phase in representing images. Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 427 (e) (f) Figure 6.2 (a) The image shown in Figure 1.4; (b) magnitude of the two-dimensional Fourier transform of (a); (c) phase of the Fourier trans- form of (a); (d) picture whose Fourier transform has magnitude as in (b) and phase equal to zero; (e) picture whose Fourier transform has magnitude equal to 1 and phase as in (c); (f) picture whose Fourier transform has phase as in (c) and magni- tude equal to that of the transform of the picture (g) shown in (g). 6.2 THE MAGNITUDE-PHASE REPRESENTATION OF THE FREQUENCY RESPONSE OF LTI SYSTEMS From the convolution property for continuous-time Fourier transforms, the transform Y(jw) of the output of an LTI system is related to the transform X(jw) of the input to the system by the equation Y(jw) = H(jw)X(jw), where H(jw) is the frequency response of the system-i.e., the Fourier transform of the system's impulse response. Similarly, in discrete time, the Fourier transforms of the input X(ei(<)) and ouput Y(ei(<)) of an LTI system with frequency response H(eiw) are related by (6.4) Thus, the effect that an LTI system has on the input is to change the complex ampli- tude of each of the frequency components of the signal. By looking at this effect in terms of the magnitude-phase representation, we can understand the nature of the effect in more"
6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems,"Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 427 (e) (f) Figure 6.2 (a) The image shown in Figure 1.4; (b) magnitude of the two-dimensional Fourier transform of (a); (c) phase of the Fourier trans- form of (a); (d) picture whose Fourier transform has magnitude as in (b) and phase equal to zero; (e) picture whose Fourier transform has magnitude equal to 1 and phase as in (c); (f) picture whose Fourier transform has phase as in (c) and magni- tude equal to that of the transform of the picture (g) shown in (g). 6.2 THE MAGNITUDE-PHASE REPRESENTATION OF THE FREQUENCY RESPONSE OF LTI SYSTEMS From the convolution property for continuous-time Fourier transforms, the transform Y(jw) of the output of an LTI system is related to the transform X(jw) of the input to the system by the equation Y(jw) = H(jw)X(jw), where H(jw) is the frequency response of the system-i.e., the Fourier transform of the system's impulse response. Similarly, in discrete time, the Fourier transforms of the input X(ei(<)) and ouput Y(ei(<)) of an LTI system with frequency response H(eiw) are related by (6.4) Thus, the effect that an LTI system has on the input is to change the complex ampli- tude of each of the frequency components of the signal. By looking at this effect in terms of the magnitude-phase representation, we can understand the nature of the effect in more 428 Time and Frequency Characterization of Signals and Systems Chap.6 detail. Specifically, in continuous time, \Y(jw )\ = \H(jw )\\X(jw )\ (6.5) and <J:Y(jw) = <J:H(jw) + <J:X(jw ), (6.6) and exactly analogous relationships hold in the discrete-time case. From eq. (6.5), we see that the effect an LTI system has on the magnitude of the Fourier transform of the sig- nal is to scale it by the magnitude of the frequency response. For this reason, \H(jw)i (or \H(ejw)\) is commonly referred to as the gain of the system. Also, from eq. (6.6), we see that the phase of the input <J:X(jw) is modified by the LTI system by adding the phase <J:H(jw) to it, and <J:H(jw) is typically referred to as the phase shift of the system. The phase shift of the system can change the relative phase relationships among the compo- nents of the input, possibly resulting in significant modifications to the time domain char- acteristics of the input even when the gain of the system is constant for all frequencies. The changes in the magnitude and phase that result from the application of an input to an LTI system may be either desirable, if the input signal is modified in a useful way, or undesirable, if the input is changed in an unwanted manner. In the latter case, the effects in eqs. (6.5) and (6.6) are commonly referred to as magnitude and phase distortions. In the following sections, we describe several concepts and tools that allow us to understand these effects a bit more thoroughly. 6.2.1 Linear and Nonlinear Phase When the phase shift at the frequency w is a linear function of w, there is a particularly straightforward interpretation of the effect in the time domain. Consider the continuous- time LTI system with frequency response H(jw) = e-jwto, (6.7) so that the system has unit gain and linear phase-i.e., \H(jw)\ = 1, <J:H(jw) = -wto. (6.8) As shown in Example 4.15, the system with this frequency response characteristic pro- duces an output that is simply a time shift of the input-i.e., y(t) = x(t - to). (6.9) In the discrete-time case, the effect oflinear phase is similar to that in the continuous- time case when the slope of the linear phase is an integer. Specifically, from Example 5.11, we know that the LTI system with frequency response e- jwno with linear phase function -wn0 produces an ouput that is a simple shift of the input-i.e., y[n] = x[n- n0 ]. Thus, a linear phase shift with an integer slope corresponds to a shift of x[n] by an integer number of samples. When the phase slope is not an integer, the effect in the time domain is some- what more complex and is discussed in Chapter 7, Section 7.5. Informally, the effect is a time shift of the envelope of the sequence values, but the values themselves may change. While linear phase shifts lead to very simple and easily understood and visualized changes in a signal, if an input signal is subjected to a phase shift that is a nonlin- ear function of w, then the complex exponential components of the input at different frequencies will be shifted in a manner that results in a change in their relative phases. When these exponentials are superimposed, we obtain a signal that may look considerably different from the input signal. This is illustrated in Figure 6.3 in the continuous-time case. 0 0 10 (a) (b) 0 0 (c) (d) Figure 6.3 (a) Continuous-time signal that is applied as the input to several systems for which the frequency response has unity magnitude; (b) response for a system with linear phase; (c) response for a system with nonlinear phase; and (d) response for a system with phase equal to the nonlinear phase of the system in part (c) plus a linear phase term. 430 Time and Frequency Characterization of Signals and Systems Chap.6 In Figure 6.3(a), we depict a signal that is applied as the input to three different systems. Figure 6.3(b) shows the output when the signal is applied as input to a system with fre- quency response H 1( jw) = e- jwto, resulting in an output that equals the input delayed by to seconds. In Figure 6.3( c), we display the output when the signal is applied to a system with unity gain and nonlinear phase function-i.e., (6.10) where 4-H2(jw) is a nonlinear function of w. Figure 6.3(d) shows the output from another system with nonlinear phase. In this case, the corresponding frequency response has a phase shift that is obtained by adding a linear phase term to 4-H2(jw )-i.e., (6.11) Thus, the output in Figure 6.3( d) can be thought of as the response to a cascade of the system H 2(jw) followed by a time shift, so that the waveforms in Figures 6.3(c) and (d) are related through a simple time shift. In Figure 6.4, we illustrate the effect of both linear and nonlinear phase in the discrete-time case. Once again, the signal in Figure 6.4(a) is applied as the input to three different LTI systems, all with unity gain (i.e., IH (e jw )j = 1) . The signals in the subse- quent parts of Figure 6.4 depict the corresponding outputs. In the case of Figure 6.4(b ), the system has linear phase characteristics with integer slope of -5, so that the output equals the input delayed by 5. The phase shifts for the systems associated with Figures 6.4( c) and (d) are nonlinear, but the difference between these two phase functions is linear with integer slope so that the signals in Figures 6.4(c) and (d) are related by a time shift. Note that all the systems considered in the examples illustrated in Figures 6.3 and 6.4 have unity gain, so that the magnitude of the Fourier transform of the input to any of these systems is passed through unchanged by the system. For this reason, such systems are commonly referred to as all-pass systems. The characteristics of an all-pass system are completely determined by its phase-shift characteristics. A more general LTI system H(jw) or H(ejw), of course, imparts both magnitude shaping through the gain jH(jw )j or jH(ejw)j and phase shift that may or may not be linear. 6.2.2 Group Delay As discussed in Section 6.2.1, systems with linear phase characteristics have the particu- larly simple interpretation as time shifts. In fact, from eqs. (6.8) and (6.9), the phase slope tells us the size of the time shift. That is, in continuous time, if 4-H(jw) = -wt0 , then the system imparts a time shift of -to or, equivalently, a delay of t0 . Similarly, in discrete time, 4-H(ejw) = -wn0 corresponds to a delay of n0 . The concept of delay can be very naturally and simply extended to include nonlin- ear phase characteristics. Suppose that we wish to examine the effects of the phase of a continuous-time LTI system on a narrowband input-i.e., an input x(t) whose Fourier transform is zero or negligibly small outside a small band of frequencies centered at w = w0 . By taking the band to be very small, we can accurately approximate the phase of this system in the band with the linear approximation 4-H(jw) = -4> - wa, (6.12) Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 431 0 n (a) 0 5 n (b) n (c) Figure 6.4 (a) Discrete-time signal that is applied as input to several sys- tems for which the frequency response has unity magnitude; (b) response for a system with linear phase with slope of -5; (c) response for a system with nonlinear phase; and (d) response for a system whose phase characteristic is that of part (c) plus a linear phase (d) term with integer slope. 432 Time and Frequency Characterization of Signals and Systems Chap.6 so that (6.13) Thus, the approximate effect of the system on the Fourier transform of this narrowband input consists of the magnitude shaping corresponding to IH(jw )I, multiplication by an overall constant complex factor e-N> and multiplication by a linear phase term e- jwa corresponding to a time delay of a seconds. This time delay is referred to as the group delay at w = w 0 , as it is the effective common delay experienced by the small band or group of frequencies centered at w = w 0 . The group delay at each frequency equals the negative of the slope of the phase at that frequency; i.e., the group delay is defined as r(w) = - d~ {1:-H(jw)}. (6.14) The concept of group delay applies directly to discrete-time systems as well. In the next example we illustrate the effect of nonconstant group delay on a signal. Example 6.1 Consider the impulse response of an all-pass system with a group delay that varies with frequency. The frequency response H (jw) for our example is the product of three factors; i.e., n3 H(jw) = H i(jw ), i=l where . 1 + (jwtwJ - 2j~i (wlwi) Hi(Jw) = (6.15) + 2 , 1 {jw!wi) + 2j~i (w!wi) w 1 = 315 rad/sec and ~1 = 0.066, w2 = 943 rad/sec and ~2 = 0.033, { w3 = 1888 radlsec and ~3 = 0.058. It is often useful to express the frequencies wi measured in radians per second in terms of frequencies fi measured in Hertz, where Wi = 27T fi. In this case, /1 =50Hz h =150Hz h =300Hz. Since the numerator of each of the factors Hi(jw) is the complex conjugate of the corresponding denominator, it follows that IHi(jw )I = 1. Consequently, we may also Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 433 conclude that IH(}w)l = 1. The phase for each H;(jw) can be determined from eq. (6.15): 1 1 <r.H;(jw) = -2 arctan 2{ (wlw·) l , [ 2 1 - (wlw;) and 3 <r.H(jw) = L <r.H;(jw). i= I If the values of <r.H (jw) are restricted to lie between -7r and 7T, we obtain the principal- phase function (i.e., the phase modulo 27T), as shown in Figure 6.5(a) where we have plotted the phase versus frequency measured in Hertz. Note that this function con- tains discontinuities of size 27T at various frequencies, making the phase function non- differentiable at those points. However, the addition or subtraction of any integer multiple of 27T to the value of the phase at any frequency leaves the original frequency response unchanged. Thus, by appropriately adding or subtracting such integer multiples of 27T from various portions of the principal phase, we obtain the unwrapped phase in Fig- ure 6.5(b ). The group delay as a function of frequency may now be computed as where <r.[H(jw )] represents the unwrapped-phase function corresponding to H(jw ). A plot of T(w) is shown in Figure 6.5(c). Observe that frequencies in the close vicinity of 50 Hz experience greater delay than frequencies in the vicinity of 150Hz or 300Hz. The effect of such nonconstant group delay can also be qualitatively observed in the impulse response (see Figure 6.5(d)) of the LTI system. Recall that ~{8(t)} = 1. The frequency components of the impulse are all aligned in time in such a way that they combine to form the impulse, which is, of course, highly localized in time. Since the all-pass system has nonconstant group delay, different frequencies in the input are delayed by different amounts. This phenomenon is referred to as dispersion. In the current example, the group delay is highest at 50 Hz. Consequently, we would expect the latter parts of the impulse response to oscillate at lower frequencies near 50 Hz. This clearly evident in Figure 6.5(d). Example 6.2 Nonconstant group delay is among the factors considered important for assessing the transmission performance of switched telecommunications networks. In a survey1 in- volving locations all across the continental United States, AT &T/Bell System reported group delay characteristics for various categories of toll calls. Figure 6.6 displays some of the results of this study for two such classes. In particular, what is plotted in each curve in Figure 6.6(a) is the nonconstant portion of the group delay for a specific cate- gory of toll calls. That is, for each category, a common constant delay corresponding to 1 ""Analog Transmission Performance on the Switched Telecommunications Network,"" by F. P. Duffy and T. W. Thatcher, Jr., in the Bell System Technical Journal, vol. 50, no. 4, April, 1971. 434 Time and Frequency Characterization of Signals and Systems Chap. 6 l Q) 0 gj .s::. 0.. -2 50 100 150 200 250 300 350 400 Frequency (Hz) (a) -5 ~ i--10 ra .s::. 0.. -15 -20L-----~-------L------~------L-----~-------L------~----~ 0 50 100 150 200 250 300 350 400 Frequency (Hz) (b) 0.08 ¥ ;:: 0.06 ra a; 0 g- 0.04 e :.!) 0.02 50 100 150 200 250 300 350 400 Frequency (Hz) (c) 400 200 0 -200 -400 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 Time (sec) (d) Figure 6.5 Phase, group delay, and impulse response for the all-pass sys- tem of Example 6.1: (a) principal phase; (b) unwrapped phase; (c) group delay; (d) impulse response. Each of these quantities is plotted versus frequency measured in Hertz. Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 435 7,000 0 Short Sho~ 'r ~ 6,000 -5 ~ I '0 c Medium \~ 8 5,000 Q) en 3 Medium~ -10 0 0 I Medium I4,ooo 1 0 dl ~ _Q a; 0 0 C\J -15 0.. Medium I e3 ,000 C) c ~ \\ I -20 en § 2,000 (.) \ c 0 z \\~ 1,000 I ~ -25 \ Short\ 0 ~ ~ Short ~ ~ -30 0 600 1 ,200 1 ,800 2,400 3,000 3,600 0 600 1,200 1,800 2,400 3,000 3,600 Frequency in (Hz) Frequency in (Hz) (a) (b) Figure 6.6 (a) Non-constant portion of the group delay; and (b) frequency re- sponse magnitude as functions of frequency for short- and medium-distance toll calls in switched telecommunications networks [after Duffy and Thatcher]. Each of these quantities is plotted versus frequency measured in Hertz. Also, as is commonly done in practice, the magnitudes of the frequency responses are plotted using a logarithmic scale in units of decibels. That is, what is plotted in (b) is 20 log10 IH(jw )I for the fre- quency responses corresponding to short- and medium-distance toll calls. The use of this logarithmic scale for the frequency-response magnitudes is discussed in detail in Section 6.2.3. the minimum of the group delay over all frequencies has been subtracted from the group delay, and the resulting difference is plotted in Figure 6.6(a). Consequently, each curve in Figure 6.6(a) represents the additional delay (beyond this common constant delay) experienced by the different frequency components of toll calls within each category. The curves labeled SHORT and MEDIUM respectively represent the results for short- distance (0-180 airline miles) and medium-distance (180-725 airline miles) toll calls. The group delay as a function of frequency is seen to be lowest at 1, 700Hz and increases monotonically as we move away from that figure in either direction. When the group delay characteristics illustrated in Figure 6.6(a) are combined with the characteristics of the magnitude of the frequency response reported in the same AT&T /Bell System survey and shown in Figure 6. 6(b ), we obtain impulse reponses of the type shown in Figure 6.7. The impulse response in Figure 6.7(a) corresponds to the short- distance category. The very low- and very high-frequency components of the response occur later than the components in the mid-frequency range. This is compatible with 436 Time and Frequency Characterization of Signals and Systems Chap.6 0.6 0.4 0.2 0 -0.2 -0.4 0 2 3 4 5 6 7 8 9 10 Time (msec) (a) 0.6 0.4 0.2 0 -0.2 -0.4 0 2 3 4 5 6 7 8 9 10 Time (msec) (b) Figure 6.7 Impulse responses associated with the group delay and magnitude char- acteristics in Figure 6.6: (a) impulse response corresponding to the short-distance cate- gory of toll calls; (b) impulse response for the medium-distance category. the corresponding group delay characteristics in Figure 6.6(a). Similarly, Figure 6.7(b) illustrates the same phenomenon for the impulse response corresponding to medium- distance toll calls. 6.2.3 Log-Magnitude and Bode Plots In graphically displaying continuous-time or discrete-time Fourier transforms and system frequency responses in polar form, it is often convenient to use a logarithmic scale for the magnitude of the Fourier transform. One of the principal reasons for doing this can be seen Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 437 from eqs. (6.5) and (6.6), which relate the magnitude and phase of the output of an LTI system to those of the input and frequency response. Note that the phase relationship is additive, while the magnitude relationship involves the product of IH(Jw )I and IX(Jw )I. Thus, if the magnitudes of the Fourier transform are displayed on a logarithmic amplitude scale, eq. (6.5) takes the form of an additive relationship, namely, log IY(jw )I = log IH(Jw )I +log IX(Jw )I, (6.16) with an exactly analogous expression in discrete time. Consequently, if we have a graph of the log magnitude and phase of the Fourier transform of the input and the frequency response of an LTI system, the Fourier transform of the output is obtained by adding the log-magnitude plots and by adding the phase plots. In a similar fashion, since the frequency response of the cascade of LTI systems is the product ofthe individual frequency responses, we can obtain plots of the log magnitude and phase of the overall frequency response of cascaded systems by adding the corresponding plots for each of the component systems. In addition, plotting the magnitude of the Fourier transform on a logarithmic scale allows detail to be displayed over a wider dynamic range. For example, on a linear scale, the detailed magnitude characteristics in the stopband of a frequency-selective filter with high attenuation are typically not evident, whereas they are on a logarithmic scale. Typically, the specific logarithmic amplitude scale used is in units of 20 log 10 , re- ferred to as decibels2 (abbreviated dB). Thus, 0 dB corresponds to a frequency response with magnitude equal to 1, 20 dB is equivalent to a gain of 10, -20 dB corresponds to an attenuation of 0.1, and so on. Also, it is useful to note that 6 dB approximately corresponds to a gain of 2. For continuous-time systems, it is also common and useful to use a logarithmic frequency scale. Plots of 20 log 10 IH(Jw )I and 4-H(jw) versus log 10(w) are referred to as Bode plots. A typical Bode plot is illustrated in Figure 6.8. Note that, as discussed in Section 4.3.3, if h(t) is real, then IH(Jw )I is an even function of w and 4-H(jw) is an odd function of w. Because of this, the plots for negative w are superfluous and can be obtained immediately from the plots for positive w. This, of course, makes it pos- sible to plot frequency response characteristics versus log 10(w) for w > 0, as in the figure. The use of a logarithmic frequency scale offers a number of advantages in continu- ous time. For example, it often allows a much wider range of frequencies to be displayed than does a linear frequency scale. In addition, on a logarithmic frequency scale, the shape 2The origin of this particular choice of units and the term decibels can be traced to the definition of power ratios in systems. Specifically, since the square of the magnitude of the Fourier transform of a signal can be interpreted as the energy per unit frequency, or power, in a signal, the square of the magnitude, IH(}w )1 2 or IH(eiw)i 2 , of the frequency response of a system can be thought of as the power ratio between the input and the output of an LTI system. In honor of Alexander Graham Bell, the inventor of the telephone, the term bel was introduced to indicate a factor of 10 in a power ratio, and decibel was used to denote one-tenth of this factor on a logarithmic scale (so that the cascade of 10 systems with 1-dB power ratios each would result in 1 bel of power amplification). Thus, 10 log 10 iH(jw )1 2 is the number of decibels of power amplification for the frequency response H(jw ), and this in tum equals 20 log 10 iH(jw )I in magnitude amplification. 438 Time and Frequency Characterization of Signals and Systems Chap.6 20 - 10 S OdB r------- I ~ -10 i -20 ~ -30 -40 -50 0 r------ 3 7T I 2 v- -1T 10 100 1.000 Figure 6.8 A typical Bode plot. (Note that w is plotted using a logarithmic scale.) of a particular response curve doesn't change if the frequency is scaled. (See Problem 6.30.) Furthermore for continuous-time LTI systems described by differential equations, an approximate sketch of the log magnitude vs. log frequency can often be easily obtained through the use of asymptotes. In Section 6.5, we will illustrate this by developing sim- ple piecewise-linear approximate Bode plots for first- and second-order continuous-time systems. In discrete time, the magnitudes of Fourier transforms and frequency responses are often displayed in dB for the same reasons that they are in continuous time. However, in discrete time a logarithmic frequency scale is not typically used, since the range of frequencies to be considered is always limited and the advantage found for differential equations (i.e., linear asymptotes) does not apply to difference equations. Typical graphi- cal representations of the magnitude and phase of a discrete-time frequency response are shown in Figure 6.9. Here, we have plotted <r:.H(ejw) in radians and iH(ejw)i in decibels [i.e., 20 log10 IH(ejw)IJ as functions of w. Note that for h[n] real, we actually need plot H ( ejw) only for 0 :=::;; w :=::;; 7T, because in this case the symmetry property of the Fourier transform implies that we can then calculate H(ejw) for -7r :=::;; w :=::;; 0 using the relations iH(ejw)i = iH(e- jw)i and <r:.H(e- jw) = - <r:.H(ejw). Furthermore, we need not consider values of lwl greater than 7T, because of the periodicity of H(ejw). Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 439 Figure 6. 9 Typical graphical representations of the magnitude and phase of a discrete-time frequency response H(ei«>). As emphasized in this section, a logarithmic amplitude scale is often useful and important. However, there are many situations in which it is convenient to use a linear amplitude scale. For example, in discussing ideal filters for which the magnitude of the frequency response is a nonzero constant over some frequency bands and zero over others, a linear amplitude scale is more appropriate. Thus, we have introduced both linear and logarithmic graphical representations for the magnitude of the Fourier transform and will use each as appropriate. 6.3 TIME-DOMAIN PROPERTIES OF IDEAL FREQUENCY-SELECTIVE FILTERS In Chapter 3, we introduced the class of frequency-selective filters, i.e., LTI systems with frequency responses chosen so as to pass one or several bands of frequencies with little or no attenuation and to stop or significantly attenuate frequencies outside those bands. As we discussed in Chapters 3, 4, and 5, there are a number of issues of importance that arise"
6.3 Time-Domain Properties of Ideal Frequency-Selective Filters,"Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 439 Figure 6. 9 Typical graphical representations of the magnitude and phase of a discrete-time frequency response H(ei«>). As emphasized in this section, a logarithmic amplitude scale is often useful and important. However, there are many situations in which it is convenient to use a linear amplitude scale. For example, in discussing ideal filters for which the magnitude of the frequency response is a nonzero constant over some frequency bands and zero over others, a linear amplitude scale is more appropriate. Thus, we have introduced both linear and logarithmic graphical representations for the magnitude of the Fourier transform and will use each as appropriate. 6.3 TIME-DOMAIN PROPERTIES OF IDEAL FREQUENCY-SELECTIVE FILTERS In Chapter 3, we introduced the class of frequency-selective filters, i.e., LTI systems with frequency responses chosen so as to pass one or several bands of frequencies with little or no attenuation and to stop or significantly attenuate frequencies outside those bands. As we discussed in Chapters 3, 4, and 5, there are a number of issues of importance that arise 440 Time and Frequency Characterization of Signals and Systems Chap.6 in frequency-selective filtering applications and that relate directly to the characteristics of frequency-selective filters. In this section, we take another look at such filters and their properties. We focus our attention here on lowpass filters, although very similar concepts and results hold for other types of frequency-selective filters such as highpass or bandpass filters. (See Problems 6.5, 6.6, 6.26, and 6.38.) As introduced in Chapter 3, a continuous-time ideallowpass filter has a frequency response of the form H( ·w) = { 1 lwl ::; We . lwl (6.17) } 0 >We This is illustrated in Figure 6.1 O(a). Similarly, a discrete-time ideallowpass filter has a frequency response (6.18) H(jw) w (a) w (b) Figure 6. 1 o (a) The frequency response of a continuous-time ideal low- pass filter; (b) the frequency response of a discrete-time ideal lowpass filter. and is periodic in w, as depicted in Figure 6.10(b). As can be seen from eqs. (6.17) and (6.18) or from Figure 6.10, ideallowpass filters have perfect frequency selectivity. That is, they pass without attenuation all frequencies at or lower than the cutoff frequency We and completely stop all frequencies in the stopband (i.e., higher than we). Moreover, these filters have zero phase characteristics, so they introduce no phase distortion. As we have seen in Section 6.2, nonlinear phase characteristics can lead to signifi- cant changes in the time-domain characteristics of a signal even when the magnitude of its Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 441 spectrum is not changed by the system, and thus, a filter with a magnitude characteristic as in eq. (6.17) or eq. (6.18), but with nonlinear phase, might produce undesirable effects in some applications. On the other hand, an ideal filter with linear phase over the passband, as illustrated in Figure 6.11, introduces only a simple time shift relative to the response of the ideallowpass filter with zero phase characteristic. IH(jw)l 11 -we 0 we w <):: H(jw)= -aw w Figure 6. 11 Continuous-time ideal lowpass filter with linear phase charac- teristic. In Examples 4.18 and 5 .12, we computed the impulse responses of ideal lowpass filters. In particular, the impulse response corresponding to the filter in eq. (6.17) is h(t) = si:~et, (6.19) which is shown in Figure 6.12(a). Similarly, the impulse response of the discrete-time ideal filter in eq. ( 6.18) is h[n] = sin Wen, (6.20) 7Tn which is depicted in Figure 6.12(b) for w c = 7T/4. If either of the ideal frequency responses of eqs. (6.17) and (6.18) is augmented with a linear phase characteristic, the impulse response is simply delayed by an amount equal to the negative of the slope of this phase function, as is illustrated in Figure 6.13 for the continuous-time impulse response. Note that in both continuous and discrete time, the width of the filter passband is proportional to we, while the width of the main lobe of the impulse is proportional to 1/we. As the bandwidth of the filter increases, the impulse response becomes narrower, and vice versa, consistent with the inverse relationship between time and frequency discussed in Chapters 4 and 5. h(t) (a) h[n] n (b) Figure 6.12 (a) The impulse response of the continuous-time ideal lowpass filter of Figure 6.1 O(a); (b) the impulse response of the discrete-time ideal lowpass filter of Figure 6.10(b) with we = 7r/4. h(t-rr) Figure 6. 13 Impulse response of an ideal lowpass filter with magnitude and phase shown in Figure 6.11. 442 Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 443 The step responses s(t) and s[n] of the ideallowpass filters in continuous time and discrete time are displayed in Figure 6.14. In both cases, we note that the step responses exhibit several characteristics that may not be desirable. In particular, for these filters, the step responses overshoot their long-term final values and exhibit oscillatory behavior, frequently referred to as ringing. Also, recall that the step response is the running integral or sum of the impulse response-i.e., n s[n] = L h[m]. m= -oo s(t) (a) s[n] • 1 2 n (b) Figure 6. 14 (a) Step response of a continuous-time ideal lowpass filter; (b) step response of a discrete-time ideal lowpass filter. 444 Time and Frequency Characterization of Signals and Systems Chap.6 Since the impulse responses for the ideal filters have main lobes extending from -7Tlwc to +'TT!wc, the step responses undergo their most significant change in value over this time interval. That is, the so-called rise time of the step response, a rough measure of the response time of the filter, is also inversely related to the bandwidth of the filter. 6.4 TIME-DOMAIN AND FREQUENCY-DOMAIN ASPECTS OF NONIDEAL FILTERS The characteristics of ideal filters are not always desirable in practice. For example, in many filtering contexts, the signals to be separated do not always lie in totally disjoint frequency bands. A typical situation might be that depicted in Figure 6.15, where the spectra of two signals overlap slightly. In such a case, we may wish to trade off the fi- delity with which the filter preserves one of these signals-say, x 1( f)-against the level to which frequency components of the second signal x2(t) are attenuated. A filter with a gradual transition from passband to stopband is generally preferable when filtering the superposition of signals with overlapping spectra. X(jw) Figure 6. 1 5 Two spectra that are w slightly overlapping. Another consideration is suggested by examining the step responses of ideallowpass filters, shown in Figure 6.14. For both continuous time and discrete time, the step response asymptotically approaches a constant equal to the value of the step. In the vicinity of the discontinuity, however, it overshoots this value and exhibits ringing. In some situations, this time-domain behavior may be undesirable. Moreover, even in cases where the ideal frequency-selective characteristics are de- sirable, they may not be attainable. For example, from eqs. (6.18) and (6.19) and Fig- ure 6.12, it is evident that the ideal lowpass filter is noncausal. When filtering is to be carried out in real time, however, causality is a necessary constraint, and thus, a causal approximation to the ideal characteristics would be required. A further consideration that motivates providing some flexibility in the filter characteristics is ease of implementation. In general, the more precisely we try to approximate or implement an ideal frequency- selective filter, the more complicated or costly the implementation becomes, whether in terms of components such as resistors, capacitors, and operational amplifiers in continu- ous time or in terms of memory registers, multipliers, and aqders in discrete time. In many contexts, a precise filter characteristic may not be essential and a simple filter will suffice. For all of these reasons, nonideal filters are of of considerable practical importance, and the characteristics of such filters are frequently specified or quantified in terms of several parameters in both the frequency and time domain. First, because the magnitude characteristics of the ideal frequency-selective filter may be unachievable or undesirable,"
6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters,"444 Time and Frequency Characterization of Signals and Systems Chap.6 Since the impulse responses for the ideal filters have main lobes extending from -7Tlwc to +'TT!wc, the step responses undergo their most significant change in value over this time interval. That is, the so-called rise time of the step response, a rough measure of the response time of the filter, is also inversely related to the bandwidth of the filter. 6.4 TIME-DOMAIN AND FREQUENCY-DOMAIN ASPECTS OF NONIDEAL FILTERS The characteristics of ideal filters are not always desirable in practice. For example, in many filtering contexts, the signals to be separated do not always lie in totally disjoint frequency bands. A typical situation might be that depicted in Figure 6.15, where the spectra of two signals overlap slightly. In such a case, we may wish to trade off the fi- delity with which the filter preserves one of these signals-say, x 1( f)-against the level to which frequency components of the second signal x2(t) are attenuated. A filter with a gradual transition from passband to stopband is generally preferable when filtering the superposition of signals with overlapping spectra. X(jw) Figure 6. 1 5 Two spectra that are w slightly overlapping. Another consideration is suggested by examining the step responses of ideallowpass filters, shown in Figure 6.14. For both continuous time and discrete time, the step response asymptotically approaches a constant equal to the value of the step. In the vicinity of the discontinuity, however, it overshoots this value and exhibits ringing. In some situations, this time-domain behavior may be undesirable. Moreover, even in cases where the ideal frequency-selective characteristics are de- sirable, they may not be attainable. For example, from eqs. (6.18) and (6.19) and Fig- ure 6.12, it is evident that the ideal lowpass filter is noncausal. When filtering is to be carried out in real time, however, causality is a necessary constraint, and thus, a causal approximation to the ideal characteristics would be required. A further consideration that motivates providing some flexibility in the filter characteristics is ease of implementation. In general, the more precisely we try to approximate or implement an ideal frequency- selective filter, the more complicated or costly the implementation becomes, whether in terms of components such as resistors, capacitors, and operational amplifiers in continu- ous time or in terms of memory registers, multipliers, and aqders in discrete time. In many contexts, a precise filter characteristic may not be essential and a simple filter will suffice. For all of these reasons, nonideal filters are of of considerable practical importance, and the characteristics of such filters are frequently specified or quantified in terms of several parameters in both the frequency and time domain. First, because the magnitude characteristics of the ideal frequency-selective filter may be unachievable or undesirable, Sec. 6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters 445 it is preferable to allow some flexibility in the behavior of the filter in the passband and in the stopband, as well as to permit a more gradual transition between the passband and stopband, as opposed to the abrupt transition characteristic of ideal filters. For example, in the case of lowpass filters, the specifications may allow some deviation from unity gain in the passband and from zero gain in the stopband, as well as including both a passband edge and stopband edge with a transition band between them. Thus, specifications for a continuous-time lowpass filter are often stated to require the magnitude of the frequency response of the filter to be restricted to the nonshaded area indicated in Figure 6.16. In this figure, a deviation from unity of plus and minus o1 is allowed in the passband, and a deviation of 82 from zero is allowed in the stopband. The amount by which the frequency response differs from unity in the passband is referred to as the passband ripple, and the amount by which it deviates from zero in the stopband is referred to as the stopband ripple. The frequency w P is referred to as the passband edge and w.1. as the stopband edge. The frequency range from w P to w.1· is provided for the transition from passband to stopband and is referred to as the transition band. Similar definitions apply to discrete-time lowpass filters, as well as to other continuous- and discrete-time frequency-selective filters. IH(iw)l ' ' ' ' ' Figure 6. 16 Tolerances for the Passband Tran' ·sjtion Stopband magnitude characteristic of a lowpass ' ' filter. The allowable passband ripple ' ' is o1 and stopband ripple is o2. The ' ~---------------------- dashed curve illustrates one possible ' ...., .. ----- frequency response that stays within w the tolerable limits. In addition to the specification of magnitude characteristics in the frequency domain, in some cases the specification of phase characteristics is also important. In particular, a linear or nearly linear phase characteristic over the passband of the filter is frequently desirable. To control the time-domain behavior, specifications are frequently imposed on the step response of a filter. As illustrated in Figure 6.17, one quantity often of interest is the rise time tr of the step response-i.e., the interval over which the step response rises toward its final value. In addition, the presence or absence of oscillatory behavior, or ringing, in the step response is often of importance. If such ringing is present, then there are three other quantities that are often used to characterize the nature of these oscillations: the overshoot ~ of the final value of the step response, the ringing frequency w r, and the settling time ts-i.e., the time required for the step response to settle to within a specified tolerance of its final value. For nonideallowpass filters, a trade-off may be observed between the width of the transition band (a frequency-domain characteristic) and the settling time of the step re- sponse (a time-domain characteristic). The following example illustrates this trade-off. 446 Time and Frequency Characterization of Signals and Systems Chap.6 s(t) Figure 6. 17 Step response of a continuous-time lowpass filter, indicating the rise time tr, overshoot A, ringing frequency wr, and settling time t5-i.e., the time at which the step response settles to within ::': o of its final value. Example 6.3 Let us consider two specific lowpass filters designed to have a cutoff frequency of 500 Hz. Each filter has a fifth-order rational frequency response and a real-valued impulse response. The two filters are of specific types, one referred to as Butterworth filters and the other as elliptic filters. Both of these classes of filters are frequently used in practice. The magnitudes of the frequency responses of the two filters are plotted (versus frequency measured in Hertz) in Figure 6.18(a). We take the transition band of each filter as the region around the cutoff frequency (500Hz) where the frequency response magnitude is neither within .05 of unity magnitude (the passband ripple) nor within .05 of zero magnitude (the stopband ripple). From Figure 6.18(a), it can be seen that the transition band of the Butterworth filter is wider than the transition band of the elliptic filter. The price paid for the narrower transition band of the elliptic filter may be observed in Figure 6.18(b ), in which the step responses of both filters are displayed. We see that the ringing in the elliptic filter's step response is more prominent than for the Butterworth step response. In particular, the settling time for the step response is longer in the case of the elliptic filter. The consideration of the trade-offs between time-domain and frequency-domain characteristics and of other issues such as the complexity and cost of filters forms the core of the important field of filter design. In the next few sections, and in several of the problems at the end of the chapter, we provide additional examples of LTI systems and filters and their time- and frequency-domain characteristics. Sec. 6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters 447 0.9 Q) en 0.8 c 0 0. en 0.7 ~ >- () c 0.6 Q) ::J ' 0"" ' ~ 0.5 '' ' 0 Q) 0.4 ' ""0 ' ' .a ' ·c: Ol 0.3 ' co '\ .....--- Butterworth filter ~ \ 0.2 \ \ \ 0.1 ' ' ' .... ... ___ _ ------~~~-~-----------------------------------~ 0 200 400 600 800 1 ,000 1 ,200 1 ,400 1 ,600 1 ,800 2,000 Frequency (Hz) 1.2 I I I I I I I I I 0.8 I I I I I I 0.6 I I I I I I 0.4 I I I I I I 0.2 I I I I I I 0 2 4 6 8 10 12 14 16 18 20 Time(msec) Figure 6. 18 Example of a fifth-order Butterworth filter and a fifth-order elliptic filter designed to have the same passband and stopband ripple and the same cutoff frequency: (a) magnitudes of the frequency responses plotted versus frequency measured in Hertz; (b) step responses. 448 Time and Frequency Characterization of Signals and Systems Chap.6 6.5 FIRST-ORDER AND SECOND-ORDER CONTINUOUS-TIME SYSTEMS LTI systems described by linear constant-coefficient differential equations are of great practical importance, because many physical systems can be modeled by such equations and because systems of this type can often be conveniently implemented. For a variety of practical reasons, high-order systems are frequently implemented or represented by combining first-order and second-order systems in cascade or parallel arrangements. Con- sequently, the properties of first- and second-order systems play an important role in an- alyzing, designing, and understanding the time-domain and frequency-domain behavior of higher order systems. In this section, we discuss these low-order systems in detail for continuous time. In Section 6.6, we examine their discrete-time counterparts. 6.5.1 First-Order Continuous-Time Systems The differential equation for a first-order system is often expressed in the form dy(t) T~ + y(t) = x(t), (6.21) where the coefficient T is a positive number whose significance will be made clear shortly. The corresponding frequency response for the first-order system is . ) 1 H(]W = . 1' (6.22) JWT + and the impulse response is (6.23) which is sketched in Figure 6.19(a). The step response of the system is s(t) = h(t) * u(t) = [1 - e- 11T]u(t). (6.24) This is sketched in Figure 6.19(b ). The parameter T is the time constant of the system, and it controls the rate at which the first-order system responds. For example, as illustrated in Figure 6.19, at t = T the impulse response has reached 11 e times its value at t = 0, and the step response is within 11 e of its final value. Therefore, as T is decreased, the impulse response decays more sharply, and the rise time of the step response becomes shorter- i.e., it rises more sharply toward its final value. Note also that the step response of a first-order system does not exhibit any ringing. Figure 6.20 depicts the Bode plot of the frequency response of eq. (6.22). In this figure we illustrate one of the advantages of using a logarithmic frequency scale: We can, without too much difficulty, obtain a useful approximate Bode plot for a continuous-time first-order system. To see this, let us first examine the plot of the log magnitude of the frequency response. Specifically, from eq. (6.22), we obtain 20log 10 IH(jw)l = -10log 2 10[(wT) + 1]. (6.25) From this, we see that for wT << 1, the log magnitude is approximately zero, while for wT >> 1, the log magnitude is approximately a linear function of log10(w ). That is, 20 log 10 IH(jw )I = 0 for w << 1/T, (6.26)"
6.5 First-Order and Second-Order Continuous-Time Systems,"448 Time and Frequency Characterization of Signals and Systems Chap.6 6.5 FIRST-ORDER AND SECOND-ORDER CONTINUOUS-TIME SYSTEMS LTI systems described by linear constant-coefficient differential equations are of great practical importance, because many physical systems can be modeled by such equations and because systems of this type can often be conveniently implemented. For a variety of practical reasons, high-order systems are frequently implemented or represented by combining first-order and second-order systems in cascade or parallel arrangements. Con- sequently, the properties of first- and second-order systems play an important role in an- alyzing, designing, and understanding the time-domain and frequency-domain behavior of higher order systems. In this section, we discuss these low-order systems in detail for continuous time. In Section 6.6, we examine their discrete-time counterparts. 6.5.1 First-Order Continuous-Time Systems The differential equation for a first-order system is often expressed in the form dy(t) T~ + y(t) = x(t), (6.21) where the coefficient T is a positive number whose significance will be made clear shortly. The corresponding frequency response for the first-order system is . ) 1 H(]W = . 1' (6.22) JWT + and the impulse response is (6.23) which is sketched in Figure 6.19(a). The step response of the system is s(t) = h(t) * u(t) = [1 - e- 11T]u(t). (6.24) This is sketched in Figure 6.19(b ). The parameter T is the time constant of the system, and it controls the rate at which the first-order system responds. For example, as illustrated in Figure 6.19, at t = T the impulse response has reached 11 e times its value at t = 0, and the step response is within 11 e of its final value. Therefore, as T is decreased, the impulse response decays more sharply, and the rise time of the step response becomes shorter- i.e., it rises more sharply toward its final value. Note also that the step response of a first-order system does not exhibit any ringing. Figure 6.20 depicts the Bode plot of the frequency response of eq. (6.22). In this figure we illustrate one of the advantages of using a logarithmic frequency scale: We can, without too much difficulty, obtain a useful approximate Bode plot for a continuous-time first-order system. To see this, let us first examine the plot of the log magnitude of the frequency response. Specifically, from eq. (6.22), we obtain 20log 10 IH(jw)l = -10log 2 10[(wT) + 1]. (6.25) From this, we see that for wT << 1, the log magnitude is approximately zero, while for wT >> 1, the log magnitude is approximately a linear function of log10(w ). That is, 20 log 10 IH(jw )I = 0 for w << 1/T, (6.26) h(t) (a) s(t) -------------------=--=-~-;;;.;----------- 'T Figure 6.19 Continuous-time first- order system: (a) impulse response; (b) (b) step response. 20 3 dB ___ j 1 0 dB ... Asymptotic I ... ~approximation 0 ci -20 .2 0 C\J -40 -60 0.1h 1h 10h 100fT w 1TI4 0 1 I \1 -1T/4 -1r12 -31T/4 0.1h 1h 10h 100fT Figure 6.20 Bode plot for a w continuous-time first-order system. 449 450 Time and Frequency Characterization of Signals and Systems Chap.6 and 20 log 10 IH(jw )I = -20 log 10(wT) (6.27) = -20log 10(w)- 20log 10(T) for w >> liT. In other words, for the first-order system, the low- and high-frequency asymptotes of the log magnitude are straight lines. The low-frequency asymptote [given by eq. (6.26)] is just the 0-dB line, while the high-frequency asymptote [specified by eq. (6.27)] corresponds to a decrease of 20 dB in IH (jw )I for every decade (i.e., factor of 10 ) in w. This is sometimes referred to as a ""20-dB-per-decade"" asymptote. Note that the two asymptotic approximations given in eqs. (6.26) and (6.27) are equal at the point log 10(w) = -log 10(T), or equivalently, w = liT. Interpreted graphically, this means that the two straight-line asymptotes meet at w = liT, which suggests a straight- line approximation to the magnitude plot. That is, our approximation to 20 log 10 IH(jw )I equals 0 for w :s liT and is given by eq. (6.27) for w 2:: liT. This approximation is also sketched (as a dashed line) in Figure 6.20. The point at which the slope of the approxima- tion changes is precisely w = liT, which, for this reason, is often referred to as the break frequency. Also, note that at w = liT the two terms [(wT)2 and 1] in the argument of the logarithm in eq. (6.25) are equal. Thus, at this point, the actual value of the magnitude is (6.28) Because of this, the point w = liT is sometimes called the 3-dB point. From the figure, we see that only near the break frequency is there any significant error in the straight-line approximate Bode plot. Thus, if we wish to obtain a more accurate sketch of the Bode plot, we need only modify the approximation near the break frequency. It is also possible to obtain a useful straight-line approximation to 1:-H(jw ): 1:-H(jw) = -tan -I (wT) 0, W :S O.liT (6.29) = -(7T/4)[log 10(wT) + 1], O.liT :S W :S 10/T. { -1T/2, W 2:: 10/T Note that this approximation decreases linearly (from 0 to -7T/2) as a function oflog 10(w) in the range 0.1 10 T T i.e., in the range from one decade below the break frequency to one decade above the break frequency. Also, zero is the correct asymptotic value of 1:-H(jw) for w << liT, and -1r12 is the correct asymptotic value of <H(jw) for w >> liT. Furthermore, the approximation agrees with the actual value of 1:-H(jw) at the break frequency w liT, at which point ·m(A) = -i· (6.30) This asymptotic approximation is also plotted in Figure 6.20, and from it we can see how, if desired, we can modify the straight-line approximation to obtain a more accurate sketch of 1:-H(jw ). Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 451 From this first-order system, we can again see the inverse relationship between time and frequency. As we make T smaller, we speed up the time response of the system [i.e., h(t) becomes more compressed toward the origin, and the rise time of the step response is reduced] and we simultaneously make the break frequency large [i.e., H(jw) becomes broader, since IH(jw )I = 1 for a larger range of frequencies]. This can also be seen by multiplying the impulse response by T and observing the relationship between rh(t) and H(jw): rh(t) = e-tiT u(t), H(jw) = jwr +I"" Thus, rh(t) is a function of tiT and H(jw) is a function of wr, and from this we see that changing Tis essentially equivalent to a scaling in time and frequency. 6.5.2 Second-Order Continuous-Time Systems The linear constant-coefficient differential equation for a second-order system is d 2y(t) dy(t) 2 2 ~ + 2~wndt + wny(t) = wnx(t). (6.31) Equations of this type arise in many physical systems, including RLC circuits and me- chanical systems, such as the one illustrated in Figure 6.21, composed of a spring, a mass, and a viscous damper or dashpot. In the figure, the input is the applied force x(t) and the output is the displacement of the mass y(t) from some equilibrium position at which the spring exerts no restoring force. The equation of motion for this system is d2y(t) = ( ) - k ( ) - bdy(t) m dt2 x t y t dt ' or 2 d y(t) + (!?_)dy(t) + (!__)y(t) = _!_x(t). dt2 m dt m m Comparing this to eq. (6.31), we see that if we identify Wn~A (6.32) and b ~=- 2Jb;z' ~ y(t) (displacement) x(t) (applied force) Figure 6.21 Second-order system consisting of a spring and dashpot attached to a moveable mass and a fixed support. 452 Time and Frequency Characterization of Signals and Systems Chap.6 then [except for a scale factor of k on x(t)] the equation of motion for the system of Figure 6.21 reduces to eq. (6.31). The frequency response for the second-order system of eq. (6.31) is 2 H(jw) = (jw)2 + 2(::(jw) + (6.33) wr The denominator of H(jw) can be factored to yield 2 H(jw) = (J' W- ~(- )' CJ JW- C2 where CJ = -(wn + WnJf2=1, (6.34) C2 = -(wn - Wn~· For ( =I= 1, c 1 and c2 are unequal, and we can perform a partial-fraction expansion of the form M M H(jw) = --- (6.35) jw- c 1 jw- c2' where (6.36) From eq. (6.35), the corresponding impulse response for the system is h(t) = M[ec 1t - ec2t]u(t). (6.37) If ( = 1, then CJ = c2 = -wn, and w2 H(jw) = (jw +nwn)2' (6.38) From Table 4.2, we find that in this case the impulse response is h(t) = w~te -wnt u(t). (6.39) Note from eqs. (6.37) and (6.39), that h(t)lwn is a function of wnt. Furthermore, eq. (6.33) can be rewritten as H(jw) = 2 1 , (jwlwn) + 2( (jwlwn) + 1 from which we see that the frequency response is a function of wlwn. Thus, changing wn is essentially identical to a time and frequency scaling. The parameter ( is referred to as the damping ratio and the parameter w n as the undamped natural frequency. The motivation for this terminology becomes clear when Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 453 we take a more detailed look at the impulse response and the step response of a second- order system. First, from eq. (6.35), we see that for 0 < ( < 1, c1 and c2 are complex, and we can rewrite the impulse response in eq. (6.37) in the form (6.40) Thus, for 0 < ( < 1, the second-order system has an impulse response that has damped oscillatory behavior, and in this case the system is referred to as being under- damped. If ( > 1, both c1 and c2 are real and negative, and the impulse response is the difference between two decaying exponentials. In this case, the system is overdamped. The case of ( = 1, when c1 = c2, is called the critically damped case. The impulse responses (multiplied by llwn) for second-order systems with different values of ( are plotted in Figure 6.22(a). (a) s(t) 2 Figure 6.22 Response of continu- ous-time second-order systems with different values of the damping ratio r (a) impulse response; (b) (b) step response. 454 Time and Frequency Characterization of Signals and Systems Chap.6 The step response of a second-order system can be calculated from eq. (6.37) for ( =I: 1. This yields the expression ecJt ec""t]J s(t) = h(t) * u(t) = { 1 + M [-c;- - ~ u(t). (6.41) For ( = 1, we can use eq. (6.39) to obtain s(t) = [1 - e-wnt - W te-w""t]u(t). 11 (6.42) The step response of a second-order system is plotted in Figure 6.22(b) for several values of (. From this figure, we see that in the underdamped case, the step response exhibits both overshoot (i.e., the step response exceeds its final value) and ringing (i.e., oscillatory behavior). For ( = 1, the step response has the fastest response (i.e., the shortest rise time) that is possible without overshoot and thus has the shortest settling time. As ( increases beyond 1, the response becomes slower. This can be seen from eqs. (6.34) and (6.41). As ( increases, c1 becomes smaller in magnitude, while c2 increases in magnitude. Therefore, although the time constant (l/jc2)) associated with ec2r decreases, the time constant (1/jc1)) associated with ec 1r increases. Consequently the term involving ec 1r in eq. (6.41) takes a longer time to decay to zero, and thus it is the time constant associated with this term that determines the settling time of the step response. As a result the step response takes longer to settle for large values of (. In terms of our spring -dashpot example, as we increase the magnitude of the damping coefficient b beyond the critical value at which (in eq. (6.33) equals 1, the motion of the mass becomes increasingly sluggish. Finally, note that, as we have said, the value of w 11 essentially controls the time scale of the responses h(t) and s(t). For example, in the underdamped case, the larger W 11 is, the more compressed is the impulse response as a function oft, and the higher is the frequency of the oscillations or ringing in both h(t) and s(t). In fact, from eq. (6.40), we see that the frequency of the oscillations in h(t) and s(t) is w 11 ~'which does increase with increasing w 11 • Note, however, that this frequency depends explicitly on the damping ratio and does not equal (and is in fact smaller than) w 11 , except in the undamped case, ( = 0. (It is for this reason that the parameter w n is traditionally referred to as the undamped natural frequency.) For the spring-dashpot example, we therefore conclude that the rate of oscillation of the mass equals w n when no dash pot is present, and the oscillation frequency decreases when we include the dashpot. In Figure 6.23, we have depicted the Bode plot of the frequency response given in eq. (6.33) for several values of(. As in the first-order case, the logarithmic frequency scale leads to linear high- and low-frequency asymptotes for the log magnitude. Specifically, from eq. (6.33), (6.43) From this expression, it follows that forw << Wn forw >> (6.44) Wn · Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 455 Asymptotic 8 approximation -20 I 0 Ol .Q -40 0 C\J -60 -80 0.1wn Wn 10wn w 0 -7T/4 3 Asymptotic ~ -7T/2 approximation v - 37T/4 w Figure 6.23 Bode plots for second-order systems with several different values of damping ratio r Therefore, the low-frequency asymptote of the log magnitude is the 0-dB line, while the high-frequency asymptote [given by eq. (6.44)] has a slope of -40 dB per decade; i.e., IH(jw )I decreases by 40 dB for every increase in w of a factor of 10. Also, note that the two straight-line asymptotes meet at the point w = wn. Thus, we obtain a straight-line approximation to the log magnitude by using the approximation given in eq. (6.44) for w ::; Wn. For this reason, Wn is referred to as the break frequency of the second-order system. This approximation is also plotted (as a dashed line) in Figure 6.23. We can, in addition, obtain a straight-line approximation to 4-H(jw ), whose exact expression can be obtained from eq. (6.33): . -I ( 2((wlwn) ) 4-H(jw) = -tan 1 - (wlwn)2 . (6.45) 456 Time and Frequency Characterization of Signals and Systems Chap. 6 The approximation is w ~ O.lwn O.lwn ~ w ~ !Own, (6.46) w 2::: !Own which is also plotted in Figure 6.23. Note that the approximation and the actual value again are equal at the break frequency w = Wn, where It is important to observe that the asymptotic approximations, eqs. (6.44) and (6.46), we have obtained for a second-order system do not depend on {, while the actual plots of !H(jw )I and <XH(jw) certainly do, and thus, to obtain an accurate sketch, especially near the break frequency w = Wn, we must take this into account by modifying the approxi- mations to conform more closely to the actual plots. The discrepancy is most pronounced for small values of{. In particular, note that in this case the actual log magnitude has a peak around w = Wn. In fact, straightforward calculations using eq. (6.43) show that, for { < J212 = 0.707, IH(jw)i has a maximum value at (6.47) and the value at this maximum point is (6.48) For { > 0. 707, however, H (jw) decreases monotonically as w increases from zero. The fact that H (jw) can have a peak is extremely important in the design of frequency-selective filters and amplifiers. In some applications, one may want to design such a circuit so that it has a sharp peak in the magnitude of its frequency response at some specified frequency, thereby providing large frequency-selective amplification for sinusoids at fre- quencies within a narrow band. The quality Q of such a circuit is defined to be a measure of the sharpness of the peak. For a second-order circuit described by an equation of the form of eq. (6.31), the quality is usually taken to be and from Figure 6.23 and eq. (6.48), we see that this definition has the proper behavior: The less damping there is in the system, the sharper is the peak in !H(jw )I. 6.5.3 Bode Plots for Rational Frequency Responses At the start of this section, we indicated that first- and second-order systems can be used as basic building blocks for more complex LTI systems with rational frequency responses. One consequence of this is that the Bode plots presented here essentially provide us with Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 457 all of the information we need to construct Bode plots for arbitrary rational frequ~ncy responses. Specifically, we have described the Bode plots for the frequency responses given by eqs. (6.22) and (6.33). In addition, we can readily obtain the Bode plots for frequency responses of the forms H(jw) = 1 + jwT (6.49) and . ) H(jw) = 1 + 2( (-.W ) + (-)'W )2 . (6.50) Wn Wn The Bode plots for eqs. (6.49) a11d (6.50) follow directly from Figures 6.20 and 6.23 and from the fact that 20 log10 IH(iw)l = -20 log10 IH(~w) I and <t(H(jw )) = - <t (H(~w)). Also, consider a system function that is a constant gain H(jw) = K. Since K = iKiej·O if K > 0 and K = iKiej7T if K < 0, we see that 20 log 10 iH(jw )I = 20 log 10 IKI -tH(1 · w) = { 0, ~f K > 0 1T, If K < 0 Since a rational frequency response can be factored into the product of a constant gain and first- and second-order terms, its Bode plot can be obtained by summing the plots for each of the terms. We illustrate further the construction of Bode plots in the next two examples. Example 6.4 Let us obtain the Bode plot for the frequency response 2 X 104 H(jw) = (jw )2 + lOOjw + 104 · First, we note that H(jw) = 2H(jw), where H(jw) has the same form as the standard second-order frequency response spec- ified by eq. (6.33). It follows that 20 log 10 iH(Jw )I = 20 log 10 2 + 20 log iH(Jw )1. 458 Time and Frequency Characterization of Signals and Systems Chap.6 By comparing H(jw) with the frequency response in eq. (6.33), we conclude that Wn = 100 and~ = 112 for H(jw). Using eq. (6.44), we may now specify the asymptotes for 20 log10 \H(}w )\: 20 log10 \H(jw )\ = 0 for w << 100, and 20 log10 \H(}w )\ = -40 log 10 w + 80 for w >> 100. It follows that 20 log10 \H(}w )\ will have the same asymptotes, except for a constant offset at all frequencies due to the addition of the 20 log10 2 term (which approxi- mately equals 6 dB). The dashed lines in Figure 6.24(a) represent these asymptotes. OdB -20 1 :I -40 0 c) .2 -60 0 C\J -80 -100~----------~----------~----------~----------~ 1 o0 1 o1 1 o2 1 o3 1 o4 w (a) ~ -TI/2 I v- w (b) Figure 6.24 Bode plot for system function in Example 6.4: (a) magnitude; (b) phase. Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 459 The solid curve in the same figure represents the actual computer-generated Bode plot for 20 log 10 IH(jw )1. Since the value of~ for H(jw) is less than J212. the actual Bode plot has a slight peak near w = 100. To obtain a plot of <rH (jw ), we note that <rH (jw) = <rH (jw) and that <rH(jw) has its asymptotes specified in accordance with eq. (6.46); that is, 0, w ~ 10 <rH(jw) = =~/2)[1og 10(w/100) + 1], 10 ~ w ~ 1,000. { w ;:::: 1,000. The asymptotes and the actual values for <r.H(jw) are plotted with dashed and solid lines, respectively, in Figure 6.24(b). Example 6.5 Consider the frequency response H ·w _ 100(1 + jw) (j ) - (10 + jw)(lOO + jw) To obtain the Bode plot for H (jw ), we rewrite it in the following factored form: Here, the first factor is a constant, the next two factors have the standard form for a first- order frequency response as specified in eq. (6.22), and the fourth factor is the reciprocal of the same first-order standard form. The Bode plot for 20 log 10 IH(jw >I is therefore the sum of the Bode plots corresponding to each of the factors. Furthermore, the asymptotes corresponding to each factor may be summed to obtain the asymptotes for the overall Bode plot. These asymptotes and the actual values of 20 log 10 IH (jw )I are displayed in Figure 6.25(a). Note that the constant factor of 1/10 accounts for an offset of -20 dB at each frequency. The break frequency at w = 1 corresponds to the ( 1 + jw) factor, which produces the 20 dB/decade rise that starts at w = 1 and is canceled by the 20 dB/decade decay that starts at the break frequency at w = 10 and is due to the 11(1 + jwll 0) factor. Finally, the 11(1 + jw/1 00) factor contributes another break frequency at w = 100 and a subsequent decay at the rate of 20 dB/decade. Similarly we can construct the asymptotic approximation for H (jw) from the in- di vidual asymptotes for each factor, as illustrated, together with a plot of the exact value of the phase, in Figure 6.25(b). In particular, the constant factor 1110 contributes 0 to the phase, while the factor ( 1 + jw) contributes an asymptotic approximation that is 0 for w < 0.1, and rises linearly as a function of log 10(w) from a value of zero at w = 0.1 to a value of 7T/2 radians at w = 10. However, this rise is canceled at w = 1 by the asymptotic approximation for the angle of 11(1 + jwll 0) which contributes a linear de- crease in angle of 7T/2 radians over the range of frequencies from w = 1 to w = 100. Finally, the asymptotic approximation for the angle of 1/(1 + jw/100) contributes an- other linear decrease in angle of 7T/2 radians over the range of frequencies from w = 10 tow = 1000. 460 Time and Frequency Characterization of Signals and Systems Chap.6 -40 0.1 10 100 1000 w (a) -rr/2 -rr/4 0 --rr/4 --rr/2 0.1 10 100 1000 w (b) Figure 6.25 Bode plot for system function in Example 6.5: (a) magnitude; (b) phase. In our discussion of first-order systems in this section, we restricted our atten- tion to values of T > 0. In fact, it is not difficult to check that if T < 0, then the causal first-order system described by eq. (6.21) has an impulse response that is not absolutely integrable, and consequently, the system is unstable. Similarly, in analyzing the second-order causal system in eq. (6.31), we required that both ( and w~ be pos- itive numbers. If either of these is not positive, the resulting impulse response is not absolutely integrable. Thus, in this section we have restricted attention to those causal first- and second-order systems that are stable and for which we can define frequency responses. Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 461 6.6 FIRST-ORDER AND SECOND-ORDER DISCRETE-TIME SYSTEMS In this section, we examine the properties of first- and second-order discrete-time LTI systems, paralleling the development in the preceding section. As in continuous time, any system with a frequency response that is a ratio of polynomials in e- jw_i.e., any discrete- time LTI system described by a linear constant-coefficient difference equation-can be written as a product or sum of first- and second-order systems, implying that these ba- sic systems are of considerable value in both implementing and analyzing more complex systems. (See, for example, Problem 6.45.) 6.6.1 First-Order Discrete-Time Systems Consider the first-order causal LTI system described by the difference equation y[n] - ay[n - 1] = x[n], (6.51) with Ia I < 1. From Example 5 .18, the frequency response of this system is (6.52) and its impulse response is (6.53) which is illustrated in Figure 6.26 for several values of a. Also, the step response of the system is 1- an+l s[n] = h[n] * u[n] 1 -a u[n], (6.54) which is illustrated in Figure 6.27. The magnitude of the parameter a plays a role similar to that of the time constant Tin the continuous-time first-order system. Specifically, lal determines the rate at which the first-order system responds. For example, from eqs. (6.53) and (6.54) and Figures 6.26 and 6.27, we see that h[n] and s[n] converge to their final value at the rate at which lain converges to zero. Therefore, the impulse response decays sharply and the step response settles quickly for lal small. For lal nearer to 1, these responses are slower. Note that unlike its continuous-time counterpart, the first-order system described by eq. (6.51) can display oscillatory behavior. This occurs when a < 0, in which case the step response exhibits both overshoot of its final value and ringing. The magnitude and phase of the frequency response of the first-order system in eq. (6.51) are, respectively, IH (e jw )I = -----=-_1_ _- -:-:-::- (6.55) (1 + a2 - 2a cos w) 112"
6.6 First-Order and Second-Order Discrete-Time Systems,"Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 461 6.6 FIRST-ORDER AND SECOND-ORDER DISCRETE-TIME SYSTEMS In this section, we examine the properties of first- and second-order discrete-time LTI systems, paralleling the development in the preceding section. As in continuous time, any system with a frequency response that is a ratio of polynomials in e- jw_i.e., any discrete- time LTI system described by a linear constant-coefficient difference equation-can be written as a product or sum of first- and second-order systems, implying that these ba- sic systems are of considerable value in both implementing and analyzing more complex systems. (See, for example, Problem 6.45.) 6.6.1 First-Order Discrete-Time Systems Consider the first-order causal LTI system described by the difference equation y[n] - ay[n - 1] = x[n], (6.51) with Ia I < 1. From Example 5 .18, the frequency response of this system is (6.52) and its impulse response is (6.53) which is illustrated in Figure 6.26 for several values of a. Also, the step response of the system is 1- an+l s[n] = h[n] * u[n] 1 -a u[n], (6.54) which is illustrated in Figure 6.27. The magnitude of the parameter a plays a role similar to that of the time constant Tin the continuous-time first-order system. Specifically, lal determines the rate at which the first-order system responds. For example, from eqs. (6.53) and (6.54) and Figures 6.26 and 6.27, we see that h[n] and s[n] converge to their final value at the rate at which lain converges to zero. Therefore, the impulse response decays sharply and the step response settles quickly for lal small. For lal nearer to 1, these responses are slower. Note that unlike its continuous-time counterpart, the first-order system described by eq. (6.51) can display oscillatory behavior. This occurs when a < 0, in which case the step response exhibits both overshoot of its final value and ringing. The magnitude and phase of the frequency response of the first-order system in eq. (6.51) are, respectively, IH (e jw )I = -----=-_1_ _- -:-:-::- (6.55) (1 + a2 - 2a cos w) 112 hI[n] 1 h[n] a=+1- 11 a=+1- •••••••••• t ••••••••••••••••••••: ••• •••••••••• Jr, •••••••••••••••••• : ••• 0 n 0 n h[n] h[n] 1 ......... ~1 a=-- 2 ol ................. :~~: .. . n n (a) (b) h[n] h[n] 11 a=+3- 11 7 0000000000 IItrr,, •.•........... : ... .......... IIIIIJttTTTtt•····:~:: ... 0 n 0 n h[n] h[n] a=-3- 7 4 a=-- 8 n 0 n (c) (d) Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 463 :2 r ·""+4! :r 8""+! ......•.. : JJIIIIIIJIJJJJJJJJJJJJJJJ 0 n ......... : tiiiiiiiiiiiiiiiiiiiiiiii 0 n :[tn] a=-.l :t[n] a=-.l 2 4 2 2 ••••••••• : tTTTTTTTTTTTTTTTTTTTTTTTT ••••••••• : tTTTTTTTTTTTTTTTTTTTTTTTT 0 n 0 n (a) (b) s[n] 8 a=+Z. 8 7 s[n] a=+3- 6 5 4 5 4 4 3 ~ ·ll ; ·ll 0 n 0 n st[n] a=-~ s[tn] a=-Z. 3 4 3 8 ••••••••• : tTtTttttttttttttttttttttt ••••••••• :_.t,T.Ttltlttttttttttttttt 0 n 0 n (c) (d) Figure 6.27 Step response s[n] of a first-order system: (a) a = ±1/4; (b) a = ±1/2; (c) a = ±3/4; (d) a = ±7/8. and <r..H (e jw) = - tan- I [ a sin w l· (6.56) 1- acosw In Figure 6.28( a), we have plotted the log magnitude and the phase of the frequency response in eq. (6.52) for several values of a > 0. The case of a < 0 is illustrated in 464 Time and Frequency Characterization of Signals and Systems Chap.6 Figure 6.28(b ). From these figures, we see that for a > 0, the system attenuates high frequencies [i.e., jH(ejw)i is smaller for w near ±1r than it is for w near 0], while when a < 0, the system amplifies high frequencies and attenuates low frequencies. Note also that for lal small, the maximum and minimum values, 1/(1 +a) and 1/(1- a), of jH(ejw)i are close together in value, and the graph ofjH(ejw)i is relatively flat. On the other hand, for lal near 1, these quantities differ significantly, and consequently jH(ejw)i is more sharply peaked, providing filtering and amplification that is more selective over a narrow band of frequencies. 20 log10 IH(ei""')l 20dB w -8 a= I 'IT 3 8 2 a=- 1 4 a=- 2 a=1 4 w (a) Figure 6.28 Magnitude and phase of the frequency response of eq. (6.52) for a first-order system: (a) plots for several values of a > 0; (b) plots for several values of a < 0. Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 465 20 log10 IH(eiw)l a=--7 8 a=-~ ~ 20dB 4 16 12 8 4 21T w -8 a=- I. 8 1T a=--3 4 2 a=--1 a=-12 4 w (b) Figure 6.28 Continued 6. 6. 2 Second-Order Discrete-Time Systems Consider next the second-order causal LTI system described by y[n] - 2r cos Oy[n - 1] + r 2y[n - 2] = x[n], (6.57) with 0 < r < 1 and 0 :::; 8 :::; 7r. The frequency response for this system is H(e jw) -- 1 1- . 2rcosoe-Jw + 2 .2 . (6.58) r e-J w The denominator of H ( ejw) can be factored to obtain jw _ 1 H(e ) - (6.59) [1 - (reJ· e )e-J.W ][1 - (re- ·e · · 1 )e-JW] 466 Time and Frequency Characterization of Signals and Systems Chap.6 For()¥= 0 or 7T, the two factors in the denominator of H(eiw) are different, and a partial- fraction expansion yields (6.60) where ej{J e-.ifJ A=--- B= (6.61) 2j sin()' 2j sin()· In this case, the impulse response of the system is h[n] = [A(rei8 yz + B(re-i 8)'1 ]u[n] _ 11 sin[(n + 1)()] [ ] (6.62) -r ·e un. Sill For() = 0 or 7T, the two factors in the denominator of eq. (6.58) are the same. When() = 0, (6.63) and h[n] = (n + l)r11 u[n]. (6.64) When() = 7T, (6.65) and h[n] = (n + 1)( -rYu[n]. (6.66) The impulse responses for second-order systems are plotted in Figure 6.29 for a range of values of rand(). From this figure and from eq. (6.62), we see that the rate of decay of h[n] is controlled by r-i.e., the closer r is to 1, the slower is the decay in h[n]. Similarly, the value of () determines the frequency of oscillation. For example, with () = 0 there is no oscillation in h[n], while for() = 7T the oscillations are rapid. The effect of different values of rand() can also be seen by examining the step response of eq. (6.57). For() ¥= 0 or 7T, (1_ ( rei8)n+l) (1 _( re-.i8)n+I )~ s[n] = h[n] * u[n] = [A 1 _ re.ifJ + B 1 _ re-JfJ ~ u[n]. (6.67) Also, using the result of Problem 2.52, we find that for () = 0, 1 r n r n] ] s[n] = [ (r- 1)2 - (r- 1)2 r + r- 1 (n + l)r u[n ' (6.68) while for() = 7T, s[n] = [(r: 1)2 + (r: !)2 (-r)"" + r ~ 1 (n + 1)(-r)""]u[n]. (6.69) The step response is plotted in Figure 6.30, again for a range of values of rand (). Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 467 r=~ r=~ 1 1 r=- r=- 4 2 0=0 1 !l=O 1~ !l=O 1l rl~ll~illllr~: :! 0 n 0 n 0 n 1 1 3 r=4 r=2 r=4 ~ !j--:4!! ~ !l=:!! 1 1 4 1 ~ e--:4!! I 0 n 0 n o!II '"" ... n 1 1 3 r=4 r=2 r=4 I !j-:!! !l=:!! 1 -2 l L !j--:2!! 1 2 I, I 0 n o l n o{ I I n 1 1 3 r= 4 r=2 r=4 e-- 3 1 1 41T I. !-j-4~ e--~4 e--~4 1 lr, of 11 r '· n o[' n of!l' n 1 1 3 r= 4 r=2 r= 4 t 1 1 !l=7r fl='IT !l=7r 1 lr r', i flor 1 TP TP"" or n n 0 n Figure 6.29 Impulse response of the second-order system of eq. (6.57) for a range of values of rand e. The second-order system given by eq. (6.57) is the counterpart of the underdamped second-order system in continuous time, while the special case of (} = 0 is the critically damped case. That is, for any value of(} other than zero, the impulse response has a damped 1 r=4 r=~ r--4~ s[n] s[n] s[n] 4 16 r=1- 4 4 r=-1 r=3- 14 2 4 3 fl=O 8=0 12 fl=O 3 10 1-l=O 2 2 8 6 4 2 0 n 0 n n s[n] s[n] s[n] 4 1 4 1 4 3 r= 4 r= 2 r=4 3 e- 3 3 fj-TI -T4I e--T4I -4 'IT 8=4 2 2 2 n 0 n 0 n s[n] s[n] s[n] 4 1 r=4 4 1 r=2 4 3 r=4 3 IJ--T2I 3 IJ--T2I 3 ll=:rr 'IT 2 8=2 2 2 2 0 n 0 n 0 n s[n] s[n] s[n] 4 1 4 1 3 r=4 4 r= 2 r=4 3 f-j-4~ 3 fl_-341T 3 e--~4 8-- 3'4TT 2 2 2 0 n 0 n n s[n] s[n] s[n] 4 1 4 1 4 3 r=4 r=2 r=4 3 3 3 fl=TI fl=TI fl=TI 8=7T 2 2 2 0 n n n *Note: The plot for r= ~ , 8=0 has a different scale from the others. Figure 6.30 Step response of the second-order system of eq. (6.57) for a range of values of rand o. 468 Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 469 oscillatory behavior, and the step response exhibits ringing and overshoot. The frequency response of this system is depicted in Figure 6.31 for a number of values of r and (}. From Figure 6.31, we see that a band of frequencies is amplified , and r determines how sharply peaked the frequency response is within this band. As we have just seen, the second-order system described in eq. (6.59) has factors with complex coefficients (unless(} = 0 or 7T). It is also possible to consider second-order systems having factors with real coefficients. Specifically, consider H(e jw ) -- . 1 . , (6 .70) (1 - d, e-.Jw)(l - d2e- .JW) where d 1 and d2 are both real numbers with ld1l, ld2 1 < 1. Equation (6.70) is the frequency response for the difference equation 8=0 (I) (a) Figure 6.31 Magnitude and phase of the frequency response of the second-order system of eq. (6.57): (a) e = 0; (b) e = TT/4; (c) e = TT/2; (d) e = 3TT!4; (e) e = TT. Each plot contains curves corresponding to r = 1/4, 1/2, and 3/4. 470 Time and Frequency Characterization of Signals and Systems Chap.6 w -12 w (b) Figure 6. 31 Continued (6.71) In this case, (6.72) where (6.73) Thus, h[n] = [Adf + Bd2Ju[n], (6.74) Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 471 24 dB 20 16 r= ~ 4 w -8 -12 e=II 2 (c) Figure 6.31 Continued which is the sum of two decaying real exponentials. Also, 11 [ (1- d + I) (1 - dn+ I) ~ s[n] = A 1 _ ~ 1 + B 1 _ ~2 ~ u[n]. (6.75) The system with frequency response given by eq. (6.70) corresponds to the cascade of two first-order systems. Therefore, we can deduce most of its properties from our un- derstanding of the first-order case. For example, the log-magnitude and phase plots for eq. (6.70) can be obtained by adding together the plots for each of the two first-order terms. Also, as we saw for first-order systems, the response of the system is fast if ld11 and ld2l are small, but the system has a long settling time if either of these magnitudes is near 1. Furthermore, if d 1 and d2 are negative, the response is oscillatory. The case when both d 1 and d2 are positive is the counterpart of the overdamped case in continuous time, with the impulse and step responses settling without oscillation. 472 Time and Frequency Characterization of Signals and Systems Chap.6 20 log10 JH(eiw)J 24 dB 20 w -12 £l_3""1T u- 4 w (d) Figure 6.31 Continued In this section, we have restricted attention to those causal first- and second-order systems that are stable and for which the frequency response can be defined. In particular, the causal system described by eq. (6.51) is unstable for I a I ~ 1. Also, the causal system described by eq. (6.56) is unstable if r ~ 1, and that described by eq. (6.71) is unstable if either I d 1 I or I d2 I exceeds 1. 6. 7 EXAMPLES OF TIME- AND FREQUENCY-DOMAIN ANALYSIS OF SYSTEMS Throughout this chapter, we have illustrated the importance of viewing systems in both the time domain and the frequency domain and the importance of being aware of trade-offs in the behavior between the two domains. In this section, we illustrate some of these issues further. In Section 6.7.1, we discuss these trade-offs for continuous time in the context of an automobile suspension system. In Section 6. 7 .2, we discuss an important class of discrete-time filters referred to as moving-average or nonrecursive systems."
6.7 Examples of Time- and Frequency-Domain Analysis of Systems,"472 Time and Frequency Characterization of Signals and Systems Chap.6 20 log10 JH(eiw)J 24 dB 20 w -12 £l_3""1T u- 4 w (d) Figure 6.31 Continued In this section, we have restricted attention to those causal first- and second-order systems that are stable and for which the frequency response can be defined. In particular, the causal system described by eq. (6.51) is unstable for I a I ~ 1. Also, the causal system described by eq. (6.56) is unstable if r ~ 1, and that described by eq. (6.71) is unstable if either I d 1 I or I d2 I exceeds 1. 6. 7 EXAMPLES OF TIME- AND FREQUENCY-DOMAIN ANALYSIS OF SYSTEMS Throughout this chapter, we have illustrated the importance of viewing systems in both the time domain and the frequency domain and the importance of being aware of trade-offs in the behavior between the two domains. In this section, we illustrate some of these issues further. In Section 6.7.1, we discuss these trade-offs for continuous time in the context of an automobile suspension system. In Section 6. 7 .2, we discuss an important class of discrete-time filters referred to as moving-average or nonrecursive systems. Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 473 w 0='1T w (e) Figure 6.31 Continued 6. 7. 1 Analysis of an Automobile Suspension System A number of the points that we have made concerning the characteristics and trade-offs in continuous-time systems can be illustrated in the interpretation of an automobile sus- pension system as a lowpass filter. Figure 6.32 shows a diagrammatic representation of a simple suspension system comprised of a spring and dashpot (shock absorber). The road surface can be thought of as a superposition of rapid small-amplitude changes in elevation (high frequencies), representing the roughness of the road surface, and gradual changes in elevation (low frequencies) due to the general topography. The automobile suspension system is generally intended to filter out rapid variations in the ride caused by the road surface (i.e., the system acts as a lowpass filter). The basic purpose of the suspension system is to provide a smooth ride, and there is no sharp, natural division between the frequencies to be passed and those to be rejected. Thus, it is reasonable to accept and, in fact, prefer a lowpass filter that has a gradual 474 Time and Frequency Characterization of Signals and Systems Chap. 6 Reference elevation Figure 6.32 Diagrammatic representation of an automotive suspension system. Here, Yo represents the distance between the chassis and the road surface when the automobile is at rest, y(t) + Yo the position of the chassis above the reference elevation, and x(t) the elevation of the road above the reference elevation. transition from passband to stopband. Furthermore, the time-domain characteristics of the system are important. If the impulse response or step response of the suspension system exhibits ringing, then a large bump in the road (modeled as an impulse input) or a curb (modeled as a step input) will result in an uncomfortable oscillatory response. In fact, a common test for a suspension system is to introduce an excitation by depressing and then releasing the chassis. If the response exhibits ringing, it is an indication that the shock absorbers need to be replaced. Cost and ease of implementation also play an important role in the design of au- tomobile suspension systems. Many studies have been carried out to determine the most desirable frequency-response characteristics for suspension systems from the point of view of passenger comfort. In situations where the cost may be warranted, such as for passenger railway cars, intricate and costly suspension systems are used. For the automotive indus- try, cost is an important factor, and simple, less costly suspension systems are generally used. A typical automotive suspension system consists simply of the chassis connected to the wheels through a spring and a dashpot. In the diagrammatic representation in Figure 6.32, y0 represents the distance be- tween the chassis and the road surface when the automobile is at rest, y(t) + y0 the position of the chassis above the reference elevation, and x(t) the elevation of the road above the reference elevation. The differential equation governing the motion of the chassis is then 2 Md y(t) + bddy(tt) + k ()- k ) bdx(t) (6.76) ~ y t - x(t + df' where M is the mass of the chassis and k and b are the spring and shock absorber constants, respectively. The frequency response of the system is . k + bjw H(jw) = (jw)2M + b(jw) + k' or w~ + 2{wn(jw) (6.77) H(jw) = (jw )2 + 2{wn(jw) + w~' Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 475 where Wn = a and '4wn = ! As in Section 6.5 .2, the parameter w n is referred to as the undamped natural frequency and ~ as the damping ratio. A Bode plot of the log magnitude of the frequency response in eq. (6.77) can be constructed by using first-order and second-order Bode plots. The Bode plot for eq. (6.77) is sketched in Figure 6.33 for several different values of the damping ratio. Figure 6.34 illustrates the step response for several different values of the damping ratio. As we saw in Section 6.5 .2, the filter cutoff frequency is controlled primarily through w 11 , or equivalently for a chassis with a fixed mass, by an appropriate choice of spring constant k. For a given w 11 , the damping ratio is then adjusted through the damping factor b associated with the shock asorbers. As the natural frequency w 11 is decreased, the suspen- sion will tend to filter out slower road variations, thus providing a smoother ride. On the other hand, we see from Figure 6.34 that the rise time of the system increases, and thus the system will feel more sluggish. On the one hand, it would be desirable to keep w 11 small to improve the lowpass filtering; on the other hand, it would be desirable to have Wn large for a rapid time response. These, of course, are conflicting requirements and illustrate the need for a trade-off between time-domain and frequency-domain characteristics. Typically, a suspension system with a low value of w n, so that the rise time is long, is characterized as ""soft"" and one with a high value of wn, so that the rise time is short, is characterized as ""hard."" From Figures 6.33 and 6.34, we observe also that, as the damping ratio decreases, the frequency response of the system cuts off more sharply, but the overshoot and ring- ing in the step response tend to increase, another trade-off between the time and frequency 20 3 0 dB I 0 Oi .Q 0 C\J -20 -40 Frequency Figure 6.33 Bode plot for the magnitude of the frequency response of the automobile suspension system for several values of the damping ratio. 476 Time and Frequency Characterization of Signals and Systems Chap.6 s(t) Figure 6.34 Step response of the automotive suspension system for vari- ous values of the damping ratio (? = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.5, 2.0, 5.0). domains. Generally, the shock absorber damping is chosen to have a rapid rise time and yet avoid overshoot and ringing. This choice corresponds to the critically damped case, with { = 1.0, considered in Section 6.5.2. 6. 7. 2 Examples of Discrete-Time Non recursive Filters In Section 3.11, we introduced the two basic classes of LTI filters described by difference equations, namely, recursive or infinite impulse response (IIR) filters and nonrecursive or finite impulse response (FIR) filters. Both of these classes of filters are of considerable importance in practice and have their own advantages and disadvantages. For example, recursive filters implemented as interconnections of the first- and second-order systems described in Section 6.6 provide a flexible class of filters that can be easily and efficiently implemented and whose characteristics can be adjusted by varying the number and the parameters of each of the component first- and second-order subsystems. On the other hand, as shown in Problem 6.64, it is not possible to design a causal, recursive filter with exactly linear phase, a property that we have seen is often desirable since, in that case, the effect of the phase on the output signal is a simple time delay. In contrast, as we show in this section, nonrecursive filters can have exactly linear phase. However, it is generally true that the same filter specifications require a higher order equation and hence more coefficients and delays when implemented with a nonrecursive equation, compared with a recursive difference equation. Consequently, for FIR filters, one of the principal trade-offs between the time and frequency domains is that increasing the flexibility in specifying the frequency domain characteristics of the filter, including, for example, achieving a higher degree of frequency selectivity, requires an FIR filter with an impulse response of longer duration. One of the most basic nonrecursive filters, introduced in Section 3.11.2, is the moving-average filter. For this class of filters, the output is the average of the values of the input over a finite window: M y[n] N M 1 L x[n - k]. (6.78) + + k= -N Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 477 The corresponding impulse response is a rectangular pulse, and the frequency response is H(e.iw) = 1 e.iw[(N-M)/2] sin[w(~ + N + 1)/2]. (6.79) N + M + 1 sm(w/2) In Figure 6.35, we show the log magnitude forM+ N + 1 = 33 and M + N + 1 = 65. The main, center lobe of each of these frequency responses corresponds to the effective pass- band of the corresponding filter. Note that, as the impulse response increases in length, the width of the main lobe of the magnitude of the frequency response decreases. This provides another example of the trade-off between the time and frequency domains. Specifically, in order to have a narrower passband, the filter in eqs. (6.78) and (6.79) must have a longer impulse response. Since the length of the impulse response of an FIR filter has a direct impact on the complexity of its implementation, this implies a trade-off between frequency selectivity and the complexity of the filter, a topic of central concern in filter design. Moving-average filters are commonly applied in economic analysis in order to at- tenuate the short-term fluctuations in a variety of economic indicators in relation to longer term trends. In Figure 6.36, we illustrate the use of a moving-average filter of the form of eq. (6.78) on the weekly Dow Jones stock market index for a 10-year period. The weekly Dow Jones index is shown in Figure 6.36(a). Figure 6.36(b) is a 51-day moving aver- age (i.e., N = M = 25) applied to that index, and Figure 6.36(c) is a 201-day moving average (i.e., N = M = 10 0) applied to the index. Both moving averages are considered useful, with the 51-day average tracking cyclical (i.e., periodic) trends that occur during the course of the year and the 20 1-day average primarily emphasizing trends over a longer time frame. The more general form of a discrete-time nonrecursive filter is M y[n] = L bkx[n - k], k=-N so that the output of this filter can be thought of as a weighted average of N t M + 1 neighboring points. The simple moving-average filter in eq. (6.78) then corresponds to setting all of these weights to the same value, namely, 1/(N + M + 1) . However, by choosing these coefficients in other ways, we have considerable flexibility in adjusting the filter's frequency response. There are, in fact, a variety of techniques available for choosing the coefficients in eq. (6.80) so as to meet certain specifications on the filter, such as sharpening the transition band as much as possible for a filter of a given length (i.e., for N+M+ 1 fixed). These pro- cedures are discussed in detail in a number of texts, 3 and although we do not discuss the procedures here, it is worth emphasizing that they rely heavily on the basic concepts and tools developed in this book. To illustrate how adjustment of the coefficients can influence 3See, for example, R. W. Hamming, Digital Filters, 3rd ed. (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1989); A. V. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs, NJ: Prentice- Hall, Inc., 1989); and L. R. Rabiner and B. Gold, Theory and Application of Digital Signal Processing (Engle- wood Cliffs, NJ: Prentice-Hall, Inc., 1975). 478 Time and Frequency Characterization of Signals and Systems Chap.6 0 dB 3 -40 ~""~"""""""""""""""""""" """" """" """"""~""~""""""""""""~ ·~ I 0 (a) 0> _Q 0 -80 N -120 0 'IT/2 w 0 dB 3 -40 ~ I 0 (b) 0> _Q ~ -80 -120 0 'IT/2 w Figure 6.35 Log-magnitude plots for the moving-average filter of eqs. (6.78) and (6.79) for (a) M + N + 1 = 33 and (b) M + N + 1 = 65. the response ofthe filter, let us consider a filter of the form of eq. (6.80), with N = M = 16 and the filter coefficients chosen to be sin(27Tk/33) bk = { 7Tk ' lkl ~ 32 (6.81) 0, lkl > 32 Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 479 400 350 1\ 300 ~ 250 J lA 200 ,. J'lI f ""\ ,fV \1\ ,/ 150 \\ ... J'j/ - 100 v lA. .. '""\ ~ 50 I f'-.. ll 0 JM JM JM JM JM JM JM JM ~n ~n JM 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 (a) 400 350 I/\ ~ 300 250 I \~ .rV 200 1\ / '""\ _/ 150 v""' 100 \ - - 50 \ I'~ 0 ~n JM ~n JM JM ~n JM JM JM ~n JM 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 (b) 400 350 300 I/~ \ Figure 6.36 Effect of lowpass fil- I ~ tering on the Dow Jones weekly stock 250 ./v market index over a 10 -year period 200 1\ ,/ using moving-average filters: (a) weekly / index; (b) 51-day moving average ap- 150 / 100 """" ...... v"" plied to (a); (c) 201-day moving average applied to (a). The weekly '-- ../f"" stock market index and the two moving 50 averages are discrete-time sequences. For clarity in the graphical display, 0 the three sequences are shown here JM JM ~n JM JM ~n JM JM JM JM JM with their individual values connected 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 by straight lines to form a continuous (c) curve. 480 Time and Frequency Characterization of Signals and Systems Chap. 6 The impulse response of the filter is sin(21Tn/33) h[n] = 1rn ' lnl ::s 32 (6.82) { 0, lnl > 32 Comparing this impulse response with eq. (6.20), we see that eq. (6.82) corresponds to truncating, for lnl > 32, the impulse response for the ideal lowpass filter with cutoff fre- quency We = 211""133. In general, the coefficients bk can be adjusted so that the cutoff is at a desired fre- quency. For the example shown in Figure 6.37, the cutoff frequency was chosen to match approximately the cutoff frequency of Figure 6.35 for N = M = 16. Figure 6.37(a) shows the impulse response of the filter, and Figure 6.37(b) shows the log magnitude of the fre- quency response in dB. Comparing this frequency response to Figure 6.35, we observe that the passband of the filter has approximately the same width, but that the transition to h[n] 2 33 n (a) 20 0 dB 3 ·~ I -20 0 Ol .2 -40 0 N -60 -80 7T 37T 7T 7T 7T 7T 37T 7T -4 -16 0 8 T6 T6 8 T6 4 w (b) Figure 6.37 (a) Impulse response for the nonrecursive filter of eq. (6.82); (b) log magnitude of the frequency response of the filter. Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 481 the stopband is sharper. In Figures 6.38(a) and (b), the magnitudes (on a linear amplitude scale) of the two filters are shown for comparison. It should be clear from the compari- son of the two examples that, by the intelligent choice of the weighting coefficients, the transition band can be sharpened. An example of a higher order lowpass filter (N = M = 125), with the coefficients determined through a numerical algorithm referred to as the Parks-McClellan algorithm,4 is shown in Figure 6.39. This again illustrates the trade-off between the time and frequency domains: If we increase the length N + M + 1 of a filter, then, by a judicious choice of the filter coefficients in eq. (6.80), we can achieve sharper transition band behavior and a greater degree of frequency selectivity. An important property of the examples we have given is that they all have zero or lin- ear phase characteristics. For example, the phase of the moving-average filter of eq. (6 . 79) is w[(N- M)/2]. Also, since the impulse response in eq. (6.82) is real and even, the im- pulse response of the filter described by that equation is real and even, and thus has zero phase. From the symmetry properties of the Fourier transform of real signals, we know that any nonrecursive filter with an impulse response that is real and even will have a frequency response H(ei(u) that is real and even and, consequently, has zero phase. Such a filter, of course, is noncausal, since its impulse response h[n] has nonzero values for n < 0. However, if a causal filter is required, then a simple change in the impulse response can achieve this, resulting in a system with linear phase. Specifically, since h[n] is the impulse response of an FIR filter, it is identically zero outside a range of values centered at the origin w Figure 6.38 Comparison, on a linear amplitude scale, of the frequency w responses of (a) Figure 6.37 and (b) (b) Figure 6.35. 4A. Y. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs. NJ: Prentice- Hall, Inc., 1989), Chap. 7. 482 Time and Frequency Characterization of Signals and Systems Chap.6 0 ill -20 ~--~ 1J- -40 I-H~ 0.0615 1011 §, 0.020 3 -60 I- ~ 0.025 -~ ""0 0.070 c I -80 1- E o o.o5 o.1o 0 Cll dl Cll <1l ..2 -100 t-O.. 0 C\1 I IIIII 1111 II 1111 I I I II II II I I I -120 - -140 - -160 I I I I I I I I I I I I I I I I I I I (!) Figure 6.39 Lowpass nonrecursive filter with 251 coefficients designed to obtain the sharpest possible cutoff. (i.e., h[n] = 0 for all Jnl > N). If we now define the nonrecursive LTI system resulting from a simple N-step delay of h[n], i.e., h1 [n] = h[n - N], (6.83) then h 1 [n] = 0 for all n < 0, so that this LTI system is causal. Furthermore, from the time- shift property for discrete-time Fomier transforms, we see that the frequency response of the system is HI (ejw) = H(ejw)e- jwN_ (6.84) Since H(ejw) has zero phase, H 1( ejw) does indeed have linear phase. 6.8 SUMMARY In this chapter, we have built on the foundation of Fourier analysis of signals and systems developed in Chapters 3-5 in order to examine in more detail the characteristics of LTI systems and the effects they have on signals. In particular, we have taken a careful look at the magnitude and phase characteristics of signals and systems, and we have introduced log-magnitude and Bode plots for LTI systems. We have also discussed the impact of phase and phase distortion on signals and systems. This examination led us to understand the special role played by linear phase characteristics, which impart a constant delay at all frequencies and which, in tum, led to the concept of nonconstant group delay and disper- sion associated with systems having nonlinear phase characteristics. Using these tools and insights, we took another look at frequency-selective filters and the time-frequency trade- offs involved. We examined the properties of both ideal and non-ideal frequency-selective filters and saw that time-frequency considerations, causality constraints, and implemen- tation issues frequently make non -ideal filters, with transition bands and tolerance limits in the passbands and stopbands, the preferred choice."
6.8 Summary,"482 Time and Frequency Characterization of Signals and Systems Chap.6 0 ill -20 ~--~ 1J- -40 I-H~ 0.0615 1011 §, 0.020 3 -60 I- ~ 0.025 -~ ""0 0.070 c I -80 1- E o o.o5 o.1o 0 Cll dl Cll <1l ..2 -100 t-O.. 0 C\1 I IIIII 1111 II 1111 I I I II II II I I I -120 - -140 - -160 I I I I I I I I I I I I I I I I I I I (!) Figure 6.39 Lowpass nonrecursive filter with 251 coefficients designed to obtain the sharpest possible cutoff. (i.e., h[n] = 0 for all Jnl > N). If we now define the nonrecursive LTI system resulting from a simple N-step delay of h[n], i.e., h1 [n] = h[n - N], (6.83) then h 1 [n] = 0 for all n < 0, so that this LTI system is causal. Furthermore, from the time- shift property for discrete-time Fomier transforms, we see that the frequency response of the system is HI (ejw) = H(ejw)e- jwN_ (6.84) Since H(ejw) has zero phase, H 1( ejw) does indeed have linear phase. 6.8 SUMMARY In this chapter, we have built on the foundation of Fourier analysis of signals and systems developed in Chapters 3-5 in order to examine in more detail the characteristics of LTI systems and the effects they have on signals. In particular, we have taken a careful look at the magnitude and phase characteristics of signals and systems, and we have introduced log-magnitude and Bode plots for LTI systems. We have also discussed the impact of phase and phase distortion on signals and systems. This examination led us to understand the special role played by linear phase characteristics, which impart a constant delay at all frequencies and which, in tum, led to the concept of nonconstant group delay and disper- sion associated with systems having nonlinear phase characteristics. Using these tools and insights, we took another look at frequency-selective filters and the time-frequency trade- offs involved. We examined the properties of both ideal and non-ideal frequency-selective filters and saw that time-frequency considerations, causality constraints, and implemen- tation issues frequently make non -ideal filters, with transition bands and tolerance limits in the passbands and stopbands, the preferred choice. Chap. 6 Problems 483 We also examined in detail the time-frequency characteristics of first- and second- order systems in both continuous and discrete time. We noted in particular the trade-off between the response time of these systems and the frequency-domain bandwidth. Since first- and second-order systems are the building blocks for more complex, higher order LTI systems, the insights developed for those basic systems are of considerable use in practice. Finally, we presented several examples of LTI systems in order to illustrate many of the points developed in the chapter. In particular, we examined a simple model of an auto- mobile suspension system to provide a concrete example of the time-response-frequency- response concerns that drive system design in practice. We also considered several examples of discrete-time nonrecursive filters, ranging from simple moving-average filters to higher order FIR filters designed to have enhanced frequency selectivity. We saw, in addition, that FIR filters can be designed so as to have exactly linear phase. These examples, the development of the tools of Fourier analysis that preceded them, and the insights those tools provide illustrate the considerable value of the methods of Fourier analysis in analyzing and designing LTI systems. Chapter 6 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 6.1. Consider a continuous-time LTI system with frequency response H(jw) = IH(jw)l eJ<tHUw) and real impulse response h(t). Suppose that we apply an input x(t) = cos(w0 t +<Po) to this system. The resulting output can be shown to be of the form y(t) = Ax(t - to), where A is a nonnegative real number representing an amplitude-scaling factor and to is a time delay. (a) Express A in terms of iHUwo)i. (b) Express to in terms of <tH(jw0 ). 6.2. Consider a discrete-time LTI system with frequency response H(eiw) iH(eiw)jei<tH(elW) and real impulse response h[n]. Suppose that we apply the input x[n] = sin(w0 n + <Po) to this system. The resulting output can be shown to be of the form y[n] = iH(eiwo)ix[n - no], provided that <tH ( eiwo) and w 0 are related in a particular way. Determine this rela- tionship. 6.3. Consider the following frequency response for a causal and stable LTI system: . 1- jw H(jw) = --.- 1 + JW"
Problems,"Chap. 6 Problems 483 We also examined in detail the time-frequency characteristics of first- and second- order systems in both continuous and discrete time. We noted in particular the trade-off between the response time of these systems and the frequency-domain bandwidth. Since first- and second-order systems are the building blocks for more complex, higher order LTI systems, the insights developed for those basic systems are of considerable use in practice. Finally, we presented several examples of LTI systems in order to illustrate many of the points developed in the chapter. In particular, we examined a simple model of an auto- mobile suspension system to provide a concrete example of the time-response-frequency- response concerns that drive system design in practice. We also considered several examples of discrete-time nonrecursive filters, ranging from simple moving-average filters to higher order FIR filters designed to have enhanced frequency selectivity. We saw, in addition, that FIR filters can be designed so as to have exactly linear phase. These examples, the development of the tools of Fourier analysis that preceded them, and the insights those tools provide illustrate the considerable value of the methods of Fourier analysis in analyzing and designing LTI systems. Chapter 6 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 6.1. Consider a continuous-time LTI system with frequency response H(jw) = IH(jw)l eJ<tHUw) and real impulse response h(t). Suppose that we apply an input x(t) = cos(w0 t +<Po) to this system. The resulting output can be shown to be of the form y(t) = Ax(t - to), where A is a nonnegative real number representing an amplitude-scaling factor and to is a time delay. (a) Express A in terms of iHUwo)i. (b) Express to in terms of <tH(jw0 ). 6.2. Consider a discrete-time LTI system with frequency response H(eiw) iH(eiw)jei<tH(elW) and real impulse response h[n]. Suppose that we apply the input x[n] = sin(w0 n + <Po) to this system. The resulting output can be shown to be of the form y[n] = iH(eiwo)ix[n - no], provided that <tH ( eiwo) and w 0 are related in a particular way. Determine this rela- tionship. 6.3. Consider the following frequency response for a causal and stable LTI system: . 1- jw H(jw) = --.- 1 + JW 484 Time and Frequency Characterization of Signals and Systems Chap. 6 (a) Show that IH(jw )I = A, and determine the value of A. (b) Determine which of the following statements is true about T(w ), the group delay of the system. (Note: T(w) = -d( -{.H(jw ))ldw, where -{.H(jw) is expressed in a form that does not contain any discontinuities.) 1. T( w) = 0 for w > 0 2. T( w) > 0 for w > 0 3. T( w) < 0 for w > 0 6.4. Consider a linear-phase discrete-time LTI system with frequency response H(eiw) and real impulse response h[n]. The group delay function for such a system is defined as d . T(W) = - dw -{.H(elw), where -{.H(eiw) has no discontinuities. Suppose that, for this system, IH(ej""12 )1 = 2, 4:H(ej0 ) = 0, and T (I) = 2 Determine the output of the syst*em) for each of the following inputs: (a) cos(¥n) (b) sinC; n + 6.5. Consider a continuous-time ideal bandpass filter whose frequency response is H(1 ' w) = { 1, We :::; lwl :::; 3we . 0, elsewhere (a) If h(t) is the impulse response of this filter, determine a function g(t) such that h(t) = ein1T:ct )g(t). (b) As we is increased, does the impulse response of the filter get more concentrated or less concentrated about the origin? 6.6. Consider a discrete-time ideal highpass filter whose frequency response is specified as 7T -We :::; lwl :::; 7T lwl < 7T- We (a) If h[n] is the impulse response of this filter, determine a function g[n] such that h[n] = ei:,n )g[n]. (b) As we is increased, does the impulse response of the filter get more concentrated or less concentrated about the origin? 6. 7. A continuous-time lowpass filter has been designed with a passband frequency of 1,000 Hz, a stopband frequency of 1,200 Hz, passband ripple of 0.1, and stopband ripple of0.05. Let the impulse response of this lowpass filter be denoted by h(t). We wish to convert the filter into a bandpass filter with impulse response g(t) = 2h(t) cos(4,0007Tt). Chap. 6 Problems 485 Assuming that IH(jw)l is negligible for lwl > 4,0001T, answer the following ques- tions: (a) If the passband ripple for the bandpass filter is constrained to be 0.1, what are the two passband frequencies associated with the bandpass filter? (b) If the stopband ripple for the bandpass filter is constrained to be 0.05, what are the two stopband frequencies associated with the bandpass filter? 6.8. A causal, nonideallowpass filter is designed with frequency response H(ejw). The difference equation relating the input x[n] and output y[n] for this filter is specified as N M y[n] = L aky[n - k] + L bkx[n- k]. k=I k=O The filter also satisfies the following specifications for the magnitude of its fre- quency response: passband frequency = w P• passband tolerance = BP • stopband frequency = w s. stopband tolerance = Bs. Now consider a causal LTI system whose input and output are related by the differ- ence equation N M y[n] = L( -l)kaky[n- k] + L( -l)kbkx[n- k]. k=I k=O Show that this filter has a passband with a tolerance of Bp, and specify the corre- sponding location of the passband. 6.9. Consider a continuous-time causal and stable LTI system whose input x(t) and out- put y(t) are related by the differential equation ddy(tt) + 5y(t) = 2x(t). What is the final value s(oo) of the step response s(t) of this filter? Also, determine the value of to for which s(t0 ) = s(oo) [I - :2 J. 6.10. For each first-order system whose frequency response is as follows, specify the straight-line approximation of the Bode magnitude plot: (a) 40(j~+O.l) (b) 0.04(!w+SO) jw+40 jw+0.2 6.11. For each second-order system whose frequency response is as follows, specify the straight- line approximation of the Bode magnitude plot: ( a ) 250 (b) 0 02 jw+50 (jw)2+50.5jw+25 ' (jw)2+0.2jw+I 486 Time and Frequency Characterization of Signals and Systems Chap. 6 6.12. A continuous-time LTI systemS with frequency response H(jw) is constructed by cascading two continuous-time LTI systems with frequency responses H 1 (jw) and H2(jw), respectively. Figures P6.12(a) and P6.12(b) show the straight-line approx- imations of the Bode magnitude plots of H1 (jw) and H(jw ), respectively. Specify H2(jw). 24dB -20 dB/decade 8 10 40 10 0 w (rad/sec) (a) 20 log1 o I H(jw)l -20dB~----------~ -40 dB/decade 8 10 10 0 w (rad/sec) (b) Figure P6.12 6.13. The straight-line approximation of the Bode magnitude plot of a second-order continuous-time LTI systemS is shown in Figure P6.13. S may be constructed by 20 log1o I H(jw)l dB 1-------.......... { -20 dB/decad~ 6 I I -26 dB - - - - - - - - - -, - - - - - - - - - { -40 dB/decade 2 10 80 10 0 w (rad/sec) Figure P6. 1 3 Chap. 6 Problems 487 either connecting two first-order systems sl and s2 in cascade or two first-order systems S3 and S4 in parallel. Determine which, if any, of the following statements are true or false. Justify your answers. (a) The frequency responses of S1 and S2 may be determined uniquely. (b) The frequency responses of S3 and S4 may be determined uniquely. 6.14. The straight-line approximation of the Bode magnitude plot of a causal and stable continuous-time LTI system S is shown in Figure P6.14. Specify the frequency re- sponse of a system that is the inverse of S. 20 log10 I H(jw)l 94 dB \_ 80 dB 0 dB/decade 12 dB 1------~ 0.1 0.2 10 50 100 w (rad/sec) Figure P6. 14 6.15. For each of the following second-order differential equations for causal and sta- ble LTI systems, determine whether the corresponding impulse response is under- damped, overdamped, or critically damped: (a) d:i~~r) + 4 d~;~n + 4y(t) = x(t) (b) sd~i~;f) +4d~~~t) +5y(t) = 7x(t) (c) d;i~~t) + 20d~;;n + y(t) = x(t) (d) 5 d""y~t) + 4 dy(f) + Sy(t) = 7 x(t) + _!_ dx(t) dt- dt 3 dt 6.16. A particular first-order causal and stable discrete-time LTI system has a step re- sponse whose maximum overshoot is 50% of its final value. If the final value is 1, determine a difference equation relating the input x[n] and output y[n] of this filter. 6.17. For each of the following second-order difference equations for causal and stable LTI systems, determine whether or not the step response of the system is oscillatory: (a) y[n] + y[n- 1] + ~y[n- 2] = x[n] (b) y[n] - y[n- 1] + ~y[n- 2] = x[n] 6.18. Consider the continuous-time LTI system implemented as the RC circuit shown in Figure P6.18. The voltage source x(t) is considered the input to this system. The voltage y(t) across the capacitor is considered the system output. Is it possible for the step response of the system to have an oscillatory behavior? 488 Time and Frequency Characterization of Signals and Systems Chap.6 R x(t) + y(t) c Figure P6. 18 6.19. Consider the continuous-time LTI system implemented as the RLC circuit shown in Figure P6.19. The voltage source x(t) is considered the input to this system. The voltage y(t) across the capacitor is considered the system output. How should R, L, and C be related so that there is no oscillation in the step response? R L 1 x(t) + y(t) c j Figure P6. 19 6.20. Consider a nonrecursive filter with the impulse response shown in Figure P6.20. What is the group delay as a function of frequency for this filter? 3 4 1 • 4 1 - - - - 0 2 3 4 n Figure P6.20 BASIC PROBLEMS 6.21. A causal LTI filter has the frequency response H(jw) shown in Figure P6.21. For each of the input signals given below, determine the filtered output signal y(t). (a) x(t) = ejt (b) x(t) = (sinwot)u(t) (c) X(jw) = (jw)(~+ 1 jw) (d) X(jw) = 2+ jw Chap. 6 Problems 489 H(jw} w Figure P6.21 6.22. Shown in Figure P6.22(a) is the frequency response H(jw) of a continuous-time filter referred to as a lowpass differentiator. For each of the input signals x(t) below, determine the filtered output signal y(t). (a) x(t) = cos(21Tt + 0) (b) x(t) = cos(41Tt+O) (c) x(t) is a half-wave rectified sine wave of period, as sketched in Figure P6.22(b). sin21Tt, m:::; t:::; (m+ ~) x(t) = { 0, (m + ~) :::; t :::; m for any integer m IH (jw}l <tH (jw} (a) x(t) 0 j_ 2 (b) Figure P6.22 6.23. Shown in Figure P6.23 is IH(jw)l for a lowpass filter. Determine and sketch the impulse response of the filter for each of the following phase characteristics: (a) 1:-H(jw) = 0 (b) 1:-H(jw) = wT, where Tis a constant 490 Time and Frequency Characterization of Signals and Systems Chap.6 w Figure P6.23 w >0 (c) -9-.H(jw) = { ~~ w <0 2 ' 6.24. Consider a continuous-time lowpass filter whose impulse response h(t) is known to be real and whose frequency response magnitude is given as: IH(jw )I = { 1, Jw I ::; 2001T 0, otherwise (a) Determine and sketch the real-valued impulse response h(t) for this filter when the corresponding group delay function is specified as: (i) r(w) = 5 (ii) r(w) = ~ (iii) r(w) = -~ (b) If the impulse response h(t) had not been specified to be real, would knowl- edge of IH(jw )J and r(w) be sufficient to determine h(t) uniquely? Justify your answer. 6.25. By computing the group delay at two selected frequencies, verify that each of the following frequency responses has nonlinear phase. (a) H(jw) = Jwl+l (b) H(jw) = (Jw~l)2 (c) H(jw) = (Jw+l)l(jw+ 2) 6.26. Consider an ideal highpass filter whose frequency response is specified as 1 Jwl >we H(jw) = { 0,• otherwise· (a) Determine the impulse response h(t) for this filter. (b) As We is increased, does h(t) get more or less concentrated about the origin? (c) Determine s(O) and s(oo), where s(t) is the step response of the filter. 6.27. The output y(t) of a causal LTI system is related to the input x(t) by the differential equation dy(t) ----;[[ + 2y(t) = x(t). (a) Determine the frequency response H(. ) = Y(jw) JW X(jw) of the system, and sketch its Bode plot. (b) Specify, as a function of frequency, the group delay associated with this system. (c) If x(t) = e-t u(t), determine Y(jw ), the Fourier transform of the output. Chap. 6 Problems 491 (d) Using the technique of partial-fraction expansion, determine the output y(t) for the input x(t) in part (c). (e) Repeat parts (c) and (d), first if the input has as its Fourier transform (i) X(jw) = l+~w 2+jw' then if (ii) X(jw) = 2+jw l+jw' and finally, if (iii) X(jw) = (2+jw)l(l+jw) . 6.28. (a) Sketch the Bode plots for the following frequency responses: (i) 1 + (jw/10) (ii) 1 - (jw/10) (iii) 16 (iv) 1-(jw/10) (Jw+2)4 l+jw (v) (jw/10)-1 (vi) I +(jw/10) I+Jw I+Jw (vii) 1-(jw/10) (viii) IO+Sjw+ IO(jw)2 (jw)2+(jw)+l I +(jw/10) (ix) 1 + jw + (jw)2 (x) 1- jw + (jw)2 ( "") (jw+ 10)(10jw+ I) XI [(jw/100+ I )][((jw )2 + jw +I)] (b) Determine and sketch the impulse response and the step response for the sys- tem with frequency response (iv). Do the same for the system with frequency response (vi). The system given in (iv) is often referred to as a non-minimum-phase system, while the system specified in (vi) is referred to as being a minimum phase. The corresponding impulse responses of (iv) and (vi) are referred to as a non -minimum-phase signal and a minimum-phase signal, respectively. By comparing the Bode plots of these two frequency responses, we can see that they have identical magnitudes; however, the magnitude of the phase of the system of (iv) is larger than for the system of (vi). We can also note differences in the time-domain behavior of the two sys- tems. For example, the impulse response of the minimum-phase system has more of its energy concentrated near t = 0 than does the impulse response of the non-minimum-phase system. In addition, the step response of (iv) initially has the opposite sign from its asymptotic value as t ~ oo, while this is not the case for the system of (vi). The important concept of minimum- and non -minimum-phase systems can be extended to more general LTI systems than the simple first-order systems we have treated here, and the distinguishing characteristics of these systems can be described far more thoroughly than we have done. 6.29. An LTI system is said to have phase lead at a particular frequency w = w0 if <r..H(jw0 ) > 0. The terminology stems from the fact that if e.iwot is the input to this system, then the phase of the output will exceed, or lead, the phase of the input. Similarly, if <r..H(jw0 ) < 0, the system is said to have phase lag at this frequency. Note that the system with frequency response 1 1 + jw'T 492 Time and Frequency Characterization of Signals and Systems Chap.6 has phase lag for all w > 0, while the system with frequency response 1 + jwT has phase lead for all w > 0. (a) Construct the Bode plots for the following two systems. Which has phase lead and which phase lag? Also, which one amplifies signals at certain frequencies? (i) 1 +(jw/10) (ii) 1 + IOjw I+ IOjw I +(jw/10) (b) Repeat part (a) for the following three frequency responses: +(jw/10))2 ("") (1 (ii) I+ jw/10 (iii) 1 + 10jw I (1 + 10jw)3 100(jw)2+ 10jw+ I O.Ol(jw)2+0.2jw+ I 6.30. Let h(t) have the Bode plot depicted in Figure P6.30. The dashed lines in the figure represent straight-line approximations. Sketch the Bode plots for 1O h(l Ot). OdBt----........ ::: ] -20 :f -40 0 Ci -60 .Q ~ -80 -100 0.1 10 100 1000 w 0.1 10 100 1000 w Figure P6.30 6.31. An integrator has as its frequency response H(jw) = _;_ + 7T 8(w ), )W where the impulse at w = 0 is a result of the fact that the integration of a constant input from t = -oo results in an infinite output. Thus, if we avoid inputs that are Chap. 6 Problems 493 constant, or equivalently, only examine H(jw) for w > 0, we see that 20logiH(Jw)l = -20log(w), -7T <f-H(jw) = 2 . In other words, the Bode plot for an integrator, as illustrated in Figure P6.31, consists of two straight-line plots. These plots reflect the principal characteristics of an inte- grator: a phase shift of -90° at all positive values of frequency and the amplification of low frequencies. (a) A useful, simple model of an electric motor is an LTI system with input equal to the applied voltage and output given by the motor shaft angle. This system can be visualized as the cascade of a stable LTI system (with the voltage as input and shaft angular velocity as output) and an integrator (representing the integra- tion of the angular velocity). Often, a model of first-order system is used for the first part of the cascade. Assuming, for example, that this first -order system has a time constant of 0.1 second, we obtain an overall motor frequency response of 40 20 3 ~ OdB 0 Oi _Q -20 0 N -40 -60 0.01 0.1 10 100 1000 w 1 0 r- :f v -1T 2 -1T f- I I I I I I 0.01 0.1 10 100 1000 w Figure P6.31 494 Time and Frequency Characterization of Signals and Systems Chap.6 the form 1 H(jw) = jw(1 + jw/10) + 7T o(w ). Sketch the Bode plot for the system for w > 0.001. (b) Sketch the Bode plot for a differentiator. (c) Do the same for systems with the following frequency responses: (i) H(jw) = I+J:::noo (ii) H(jw) = (1 +(jw)/16:(jw)211QO) 6.32. Consider the system depicted in Figure P6.32. This ""compensator"" box is a continu- ous-time LTI system. (a) Suppose that it is desired to choose the frequency response of the compensator so that the overall frequency response H(jw) of the cascade satisfies the following two conditions: 1. The log magnitude of H(jw) has a slope of -40 dB/decade beyond w = 1,000. 2. For 0 < w < 1,000, the log magnitude of H(jw) should be between -10 dB and 10 dB. Design a suitable compensator (that is, determine a frequency response for a compensator that meets the preceding requirements), and draw the Bode plot for the resulting H (j w). (b) Repeat (a) if the specifications on the log magnitude of H(jw) are as follows: 1. It should have a slope of +20 dB/decade for 0 < w < 10. 2. It should be between+ 10 and +30 dB for 10 < w < 100. 3. It should have a slope of -20 dB/decade for 100 < w < 1,000. 4. It should have a slope of -40 dB/decade for w > 1,000. 1 x(t)-J Compensator J •I- I • y(t) jW +50 Figure P6.32 6.33. Figure P6.33 shows a system commonly used to obtain a highpass filter from a lowpass filter and vice versa. (a) Show that, if H(jw) is a lowpass filter with cutoff frequency w 1P, the overall system corresponds to an ideal highpass filter. Determine the system's cutoff frequency and sketch its impulse response. (b) Show that, if H(jw) is an ideal highpass filter with cutoff frequency whP' the overall system corresponds to an ideallowpass filter, and determine the cutoff frequency of the system. (c) If the interconnection of Figure P6.33 is applied to an ideal discrete-time low- pass filter, will the resulting system be an ideal discrete-time highpass filter? x(t) -~lt---!•~81---......;;~J;.__ y(t) Figure P6.33 Chap. 6 Problems 495 6.34. In Problem 6.33, we considered a system commonly used to obtain a highpass filter from a lowpass filter and vice versa. In this problem, we explore the system further and, in particular, consider a potential difficulty if the phase of H(jw) is not properly chosen. (a) Referring to Figure P6.33, let us assume that H(jw) is real and as shown in Figure P6.34. Then 1 - o1 < H(jw) < 1 + o1, 0 ::; w ::; w 1, -02 < H(jw) < +02, w2 < w. Determine and sketch the resulting frequency response of the overall system of Figure P6.33. Does the resulting system correspond to an approximation to a highpass filter? (b) Now let H(jw) in Figure P6.33 be of the form (P6.34-1) where H1 (jw) is identical to Figure P6.34 and O(w) is an unspecified phase characteristic. With H(jw) in this more general form, does it still correspond to an approximation to a lowpass filter? (c) Without making any assumptions about O(w ), determine and sketch the toler- ance limits on the magnitude of the frequency response of the overall system of Figure P6.33. (d) If H(jw) in Figure P6.33 is an approximation to a lowpass filter with unspec- ified phase characteristics, will the overall system in that figure necessarily correspond to an approximation to a highpass filter? H(jw} 1 + 01 ~c----._---..._....- 1 -51 1--,..__---~ Figure P6.34 6.35. Shown in Figure P6.35 is the frequency response H(eiw) of a discrete-time differ- entiator. Determine the output signal y[n] as a function of w 0 if the input x[n] is x[n] = cos[won + 0]. 496 Time and Frequency Characterization of Signals and Systems Chap.6 -1T 1T w - -1T w ~----~-~ --- Figure P6.35 6.36. Consider a discrete-time lowpass filter whose impulse response h[n] is known to be real and whose frequency response magnitude in the region -7r :::s w :::s 7T is given as: iwi:::; * otherwise· Determine and sketch the real-valued impulse response h[n] for this filter when the corresponding group delay function is specified as: (a) T( W) = 5 (b) T( W) = ~ (C) T( W) = - ~ 6.37. Consider a causal LTI system whose frequency response is given as: . . 1 - lejw H(elw) = e- JW 2 .. 1 - le- JW 2 (a) Show that iH(ejw)i is unity at all frequencies. (b) Show that 1:-H ( ejw) = - w - 2 tan- 1 ( ! s!i n w )· 1 - cosw (c) Show that the group delay for this filter is given by 3 T(w) = 4 i- cosw. Sketch T(w ). (d) What is the output of this filter when the input is cos( ~n)? 6.38. Consider an ideal bandpass filter whose frequency response in the region -7T ::; w :::s 7T is specified as 1T H(elw. ) = { 1' 2f- We :::; iwi :::; 2 +we 0, otherwise Chap. 6 Problems 497 Determine and sketch the impulse response h[n] for this filter when (a) We = ~ (b) We = * (c) We = ~ As we is increased, does h[n] get more or less concentrated about the origin? 6.39. Sketch the log magnitude and phase of each of the following frequency responses. (a) 1 + ~e- Jw (b) 1 + 2e- Jw (c) 1 - 2e- Jw (d) 1 + 2e- i 2w l (O l+~e-jw (e) (l + ~ r jw )3 l - ~ e- jw (g) 1+2e-;w (h) 1-2e-jw l+~e-jw 1+~e-Jw (i) 1 (j) 1 (l - ~ r jw )(l - ~ e- jw) (l- ~ r jw )(l + ~ e- jw) (k) 1+2e-Zjw (1-~e-iw)2 6.40. Consider an ideal discrete-time lowpass filter with impulse response h[n] and for which the frequency response H(eiw) is that shown in Figure P6.40. Let us consider obtaining a new filter with impulse response h 1 [n] and frequency response H 1( eiw) as follows: ht [n] = { h[n/2], n even 0, n odd This corresponds to inserting a sequence value of zero between each sequence value of h[n]. Determine and sketch H 1( eiw) and state the class of ideal filters to which it belongs (e.g., lowpass, highpass, bandpass, multiband, etc.). I H(eiw) I <l::H(eiw)=O ~ I ~ -2TI -TI -we We 1T 2TI w Figure P6.40 6.41. A particular causal LTI system is described by the difference equation j2 1 y[n]- Ty[n- 1] + y[n- 2j = x[n]- x[n- 1]. 4 (a) Find the impulse response of this system. (b) Sketch the log magnitude and the phase of the frequency response of the system. 6.42. (a) Consider two LTI systems with the following frequency responses: 498 Time and Frequency Characterization of Signals and Systems Chap.6 Show that both of these frequency responses have the same magnitude function [i.e., \H 1 (e.iw)\ = \H2(e.iw)\], but the group delay of H 2(e.iw) is greater than the group delay of H 1 (e.i(u) for w > 0. (b) Determine and sketch the impulse and step responses of the two systems. (c) Show that H2(ejw) = G(e.iw)HI (eiw), where G(e.i(u) is an all-pass s_vstem [i.e., \G(e.iw)\ = 1 for all w ]. 6.43. When designing filters with highpass or bandpass characteristics, it is often conve- nient first to design a lowpass filter with the desired passband and stopband specifi- cations and then to transform this prototype filter to the desired highpass or bandpass filter. Such transformations are called lowpass-to-highpass or highpass-to-lowpass transformations. Designing filters in this manner is convenient because it requires us only to formulate our filter design algorithms for the class of filters with low- pass characteristics. As one example of such a procedure, consider a discrete-time lowpass filter with impulse response h1P[n] and frequency response H 1p(eiw), as sketched in Figure P6.43. Suppose the impulse response is modulated with these- quence ( -1)11 to obtain hhp[n] = ( -1)11 h1p[n]. (a) Determine and sketch Hhp( e.iw) in terms of H 1p( e.iw ). Show in particular that, for H 1p(e.iw) as shown in Figure P6.43, Hhp(e.iw) corresponds to a highpass filter. (b) Show that modulation of the impulse response of a discrete-time high pass filter by ( -1 )11 will transform it to a lowpass filter. HtP (eiw) 1 ___J_____~~--c..__;h__L~_~L___L._ w 1T Figure P6.43 6.44. A discrete-time system is implemented as shown in Figure P6.44. The system S shown in the figure is an LTI system with impulse response h1p[n]. (a) Show that the overall system is time invariant. (b) If h1p[n] is a lowpass filter, what type of filter does the system of the figure implement? x[n] ·I s •~y[n] ·~ h1p[n] t (-1t (-1t Figure P6.44 Chap. 6 Problems 499 6.45. Consider the following three frequency responses for causal and stable third-order LTI systems. By utilizing the properties of first- and second-order systems dis- cussed in Section 6.6, determine whether or not the impulse response of each of the third-order systems is oscillatory. (Note: You should be able to answer this ques- tion without taking the inverse Fourier transforms of the frequency responses of the third-order systems.) 1 H1 (e.iw) = (1 - .!.e- .iw)(l - .!.e- .iw)(l - .!.e- .iw)' 2 3 4 1 H2(e.iw) = ---:---------=------:---- (1 + ~e- .iw)(l - .!.e- .iw)(1 - .!.e- .iw)' - 3 4 H3(ejw) = -----------,--1- -----,------- (1 - 4e- .iw)(l - ~e- jw + ~e- j2w) · 6.46. Consider a causal, nonrecursive (FIR) filter whose real-valued impulse response h[n] is zero for n ~ N. (a) Assuming that N is odd, show that if h[n] is symmetric about (N- 1)/2 (i.e., if h[(N- 1)/2 + n] = h[(N- 1)/2- n]), then H(e.iw) = A(w)e-j[(N-I)!2Jw, where A(w) is a real-valued function of w. We conclude that the filter has linear phase. (b) Give an example of the impulse response h[n] of a causal, linear-phase FIR filter such that h[n] = 0 for n ~ 5 and h[n] =/= 0 for 0 ::; n ::; 4. (c) Assuming that N is even, show that if h[n] is symmetric about (N- 1)/2 (i.e., if h[(N/2) + n] = h[N/2- n- 1]), then H(e.iw) = A(w)e-j[(N-1)12lw, where A(w) is a real-valued function of w. (d) Give an example of the impulse response h[n] of a causal, linear-phase FIR filter such that h[n] = 0 for n ~ 4 and h[n] =/= 0 for 0 ::; n ::; 3. 6.47. A three-point symmetric moving average, referred to as a weighted moving average, is of the form y[n] = b{ax[n - 1] + x[n] + ax[n + 1]}. (P6.47-1) (a) Determine, as a function of a and b, the frequency response H(e.iw) of the three- point moving average in eq. (P6.47-1). (b) Determine the scaling factor b such that H ( e.iw) has unity gain at zero frequency. (c) In many time-series analysis problems, a common choice for the coefficient a in the weighted moving average in eq. (P6.47-1) is a = 112. Determine and sketch the frequency response of the resulting filter. 6.48. Consider a four-point, moving-average, discrete-time filter for which the difference equation is y[n] = box[n] + b1 x[n- 1] + b2x[n- 2] + b3x[n- 2]. 500 Time and Frequency Characterization of Signals and Systems Chap.6 Determine and sketch the magnitude of the frequency response for each of the fol- lowing cases: (a) bo = b3 = 0, b1 = b2 (b) b1 = b2 = 0, bo = b3 (c) bo = b 1 = b2 = b3 (d) bo = -hi = b2 = -b3 ADVANCED PROBLEMS 6.49. The time constant provides a measure of how fast a first-order system responds to inputs. The idea of measuring the speed of response of a system is also important for higher order systems, and in this problem we investigate the extension of the time constant to such systems. (a) Recall that the time constant of a first-order system with impulse response h(t) = ae -at u(t), a > 0, is 1/a, which is the amount of time from t = 0 that it takes the system step response s(t) to settle within lie of its final value [i.e., s(oo) = limr~""""s(t)]. Using this same quantitative definition, find the equation that must be solved in order to determine the time constant of the causal LTI system described by the differential equation d2y(t) + 11 dy(t) 10 () = 9 ( ) dt2 dt + y t X t . (P6.49-1) (b) As can be seen from part (a), if we use the precise definition of the time constant set forth there, we obtain a simple expression for the time constant of a first- order system, but the calculations are decidedly more complex for the system of eq. (P6.49-1). However, show that this system can be viewed as the parallel interconnection of two first-order systems. Thus, we usually think of the system of eq. (P6.49-1) as having two time constants, corresponding to the two first- order factors. What are the two time constants for this system? (c) The discussion given in part (b) can be directly generalized to all systems with impulse responses that are linear combinations of decaying exponentials. In any system of this type, one can identify the dominant time constants of the system, which are simply the largest of the time constants. These represent the slowest parts of the system response, and consequently, they have the dominant effect on how fast the system as a whole can respond. What is the dominant time constant of the system of eq. (P6.49-1)? Substitute this time constant into the equation determined in part (a). Although the number will not satisfy the equation exactly, you should see that it nearly does, which is an indication that it is very close to the time constant defined in part (a). Thus, the approach we have outlined in part (b) and here is of value in providing insight into the speed of response of LTI systems without requiring excessive calculation. (d) One important use of the concept of dominant time constants is in the reduction of the order of LTI systems. This is of great practical significance in problems Chap. 6 Problems 501 s(t) ! Figure P6.49 involving the analysis of complex systems having a few dominant time con- stants and other very small time constants. In order to reduce the complexity of the model of the system to be analyzed, one often can simplify the fast parts of the system. That is, suppose we regard a complex system as a parallel in- terconnection of first- and second-order systems. Suppose also that one of these subsystems, with impulse response h(t) and step response s(t), is fast-that is, that s(t) settles to its final values(o o) very quickly. Then we can approximate this subsystem by the subsystem that settles to the same final value instantaneously. That is, if s(t) is the step response to our approximation, then s(t) = s(oo)u(t). This is illustrated in Figure P6.49. Note that the impulse response of the ap- proximate system is then h(t) = s(oo)o(t), which indicates that the approximate system is memory less. Consider again the causal LTI system described by eq. (P6.49-l) and, in particular, the representation of it as a parallel interconnection of two first -order systems, as described in part (b). Use the method just outlined to replace the faster of the two subsystems by a memory less system. What is the differential equation that then describes the resulting overall system? What is the frequency response of this system? Sketch IH(jw )I (not log IH(jw )I) and <r:.H(jw) for both the original and approximate systems. Over what range of frequencies are these frequency responses nearly equal? Sketch the step responses for both systems. Over what range of time are the step responses nearly equal? From your plots, you will see some of the similarities and differences between the original sys- tem and its approximation. The utility of an approximation such as this de- pends upon the specific application. In particular, one must take into account both how widely separated the different time constants are and also the nature of the inputs to be considered. As you will see from your answers in this part of the problem, the frequency response of the approximate system is essentially the same as the frequency response of the original system at low frequencies. That is, when the fast parts of the system are sufficiently fast compared to the rate of fluctuation of the input, the approximation becomes useful. 502 Time and Frequency Characterization of Signals and Systems Chap.6 6.50. The concepts associated with frequency-selective filtering are often used to sepa- rate two signals that have been added together. If the spectra of the two signals do not overlap, ideal frequency-selective filters are desirable. However, if the spectra overlap, it is often preferable to design the filter to have a gradual transition between passband and stopband. In this problem, we explore one approach for determining the frequency response of a filter to be used for separating signals with overlapping spectra. Let x(t) denote a composite continuous-time signal consisting of the sum of two signals s(t) + w(t). As indicated in Figure P6.50(a), we would like to design an LTI filter to recover s(t) from x(t). The filter's frequency response H(jw) is to be chosen so that, in some sense, y(t) is a ""good"" approximation to s(t). Let us define a measure of the error between y(t) and s(t) at each frequency w as E(w) ~ IS(jw) - Y(jw )1 2, where S(jw) and Y(jw) are the Fourier transforms of s(t) and y(t), respectively. (a) Express E(w) in terms of S(jw), H(jw), and W(jw), where W(jw) is the Fourier transform of w(t). (b) Let us restrictH(jw) to be real, so thatH(jw) = H*(jw ). By setting the deriva- tive of E(w) with respect to H(jw) to be zero, determine the H(jw) required to minimize the error E( w ). (c) Show that if the spectra of S(jw) and W(jw) are non-overlapping, the result in part (b) reduces to an ideal frequency-selective filter. (d) From your result in part (b), determine and sketch H(jw) if S(jw) and W(jw) are as shown in Figure P6.50(b ). x(t) ~ s(t) + w(t)-8-- y(t) (a) S(jw) 11 -2 2 w W(jw) 1+ I -1 w (b) Figure P6.50 Chap. 6 Problems 503 6.51. An ideal bandpass filter is a bandpass filter that passes only a range of frequencies, without any change in amplitude or phase. As shown in Figure P6.5l(a), let the passband be w w wo - 2 :::; lwl :::; wo + 2· (a) What is the impulse response h(t) of this filter? (b) We can approximate an ideal bandpass filter by cascading a first-order lowpass and a first-order highpass filter, as shown in Figure P6.5l(b). Sketch the Bode diagrams for each of the two filters H 1( jw) and H 2(jw ). (c) Determine the Bode diagram for the overall bandpass filter in terms of your results from part (b). w (a) x(t)-[~~}----~y(t) H ("" ) 103 H ("" ) jw 1 jw = 103 +jw 2 jw = 100+jw (b) Figure P6.51 6.52. In Figure P6.52(a), we show the magnitude of the frequency response for an ideal continuous-time differentiator. A nonideal differentiator would have a frequency response that is some approximation to the frequency response in the figure. (a) Consider a nonideal differentiator with frequency response G(jw) for which IG(jw )I is constrained to be within ± 10% of the magnitude of the frequency response of the ideal differentiator at all frequencies; that is, -O.liH(jw)l :::; [IG(jw)I-IH(jw)IJ :::; O.liH(jw)l. Sketch the region in a plot of G(jw) vs. w where IG(jw )I must be confined to meet this specification. 504 Time and Frequency Characterization of Signals and Systems Chap.6 I H (jw) I I H (jw) I= lwl ~peo1 w (a) Multiplication by 1fT y(t) =tw(t) Ideal delay Tsec. ....__ __ __. x(t-T) (b) Figure P6.52 (b) The system in Figure P6.52(b), incorporating an ideal delay ofT seconds, is sometimes used to approximate a continuous-time differentiator. ForT = 10-2 second, determine the frequency range over which the magnitude of the fre- quency response of the system in the figure is within ± 10% of that for an ideal differentiator. 6.53. In many filtering applications, it is often undesirable for the step response of a filter to overshoot its final value. In processing pictures, for example, the overshoot in the step response of a linear filter may produce flare-that is, an increase in intensity- at sharp boundaries. It is possible, however, to eliminate overshoot by requiring that the impulse response of the filter be positive for all time. Show that if h(t), the impulse response of a continuous-time LTI filter, is al- ways greater than or equal to zero, the step response of the filter is a monotonically nondecreasing function and therefore will not have overshoot. 6.54. By means of a specific filter design procedure, a nonideal continuous-time lowpass filter with frequency response H0(jw ), impulse response h0(t), and step response s0(t) has been designed. The cutoff frequency of the filter is at w = 27T X 102 rad/sec, and the step response rise time, defined as the time required for the step response to go from 10% of its final value to 90% of its final value, is Tr = 10-2 second. From this design, we can obtain a new filter with an arbitrary cutoff fre- quency we by the use of frequency scaling. The frequency response of the resulting filter is then of the form Hip(jw) = Ho(jaw ), where a is an appropriate scale factor. (a) Determine the scale factor a such that H1p(jw) has a cutoff frequency of We. Chap. 6 Problems 505 (b) Determine the impulse response h1p(t) of the new filter in terms of We and h0 (t). (c) Determine the step response s1p(t) of the new filter in terms of we and s0(t). (d) Determine and sketch the rise time of the new filter as a function of its cutoff frequency we. This is one illustration of the trade-off between time-domain and frequency-domain characteristics. In particular, as the cutoff frequency de- creases, the rise time tends to increase. 6.55. The square of the magnitude of the frequency response of a class of continuous-time lowpass filters, known as Butterworth filters, is . 12 1 IB (jw) = 1 + (w/wc)2N. Let us define the passband edge frequency w P as the frequency below which IB(jw )1 2 is greater than one-half of its value at w = 0; that is, Now let us define the stopband edge frequency Ws as the frequency above which IB(jw )1 2 is less than 10-2 of its value at w = 0; that is, The transition band is then the frequency range between w P and w 5 • The ratio wsfw P is referred to as the transition ratio. For fixed w P' and making reasonable approximations, deteimine and sketch the transition ratio as a function of N for the class of Butterworth filters. 6.56. In this problem, we explore some of the filtering issues involved in the commercial version of a typical system that is used in most modem cassette tape decks to reduce noise. The primary source of noise is the high-frequency hiss in the tape playback process, which, in some part, is due to the friction between the tape and the playback head. Let us assume that the noise hiss that is added to the signal upon playback has the spectrum of Figure P6.56(a) when measured in decibels, with 0 dB equal to the signal level at 100Hz. The spectrum S(jw) of the signal has the shape shown in Figure P6.56(b ). The system that we analyze has a filter H 1 (jw) which conditions the signal s(t) before it is recorded. Upon playback, the hiss n(t) is added to the signal. The system is represented schematically in Figure P6.56(c). Suppose we would like our overall system to have a signal-to-noise ratio of 40 dB over the frequency range 50 Hz< w/27T <20kHz. (a) Determine the transfer characteristic of the filter H 1( jw ). Sketch the Bode plot of H 1(jw). (b) If we were to listen to the signal p(t), assuming that the playback process does nothing more than add hiss to the signal, how do you think it would sound? (c) What should the Bode plot and transfer characteristic of the filter H 2(jw) be in order for the signal s(t) to sound similar to s(t)? 506 Time and Frequency Characterization of Signals and Systems Chap.6 20 log1 0 I N(jw) I -28dBf / -40dB ~~---· ~I I 5kHz 10kHz 20kHz w f = 21T (a) 20 log10 I S(jw) I ~: :: fl--'--~- '----'------------'------'--'""""- 50 100 1kHz 10kHz 20kHz (b) n(t) p(t) s(t) (c) Figure P6.56 6.57. Show that if h[n], the impulse response of a discrete-time LTI filter, is always greater than or equal to zero, the step response of the filter is a monotonically nondecreasing function and therefore will not have overshoot. 6.58. In the design of either analog or digital filters, we often approximate a specified magnitude characteristic without particular regard to the phase. For example, stan- dard design techniques for lowpass and bandpass filters are typically derived from a consideration of the magnitude characteristics only. In many filtering problems, one would ideally like the phase characteristics to be zero or linear. For causal filters, it is impossible to have zero phase. However, for many digital filtering applications, it is not necessary that the unit sample response of the filter be zero for n < 0 if the processing is not to be carried out in real time. One technique commonly used in digital filtering when the data to be filtered are of finite duration and stored, for example, on a disc or magnetic tape is to process the data forward and then backward through the same filter. Let h[n] be the unit sample response of a causal filter with an arbitrary phase characteristic. Assume that h[n] is real, and denote its Fourier transform by H(ejw). Let x[n] be the data that we want to filter. The filtering operation is performed as follows: Chap. 6 Problems 507 x[n]----!~&- g[n] g[-n]----!~&- r[n] s[n] = r[-n] (a) x[n] &-g[n] x[-n] &-r[n] (b) 1I 3TI 'TT W 4 4 (c) Figure P6.58 (a) Method A: Process x[n] to get s[n], as indicated in Figure P6.58(a). 1. Determine the overall unit sample response h1 [n] that relates x[n] and s[n], and show that it has zero phase characteristic. 2. Determine IH1( eiw)l and express it in terms of IH(eiw)l and <r:H(eiw). (b) Method B: Process x[n] through the filter h[n] to get g[n] [Figure P6.58(b)]. Also, process x[n] backward through h[n] to get r[n]. The output y[n] is taken to be the sum of g[n] and r[ -n]. The composite set of operations can be repre- sented by a filter with input x[n], output y[n], and unit sample response h2 [n]. 1. Show that the composite filter h2 [n] has zero phase characteristic. 2. Determine IH2(eiw)1, and express it in terms of IH(eiw)l and <r:H(eiw). (c) Suppose that we are given a sequence of finite duration on which we would like to perform bandpass, zero-phase filtering. Furthermore, assume that we 508 Time and Frequency Characterization of Signals and Systems Chap.6 are given the bandpass filter h[n] with frequency response as specified in Fig- ure P6.58(c) and with magnitude cparacteristic that we desire, but with linear phase. To achieve zero phase, we could use either of the preceding methods, A or B. Determine and sketch IH1 (e.iw)l and IH2(e.iw)1. From these results, which method would you use to achieve the desired bandpass filtering operation? Ex- plain why. More generally, if h[n] has the desired magnitude, but a nonlinear phase characteristic, which method is preferable to achieve a zero phase char- acteristic? 6.59. Let hc~[n] denote the unit sample response of a desired ideal system with frequency response Hc~(e.iw), and let h[n] denote the unit sample response for an FIR system of length Nand with frequency response H(e.iw). In this problem, we show that a rectangular window of length N samples applied to he~ [ n] will produce a unit sample response h[n] such that the mean square error is minimized. (a) The error function E(e.iw) = Hc~(e.iw)- H(e.iw) can be expressed as the power series n=-x Find the coefficients e[n] in terms of hc~[n] and h[n]. (b) Using Parsevars relation, express the mean square error E 2 in terms of the co- efficients e[n]. (c) Show that for a unit sample response h[n] of length N samples, E 2 is minimized when h[n] = { hc~[n], O~n~N-1 0, otherwise That is, simple truncation gives the best mean square approximation to a desired frequency response for a fixed value of N. 6.60. In Problem 6.50, we considered one specific criterion for determining the frequency response of a continuous-time filter that would recover a signal from the sum of two signals when their spectra overlapped in frequency. For the discrete-time case, develop the result corresponding to that obtained in part (b) of Problem 6.50. 6.61. In many situations we have available an analog or digital filter module, such as a basic hardware element or computer subroutine. By using the module repetitively or by combining identical modules, it is possible to implement a new filter with improved passband or stopband characteristics. In this and the next problem, we consider two procedures for doing just that. Although the discussion is phrased in terms of discrete-time filters, much of it applies directly to continuous-time filters as well. Chap. 6 Problems 509 I H (ei""') I 1 + 01 1-------i 1 -01 ~----------- 1T w Figure P6.61 Consider a lowpass filter with frequency response H(eiw) for which IH(e.iw)l falls within the tolerance limits shown in Figure P6.61; that is, 1 - 81 :S IH(eiw)l :S 1 + 81, 0 :S W :S w1, 0 :S IH(ejw )I :S 82, w2 :S w :S 7T. A new filter with frequency response G(eiw) is formed by cascading two identical filters, both with frequency response H(eiw ). (a) Determine the tolerance limits on IG(eiw)l. (b) Assuming that H ( eiw) is a good approximation to a lowpass filter, so that o1 < < 1 and 82 << 1, determine whether the passband ripple for G(eiw) is larger or smaller than the passband ripple for H(e.iw). Also, determine whether the stop- band ripple for G(e.iw) is larger or smaller than the stopband ripple for H(ei(u). (c) If N identical filters with frequency response H(eiw) are cascaded to obtain a new frequency response G( eiw ), then, again assuming that o1 < < 1 and 82 < < I, determine the approximate tolerance limits on IG(e.iw)l. 6.62. In Problem 6.61, we considered one method for using a basic filter module repcti tively to implement a new filter with improved characteristics. Let us now con""idet an alternative approach, proposed by J. W. Tukey in the book, Explorator."" I >uru Analysis (Reading, MA: Addison-Wesley Publishing Co., Inc., 1976). The pwce- dure is shown in block diagram form in Figure P6.62(a). (a) Suppose that H ( eiw) is real and has a passband ripple of :±: o1 and d stopband ripple of :±:82 (i.e., H(e.iw) falls within the tolerance limits indicated in Fig- ure P6.62(b)). The frequency response G(eiw) of the overall system in Figure P6.62(a) falls within the tolerance limits indicated in Figure P6.62(c). Deter- mine A, B, C, and Din terms of 81 and 82. (b) If o1 < < I and 82 < < 1, what is the approximate passband ripple and stopband ripple associated with G(eiw)? Indicate in particular whether the passband rip- ple for G(e.i(u) is larger or smaller than the passband ripple for H(eiw). Also, indicate whether the stopband ripple for G(eiw) is larger or smaller than the stopband ripple for H(e.iw). (c) In parts (a) and (b), we assumed that H(eiw) is real. Now consider H(e.iw) to have the more general form H(ejw) = Hl(e.iw)eje(wl, where H 1( ei«J) is real and O(w) is an unspecified phase characteristic. If IH(e.iw)l 510 Time and Frequency Characterization of Signals and Systems Chap.6 g[n] x[n] _""""'!""""""_...,_...,.~I H(eiw) y[n] - I I_ -- - -- - --- - - --- - - - - - - --- - - - -- - _I (a) Wp I Ws 111' W ~-----.....,j (b) AI------ A ::::; G (eiw) :S B 0 :S w :S wp C :S G (eiw) :S D w5 :S w ::::; 11' 81-----...., c.....----.... I w5 o'-----• (c) Figure P6.62 is a reasonable approximation to an ideallowpass filter, williG(eiw)l necessarily be a reasonable approximation to an ideallowpass filter? (d) Now assume that H(efw) is an FIR linear-phase lowpass filter, so that H(eiw) = HI (efw)efMw, Chap. 6 Problems 511 where H 1( eiw) is real and M is an integer. Show how to modify the system in Figure P6.62(a) so that the overall system will approximate a lowpass filter. 6.63. In the design of digital filters, we often choose a filter with a specified magnitude characteristic that has the shortest duration. That is, the impulse response, which is the inverse Fourier transform of the complex frequency spectrum, should be as narrow as possible. Assuming that h[n] is real, we wish to show that if the phase (}(w) associated with the frequency response H(eiw) is zero, the duration of the impulse response is minimal. Let the frequency response be expressed as and let us consider the quantity n =-ex n= -ex to be a measure of the duration of the associated impulse response h[n]. (a) Using the derivative property of the Fourier transform and Parseval's relation, express Din terms of H(eiw). (b) By expressing H(eiw) in terms of its magnitude IH(eiw)l and phase (}(w ), use your result from part (a) to show that Dis minimized when (}(w) = 0. 6.64. For a discrete-time filter to be causal and to have exactly linear phase, its impulse response must be of finite length and consequently the difference equation must be nonrecursive. To focus on the insight behind this statement, we consider a particular case, that of a linear phase characteristic for which the slope of the phase is an integer. Thus, the frequency response is assumed to be of the form (P6.64-1) where Hr(eiw) is real and even. Let h[n] denote the impulse response of the filter with frequency response H(eiw) and let hr[n] denote the impulse response of the filter with frequency re- sponse Hr(eiw). (a) By using the appropriate properties in Table 5.1, show that: 1. hr[n] = hr[- n] (i.e., hr[n] is symmetric about n = 0). 2. h[n] = hr[n - M]. (b) Using your result in part (a), show that with H(eiw) of the form shown in eq. (P6.64-1), h[n] is symmetric about n = M, that is, h[M + n] = h[M - n]. (P6.64-2) (c) According to the result in part (b), the linear phase characteristic in eq. (P6.64-1) imposes a symmetry in the impulse response. Show that if h[n] is causal and has the symmetry in eq. (P6.64-2), then h[n] = 0, n < 0 and n >2M (i.e., it must be of finite length). 512 Time and Frequency Characterization of Signals and Systems Chap.6 6.65. For a class of discrete-time lowpass filters, known as Butterworth filters, the squared magnitude of the frequency response is given by jB(efw)l2 = 1 2N' 1 + ( tan(w/2) ) tan(wJ2) where We is the cutoff frequency (which we shall take to be 'TT'/2) and N is the order of the filter (which we shall consider to beN = 1). Thus, we have B(efw )j2 = 1 1 + tan2(w/2) · ' (a) Using trigonometric identities, show that IB(efw)l2 = cos2(w/2). (b) Let B(eiw) = acos(w/2). For what complex values of a is jB(efw)l2 the same as in part (a)? (c) Show that B(efw) from part (b) is the transfer function corresponding to a dif- ference equation of the form y[n] = ax[n] + f3x[n- y]. Determine a, f3, and 'Y. 6.66. In Figure P6.66(a) we show a discrete-time system consisting of a parallel combi- nation of N LTI filters with impulse response hk[n], k = 0, 1, · · ·, N- 1. For any k, hk[n] is related to h0 [n] by the expression hk[n] = ei(27Tnk!N) ho[n]. (a) If h0 [n] is an ideal discrete-time lowpass filter with frequency response Ho( efw) as shown in Figure P6.66(b), sketch the Fourier transforms of h1 [n] and hN-I [n] for win the range -'TT' < w :::; +'TT'. Yo[n] N-1 x[n] y[n] = !. Yk[n] k=O Figure P6.66a Chap. 6 Problems 513 1T w Figure P6.66b (b) Determine the value of the cutoff frequency We in Figure P6.66(b) in terms of N (0 < we ~ 7T) such that the system of Figure P6.66(a) is an identity system; that is, y[n] = x[n] for all nand any input x[n]. (c) Suppose that h[n] is no longer restricted to be an ideallowpass filter. If h[n] denotes the impulse response of the entire system in Figure P6.66(a) with input x[n] and output y[n], then h[n] can be expressed in the form h[n] = r[n]ho[n]. Determine and sketch r[ n]. (d) From your result of part (c), determine a necessary and sufficient condition on h0 [n] to ensure that the overall system will be an identity system (i.e., such that for any input x[n], the output y[n] will be identical to x[n]). Your answer should not contain any sums. 7 SAMPLING 7.0 INTRODUCTION Under certain conditions, a continuous-time signal can be completely represented by and recoverable from knowledge of its values, or samples, at points equally spaced in time. This somewhat surprising property follows from a basic result that is referred to as the sampling theorem. This theorem is extremely important and useful. It is exploited, for example, in moving pictures, which consist of a sequence of individual frames, each of which represents an instantaneous view (i.e., a sample in time) of a continuously changing scene. When these samples are viewed in sequence at a sufficiently fast rate, we perceive an accurate representation of the original continuously moving scene. As another example, printed pictures typically consist of a very fine grid of points, each corresponding to a sample of the spatially continuous scene represented in the picture. If the samples are sufficiently close together, the picture appears to be spatially continuous, although under a magnifying glass its representation in terms of samples becomes evident. Much of the importance of the sampling theorem also lies in its role as a bridge between continuous-time signals and discrete-time signals. As we will see in this chapter, the fact that under certain conditions a continuous-time signal can be completely recovered from a sequence of its samples provides a mechanism for representing a continuous-time signal by a discrete-time signal. In many contexts, processing discrete-time signals is more flexible and is often preferable to processing continuous-time signals. This is due in large part to the dramatic development of digital technology over the past few decades, result- ing in the availability of inexpensive, lightweight, programmable, and easily reproducible discrete-time systems. The concept of sampling, then, suggests an extremely attractive and widely employed method for using discrete-time system technology to implement continuous-time systems and process continuous-time signals: We exploit sampling to 514"
7 Sampling,"7 SAMPLING 7.0 INTRODUCTION Under certain conditions, a continuous-time signal can be completely represented by and recoverable from knowledge of its values, or samples, at points equally spaced in time. This somewhat surprising property follows from a basic result that is referred to as the sampling theorem. This theorem is extremely important and useful. It is exploited, for example, in moving pictures, which consist of a sequence of individual frames, each of which represents an instantaneous view (i.e., a sample in time) of a continuously changing scene. When these samples are viewed in sequence at a sufficiently fast rate, we perceive an accurate representation of the original continuously moving scene. As another example, printed pictures typically consist of a very fine grid of points, each corresponding to a sample of the spatially continuous scene represented in the picture. If the samples are sufficiently close together, the picture appears to be spatially continuous, although under a magnifying glass its representation in terms of samples becomes evident. Much of the importance of the sampling theorem also lies in its role as a bridge between continuous-time signals and discrete-time signals. As we will see in this chapter, the fact that under certain conditions a continuous-time signal can be completely recovered from a sequence of its samples provides a mechanism for representing a continuous-time signal by a discrete-time signal. In many contexts, processing discrete-time signals is more flexible and is often preferable to processing continuous-time signals. This is due in large part to the dramatic development of digital technology over the past few decades, result- ing in the availability of inexpensive, lightweight, programmable, and easily reproducible discrete-time systems. The concept of sampling, then, suggests an extremely attractive and widely employed method for using discrete-time system technology to implement continuous-time systems and process continuous-time signals: We exploit sampling to 514"
7.0 Introduction,"7 SAMPLING 7.0 INTRODUCTION Under certain conditions, a continuous-time signal can be completely represented by and recoverable from knowledge of its values, or samples, at points equally spaced in time. This somewhat surprising property follows from a basic result that is referred to as the sampling theorem. This theorem is extremely important and useful. It is exploited, for example, in moving pictures, which consist of a sequence of individual frames, each of which represents an instantaneous view (i.e., a sample in time) of a continuously changing scene. When these samples are viewed in sequence at a sufficiently fast rate, we perceive an accurate representation of the original continuously moving scene. As another example, printed pictures typically consist of a very fine grid of points, each corresponding to a sample of the spatially continuous scene represented in the picture. If the samples are sufficiently close together, the picture appears to be spatially continuous, although under a magnifying glass its representation in terms of samples becomes evident. Much of the importance of the sampling theorem also lies in its role as a bridge between continuous-time signals and discrete-time signals. As we will see in this chapter, the fact that under certain conditions a continuous-time signal can be completely recovered from a sequence of its samples provides a mechanism for representing a continuous-time signal by a discrete-time signal. In many contexts, processing discrete-time signals is more flexible and is often preferable to processing continuous-time signals. This is due in large part to the dramatic development of digital technology over the past few decades, result- ing in the availability of inexpensive, lightweight, programmable, and easily reproducible discrete-time systems. The concept of sampling, then, suggests an extremely attractive and widely employed method for using discrete-time system technology to implement continuous-time systems and process continuous-time signals: We exploit sampling to 514 Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 51 s convert a continuous-time signal to a discrete-time signal, process the discrete-time signal using a discrete-time system, and then convert back to continuous time. In the following discussion, we introduce and develop the concept of sampling and the process of reconstructing a continuous-time signal from its samples. In this discus- sion, we both identify the conditions under which a continuous-time signal can be exactly reconstructed from its samples and examine the consequences when these conditions are not satisfied. Following this, we explore the processing of continuous-time signals that have been converted to discrete-time signals through sampling. Finally, we examine the sampling of discrete-time signals and the related concepts of decimation and interpola- tion. 7.1 REPRESENTATION OF A CONTINUOUS-TIME SIGNAL BY ITS SAMPLES: THE SAMPLING THEOREM In general, in the absence of any additional conditions or information, we would not expect that a signal could be uniquely specified by a sequence of equally spaced samples. For example, in Figure 7.1 we illustrate three different continuous-time signals, all of which have identical values at integer multiples ofT; that is, Clearly, an infinite number of signals can generate a given set of samples. As we will see, however, if a signal is band limited-i.e., if its Fourier transform is zero outside a finite band of frequencies-and if the samples are taken sufficiently close together in relation to the highest frequency present in the signal, then the samples uniquely specify the signal, and we can reconstruct it perfectly. This result, known as the sampling theorem, is of profound importance in the practical application of the methods of signal and system analysis. Figure 7.1 Three continuous-time signals with identical values at integer multiples of T."
7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem,"Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 51 s convert a continuous-time signal to a discrete-time signal, process the discrete-time signal using a discrete-time system, and then convert back to continuous time. In the following discussion, we introduce and develop the concept of sampling and the process of reconstructing a continuous-time signal from its samples. In this discus- sion, we both identify the conditions under which a continuous-time signal can be exactly reconstructed from its samples and examine the consequences when these conditions are not satisfied. Following this, we explore the processing of continuous-time signals that have been converted to discrete-time signals through sampling. Finally, we examine the sampling of discrete-time signals and the related concepts of decimation and interpola- tion. 7.1 REPRESENTATION OF A CONTINUOUS-TIME SIGNAL BY ITS SAMPLES: THE SAMPLING THEOREM In general, in the absence of any additional conditions or information, we would not expect that a signal could be uniquely specified by a sequence of equally spaced samples. For example, in Figure 7.1 we illustrate three different continuous-time signals, all of which have identical values at integer multiples ofT; that is, Clearly, an infinite number of signals can generate a given set of samples. As we will see, however, if a signal is band limited-i.e., if its Fourier transform is zero outside a finite band of frequencies-and if the samples are taken sufficiently close together in relation to the highest frequency present in the signal, then the samples uniquely specify the signal, and we can reconstruct it perfectly. This result, known as the sampling theorem, is of profound importance in the practical application of the methods of signal and system analysis. Figure 7.1 Three continuous-time signals with identical values at integer multiples of T. 516 Sampling Chap. 7 7. 1 . 1 Impulse-Train Sampling In order to develop the sampling theorem, we need a convenient way in which to represent the sampling of a continuous-time signal at regular intervals. A useful way to do this is through the use of a periodic impulse train multiplied by the continuous-time signal x(t) that we wish to sample. This mechanism, known as impulse-train sampling, is depicted in Figure 7 .2. The periodic impulse train p(t) is referred to as the sampling function, the period T as the sampling period, and the fundamental frequency of p(t), Ws = 27r/T, as the sampling frequency. In the time domain, Xp(t) = x(t)p(t), (7.1) where +oc p(t) = 2, D(t - nT). (7.2) n= -oc Because of the sampling property of the unit impulse discussed in Section 1.4.2, we know that multiplying x(t) by a unit impulse samples the value of the signal at the point at which the impulse is located; i.e., x(t)D(t- to) = x(t0 )B(t- t0 ). Applying this to eq. (7.1), we see, as illustrated in Figure 7.2, that Xp(t) is an impulse train with the amplitudes of p(t) x(t) --.•.- --f~...,._--1•~ xp(t) 0 1-r-j p(t) 1 t t 1 t t t t 0 / / 0 Figure 7.2 Impulse-train sampling. Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 517 the impulses equal to the samples of x(t) at intervals spaced by T; that is, +oc Xp(t) = L x(nT)8(t - nT). (7.3) n= -oo From the multiplication property (Section 4.5), we know that X/}w) = -}J +x X(j8)P(j(w - 8))d8. (7.4) 27T -X and from Example 4.8, 2 L+oc P(jw) = ; 8(w - kws). (7.5) k= -00 Since convolution with an impulse simply shifts a signal [i.e., X(jw) * 8(w - w 0 ) = X(j(w - w0))], it follows that 1 +oc Xp(jw) = T L X(j(w - kws)). (7.6) k= -00 That is, Xp(jw) is a periodic function of w consisting of a superposition of shifted replicas of X(jw), scaled by 1/T, as illustrated in Figure 7.3. In Figure 7.3(c), WM < (ws- wM), or equivalently, Ws > 2wM, and thus there is no overlap between the shifted replicas of X(jw ), whereas in Figure 7.3(d), with Ws < 2wM, there is overlap. For the case illustrated in Figure 7.3(c), X(jw) is faithfully reproduced at integer multiples of the sampling fre- quency. Consequently, if Ws > 2wM, x(t) can be recovered exactly from xp(t) by means of X(jw) l -wM WM w (a) P(jw) 2;1 t t t t t ... Figure 7.3 Effect in the frequency -2w 0 2w domain of sampling in the time do- 5 -ws Ws 5 main: (a) spectrum of original signal; (b) (b) spectrum of sampling function; 518 Sampling Chap. 7 XP(jw) 1\1\lhl\1\1\ -wM 0 WM t W5 W (c) (ws- wM) Figure 7.3 Continued (c) spectrum of sampled signal with ws > 2wM; w (d) spectrum of sampled signal with Ws < 2wM. a lowpass filter with gain T and a cutoff frequency greater than w M and less than w.1. - w M, as indicated in Figure 7 .4. This basic result, referred to as the sampling theorem, can be stated as follows: 1 Sampling Theorem: Let x(t) be a band-limited signal with X(jw) = 0 for lwl > WM. Then x(t) is uniquely determined by its samples x(nT), n = 0, :±: 1, ±2, ... , if Ws > 2wM, where Ws Given these samples, we can reconstruct x(t) by generating a periodic impulse train in which successive impulses have amplitudes that are successive sample values. This impulse train is then processed through an ideal lowpass filter with gain T and cutoff frequency greater than w M and less than w .1· - w M. The resulting output signal will exactly equal x(t). 1 The important and elegant sampling theorem was available for many years in a variety of forms in the mathematics literature. See, for example, J. M. Whittaker, ""Interpolatory Function Theory,"" (New York: Stecher-Hafner Service Agency, 1964), chap. 4. It did not appear explicitly in the literature of communication theory until the publication in 1949 of the classic paper by Shannon entitled ""Communication in the Presence of Noise"" (Proceedings of the IRE, January 1949, pp. 10-21 ). However, H. Nyquist in 1928 and D. Gabor in 1946 had pointed out, based on the use of the Fourier Series, that 2TW numbers are sufficient to represent a function of duration T and highest frequency W. [H. Nyquist, ""Certain Topics in Telegraph Transmission Theory,"" AlEE Transactions, 1928, p. 617; D. Gabor, ""Theory of Communication,""Journa/ of lEE 93, no. 26 (1946), p. 429.] Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 519 p(t) = I S(t - nT) n = -x Xp(jw) x(t) --....,.•~~ Xp(l) • 81-----.....j•~xr(t) (a) X(jw) ~ w (b) XP(jw) ~AL(WM w (c) H(jw) T r----f-1--w-M..,<wc <(ws -wM) w (d) Figure 7.4 Exact recovery of a continuous-time signal from its sam- Xr(jw) ples using an ideal lowpass filter: A (a) system for sampling and recon- struction; (b) representative spectrum for x(t); (c) corresponding spectrum for xp(t); (d) ideal lowpass filter to re- cover X(jw) from Xp(jw ); (e) spectrum (e) Of Xr(t). The frequency 2w M, which, under the sampling theorem, must be exceeded by the sam- pling frequency, is commonly referred to as the Nyquist rate. 2 As discussed in Chapter 6, ideal filters are generally not used in practice for a va- riety of reasons. In any practical application, the ideallowpass filter in Figure 7.4 would be 2The frequency wM corresponding to one-half the Nyquist rate is often referred to as the Nyquist fre- quency. 520 Sampling Chap. 7 replaced by a nonideal filter H(jw) that approximated the desired frequency character- istic accurately enough for the problem of interest (i.e., H (jw) = 1 for lw I < w M, and H(jw) = 0 for jwj > ws- WM ). Obviously, any such approximation in the lowpass filter- ing stage will lead to some discrepancy between x(t) and x,.(t) in Figure 7.4 or, equiva- lently, between X(jw) and X,.(jw ). The particular choice of nonideal filter is then dictated by the acceptable level of distortion for the application under consideration. For conve- nience and to emphasize basic principles such as the sampling theorem, we will regularly assume the availability and use of ideal filters throughout this and the next chapter, with the understanding that in practice such a filter must be replaced by a nonideal filter designed to approximate the ideal characteristics accurately enough for the problem at hand. 7. 1 .2 Sampling with a Zero-Order Hold The sampling theorem, which is most easily explained in terms of impulse-train sampling, establishes the fact that a band-limited signal is uniquely represented by its samples. In practice, however, narrow, large-amplitude pulses, which approximate impulses, are also relatively difficult to generate and transmit, and it is often more convenient to generate the sampled signal in a form referred to as a zero-order hold. Such a system samples x(t) at a given instant and holds that value until the next instant at which a sample is taken, as illustrated in Figure 7.5. The reconstruction of x(t) from the output of a zero-order hold can again be carried out by lowpass filtering. However, in this case, the required filter no longer has constant gain in the passband. To develop the required filter characteristic, we first note that the output x0 (t) of the zero-order hold can in principle be generated by impulse-train sampling followed by an LTI system with a rectangular impulse response, as depicted in Figure 7 .6. To reconstruct x(t) from x0 (t), we consider processing x0 (t) with an LTI system with impulse response h,.(t) and frequency response H,.(jw ). The cascade of this system with the system of Figure 7.6 is shown in Figure 7.7, where we wish to specify H,.(jw) so that r(t) = x(t). Comparing the system in Figure 7.7 with that in Figure 7.4, we see that r(t) = x(t) if the cascade combination of h0(t) and h,.(t) is the ideallowpass filter H(jw) used in Figure 7.4. Since, from Example 4.4 and the time-shifting property in Section 4.3.2, Ho J.W ) -_ e -j·w T/'2 [2sin(wT/2)] ( , (7.7) w this requires that e.icvT/'2 H(jw) H,.(jw) = 2sin(wT/2) · (7.8) w x(t) __ Zerhoo-oldrd er Xo (t) ...,.~ Figure 7.5 Sampling utilizing a zero-order hold. Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 521 p(t) h0 (t) x(t)-.....,.~ 1b_ .,...._ _, .._ x (t) 0 0 T t x(t) /.-- ........... / ' / / ' --- ...... I I ' ' I "" x0 (t) Figure 7.6 Zero-order hold as \ impulse-train sampling followed by an LTI system with a rectangular impulse response. H(jw) r-------------------------------- p(t) 1 I I I Xp (t)l x0 (t) x(t) ---t~~ 1---+--~ r(t) I I I 0 Tt I IL ______________________________ _ Figure 7.7 Cascade of the representation of a zero-order hold (Figure 7.6) with a reconstruction filter. 522 Sampling Chap. 7 I Hr(jw)l (l) Figure 7.8 Magnitude and phase for the reconstruction filter for a zero- order hold. For example, with the cutoff frequency of H(jw) equal to w.J2, the ideal magnitude and phase for the reconstruction filter following a zero-order hold is that shown in Figure 7 .8. Once again, in practice the frequency response in eq. (7.8) cannot be exactly realized, and thus an adequate approximation to it must be designed. In fact, in many situations, the output of the zero-order hold is considered an adequate approximation to the original signal by itself, without any additionallowpass filtering, and in essence represents a possible, al- though admittedly very coarse, interpolation between the sample values. Alternatively, in some applications, we may wish to perform some smoother interpolation between sample values. In the next section, we explore in more detail the general concept of interpreting the reconstruction of a signal from its samples as a process of interpolation. 7.2 RECONSTRUCTION OF A SIGNAL FROM ITS SAMPLES USING INTERPOLATION Interpolation, that is, the fitting of a continuous signal to a set of sample values, is a commonly used procedure for reconstructing a function, either approximately or exactly, from samples. One simple interpolation procedure is the zero-order hold discussed in Section 7 .1. Another useful form of interpolation is linear interpolation, whereby adja- cent sample points are connected by a straight line, as illustrated in Figure 7. 9. In more Figure 7. 9 Linear interpolation be- tween sample points. The dashed curve represents the original signal and the solid curve the linear interpolation."
7.2 Reconstruction of a Signal from Its Samples Using Interpolation,"522 Sampling Chap. 7 I Hr(jw)l (l) Figure 7.8 Magnitude and phase for the reconstruction filter for a zero- order hold. For example, with the cutoff frequency of H(jw) equal to w.J2, the ideal magnitude and phase for the reconstruction filter following a zero-order hold is that shown in Figure 7 .8. Once again, in practice the frequency response in eq. (7.8) cannot be exactly realized, and thus an adequate approximation to it must be designed. In fact, in many situations, the output of the zero-order hold is considered an adequate approximation to the original signal by itself, without any additionallowpass filtering, and in essence represents a possible, al- though admittedly very coarse, interpolation between the sample values. Alternatively, in some applications, we may wish to perform some smoother interpolation between sample values. In the next section, we explore in more detail the general concept of interpreting the reconstruction of a signal from its samples as a process of interpolation. 7.2 RECONSTRUCTION OF A SIGNAL FROM ITS SAMPLES USING INTERPOLATION Interpolation, that is, the fitting of a continuous signal to a set of sample values, is a commonly used procedure for reconstructing a function, either approximately or exactly, from samples. One simple interpolation procedure is the zero-order hold discussed in Section 7 .1. Another useful form of interpolation is linear interpolation, whereby adja- cent sample points are connected by a straight line, as illustrated in Figure 7. 9. In more Figure 7. 9 Linear interpolation be- tween sample points. The dashed curve represents the original signal and the solid curve the linear interpolation. Sec. 7.2 Reconstruction of a Signal from Its Samples Using Interpolation 523 complicated interpolation formulas, sample points may be connected by higher order poly- nomials or other mathematical functions. As we have seen in Section 7.1, for a band-limited signal, if the sampling instants are sufficiently close, then the signal can be reconstructed exactly; i.e., through the use of a lowpass filter, exact interpolation can be carried out between the sample points. The interpretation of the reconstruction of x(t) as a process of interpolation becomes evident when we consider the effect in the time domain of the lowpass filter in Figure 7 .4. In particular, the output is Xr(t) = Xp(t) * h(t) or, with Xp(t) given by eq. (7.3), +oo Xr(t) = L x(nT)h(t - nT). (7.9) n= -oc Equation (7 .9) describes how to fit a continuous curve between the sample points x(nT) and consequently represents an interpolation formula. For the ideallowpass filter H(jw) in Figure 7.4, WeT sin(wet) h(t) = ---- (7.10) 7TWet so that Xr(t) = f x(nT) WeT sin(wc(t - nT)). (7.11) n= 7T We(t- nT) -oo The reconstruction according to eq. (7.11) with We = w 5 /2 is illustrated in Figure 7.10. Figure 7.10(a) represents the original band-limited signal x(t), and Figure 7.10(b) rep- resents x p(t), the impulse train of samples. In Figure 7.1 0( c), the superposition of the individual terms in eq. (7.11) is illustrated. Interpolation using the impulse response of an ideallowpass filter as in eq. (7 .11) is commonly referred to as band-limited interpolation, since it implements exact re- construction if x(t) is band limited and the sampling frequency satisfies the condi- tions of the sampling theorem. As we have indicated, in many cases it is preferable to use a less accurate, but simpler, filter or, equivalently, a simpler interpolating func- tion than the function in eq. (7.10). For example, the zero-order hold can be viewed as a form of interpolation between sample values in which the interpolating function h(t) is the impulse response h0(t) depicted in Figure 7.6. In that sense, with x0(t) in the figure corresponding to the approximation to x(t), the system h0(t) represents an approximation to the ideal lowpass filter required for the exact interpolation. Fig- ure 7.11 shows the magnitude of the transfer function of the zero-order-hold interpo- lating filter, superimposed on the desired transfer function of the exact interpolating filter. · Both from Figure 7.11 and from Figure 7 .6, we see that the zero-order hold is a very rough approximation, although in some cases it is sufficient. For example, if additional 524 Sampling Chap. 7 x(t) (a) ,.-- .... I I ' I ' I (b) Figure 7. 1 0 Ideal band-limited in- terpolation using the sine function: (a) band-limited signal x(t); (b) im- pulse train of samples of x(t); (c) ideal band-limited interpolation in which the impulse train is replaced by a superpo- (c) sition of sine functions [eq. (7.11 )]. T ~Ideal interpolating filter Figure 7. 11 Transfer function for 0 the zero-order hold and for the ideal interpolating filter. lowpass filtering is naturally applied in a given application, it will tend to improve the overall interpolation. This is illustrated in the case of pictures in Figure 7 .12. Fig- ure 7.12(a) shows pictures with impulse sampling (i.e., sampling with spatially nar- row pulses). Figure 7.12(b) is the result of applying a two-dimensional zero-order hold to Figure 7.12(a), with a resulting mosaic effect. However, the human visual system inherently imposes lowpass filtering, and consequently, when viewed at a dis- tance, the discontinuities in the mosaic are smoothed. For example, in Figure 7.12(c) a (a) (b) (c) Figure 7.12 (a) The original pictures of Figures 6.2(a) and (g) with impulse sam- pling; (b) zero-order hold applied to the pictures in (a). The visual system naturally introduces lowpass filtering with a cutoff frequency that decreases with distance. Thus, when viewed at a distance, the discontinuities in the mosaic in Figure 7.12(b) are smoothed; (c) result of applying a zero-order hold after impulse sampling with one-fourth the horizontal and vertical spacing used in (a) and (b). 525 526 Sampling Chap. 7 zero-order hold is again used, but here the sample spacing in each direction is one-fourth that in Figure 7.12(a). With normal viewing, considerable lowpass filtering is naturally applied, although the mosaic effect is still evident. If the crude interpolation provided by the zero-order hold is insufficient, we can use a variety of smoother interpolation strategies, some of which are known collectively as higher order holds. In particular, the zero-order hold produces an output signal, as in Fig- ure 7.5, that is discontinuous. In contrast, linear interpolation, as illustrated in Figure 7.9, yields reconstructions that are continuous, although with discontinous derivatives due to the changes in slope at the sample points. Linear interpolation, which is sometimes referred to as a first-order hold, can also be viewed as interpolation in the form of Figure 7.4 and eq. (7 .9) with h(t) triangular, as illustrated in Figure 7 .13. The associated transfer function is also shown in the figure and is 2 H(. ) = 2_ [sin(wT/2)] JW (7.12) T w/2 The transfer function of the first -order hold is shown superimposed on the transfer function for the ideal interpolating filter. Figure 7.14 corresponds to the same pictures as those in Figure 7 .12(b ), but with a first-order hold applied to the sampled picture. In an analogous fashion, we can define second- and higher order holds that produce reconstructions with a higher degree of smoothness. For example, the output of a second-order hold provides an interpolation of the sample values that is continuous and has a continuous first derivative and discontinuous second derivative. p(t) ~ Xp(t) h(t) x(t) --.-1 X !--..__.....,.~ H(jw) 1---~ Xr(t) (a) T 2T (b) Figure 7. 1 3 Linear interpolation h(t) (first-order hold) as impulse-train sam- ;l pling followed by convolution with a triangular impulse response: (a) sys- tem for sampling and reconstruction; (b) impulse train of samples; (c) im- -T T pulse response representing a first- (c) order hold; Sec. 7.3 The Effect of Undersampling: Aliasing 527 (d) H(jw) T Ideal interpolating filter Figure 7.13 Continued (d) first- order hold applied to the sampled sig- 0 nal; (e) comparison of transfer function of ideal interpolating filter and first- (e) order hold. 7.3 THE EFFECT OF UNDERSAMPLING: ALIASING In previous sections in this chapter, it was assumed that the sampling frequency was sufficiently high that the conditions of the sampling theorem were met. As illustrated in Figure 7 .3, with Ws > 2w M, the spectrum of the sampled signal consists of scaled repli- cations of the spectrum of x(t), and this forms the basis for the sampling theorem. When (a) (b) Figure 7. 14 Result of applying a first-order hold rather than a zero-order hold af- ter impulse sampling with one-third the horizontal and vertical spacing used in Fig- ures 7.12(a) and (b)."
7.3 The Effect of Undersampling: Aliasing,"Sec. 7.3 The Effect of Undersampling: Aliasing 527 (d) H(jw) T Ideal interpolating filter Figure 7.13 Continued (d) first- order hold applied to the sampled sig- 0 nal; (e) comparison of transfer function of ideal interpolating filter and first- (e) order hold. 7.3 THE EFFECT OF UNDERSAMPLING: ALIASING In previous sections in this chapter, it was assumed that the sampling frequency was sufficiently high that the conditions of the sampling theorem were met. As illustrated in Figure 7 .3, with Ws > 2w M, the spectrum of the sampled signal consists of scaled repli- cations of the spectrum of x(t), and this forms the basis for the sampling theorem. When (a) (b) Figure 7. 14 Result of applying a first-order hold rather than a zero-order hold af- ter impulse sampling with one-third the horizontal and vertical spacing used in Fig- ures 7.12(a) and (b). 528 Sampling Chap. 7 Ws < 2wM, X(jw), the spectrum of x(t), is no longer replicated in Xp(jw) and thus is no longer recoverable by lowpass filtering. This effect, in which the individual terms in eq. (7 .6) overlap, is referred to as aliasing, and in this section we explore its effect and consequences. Clearly, if the system of Figure 7.4 is applied to a signal with Ws < 2wM, the reconstructed signal Xr(t) will no longer be equal to x(t). However, as explored in Problem 7.25, the original signal and the signal Xr(t) that is reconstructed using band- limited interpolation will always be equal at the sampling instants; that is, for any choice ofws, Xr(nT) = x(nT), n = 0, ±1, ±2, .... (7.13) Some insight into the relationship between x(t) and Xr(t) when Ws < 2wM is pro- vided by considering in more detail the comparatively simple case of a sinusoidal signal. Thus, let x(t) = cos wot, (7.14) with Fourier transform X(jw) as indicated in Figure 7.15(a). In this figure, we have graphically distinguished the impulse at w0 from that at -w0 for convenience. Let us consider X p(jw ), the spectrum of the sampled signal, and focus in particular on the effect of a change in the frequency w 0 with the sampling frequency w s fixed. In Fig- ures 7.15(b)-(e), we illustrate Xp(jw) for several values of w0. Also indicated by a dashed line is the passband of the lowpass filter of Figure 7.4 with We = ws/2. Note that no aliasing occurs in (b) and (c), since w0 < w sf2, whereas aliasing does occur in (d) and (e). For each of the four cases, the lowpass filtered output Xr(t) is given as follows: Ws (a) wo = 6' Xr(t) = cos wot = x(t) 2ws (b) wo = Xr(t) = cos wot = x(t) 6 ' 4ws (c) wo = Xr(t) = cos(ws - wo)t ~ x(t) 6 ' 5ws (d) wo = Xr(t) = cos(ws - wo)t ~ x(t). 6 ' When aliasing occurs, the original frequency w 0 takes on the identity of a lower fre- quency, Ws- wo. For ws/2 < wo < Ws, as wo increases relative toWs, the output frequency w s - w0 decreases. When w s = w0 , for example, the reconstructed signal is a constant. This is consistent with the fact that, when sampling once per cycle, the samples are all equal and would be identical to those obtained by sampling a constant signal (w0 = 0). In Figure 7.16, we have depicted, for each of the four cases in Figure 7.15, the signal x(t), its samples, and the reconstructed signal Xr(t). From the figure, we can see how the lowpass Aliasing Figure 7. 1 5 Effect in the frequency domain of oversampling and under- sampling: (a) spectrum of original si- nusoidal signal; (b), (c) spectrum of sampled signal with ws > 2wo; (d), (e) spectrum of sampled signal with ws < 2wo. As we increase wo in mov- ing from (b) through (d), the impulses drawn with solid lines move to the right, while the impulses drawn with dashed lines move to the left. In (d) and (e), these impulses have moved sufficiently that there is a change in the ones falling within the passband of the ideal lowpass filter. filter interpolates between the samples, in particular always fitting a sinusoid of frequency less than wsf2 to the samples of x(t). As a variation on the preceding examples, consider the signal x(t) = cos(wot + </>). (7.15) V1 / Original signal IN 0 / Reconstructed signal wo=~ 6 ' ', / ' / ' ' ' / ' ' / ' ' ' (a) ' 2w wo=65 ' / / \ ' \ I /f ' \ I ' \ I ' \ \ I \ I \ I \ \ I \ I \ I \ I \ \ I I \ \ \ I I ' / ' / ' / ' / (b) Figure 7. 16 Effect of aliasing on a sinusoidal signal. For each of four values of wo. the original sinusoidal signal (solid curve), its samples, and the reconstructed sig- nal (dashed curve) are illustrated: (a) w0 = w5/6; (b) w0 = 2w5/6; (c) w0 = 4wsf6; (d) w0 = 5w5/6. In (a) and (b) no aliasing occurs, whereas in (c) and (d) there is aliasing. ii<O ii<O II II 0 0 3 3 I / ' I / / / / / I I I I ' I / / ' ' ' \ / \ I I I '1c:u:J :::::s :§ c::: 0 <:.,:) ' I ..0 ~ / ~ ,..: / I / ~ I :I at Li: ' I ' ' ' / ' \ I I / / ' \ I / / / / / I I I 531 532 Sampling Chap. 7 In this case, the Fourier transform of x(t) is essentially the same as Figure 7.15(a), ex- cept that the impulse indicated with a solid line now has amplitude 7TeN>, while the impulse indicated with a dashed line has amplitude with the opposite phase, namely, 7Te- N>. If we now consider the same set of choices for w 0 as in Figure 7.15, the re- sulting spectra for the sampled versions of cos(w0t + cp) are exactly as in the figure, with all solid impulses having amplitude 7TeN> and all dashed ones having amplitude 7Te- N>. Again, in cases (b) and (c) the condition of the sampling theorem is met, so that x,(t) = cos(w0t + cp) = x(t), while in (d) and (e) we again have aliasing. How- ever, we now see that there has been a reversal in the solid and dashed impulses ap- pearing in the passband of the lowpass filter. As a result, we find that in these cases, xr(t) = cos[(ws -w0 )t-cp], where we have a change in the sign of the phase cp, i.e., a phase reversal. It is important to note that the sampling theorem explicity requires that the sampling frequency be greater than twice the highest frequency in the signal, rather than greater than or equal to twice the highest frequency. The next example illustrates that sampling a sinusoidal signal at exactly twice its frequency (i.e., exactly two samples per cycle) is not sufficient. Example 7.1 Consider the sinusoidal signal and suppose that this signal is sampled, using impulse sampling, at exactly twice the frequency of the sinusoid, i.e., at sampling frequency Ws. As shown in Problem 7.39, if this impulse-sampled signal is applied as the input to an ideallowpass filter with cutoff frequency w sf2, the resulting output is x,(l) ~ (cos c/>) cos ( ~' t). As a consequence, we see that perfect reconstruction of x(t) occurs only in the case in which the phase l/J is zero (or an integer multiple of 27T). Otherwise, the signal Xr(t) does not equal x(t). As an extreme example, consider the case in which lfJ = -7T/2, so that This signal is sketched in Figure 7.17. We observe that the values of the signal at integer multiples of the sampling period 21Tiws are zero. Consequently, sampling at this rate produces a signal that is identically zero, and when this zero input is applied to the ideal lowpass filter, the resulting output Xr(t) is also identically zero. Sec. 7.3 The Effect of Undersampling: Aliasing 533 ~v(\v( \-T(v\v,(T\3\(J \v(\ v( Figure 7. 1 7 Sinusoidal signal for Example 7.1. The effect of undersampling, whereby higher frequencies are reflected into lower frequencies, is the principle on which the stroboscopic effect is based. Consider, for exam- ple, the situation depicted in Figure 7 .18, in which we have a disc rotating at a constant rate with a single radial line marked on the disc. The flashing strobe acts as a sampling system, since it illuminates the disc for extremely brief time intervals at a periodic rate. When the strobe frequency is much higher than the rotational speed of the disc, the speed of rotation of the disc is perceived correctly. When the strobe frequency becomes less than twice the rotational frequency of the disc, the rotation appears to be at a lower frequency than is actu- ally the case. Furthermore, because of phase reversal, the disc will appear to be rotating in the wrong direction! Roughly speaking, if we track the position of a fixed line on the disc at successive samples, then when w0 < Ws < 2w0 , so that we sample somewhat more fre- quently than once per revolution, samples of the disc will show the fixed line in positions that are successively displaced in a counterclockwise direction, opposite to the clockwise rotation of the disc itself. At one flash per revolution, corresponding tows = w 0 , the radial line appears stationary (i.e., the rotational frequency of the disc and its harmonics have been aliased to zero frequency). A similar effect is commonly observed in Western movies, Strobe Figure 7. 18 Strobe effect. 534 Sampling Chap. 7 where the wheels of a stagecoach appear to be rotating more slowly than would be consis- tent with the coach's forward motion, and sometimes in the wrong direction. In this case, the sampling process corresponds to the fact that moving pictures are a sequence of indi- vidual frames with a rate (usually between 18 and 24 frames per second) corresponding to the sampling frequency. The preceding discussion suggests interpreting the stroboscopic effect as an exam- ple of a useful application of aliasing due to undersampling. Another practical application of aliasing arises in a measuring instrument referred to as a sampling oscilloscope. This instrument is intended for observing very high-frequency waveforms and exploits the prin- ciples of sampling to alias these frequencies into ones that are more easily displayed. The sampling oscilloscope is explored in more detail in Problem 7 .38. 7.4 DISCRETE-TIME PROCESSING OF CONTINUOUS-TIME SIGNALS In many applications, there is a significant advantage offered in processing a continuous- time signal by first converting it to a discrete-time signal and, after discrete-time process- ing, converting back to a continuous-time signal. The discrete-time signal processing can be implemented with a general- or special-purpose computer, with microprocessors, or with any of the variety of devices that are specifically oriented toward discrete-time signal processing. In broad terms, this approach to continuous-time signal processing can be viewed as the cascade of three operations, as indicated in Figure 7 .19, where xc(t) and Yc(t) are continuous-time signals and xd[n] and Yc~[n] are the discrete-time signals corresponding to Xc(t) and Yc(t). The overall system is, of course, a continuous-time system in the sense that its input and output are both continuous-time signals. The theoretical basis for converting a continuous-time signal to a discrete-time signal and reconstructing a continuous-time signal from its discrete-time representation lies in the sampling theorem, as discussed in Section 7 .1. Through the process of periodic sampling with the sampling frequency consistent with the conditions of the sampling theorem, the continuous-time signal Xc(t) is exactly represented by a sequence of instantaneous sample values Xc(nT); that is, the discrete-time sequence xd[n] is related to Xc(t) by Xd[n] = Xc(nT). (7.16) ------------------------------------------- 1 xd[n] Conversion to Discrete-time Conversion to discrete time system continous time Figure 7. 19 Discrete-time processing of continuous-time signals."
7.4 Discrete-Time Processing of Continuous-Time Signals,"534 Sampling Chap. 7 where the wheels of a stagecoach appear to be rotating more slowly than would be consis- tent with the coach's forward motion, and sometimes in the wrong direction. In this case, the sampling process corresponds to the fact that moving pictures are a sequence of indi- vidual frames with a rate (usually between 18 and 24 frames per second) corresponding to the sampling frequency. The preceding discussion suggests interpreting the stroboscopic effect as an exam- ple of a useful application of aliasing due to undersampling. Another practical application of aliasing arises in a measuring instrument referred to as a sampling oscilloscope. This instrument is intended for observing very high-frequency waveforms and exploits the prin- ciples of sampling to alias these frequencies into ones that are more easily displayed. The sampling oscilloscope is explored in more detail in Problem 7 .38. 7.4 DISCRETE-TIME PROCESSING OF CONTINUOUS-TIME SIGNALS In many applications, there is a significant advantage offered in processing a continuous- time signal by first converting it to a discrete-time signal and, after discrete-time process- ing, converting back to a continuous-time signal. The discrete-time signal processing can be implemented with a general- or special-purpose computer, with microprocessors, or with any of the variety of devices that are specifically oriented toward discrete-time signal processing. In broad terms, this approach to continuous-time signal processing can be viewed as the cascade of three operations, as indicated in Figure 7 .19, where xc(t) and Yc(t) are continuous-time signals and xd[n] and Yc~[n] are the discrete-time signals corresponding to Xc(t) and Yc(t). The overall system is, of course, a continuous-time system in the sense that its input and output are both continuous-time signals. The theoretical basis for converting a continuous-time signal to a discrete-time signal and reconstructing a continuous-time signal from its discrete-time representation lies in the sampling theorem, as discussed in Section 7 .1. Through the process of periodic sampling with the sampling frequency consistent with the conditions of the sampling theorem, the continuous-time signal Xc(t) is exactly represented by a sequence of instantaneous sample values Xc(nT); that is, the discrete-time sequence xd[n] is related to Xc(t) by Xd[n] = Xc(nT). (7.16) ------------------------------------------- 1 xd[n] Conversion to Discrete-time Conversion to discrete time system continous time Figure 7. 19 Discrete-time processing of continuous-time signals. Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 535 The transformation of Xc(t) to xd[n] corresponding to the first system in Figure 7.19 will be referred to as continuous-to-discrete-time conversion and will be abbreviated C/D. The reverse operation corresponding to the third system in Figure 7.19 will be abbreviated D/C, representing discrete-time to continuous-time conversion. The D/C operation performs an interpolation between the sample values provided to it as input. That is, the D/C operation produces a continuous-time signal YcU) which is related to the discrete-time signal Yd[n] by Yd[n] = Yc(nT). This notation is made explicit in Figure 7 .20. In systems such as digital computers and digital systems for which the discrete-time signal is represented in digital form, the device commonly used to implement the C/D conversion is referred to as an analog-to-digital (A- to-D) converter, and the device used to implement the D/C conversion is referred to as a digital-to-analog (D-to-A) converter. r-------, xd [n] = Xc (nT) Yd [n] = Yc (nT) Xc (t) 1-------t~ Yc (t) Discrete -Time t-----+1 System T T Figure 7.20 Notation for continuous-to-discrete-time conversion and discrete-to-continuous-time conversion. T represents the sampling period. To understand further the relationship between the continuous-time signal xc(t) and its discrete-time representation xd [ n], it is helpful to represent C/D as a process of periodic sampling followed by a mapping of the impulse train to a sequence. These two steps are illustrated in Figure 7 .21. In the first step, representing the sampling process, the impulse train Xp(t) corresponds to a sequence of impulses with amplitudes corresponding to the samples of Xc(t) and with a time spacing equal to the sampling period T. In the conver- sion from the impulse train to the discrete-time sequence, we obtain xd[n], corresponding to the same sequence of samples of Xc(t), but with unity spacing in terms of the new in- dependent variable n. Thus, in effect, the conversion from the impulse train sequence of samples to the discrete-time sequence of samples can be thought of as a normalization in time. This normalization in converting xp(t) to xd[n] is evident in Figures 7.2l(b) and (c), in which Xp(t) and xd[n] are respectively illustrated for sampling rates ofT = T 1 and T = 2Tt. It is also instructive to examine the processing stages in Figure 7.19 in the frequency domain. Since we will be dealing with Fourier transforms in both continuous and dis- crete time, in this section only we distinguish the continuous-time and discrete-time fre- quency variables by using w in continuous time and 0 in discrete time. For example, the continuous-time Fourier transforms of Xc(t) and Yc(t) are Xc(Jw) and Yc(jw ), respectively, while the discrete-time Fourier transforms of xd[n] and Yd[n] are Xd(ej0 ) and Yd(ej0 ), respectively. 536 Sampling Chap. 7 C/O conversion ----------------, I I p(t) : ~ Conversion of x (t) ~ x Xp (t) impulse tr~in xd[n] c 1 to d1screte-t1me sequence 1 I I_-------------- _I (a) Xp (t) = - - - / / / / / --r-- / / / ' / / / / / ~ 0 T 2T 0 T 2T t (b) x[n] -4 -3 -2-1 0 1 2 3 4 n -4 -3 -2 -1 0 1 2 3 4 n (c) Figure 7.21 Sampling with a periodic impulse train followed by conversion to a discrete-time sequence: (a) overall system; (b) xp(t) for two sampling rates. The dashed envelope represents Xc(t); (c) the output sequence for the two different sampling rates. To begin let us express Xp(jw ), the continuous-time Fourier transform of Xp(t), in terms of the sample values of Xc(t) by applying the Fourier transform to eq. (7 .3). Since +:xo Xp(t) = ~ Xc(nT)8(t - nT), (7.17) n=-:xo and since the transform of 8(t- nT) is e- jwnT, it follows that +oo Xp(jw) = ~ Xc(nT)e- jwnT. (7.18) n= -oo Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 537 Now consider the discrete-time Fourier transform of xd[n], that is, +oo Xd(eifl) = L xd[n]e- Jfln, (7.19) n= -oo or, using eq. (7.16), +oo Xd(eifl) = L Xc(nT)e- jfln. (7.20) n= -oo Comparing eqs. (7.18) and (7.20), we see that Xd(eifl) and Xp(}w) are related through Xd(eifl) = Xp(}fl/T). (7.21) Also, recall that, as developed in eq. (7.6) and illustrated in Figure 7.3, 1 +oo Xp(}w) = T L Xc(j(w - kws)). (7.22) k= -00 Consequent! y, Xd(eifl) = ~ f Xc(j(fl - 27rk)IT). (7.23) k= -00 The relationship among Xc(}w ), Xp(}w ), and Xd(eifl) is illustrated in Figure 7.22 for two different sampling rates. Here, Xd(eifl) is a frequency-scaled version of Xp(}w) Xc (jw) Xc (jw) i i w w Xp(jw) i T= T1 1\ 1\ _2'1T 0 2'1T w T1 T1 xd (ejn) ~i~ -2'1T 2'1T n Figure 7.22 Relationship between Xc(jw),Xp(jw), and Xd(ejn) for two dif- ferent sampling rates. 538 Sampling Chap. 7 and, in particular, is periodic in !1 with period 27T. This periodicity is, of course, charac- teristic of any discrete-time Fourier transform. The spectrum of xd[n] is related to that of xc(t) through periodic replication, represented by eq. (7.22), followed by linear frequency scaling, represented by eq. (7.21). The periodic replication is a consequence of the first step in the conversion process in Figure 7.21, namely, the impulse-train sampling. The linear frequency scaling in eq. (7.21) can be thought of informally as a consequence of the normalization in time introduced by converting from the impulse train xp(t) to the discrete-time sequence xd[n]. From the time-scaling property of the Fourier transform in Section 4.3.5, scaling of the time axis by liT will introduce a scaling of the frequency axis by T. Thus, the relationship n = w T is consistent with the notion that, in converting from Xp(t) to xd[n], the time axis is scaled by liT. In the overall system of Figure 7.19, after processing with a discrete-time system, the resulting sequence is converted back to a continuous-time signal. This process is the reverse of the steps in Figure 7.21. Specifically, from the sequence Yd[n], a continuous- time impulse train Yp(t) can be generated. Recovery of the continuous-time signal YcU) from this impulse train is then accomplished by means of lowpass filtering, as illustrated in Figure 7.23. 0/C conversion ~------------------------------ 1 I I I Conversion of I Yp (t) discrete-time _, IT Yd[n]~ ... sequence to I I T~ Yc (t) I w w I impulse train - Ws s 2 2 Figure 7.23 Conversion of a I I discrete-time sequence to a continuous- I time signal. Now let us consider the overall system of Figure 7.19, represented as shown in Fig- ure 7.24. Clearly, if the discrete-time system is an identity system (i.e., xd[n] = Yd[n]), then, assuming that the conditions of the sampling theorem are met, the overall system will be an identity system. The characteristics of the overall system with a more general frequency response Hd(ejfl) are perhaps best understood by examining the representative example depicted in Figure 7.25. On the left-hand side of the figure are the representative p(t) l Conversion of xd [n] Yd [n] Conversion of Yp (t) impulse train ....,._......,~ ....,._......,~ sequence to Yc (t) to sequence impulse train 2 2 Figure 7.24 Overall system for filtering a continuous-time signal using a discrete- time filter. (!) (a) (d) XP (jw) Hp (jw), XP (jw) 1\Lh/\ AihA (!) - WM - De 0 De WM (!) T T (b) (e) xd (ein) He (jw), Xc (jw) ~JhL 0 (!) (c) (f) Figure 7.25 Frequency-domain illustration of the system of Figure 7.24: (a) continuous- time spectrum XcUw ); (b) spectrum after impulse-train sampling; (c) spectrum of discrete-time sequence xd[n]; (d) Hd(ei0 ) and Xd(ei0 ) that are multiplied to form Yd(ei0 ); (e) spectra that are multiplied to form Yp{jw}; (f) spectra that are multiplied to form YcUw ). 540 Sampling Chap. 7 spectra Xc(jw ), Xp(jw ), and Xd(ei0 ), where we assume that WM < w.J2, so that there is no aliasing. The spectrum Yd ( ei0 ) corresponding to the output of the discrete-time filter is the product of Xd(ei0 ) and Hd(ei0 ), and this is depicted in Figure 7.25(d) by overlaying Hd(ei0 ) and Xd(e.i0 ). The transformation to Yc(}w) then corresponds to applying a fre- quency scaling and lowpass filtering, resulting in the spectra indicated in Figure 7 .25( e) and (f). Since Yd(e.i0 ) is the product of the two overlaid spectra in Figure 7.25(d), the frequency scaling and lowpass filtering are applied to both. In comparing Figures 7.25(a) and (f), we see that (7.24) Consequently, for inputs that are sufficiently band limited, so that the sampling theorem is satisfied, the overall system of Figure 7.24 is, in fact, equivalent to a continuous-time LTI system wit~ frequency response Hc(jw) which is related to the discrete-time frequency response Hd(e.i0 ) through lwl < Ws/2 lwl (7.25) > w.J2 · The equivalent frequency response for this continuous-time filter is one period of the frequency response of the discrete-time filter with a linear scale change applied to the frequency axis. This relationship between the discrete-time frequency response and the equivalent continuous-time frequency response is illustrated in Figure 7 .26. The equivalence of the overall system of Figure 7.24 to an LTI system is somewhat surprising in view of the fact that multiplication by an impulse train is not a time-invariant operation. In fact, the overall system of Figure 7.24 is not time invariant for arbitrary in- puts. For example, if Xc(t) was a narrow rectangular pulse of duration less than T, then a time shift of Xc(t) could generate a sequence x[n] that either had all zero values or had one nonzero value, depending on the alignment of the rectangular pulse relative to the Figure 7.26 Discrete-time fre- quency response and the equivalent w continuous-time frequency response for the system of Figure 7.24. Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 541 sampling impulse train. However, as suggested by the spectra of Figure 7.25, for band- limited input signals with a sampling rate sufficiently high so as to avoid aliasing, the system of Figure 7.24 is equivalent to a continuous-time LTI system. For such inputs, Figure 7.24 and eq. (7.25) provide the conceptual basis for continuous-time processing using discrete-time filters. This is now explored further in the context of some examples. 7 .4.1 Digital Differentiator Consider the discrete-time implementation of a continuous-time band-limited differenti- ating filter. As discussed in Section 3.9.1, the frequency response of a continuous-time differentiating filter is Hc(jw) = jw, (7.26) and that of a band-limited differentiator with cutoff frequency we is Hc(jw) = { jw, iwi <we (7.27) 0, iwi >we' as sketched in Figure 7.27. Using eq. (7.25) with a sampling frequency Ws = 2wc, we see that the corresponding discrete-time transfer function is (7.28) as sketched in Figure 7.28. With this discrete-time transfer function, Yc(t) in Figure 7.24 will be the derivative of Xc(t), assuming that there is no aliasing in sampling xc(t). I He (jw) I w ~ .......- -.., 2 w 1T Figure 7.27 Frequency response 2 of a continuous-time ideal band-limited differentiator HcUw) = jw, lwl < we. 542 Sampling Chap. 7 1T 2 Figure 7.28 Frequency response n of discrete-time filter used to imple- 1T 2 ment a continuous-time band-limited differentiator. Example 7.2 By considering the output of the digital differentiator for a continuous-time sine input, we may conveniently determine the impulse response hd [n] of the discrete-time filter in the implementation of the digital differentiator. With reference to Figure 7 .24, let sin( 7rt/T) Xc(t) = --- (7.29) 'TTl where T is the sampling period. Then lwl < 7riT otherwise ' which is sufficiently band limited to ensure that sampling Xc(t) at frequency Ws = 27r/T does not give rise to any aliasing. It follows that the output of the digital differentiator is (t) = !!__ x (t) = cos( 7rt/T) _ sin( 7rt/T) y( dt Tt 7rt2 (7.30) c For Xc(t) as given by eq. (7.29), the corresponding signal xd[n] in Figure 7.24 may be expressed as (7.31) That is, for n =F 0, Xc(nT) = 0, while 1 Xd [0] = Xc(O) = T which can be verified by l'H6pital's rule. We can similarly evaluate Yd[n] in Figure 7.24 corresponding to Yc(t) in eq. (7.30). Specifically n=FO (7.32) n = 0 Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 543 which can be verified for n =P 0 by direct substitution into eq. (7.30) and for n = 0 by application of l'H6pital's rule. Thus when the input to the discrete-time filter given by eq. (7.28) is the scaled unit impulse in eq. (7.31), the resulting output is given by eq. (7.32). We then conclude that the impulse response of this filter is given by ( -1)11 hd [n] = ~' n =P 0 { 0, n = 0 7 .4.2 Half-Sample Delay In this section, we consider the implementation of a time shift (delay) of a continuous-time signal through the use of a system in the form of Figure 7.19. Thus, we require that the input and output of the overall system be related by Yc(t) = Xc(t- d) (7.33) when the input Xc(t) is band limited and the sampling rate is high enough to avoid alias- ing and where d represents the delay time. From the time-shifting property derived in Section 4.3.2 Y c(jw) = e- jwb. Xc(jW ). From eq. (7.25), the equivalent continuous-time system to be implemented must be band limited. Therefore, we take - jwb. Hc(jw) = e ' lwl <we (7.34) { 0, otherwise' where We is the cutoff frequency of the continuous-time filter. That is, Hc(jw) corresponds to a time shift as in eq. (7.33) for band-limited signals and rejects all frequencies greater than We. The magnitude and phase of the frequency response are shown in Figure 7.29(a). With the sampling frequency w.1· taken as w.1· = 2wc, the corresponding discrete-time I He(jw) I I Hd(eifl) I 11 1 I I I -we We w -1T 1T n < Hd(ein) __...-o1 ~T ar ~ ~~~~ n (a) (b) Figure 7.29 (a) Magnitude and phase of the frequency response for a continuous-time delay; (b) magnitude and phase of the frequency response for the corresponding discrete-time delay. 544 Sampling Chap. 7 frequency response is (7.35) and is shown in Figure 7.29(b). For appropriately band-limited inputs, the output of the system of Figure 7.24 with Hd(ejf!) as in eq. (7.35) is a delayed replica of the input. For il.IT an integer, the sequence y d [ n] is a delayed replica of xd [ n]; that is, y d [ n] = xd [ n - ~ l· (7 .36) For il.!T not an integer, eq. (7.36), as written, has no meaning, since sequences are defined only at integer values of the index. However, we can interpret the relationship between xd[n] and Yd[n] in these cases in terms of band-limited interpolation. The signals Xc(t) and xd[n] are related through sampling and band-limited interpolation, as are Yc(t) and Yd[n]. With Hd(ejf!) in eq. (7.35), Yd[n] is equal to samples of a shifted version of the band-limited interpolation of the sequence xd[n]. This is illustrated in Figure 7.30 with il./T = 1/2, which is sometimes referred to as a half-sample delay. 0 T 2T (a) ,f'I--r,'f--I,,r-I,,id~""~ = Yc(nT) = '""''""-~)T] Figure 7.30 (a) Sequence of sam- 0 T 2T ples of a continuous-time signal Xc(t); (b) sequence in (a) with a half-sample (b) delay. Example 7.3 The approach in Example 7.2 is also applicable to determining the impulse response hd[n] of the discrete-time filter in the half-sample delay system. With reference to Fig- ure 7.24, let sin('TT't/T) Xc(t) = --- (7.37) 'TT't It follows from Example 7.2 that 1 xd[n] = Xc(nT) = T8[n]. Sec. 7.5 Sampling of Discrete-Time Signals 545 Also, since there is no aliasing for the band-limited input in eq. (7 .37), the output of the half-sample delay system is Yc(t) = Xc(t- T/2) = sin( 7r(t- T/2)/T) 1r(t- T/2) and the sequence Yd[n] in Figure 7.24 is sin(1r(n- ~)) Yc~[n] = Yc(nT) = T1r(n- 1 • 2) We conclude that sin( 1r(n - ~)) h[n] = 7T(n- 21 • ) 7.5 SAMPLING OF DISCRETE-TIME SIGNALS Thus far in this chapter, we have considered the sampling of continuous-time signals, and in addition to developing the analysis necessary to understand continuous-time sampling, we have introduced a number of its applications. As we will see in this section, a very sim- ilar set of properties and results with a number of important applications can be developed for sampling of discrete-time signals. 7.5.1 Impulse-Train Sampling In analogy with continuous-time sampling as carried out using the system of Figure 7.2, sampling of a discrete-time signal can be represented as shown in Figure 7 .31. Here, the new sequence x P [ n] resulting from the sampling process is equal to the original sequence x[n] at integer multiples of the sampling period Nand is zero at the intermediate samples; that is, [ ] = { x[n], if n = an integer multiple of N Xp n O, (7.38) otherwise As with continuous-time sampling in Section 7.1, the effect in the frequency do- main of discrete-time sampling is seen by using the multiplication property developed in Section 5.5. Thus, with +oc Xp[n] = x[n]p[n] L x[kN]8[n- kN], (7.39) k= -oc we have, in the frequency domain, (7.40)"
7.5 Sampling of Discrete-Time Signals,"Sec. 7.5 Sampling of Discrete-Time Signals 545 Also, since there is no aliasing for the band-limited input in eq. (7 .37), the output of the half-sample delay system is Yc(t) = Xc(t- T/2) = sin( 7r(t- T/2)/T) 1r(t- T/2) and the sequence Yd[n] in Figure 7.24 is sin(1r(n- ~)) Yc~[n] = Yc(nT) = T1r(n- 1 • 2) We conclude that sin( 1r(n - ~)) h[n] = 7T(n- 21 • ) 7.5 SAMPLING OF DISCRETE-TIME SIGNALS Thus far in this chapter, we have considered the sampling of continuous-time signals, and in addition to developing the analysis necessary to understand continuous-time sampling, we have introduced a number of its applications. As we will see in this section, a very sim- ilar set of properties and results with a number of important applications can be developed for sampling of discrete-time signals. 7.5.1 Impulse-Train Sampling In analogy with continuous-time sampling as carried out using the system of Figure 7.2, sampling of a discrete-time signal can be represented as shown in Figure 7 .31. Here, the new sequence x P [ n] resulting from the sampling process is equal to the original sequence x[n] at integer multiples of the sampling period Nand is zero at the intermediate samples; that is, [ ] = { x[n], if n = an integer multiple of N Xp n O, (7.38) otherwise As with continuous-time sampling in Section 7.1, the effect in the frequency do- main of discrete-time sampling is seen by using the multiplication property developed in Section 5.5. Thus, with +oc Xp[n] = x[n]p[n] L x[kN]8[n- kN], (7.39) k= -oc we have, in the frequency domain, (7.40) 546 Sampling Chap. 7 x[n] --+-~ X J---~ xp[n] +oo p[n] = ~ o [n - kN] k=-00 x[n] tiiiiiiiiiiiiiiiiii n p[n: . .I .. I • • I • • I.. I • • I.. I • • n I xp[n] • • I • • • • I • • I • • I.. I• • I. Figure 7.31 Discrete-time n sampling. As in Example 5.6, the Fourier transform of the sampling sequence p[n] is 2 +oo P(el.w ) = N7 TL"" ""' 8(w- kws), (7.41) k= -00 where Ws, the sampling frequency, equals 27T/N. Combining eqs. (7.40) and (7.41), we have (7.42) Equation (7 .42) is the counterpart for discrete-time sampling of eq. (7 .6) for continuous-time sampling and is illustrated in Figure 7 .32. In Figure 7 .32( c), with Ws- WM > WM, or equivalently, Ws > 2wM, there is no aliasing [i.e., the nonzero portions of the replicas of X(ejw) do not overlap], whereas with Ws < 2wM, as in Figure 7.32(d), frequency-domain aliasing results. In the absence of aliasing, X(ejw) is faithfully repro- duced around w = 0 and integer multiples of 27T. Consequently, x[n] can be recovered from xp[n] by means of a lowpass filter with gain Nand a cutoff frequency greater than Sec. 7.5 Sampling of Discrete-Time Signals 547 X(eiw) zL -wM 0 WM (a) P(eiw) t t t 2~ 1 t t t ws (b) Xp(eiw) ~~~~~6~ - WM WM """" Ws (c) (w5 -wM) (d) Figure 7.32 Effect in the frequency domain of impulse-train sampling of a discrete-time signal: (a) spectrum of original signal; (b) spectrum of sampling sequence; (c) spectrum of sampled signal with w 5 > 2wM; (d) spectrum of sampled signal with w 5 < 2wM. Note that aliasing occurs. WM and less than w 5 - WM, as illustrated in Figure 7.33, where we have specified the cutoff frequency of the lowpass filter as wsf2. If the overall system of Figure 7.33(a) is ap- plied to a sequence for which Ws < 2wM, so that aliasing results, Xr[n] will no longer be equal to x[n]. However, as with continuous-time sampling, the two sequences will be equal at multiples of the sampling period; that is, corresponding to eq. (7 .13 ), we have Xr[kN] = x[kN], k = 0, :±: 1, :±:2, ... , (7.43) independently of whether aliasing occurs. (See Problem 7.46.) 548 Sampling Chap. 7 p[n] x[n] .~ xP[n ] • EH(jw)J --Xr[n] (a) X(jw) ~ ~ ~ -21T -wM WM 21T w XP(jw) (0 ,A.. /\w?hM(> ,A..~ 21T (b) Figure 7.33 Exact recovery of a discrete-time signal from its samples us- ing an ideal lowpass filter: (a) block diagram for sampling and reconstruction of a band-limited signal from its samples; (b) spectrum of the signal x[n]; (c) spectrum of Xp[n]; (d) frequency response of an ideal lowpass filter with cutoff frequency ws/2; (e) spectrum of the reconstructed signal x,[n]. For the example depicted here w5 > 2wM so that no aliasing occurs and consequently x,[n] = x[n]. Example 7.4 Consider a sequence x[n] whose Fourier transform X(eiw) has the property that X(eiw) = 0 for 27T/9 ~ iwi ~ 7T. Sec. 7.5 Sampling of Discrete-Time Signals 549 To determine the lowest rate at which x[n] may be sampled without the possibility of aliasing, we must find the largest N such that N21T (27T) 2 2 9 ==? N =:::; 9/2. We conclude that Nmax = 4, and the corresponding sampling frequency is 2n/4 = 11""12. The reconstruction of x[n] through the use of a lowpass filter applied to xp[n] can be interpreted in the time domain as an interpolation formula similar to eq. (7.11). With h[n] denoting the impulse response of the lowpass filter, we have (7.44) The reconstructed sequence is then Xr[n] = Xp[n] * h[n], (7.45) or equivalently, Xr[n] = f x[kN]Nwc sinwc(n- kN)_ (7.46) k= -oc 7T Wc(n- kN) Equation (7.46) represents ideal band-limited interpolation and requires the implemen- tation of an ideal lowpass filter. In typical applications a suitable approximation for the lowpass filter in Figure 7.33 is used, in which case the equivalent interpolation formula is of the form +co Xr[n] = L x[kN]hr[n- kN], (7.47) k= -oc where hr [ n] is the impulse response of the interpolating filter. Some specific examples, in- eluding the discrete-time counterparts of the zero-order hold and first-order hold discussed in Section 7.2 for continuous-time interpolation, are considered in Problem 7.50. 7.5.2 Discrete-Time Decimation and Interpolation There are a variety of important applications of the principles of discrete-time sampling, such as in filter design and implementation or in communication applications. In many of these applications it is inefficient to represent, transmit, or store the sampled sequence xp[n] directly in the form depicted in Figure 7.31, since, in between the sampling instants, xp[n] is known to be zero. Thus, the sampled sequence is typically replaced by a new sequence Xb[n], which is simply every Nth value of xp[n]; that is, (7.48) Also, equivalently, Xb[n] = x[nN], (7 .49) 550 Sampling Chap. 7 since x P [ n] and x[ n] are equal at integer multiples of N. The operation of extracting every Nth sample is commonly referred to as decimation. 3 The relationship between x[n], xp[n], and xb[n] is illustrated in Figure 7.34. To determine the effect in the frequency domain of decimation, we wish to determine the relationship between Xb(eiw)-the Fourier transform of xb[n]-and X(eiw). To this end, we note that +oc Xb(eiw) = L Xb[k]e- jwk, (7.50) k= -00 or, using eq. (7.48), +oo Xb(eiw) = L Xp[kN]e- }wk. (7.51) k= -00 If we let n = kN, or equivalently k = n/N, we can write n=integer multiple of N and since Xp[n] = 0 when n is not an integer multiple of N, we can also write +oo Xb(eiw) = L Xp[n]e- jwn!N. (7.52) n= -oo x[n] tl IIIIJJIII Jlllll It 0 n ~~ . I. .~1 • • r• • I ..1.. l~ .. 1• • 0 n tlnllr Figure 7.34 Relationship between Xp[n] corresponding to sampling and 0 n xb[n] corresponding to decimation. 3Technically, decimation would correspond to extracting every tenth sample. However, it has become common terminology to refer to the operation as decimation even when N is not equal to 10. Sec. 7.5 Sampling of Discrete-Time Signals 551 Furthermore, we recognize the right-hand side of eq. (7.52) as the Fourier transform of Xp[n]; that is, L+x Xp[n]e- jwn/N = Xp(eiw!N). (7.53) n= -oo Thus, from eqs. (7.52) and (7.53), we conclude that Xb(eiw) = Xp(ejw/N). (7.54) This relationship is illustrated in Figure 7 .35, and from it, we observe that the spectra for the sampled sequence and the decimated sequence differ only in a frequency scaling or normalization. If the original spectrum X(eiw) is appropriately band limited, so that there is no aliasing present in Xp(eiw), then, as shown in the figure, the effect of decimation is to spread the spectrum of the original sequence over a larger portion of the frequency band. 1 7T 27T w 7T 27T w Xb(eiw) ~~~ Figure 7.35 Frequency-domain illustration of the relationship between sampling and decimation. If the original sequence x[n] is obtained by sampling a continuous-time signal, the process of decimation can be viewed as reducing the sampling rate on the signal by a factor of N. To avoid aliasing, X(eiw) cannot occupy the full frequency band. In other words, if the signal can be decimated without introducing aliasing, then the original continuous- time signal was oversampled, and thus, the sampling rate can be reduced without aliasing. With the interpretation of the sequence x[n] as samples of a continuous-time signal, the process of decimation is often referred to as downsampling. 552 Sampling Chap. 7 C/D xd[n] Discrete time conversion lowpass filter Hd(eiw) Xe(jw) Lh -wM WM w Xd(eiw) ~ -21T w 1T 21T w Hd(eiw) Ib Figure 7.36 Continuous-time sig- D D nal that was originally sampled at the Nyquist rate. After discrete-time fil- tering, the resulting sequence can be -21T2 -we We 21T w further downsampled. Here Xc(jw) Yd(eiw) is the continuous-time Fourier trans- form of Xc(t), Xd(eiw) and Yd(eiw) are ch the discrete-time Fourier transforms Q Q of xd[n] and Yd[n] respectively, and Hd ( eiw) is the frequency response of the discrete-time lowpass filter de- -21T -we We 21T w picted in the block diagram. In some applications in which a sequence is obtained by sampling a continuous- time signal, the original sampling rate may be as low as possible without introducing aliasing, but after additional processing and filtering, the bandwidth of the sequence may be reduced. An example of such a situation is shown in Figure 7.36. Since the output of the discrete-time filter is band limited, downsampling or decimation can be applied. Just as in some applications it is useful to downsample, there are situations in which it is useful to convert a sequence to a higher equivalent sampling rate, a process referred to as upsampling or interpolation. Upsampling is basically the reverse of decimation or downsampling. As illustrated in Figures 7.34 and 7 .35, in decimation we first sample and then retain only the sequence values at the sampling instants. To upsample, we reverse the process. For example, referring to Figure 7 .34, we consider upsampling the sequence xb[n] to obtain x[n]. From xb[n], we form the sequence xp[n] by inserting N - 1 points with zero amplitude between each of the values in xb[n]. The interpolated sequence x[n] is then obtained from xp[n] by lowpass filtering. The overall procedure is summarized in Figure 7.37. Conversion of Ideal lowpass decimated sequence xb[n] filter -----+- x[n] to sampled H(ejw) sequence (a) xb[n] Xb(ejw) ~ & / n -2'1T 2'1T w xp[n] n -2TI -'IT 'IT 'IT '1T 2'1T w 2 2 x[n] X(ejw) \ I i- t\ I L n -2TI '1T 2'1T w Figure 7.37 Upsampling: (a) overall system; (b) associated sequences and spectra for upsampling by a factor of 2. U1 U1 1.\1 554 Sampling Chap. 7 Example 7.5 In this example, we illustrate how a combination of interpolation and decimation may be used to further downsample a sequence without incurring aliasing. It should be noted that maximum possible downsampling is achieved once the non -zero portion of one period of the discrete-time spectrum has expanded to fill the entire band from -7r to 1T. Consider the sequence x[n] whose Fourier transform X(eiw) is illustrated in Figure 7.38(a). As discussed in Example 7.4, the lowest rate at which impulse-train sampling may be used on this sequence without incurring aliasing is 27T/4. This corresponds to 27r 0 27r 'IT w -9 9 (a) ,/'... -271"" 1071"" -'IT 87r 0 87r -9 -9 9 (b) w ub(eJw) .. I I I I .. -271"" -'IT 0 'IT 27r w (d) Figure 7.38 Spectra associated with Example 7.5. (a) Spectrum of x[n]; (b) spectrum after downsampling by 4; (c) spectrum after upsampling x[n] by a factor of 2; (d) spectrum after upsampling x[n] by 2 and then downsampling by 9. Sec. 7.6 Summary 555 sampling every 4th value of x[ n]. If the result of such sampling is decimated by a factor of 4, we obtain a sequence xb[n] whose spectrum is shown in Figure 7.38(b). Clearly, there is still no aliasing of the original spectrum. However, this spectrum is zero for 87r/9 :s: Jw J :s: Tr, which suggests there is room for further downsampling. ' Specifically, examining Figure 7 .38(a) we see that if we could scale frequency by a factor of 9/2, the resulting spectrum would have nonzero values over the entire frequency interval from -7r to Tr. However, since 9/2 is not an integer, we can't achieve this purely by downsampling. Rather we must first upsample x[n] by a factor of 2 and then downsample by a factor of 9. In particular, the spectrum of the signal xu [ n] obtained when x[n] is upsampled by a factor of 2, is displayed in Figure 7.38(c). When xu[n] is then downsampled by a factor of 9, the spectrum of the resulting sequence Xub[n] is as shown in Figure 7.38(d). This combined result effectively corresponds to downsampling x[n] by a noninteger amount, 9/2. Assuming that x[n] represents unaliased samples of a continuous-time signal Xc(t), our interpolated and decimated sequence represents the maximum possible (aliasing-free) downsampling of Xc(t). 7.6 SUMMARY In this chapter we have developed the concept of sampling, whereby a continuous-time or discrete-time signal is represented by a sequence of equally spaced samples. The con- ditions under which the signal is exactly recoverable from the samples is embodied in the sampling theorem. For exact reconstruction, this theorem requires that the signal to be sampled be band limited and that the sampling frequency be greater than twice the high- est frequency in the signal to be sampled. Under these conditions, exact reconstruction of the original signal is carried out by means of ideal lowpass filtering. The time-domain interpretation of this ideal reconstruction procedure is often referred to as ideal band- limited interpolation. In practical implementations, the lowpass filter is approximated and the interpolation in the time domain is no longer exact. In some instances, simple inter- polation procedures such as a zero-order hold or linear interpolation (a first-order hold) suffice. If a signal is undersampled (i.e., if the sampling frequency is less than that required by the sampling theorem), then the signal reconstructed by ideal band-limited interpolation will be related to the original signal through a form of distortion referred to as aliasing. In many instances, it is important to choose the sampling rate so as to avoid aliasing. However, there are a variety of important examples, such as the stroboscope, in which aliasing is exploited. Sampling has a number of important applications. One particularly significant set of applications relates to using sampling to process continuous-time signals with discrete- time systems, by means of minicomputers, microprocessors, or any of a variety of devices specifically oriented toward discrete-time signal processing. The basic theory of sampling is similar for both continuous-time and discrete- time signals. In the discrete-time case there is the closely related concept of decimation, whereby the decimated sequence is obtained by extracting values of the original sequence at equally spaced intervals. The difference between sampling and decimation lies in the fact that, for the sampled sequence, values of zero lie in between the sample values, whereas in the decimated sequence these zero values are discarded, thereby compressing the sequence in time. The inverse of decimation is interpolation. The ideas of decima-"
7.6 Summary,"Sec. 7.6 Summary 555 sampling every 4th value of x[ n]. If the result of such sampling is decimated by a factor of 4, we obtain a sequence xb[n] whose spectrum is shown in Figure 7.38(b). Clearly, there is still no aliasing of the original spectrum. However, this spectrum is zero for 87r/9 :s: Jw J :s: Tr, which suggests there is room for further downsampling. ' Specifically, examining Figure 7 .38(a) we see that if we could scale frequency by a factor of 9/2, the resulting spectrum would have nonzero values over the entire frequency interval from -7r to Tr. However, since 9/2 is not an integer, we can't achieve this purely by downsampling. Rather we must first upsample x[n] by a factor of 2 and then downsample by a factor of 9. In particular, the spectrum of the signal xu [ n] obtained when x[n] is upsampled by a factor of 2, is displayed in Figure 7.38(c). When xu[n] is then downsampled by a factor of 9, the spectrum of the resulting sequence Xub[n] is as shown in Figure 7.38(d). This combined result effectively corresponds to downsampling x[n] by a noninteger amount, 9/2. Assuming that x[n] represents unaliased samples of a continuous-time signal Xc(t), our interpolated and decimated sequence represents the maximum possible (aliasing-free) downsampling of Xc(t). 7.6 SUMMARY In this chapter we have developed the concept of sampling, whereby a continuous-time or discrete-time signal is represented by a sequence of equally spaced samples. The con- ditions under which the signal is exactly recoverable from the samples is embodied in the sampling theorem. For exact reconstruction, this theorem requires that the signal to be sampled be band limited and that the sampling frequency be greater than twice the high- est frequency in the signal to be sampled. Under these conditions, exact reconstruction of the original signal is carried out by means of ideal lowpass filtering. The time-domain interpretation of this ideal reconstruction procedure is often referred to as ideal band- limited interpolation. In practical implementations, the lowpass filter is approximated and the interpolation in the time domain is no longer exact. In some instances, simple inter- polation procedures such as a zero-order hold or linear interpolation (a first-order hold) suffice. If a signal is undersampled (i.e., if the sampling frequency is less than that required by the sampling theorem), then the signal reconstructed by ideal band-limited interpolation will be related to the original signal through a form of distortion referred to as aliasing. In many instances, it is important to choose the sampling rate so as to avoid aliasing. However, there are a variety of important examples, such as the stroboscope, in which aliasing is exploited. Sampling has a number of important applications. One particularly significant set of applications relates to using sampling to process continuous-time signals with discrete- time systems, by means of minicomputers, microprocessors, or any of a variety of devices specifically oriented toward discrete-time signal processing. The basic theory of sampling is similar for both continuous-time and discrete- time signals. In the discrete-time case there is the closely related concept of decimation, whereby the decimated sequence is obtained by extracting values of the original sequence at equally spaced intervals. The difference between sampling and decimation lies in the fact that, for the sampled sequence, values of zero lie in between the sample values, whereas in the decimated sequence these zero values are discarded, thereby compressing the sequence in time. The inverse of decimation is interpolation. The ideas of decima- 556 Sampling Chap. 7 tion and interpolation arise in a variety of important practical applications of signals and systems, including communication systems, digital audio, high-definition television, and many other applications. Chapter 1 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 7.1. A real-valued signal x(t) is known to be uniquely determined by its samples when the sampling frequency is Ws = 10,000'77'. For what values of w is X(jw) guaranteed to be zero? 7.2. A continuous-time signal x(t) is obtained at the output of an ideal lowpass filter with cutoff frequency We = 1,000'77'. If impulse-train sampling is performed on x(t), which of the following sampling periods would guarantee that x(t) can be recovered from its sampled version using an appropriate lowpass filter? (a) T = 0.5 x 10-3 (b) T = 2 X 1o -3 (c) T = 10-4 7 .3. The frequency which, under the sampling theorem, must be exceeded by the sam- pling frequency is called the Nyquist rate. Determine the Nyquist rate corresponding to each of the following signals: (a) x(t) = 1 + cos(2,000'7Tt) + sin(4,000'7Tt) (b) x(t) = sin(4,0007Tt) 7Tt (c) x(t) = ( sin(4,~~0m) t 7.4. Let x(t) be a signal with Nyquist rate w 0 . Determine the Nyquist rate for each of the following signals: (a) x(t) + x(t - 1) (b) d~;t) (c) x2(t) (d) x(t) cos wot 7.5. Let x(t) be a signal with Nyquist rate w 0 . Also, let y(t) = x(t)p(t- 1),"
Problems,"556 Sampling Chap. 7 tion and interpolation arise in a variety of important practical applications of signals and systems, including communication systems, digital audio, high-definition television, and many other applications. Chapter 1 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 7.1. A real-valued signal x(t) is known to be uniquely determined by its samples when the sampling frequency is Ws = 10,000'77'. For what values of w is X(jw) guaranteed to be zero? 7.2. A continuous-time signal x(t) is obtained at the output of an ideal lowpass filter with cutoff frequency We = 1,000'77'. If impulse-train sampling is performed on x(t), which of the following sampling periods would guarantee that x(t) can be recovered from its sampled version using an appropriate lowpass filter? (a) T = 0.5 x 10-3 (b) T = 2 X 1o -3 (c) T = 10-4 7 .3. The frequency which, under the sampling theorem, must be exceeded by the sam- pling frequency is called the Nyquist rate. Determine the Nyquist rate corresponding to each of the following signals: (a) x(t) = 1 + cos(2,000'7Tt) + sin(4,000'7Tt) (b) x(t) = sin(4,0007Tt) 7Tt (c) x(t) = ( sin(4,~~0m) t 7.4. Let x(t) be a signal with Nyquist rate w 0 . Determine the Nyquist rate for each of the following signals: (a) x(t) + x(t - 1) (b) d~;t) (c) x2(t) (d) x(t) cos wot 7.5. Let x(t) be a signal with Nyquist rate w 0 . Also, let y(t) = x(t)p(t- 1), Chap. 7 Problems 557 where ~ 2 p(t) = L o(t- nT), and T < ____!!__ n=-~ WQ Specify the constraints on the magnitude and phase of the frequency response of a filter that gives x(t) as its output when y(t) is the input. 7.6. In the system shown in Figure P7.6, two functions of time, XI (t) and x2(t), are mul- tiplied together, and the product w(t) is sampled by a periodic impulse train. XI (t) is band limited tow 1, and x2(t) is band limited to w2 ; that is, XI(jw) = 0, lwl ~WI, X2(jw) = 0, lwl ~ w2. Determine the maximum sampling interval T such that w(t) is recoverable from wp(t) through the use of an ideallowpass filter. p(t) = ~ o(t -nT) x1(t)---p-.X_ _w_ (t)~-~~o~-x • Wp(l) x2(t) - X1(jw) ch Figure P7.6 7.7. A signal x(t) undergoes a zero-order hold operation with an effective sampling pe- riod T to produce a signal x0(t). Let XI (t) denote the result of a first-order hold operation on the samples of x(t); i.e., XI (t) = L x(nT)hi (t- nT), n= -oo where hi (t) is the function shown in Figure P7.7. Specify the frequency response of a filter that produces x 1 (t) as its output when x0(t) is the input. 558 Sampling Chap. 7 -T 0 T Figure P7.7 7.8. Consider a real, odd, and periodic signal x(t) whose Fourier series representation may be expressed as x(t) = ~5 (1 )k 2 sin(k1rt). Let x(t) represent the signal obtained by performing impulse-train sampling on x(t) using a sampling period of T = 0. 2. (a) Does aliasing occur when this impulse-train sampling is performed on x(t)? (b) If x(t) is passed through an ideallowpass filter with cutoff frequency 1riT and passband gain T, determine the Fourier series representation of the output signal g(t). 7.9. Consider the signal which we wish to sample with a sampling frequency of Ws = 1507T to obtain a signal g(t) with Fourier transform G(jw ). Determine the maximum value of w0 for which it is guaranteed that G(jw) = 75X(jw) for lwl ::s wo, where X(jw) is the Fourier transform of x(t). 7.10. Determine whether each of the following statements is true or false: (a) The signal x(t) = u(t + T0 ) - u(t- T0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 2T0 . (b) The signal x(t) with Fourier transform X(jw) = u(w + w0)- u(w- w0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 7Tiwo. (c) The signal x(t) with Fourier transform X(jw) = u(w)- u(w- w0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 27Tiwo. 7.11. Let Xc(t) be a continuous-time signal whose Fourier transform has the property that Xc(jw) = 0 for lwl ~ 2,0001T. A discrete-time signal xd[n] = Xc(n(0.5 X 10-3)) Chap. 7 Problems 559 is obtained. For each of the following constraints on the Fourier transform Xd(ejw) of xd[n], determine the corresponding constraint on Xc(jw ): (a) Xd(ejw) is real. (b) The maximum value of Xd(ejw) over all w is 1. (c) Xd(ejw) = 0 for 3 ; ::; lw I ::; 1T. (d) Xd(ejw) = Xd(ej(w-1T)). 7.12. A discrete-time signal xd[n] has a Fourier transform Xd(ejw) with the property that Xd(ejw) = 0 for 37T/4 ::; lwl ::; 1T. The signal is converted into a continuous-time signal Loo sin ¥U - nT) Xc(t) = T xd[n] 1T(t _ nT) , n= -oo where T = 1o -3 . Determine the values of w for which the Fourier transform Xc(jw) of xc(t) is guaranteed to be zero. 7.13. With reference to the filtering approach illustrated in Figure 7.24, assume that the sampling period used is T and the input Xc(t) is band limited, so that Xc(jw) = 0 for lwl 2:: 1TIT. If the overall system has the property that Yc(t) = xc(t-2T), determine the impulse response h[n] of the discrete-time filter in Figure 7.24. 7.14. Repeat the previous problem, except this time assume that 7.15. Impulse-train sampling of x[n] is used to obtain g[n] = L x[n]S[n- kN]. k= -00 If X(ejw) = 0 for 37T/ 7 ::; lwl ::; 1T, determine the largest value for the sampling interval N which ensures that no aliasing takes place while sampling x[n]. 7.16. The following facts are given about the signal x[n] and its Fourier transform: 1. x[n] is real. 2. X(ejw) -:t= 0 for 0 < w < 1T. 3. x[n]L~=-oo8[n- 2k] = S[n]. Determine x[n]. You may find it useful to note that the signal (sin ~n)/( 1Tn) satisfies two of these conditions. 560 Sampling Chap. 7 7.17. Consider an ideal discrete-time bandstop filter with impulse response h[ n] for which the frequency response in the interval -7r ::; w ::; 1T is 3 lwl ::; *and lwl ~ ; . elsewhere Determine the frequency response of the filter whose impulse response is h[2n]. 7 .18. Suppose the impulse response of an ideal discrete-time lowpass filter with cutoff fre- quency 1r12 is interpolated (in accordance with Figure 7 .37) to obtain an upsampling by a factor of 2. What is the frequency response corresponding to this upsampled impulse response? 7.19. Consider the system shown in Figure P7.19, with input x[n] and the correspond- ing output y[n]. The zero-insertion system inserts two points with zero amplitude between each of the sequence values in x[n]. The decimation is defined by y[n] = w[5n], where w[n] is the input sequence for the decimation system. If the input is of the form sinw1n x[n] = --- 1Tn determine the output y[n] for the following values of w1: (a) 3 WI ::; ; (b) WI > 3 ; H(eiw) - - 1 w[n] Zero insertion Decimation x[n] y[n] -7T -7T/5 7T/5 7T Figure P7. 1 9 7 .20. Two discrete-time systems S I and S2 are proposed for implementing an ideal low- pass filter with cutoff frequency 7T/4. System S1 is depicted in Figure P7.20(a). System S2 is depicted in Figure P7 .20(b ). In these figures, SA corresponds to a zero- insertion system that inserts one zero after every input sample, while S 8 corresponds to a decimation system that extracts every second sample of its input. (a) Does the proposed system S1 correspond to the desired ideallowpass filter? (b) Does the proposed system S2 correspond to the desired ideallowpass filter? Chap. 7 Problems 561 -·~l._ __ ill sA_ _ ill Ss -x[n-] I y[n] -'IT/8 0 'IT/8 (a) -x[n-]-•~1._ __ss_ _ • ]1 SA I ]1 y[n) -'IT/2 0 'IT/2 -'IT/2 0 'IT/2 (b) Figure P7.20 BASIC PROBLEMS 7.21. A signal x(t) with Fourier transform X(jw) undergoes impulse-train sampling to generate <X) X p(t) = L x(nT) o(t - nT) n= -oo where T = 10-4 • For each of the following sets of constraints on x(t) and/or X(jw ), does the sampling theorem (see Section 7.1.1) guarantee that x(t) can be recovered exactly from x p(t)? (a) X(jw) = 0 for lwl > 50007T (b) X(jw) = 0 for lwl > 150007T (c) (Jl.e{X(jw)} = 0 for lwl > 50007T (d) x(t) real and X(jw) = 0 for w > 50007T (e) x(t) real and X(jw) = 0 for w < -150007T (f) X(jw) * X(jw) = 0 for lwl > 150007T (g) IX(jw )I = 0 for w > 50007T 7.22. The signal y(t) is generated by convolving a band-limited signal XI (t) with another band-limited signal x2(t), that is, y(t) = XI (t) * X2(t) where X1(jw) = 0 for lw I > 10 007T X2(jw) = 0 for lw I > 20007T. Impulse-train sampling is performed on y(t) to obtain 562 Sampling Chap. 7 +oo Yp(t) = L y(nT)o(t - nT). n= -oo Specify the range of values for the sampling period T which ensures that y(t) is recoverable from Yp(t). 7.23. Shown in Figure P7.23 is a system in which the sampling signal is an impulse train with alternating sign. The Fourier transform of the input signal is as indicated in the figure. (a) Ford< 7rl(2wM), sketch the Fourier transform of Xp(t) and y(t). (b) Ford< 7rl(2wM), determine a system that will recover x(t) from Xp(t). (c) Ford< 7rl(2wM ), determine a system that will recover x(t) from y(t). (d) What is the maximum value of din relation to WM for which x(t) can be recov- ered from either xp(t) or y(t)? p(t) x(t)--,.._1 p(t) ... t 1 i Ll t l _J 2Ll l t X(jw) ~ H(jw) 1 D t D 3'TT (I) T Figure P7.23 7 .24. Shown in Figure P7 .24 is a system in which the input signal is multiplied by a periodic square wave. The period of s(t) is T. The input signal is band limited with IX(jw)l = 0 for lwl ;:::: WM. Chap. 7 Problems 563 (a) For~ = T/3, determine, in terms of WM, the maximum value ofT for which there is no aliasing among the replicas of X(jw) in W(jw ). (b) For ~ = T 14, determine, in terms of w M, the maximum value of T for which there is no aliasing among the replicas of X(jw) in W(jw ). x(t)---~~~ w(t) t s(t) Figure P7 .24 7 .25. In Figure P7 .25 is a sampler, followed by an ideallowpass filter, for reconstruction of x(t) from its samples x p(t). From the sampling theorem, we know that if w s = 27TIT is greater than twice the highest frequency present in x(t) and We = wsf2, then the reconstructed signal Xr(t) will exactly equal x(t). If this condition on the bandwidth of x(t) is violated, then Xr(t) will not equal x(t). We seek to show in this problem that if We = wsf2, then for any choice ofT, Xr(t) and x(t) will always be equal at the sampling instants; that is, Xr(kT) = x(kT), k = 0, ± 1, ±2, .... +oo p(t) = l o(t -nT) n = -oo H(jw) ill 1----~ Xr (t) Figure P7.25 To obtain this result, consider eq. (7.11), which expresses Xr(t) in terms of the samples of x(t): Xr (t ) = ~ ( T)TWe sin[we(t- nT)] L X n ( T) . n=-oo 7T Wet-n With We = wsf2, this becomes oo sin [ f (I - nT)] Xr(t) = L x(nT) 7T • (P7.25-l) n= -oo T(t- nT) 564 Sampling Chap. 7 By considering the values of a for which [sin(a)]/a = 0, show from eq. (P7.25-l) that, without any restrictions on x(t), Xr(kT) = x(kT) for any integer value of k. 7.26. The sampling theorem, as we have derived it, states that a signal x(t) must be sam- pled at a rate greater than its bandwidth (or equivalently, a rate greater than twice its highest frequency). This implies that if x(t) has a spectrum as indicated in Figure P7.26(a) then x(t) must be sampled at a rate greater than 2w2• However, since the signal has most of its energy concentrated in a narrow band, it would seem reason- able to expect that a sampling rate lower than twice the highest frequency could be used. A signal whose energy is concentrated in a frequency band is often referred to as a bandpass signal. There are a variety of techniques for sampling such signals, generally referred to as bandpass-sampling techniques. X(jw) 1t w (a) +co x(t) ----:~I~~:~~T) ·1,.....---H-(J-.W-) ___,~ x,(t) 1 1 1 1 T H(jw) At (b) Figure P7 .26 Chap. 7 Problems 565 To examine the possibility of sampling a bandpass signal as a rate less than the total bandwidth, consider the system shown in Figure P7.26(b). Assuming that w 1 > w2 - w 1, find the maximum value ofT and the values of the constants A, wa, and wb such that Xr(t) = x(t). 7.27. In Problem 7 .26, we considered one procedure for bandpass sampling and recon- struction. Another procedure, used when x(t) is real, consists of multiplying x(t) by a complex -exponential and then sampling the product. The sampling system is shown in Figure P7.27(a). With x(t) real and with X(jw) nonzero only for w 1 < lwl < w2, the frequency is chosen to be w0 = (112)(w 1 + w2), and the lowpass filter H 1(jw) has cutoff frequency (112)(w2 - w 1) . (a) For X(jw) as shown in Figure P7.27(b), sketch Xp(jw ). (b) Determine the maximum sampling period T such that x(t) is recoverable from Xp(t). (c) Determine a system to recover x(t) from xp(t). x(t) -----+-@ ·I H(jw) t e-iwot +70 p(t) = ~ 8(t-nT) n = -x (a) X(jw) L1 1t -w2 -w1 (b) Figure P7.27 7.28. Figure P7.28(a) shows a system that converts a continuous-time signal to a discrete- time signal. The input x(t) is periodic with a period of 0.1 second. The Fourier series coefficients of x(t) are 1 ak = ( ""2 Jkl , -oo < k < +oo. The lowpass filter H(jw) has the frequency response shown in Figure P7.28(b). The sampling period T = 5 x 10-3 second. (a) Show that x[n] is a periodic sequence, and determine its period. (b) Determine the Fourier series coefficients of x[n]. 566 Sampling Chap. 7 Conversion Lowpass Xc(t) of an x(t) filter X impulse train x[n] = xc (nT) H(jw) to a sequence p(t) = I 8(t -nT) n = -oo (a) H(jw) 11 I -205'7T 205'7T w (b) Figure P7.28 7 .29. Figure P7 .29( a) shows the overall system for filtering a continuous-time signal using a discrete-time filter. If Xc(jw) and H(eiw) are as shown in Figure P7.29(b), with liT = 20kHz, sketch Xp(jw ), X(eiw), Y(eiw), Yp(jw ), and Yc(jw ). H(jw) Xc (t) Xp (t) Conversion x[n] = Xc (nT) y[n] = Yc (nT) h [n] Conversion Yp (t) Yc (t) X to a H(eiw) to an ~ ~ sequence impulse train m -'lTIT 1T/T p(t) =I 8(t-nT) n = -oo (a) Xc(jw) H(eiw) A 1 I I -'lT X104 '7T X104 w -41T 1T 4 w (b) Figure P7 .29 Chap. 7 Problems 567 7.30. Figure P7.30 shows a system consisting of a continuous-time LTI system followed by a sampler, conversion to a sequence, and an LTI discrete-time system. The continuous-time LTI system is causal and satisfies the linear, constant-coefficient differential equation dyc(t) d[ + Yc(t) = Xc(t). The input Xc(t) is a unit impulse o(t). (a) Determine Yc(t). (b) Determine the frequency response H(ejw) and the impulse response h[n] such that w[n] = o[n]. t LTI y, ( ) , r- Conversion of impu;~~ y[n] train I '--------J- l w[n] sequence p(t) = !,o(t-nT) y[n] = Yc (nT) n = -x Figure P7.30 7 .31. Shown in Figure P7 .31 is a system that processes continuous-time signals using a digital filter h[n] that is linear and causal with difference equation 1 y[n] = 2y [n - 1] + x[n]. For input signals that are band limited such that Xc(jw) = 0 for lw I > niT, the system in the figure is equivalent to a continuous-time LTI system. Determine the frequency response Hc(jw) of the equivalent overall system with input xc(t) and output Yc(t). (t) Conversion of x[n] y[n] Conversion of y(t) Ideal lowpass x (t) ~~xP impulse train h[n] sequence filter cutoff c to a to a frenquency sequence impulse train 1r/T p(t) =!, o(t-nT) n = -x. Hgute ~1.3' 7.32. A signal x[n] has a Fourier transform X(ejw) that is zero for ( 7T/4) ~ lwl ~ 7T. Another signal 568 Sampling Chap. 7 g[n] = x[n] L, o[n- 1 - 4k] k=-% is generated. Specify the frequency response H(e.iw) of a lowpass filter that produces x[n] as output when g[n] is the input. 7.33. A signal x[n] with Fourier transform X(e.iw) has the property that G[n] '~% ll[n- 3k] )• (si*!n) = x[n]. For what values of w is it guaranteed that X ( e.iw) = 0? 7.34. A real-valued discrete-time signal x[n] has a Fourier transform X(e.iw) that is zero for 31TI14 ~ lwl ~ 1T. The nonzero portion of the Fourier transform of one period of X(e.iw) can be made to occupy the region lwl < 1T by first performing upsampling by a factor of L and then performing downsampling by a factor of M. Specify the values of L and M. 7.35. Consider a discrete-time sequence x[n] from which we form two new sequences, xp[n] and xd[n], where Xp[n] corresponds to sampling x[n] with a sampling period of 2 and xd[n] corresponds to decimating x[n] by a factor of 2, so that Xp[n] = { x,[n], n = 0, ±2, ±4, .. . 0 n = ±1, ±3, .. . and xd [n] = x[2n]. (a) If x[n] is as illustrated in Figure P7.35(a), sketch the sequences Xp[n] and xd[n]. (b) If X(e.iw) is as shown in Figure P7.35(b), sketch Xp(e.iw) and Xd(e.iw). • • ' l I I II I I I I II I t~l . 0 n (a) / 0 37T 57T 117T w 4 4 4 (b) Figure P7. 3 5 ADVANCED PROBLEMS 7.36 Letx(t)beaband-limitedsignalsuchthatX(jw) = Oforlwl2 ¥· (a) If x(t) is sampled using a sampling period T, determine an interpolating function Chap. 7 Problems 569 g(t) such that dx(t) L x(nT)g(t - nT). dt n=-x (b) Is the function g(t) unique? 7.37. A signal limited in bandwidth to lw I < W can be recovered from nonuniformly spaced samples as long as the average sample density is 2(W/27T) samples per sec- ond. This problem illustrates a particular example of nonuniform sampling. Assume that in Figure P7.37(a): 1. x(t) is band limited; X(jw) = 0, lwl > W. 2. p(t) is a nonuniformly spaced periodic pulse train, as shown in Figure P7.37(b). 3. f(t) is a periodic waveform with period T = 27TIW. Since f(t) multiplies an impulse train, only its values f(O) = a and f(~) = b at t = 0 and t = ~' re- spectively, are significant. 4. H 1( jw) is a 90° phase shifter; that is, H1(jw) = { j, . w >0 - ], w <0' f(t) l ~~ y, (t) L___j ~ y, (t) Sampled x(t) x(t) --~1 x 1----.. 0-----1 H2(jw) 1---...-1•~ z(t) t Y3 (t) p(t) (a) t t 1 (b) Figure P7. 3 7 570 Sampling Chap. 7 5. H 2(jw) is an ideallowpass filter; that is, K, O<w < W H2(jw) = K*, -W<w<O, { 0, lwi>W where K is a (possibly complex) constant. (a) Find the Fourier transforms of p(t), Y1 (t), Y2(t), and y3(t). (b) Specify the values of a, b, and K as functions of d such that z(t) = x(t) for any band-limited x(t) and any d such that 0 < d < 1riW. 7 .38. It is frequently necessary to display on an oscilloscope screen waveforms having very short time structures-for example, on the scale of thousandths of a nanosec- ond. Since the rise time of the fastest oscilloscope is longer than this, such displays cannot be achieved directly. If however, the waveform is periodic, the desired result can be obtained indirectly by using an instrument called a sampling oscilloscope. The idea, as shown in Figure P7.38(a), is to sample the fast waveform x(t) once each period, but at successively later points in successive periods. The increment d should be an appropriately chosen sampling interval in relation to the bandwidth of x(t). If the resulting impulse train is then passed through an appropriate interpolat- (a) x(t) ---~~ x )-------!~ H(jw) t------!)1~ y(t) Periodic with period T; I X Gw) I= o for I w I >Wx 00 p(t) = ~ 3[t-n(T + Ll)] 1,lwl< -1- H(jw) = 2(T + Ll) { 0, elsewhere (b) Figure P7.38 Chap. 7 Problems 571 ing lowpass filter, the output y(t) will be proportional to the original fast waveform slowed down or stretched out in time [i.e., y(t) is proportional to x(at), where a < 1]. For x(t) = A+ B cos[(27TIT)t + 8], find a range of values of~ such that y(t) in Figure P7.38(b) is proportional to x(at) with a < 1. Also, determine the value of a in terms ofT and~. 7.39 A signal xp(t) is obtained through impule-train sampling of a sinusoidal signal x(t) whose frequency is equal to half the sampling frequency Ws. x(t) = cos ( ~' t + </.>) and +cc Xp(t) = L x(nT)8(t- nT) n=-cc where T = 27TIWs. (a) Find g(t) such that x(t) = cos( cf>) cos 2WI' t ) + g(t). ( (b) Show that g(nT) = 0 for n = 0, ± 1, ±2, · · · (c) Using the results of the previous two parts, show that if xp(t) is applied as the input to an ideallowpass filter with cutoff frequency wsf2, the resulting output lS y(t) = cos(cf>) cos Ws ) ( 2 t . 7 .40. Consider a disc on which four cycles of a sinusoid are painted. The disc is rotated at approximately 15 revolutions per second, so that the sinusoid, when viewed through a narrow slit, has a frequency of 60 Hz. The arrangement is indicated in Figure P7 .40. Let v(t) denote the position of the line seen through the slit. Then v(t) = A cos(w0 t + cf> ), w 0 = 1207T. Position of line varies sinusoidally at 60 cycles per second + ---- ......... / / / 1- ....... """"-. Disk rotating at / I \ f \ ""\ 15 rps I \ ""I I /~-lo. -- \ ....... I \ \ f L- \ \ \ \ ----- , __ f I ....... / \ I \ I '\ \ f I \ ""-._ - -- I / ....... / / ' - / Figure P7.40 572 Sampling Chap. 7 For notational convenience, we will normalize v(t) so that A = 1. At 60Hz, the eye is not able to follow v(t), and we will assume that this effect can be explained by modeling the eye as an ideallowpass filter with cutoff frequency 20 Hz. Sampling of the sinusoid can be accomplished by illuminating the disc with a strobe light. Thus, the illumination can be represented by an impulse train; that is, +oo i(t) = L 8(t - kT), k= -oc where liT is the strobe frequency in hertz. The resulting sampled signal is the prod- uct r(t) = v(t)i(t). Let R(jw ), V(jw ), and l(jw) denote the Fourier transforms of r(t), v(t), and i(t), respectively. (a) Sketch V(jw ), indicating clearly the effect of the parameters cp and w 0 . (b) Sketch /(jw ), indicating the effect ofT. (c) According to the sampling theorem, there is a maximum value for T in terms of w 0 such that v(t) can be recovered from r(t) using a lowpass filter. Determine this value ofT and the cutoff frequency of the lowpass filter. Sketch R(jw) when T is slightly less than the maximum value. If the sampling period T is made greater than the value determined in part (c), aliasing of the spectrum occurs. As a result of this aliasing, we perceive a lower frequency sinusoid. (d) Suppose that 27T/T = w 0 + 207T. Sketch R(jw) for lwl < 407T. Denote by va(t) ~he apparent position of the line as we perceive it. Assuming that the eye be- haves as an ideallowpass filter with 20-Hz cutoff and unity gain, express va(t) in the form Va(t) = Aa cos(wa + cf>a), where Aa is the apparent amplitude, Wa the apparent frequency, and cf>a the apparent phase of Va(t). (e) Repeat part ~d) for 27T/T = w 0 - 207T. 7.41. In many practical situations a signal is recorded in the presence of an echo, which we would like to remove by appropriate processing. For example, in Figure P7.41(a), we illustrate a system in which a receiver simultaneously receives a signal x(t) and an echo represented by an attenuated delayed replication of x(t). Thus, the receiver output is s(t) = x(t) + ax(t- T0 ), where Ia I < 1. This output is to be processed to recover x(t) by first converting to a sequence and then using an appropriate digital filter h[n], as indicated in Figure P7.4l(b). Assume that x(t) is band limited [i.e., X(jw) = 0 for lwl > WM] and that lal < 1. (a) IfT0 < 7T/wM,andthesamplingperiodistakentobeequaltoT0 (i.e.,T =To), determine the difference equation for the digital filter h[n] such that Yc(t) is proportional to x(t). (b) With the assumptions of part (a), specify the gain A of the ideallowpass filter such that Yc(t) = x(t). (c) Now suppose that 7TIWM <To< 27T/wM. Determine a choice for the sampling period T, the lowpass filter gain A, and the frequency response for the digital filter h[n] such that Yc(t) is proportional to x(t). Chap. 7 Problems 573 ~//////////& ~~t-T0) ~ Receiver output s(t) = x(t) +a x(t-T0) (a) ldeallowpass filter sc(t) = x(t) +ax(t-T ) ---L 0 Conversion of Conversion of sp(t) impulse train s[n] y[n] sequence to a h[n] to sequence impulse train -~ 'IT T T p(t) = ~ 8(t-kT) k =-X (b) Figure P7.41 7.42. Consider a band-limited signal xc(t) that is sampled at a rate higher than the Nyquist rate. The samples, spaced T seconds apart, are then converted to a sequence x[n], as indicated in Figure P7.42. p(t) = ~ 8(t-nT) n = -x 1 xp(t) Conversion of Xc(t) ----~1 x 1----~~ impulse train 1-----i~ x[n] = Xc (nT) to sequence Figure P7.42 Determine the relation between the energy Ed of the sequence, the energy Ec of the original signal, and the sampling interval T. The energy of a sequence x[n] is defined as n= -cxc and the energy in a continuous-time function Xc(t) is defined as +oc Ec = I-c 2 xc lxc(t)j dt. 574 Sampling Chap. 7 7.43. Figure P7.43(a) depicts a system for which the input and output are discrete-time signals. The discrete-time input x[n] is converted to a continuous-time impulse train Xp(t). The continuous-time signal Xp(t) is then filtered by an LTI system to produce the output Yc(t), which is then converted to the discrete-time signal y[n]. The LTI system with input Xc(t) and output Yc(t) is causal and is characterized by the linear constant -coefficient differential equation 2 d yc(t) + 4 dyc(t) () _ () ~ ~ + 3 Yc t - Xc t . The overall system is equivalent to a causal discrete-time LTI system, as indicated in Figure P7 .43(b ). Determine the frequency response H(eiw) and the unit sample response h[n] of the equivalent LTI system. +oo ~ 8(t-nT) n = -x HOw) x[n] Conversion Ih Conversion ~ toan h(t) to a y[n] impulse train sequence -'ITIT 'ITIT +-x xp(t) = ~ x[n] 8(t-nT) n =-x +x Yp(t) = Yc(t) ~ 8(t -nT) n =-x y[n] = Yc(nT) (a) h[n]; H(eiw) x[n] ---~ equivalent t----~ y[n] LTI system (b) Figure P7.43 7 .44. Suppose we wish to design a continuous-time generator that is capable of producing sinusoidal signals at any frequency satisfying where w 1 and w2 are given positive numbers. Our design is to take the following form: We have stored a discrete-time cosine wave of period N; that is, we have stored x[O], ... , x[N - 1], where Chap. 7 Problems 575 x[k] =cos N21 Tk) ( . Every T seconds we output an impulse weighted by a value of x[k], where we pro- ceed through the values of k = 0, l, ... , N - 1 in a cyclic fashion. That is, Yp(kT) = x(k modulo N), or equivalently, Yp(kT) = cos 21Tk) (N , and +o Yp(t) = k~oo o (2 cos ~ k) o(t - kT). (a) Show that by adjusting T, we can adjust the frequency of the cosine signal being sampled. That is, show that +oo Yp(t) = (cos wot) ~ o(t - kT), k= -oo where w 0 = 21TI NT. Determine a range of values for T such that yp Ct) can rep- resent samples of a cosine signal with a frequency that is variable over the full range (b) Sketch Y p(jw ). The overall system for generating a continuous-time sinusoid is depicted in Figure P7.44(a). H(jw) is an idea11owpass filter with unity gain in its pass- band; that is, 1 H(jw) = { ' lwl <We 0, otherwise· x[O] ~._I_H_(j_w_)- ---~~-----·... ... y(t) x[N-1] y(t)--.... G(jw) t----•.,._ cos wt (a) (b) Figure P7 .44 576 Sampling Chap. 7 The parameter We is to be determined so that y(t) is a continuous-time cosine signal in the desired frequency band. (c) Consider any value of T in the range determined in part (a). Determine the minimum value of Nand some value for we such that y(t) is a cosine signal in the range w 1 :5 w :5 w2. (d) The amplitude of y(t) will vary, depending upon the value of w chosen between w 1 and w 2 . Thus, we must design a system G(jw) that normalizes the signal as shown in Figure P7.44(b). Find such a G(jw ). 7.45. In the system shown in Figure P7.45, the input Xc(t) is band limited with Xc(jw) = 0, lwl > 27T X 104 . The digital filter h[n] is described by the input-output relation HUw) Conversion x[n] = Xc(nT) y[n] Conversion Yp(t) Yc(t) ·~ to a h[n] to an _ill_ ~ sequence impulse train l -~ 1T T T +oo p(t) = L o(t-nT) n = -x Figure P7 .45 II y[n] = T L x[k]. (P7.45-l) k= -'X (a) What is the maximum value of T allowed if aliasing is to be avoided in the transformation from Xc(t) to Xp(t). (b) With the discrete-time LTI system specified through eq. (P7.45-l), determine its impulse response h[n]. (c) Determine whether there is any value ofT for which ,!~ y[ n] = )~II! Lx ,.( T) d T. (P7 .45-2) If so, determine the maximum value. If not, explain and specify how T would be chosen so that the equality in eq. (P7.45-2) is best approximated. (Think carefully about this part; it is easy to jump to the wrong conclusion!) 7.46 A signal x[n] is sampled in discrete time as shown in Figure P7.46. hr[n] is an ideal lowpass filter with frequency response lwl < ~ ~ < lwl < 1T From eqs. (7.46) and (7.47), the filter output is expressible as x,[n] = k""!t;~ x[kN]h,[n- kN] = k'!toc x[kN]N;, si:~~(~ ~~~) Chap. 7 Problems 577 x[n] +oo p[n] = l o(n -kN) k =-'X Figure P7.46 where We = 2n""/N. Show that independent of whether the sequence x[n] is sam- pled above or below the Nyquist rate, Xr[mN] = x[mN], where m is any positive or negative integer. 7.47. Suppose x[n] has a Fourier transform that is zero for 7T/3 :::; lwl :::; 'TT. Show that oo (sin(-~(n - 3k))) x[n] = k~oo x[3k] ~(n - 3k) . 7.48. Ifx[n] = cos(~n+<f>0)with0:::; <Po< 27Tandg[n] = x[n]L~=-ooo[n-4k],what additional constraints must be imposed on <Po to ensure that sin !!_n) g[n] * ( ~~ = x[n]? 7.49. As discussed in Section 7.5 and illustrated in Figure 7.37, the procedure for interpo- lation or upsampling by an integer factor N can be thought of as the cascade of two operations. The first operation, involving system A, corresponds to inserting N - 1 zero-sequence values between each sequence value of x[ n], so that n = 0, ±N, ±2N, ... otherwise For exact band-limited interpolation, H(eiw) is an ideallowpass filter. (a) Determine whether or not system A is linear. (b) Determine whether or not system A is time invariant. (c) For Xd(eiw) as sketched in Figure P7.49 and with N = 3, sketch Xp(eiw). (d) For N = 3, Xd(eiw) as in Figure P7.49, and H(eiw) appropriately chosen for exact band-limited interpolation, sketch X(eiw). -'IT 'IT w Figure P7 .49 578 Sampling Chap. 7 7 .50. In this problem, we consider the discrete-time counterparts of the zero-order hold and first-order hold, which were discussed for continuous time in Sections 7 .1.2 and 7 .2. Let x[n] be a sequence to which discrete-time sampling, as illustrated in Fig- ure 7.31, has been applied. Suppose the conditions of the discrete-time sampling theorem are satisfied; that is, w s > 2w M, where Ws is the sampling frequency and X(ejw) = 0, WM < lwl ::=; 1T. The original signal x[n] is then exactly recoverable from Xp[n] by ideal lowpass filtering, which, as discussed in Section 7.5, corre- sponds to band-limited interpolation. The zero-order hold represents an approximate interpolation whereby every sample value is repeated (or held) N - 1 successive times, as illustrated in Figure P7.50(a) for the case of N = 3. The first-order hold represents a linear interpolation between samples, as illustrated in the same figure. x[n] I II I J I I r I I IJ JI I I J I t n xp[n] J • • I . . I . . I • • L. I• • I n rn-[JJlrJ1JTilllirt-:""[""~OH n x1 [n] FOH rfllflll1II1111111l (a) n ZOH x0[n]~x0[n] x0[n]-Et--- x[n] (b) (c) FOH ><p[n]~x1 [n] (d) Figure P7.50 Chap. 7 Problems 579 (a) The zero-order hold can be represented as an interpolation in the form of eq. (7.47) or, equivalently, the system in Figure P7.50(b). Determine and sketch h0 [n] for the general case of a sampling period N. (b) x[n] can be exactly recovered from the zero-order-hold sequence x0 [n] using an appropriate LTI filter H(efw), as indicated in Figure P7.50(c). Determine and sketch H(efw). (c) The first-order-hold (linear interpolation) can be represented as an interpolation in the form of eq. (7.47) or, equivalently, the system in Figure P7.50(d). Deter- mine and sketch h 1 [ n] for the general case of a sampling period N. (d) x[n] can be exactly recovered from the first-order-hold sequence x 1 [n] using an appropriate LTI filter with frequency response H(efw). Determine and sketch H(efw). 7.51. As shown in Figure 7.37 and discussed in Section 7.5.2, the procedure for inter- polation or upsampling by an integer factor N can be thought of as a cascade of two operations. For exact band-limited interpolation, the filter H(efw) in Figure 7.37 is an ideal lowpass filter. In any specific application, it would be necessary to implement an approximate lowpass filter. In this problem, we explore some use- ful constraints that are often imposed on the design of these approximate lowpass filters. (a) Suppose that H(efw) is approximated by a zero-phase FIR filter. The filter is to be designed with the constraint that the original sequence values xd [ n] get reproduced exactly; that is, x[n] ~ xd [I]• n ~ 0, ±L, ±2L, .... (P7 .51-1) This guarantees that, even though the interpolation between the original se- quence values may not be exact, the original values are reproduced exactly in the interpolation. Determine the constraint on the impulse response h[n] of the lowpass filter which guarantees that eq. (P7 .51-1) will hold exactly for any se- quence xd[ n]. (b) Now suppose that the interpolation is to be carried out with a linear-phase, causal, symmetric FIR filter of length N; that is h[n] = 0, n < 0, n > N - l, (P7.51-2) (P7.51-3) where HR(e.iw) is real. The filter is to be designed with the constraint that the original sequence values xd [ n] get reproduced exactly, but with an integer delay a, where a is the negative of the slope of the phase of H(efw); that is, x[n] = xd [-nL- a-] , n - a = 0, ±L, ±2L, ... (P7.51-4) Determine whether this imposes any constraint on whether the filter length N is odd or even. 580 Sampling Chap. 7 (c) Again, suppose that the interpolation is to be carried out with a linear-phase, causal, symmetric FIR filter, so that H(ejw) = HR(ejw)e- jf3w, where H R( ejw) is real. The filter is to be designed with the constraint that the original sequence values xd[n] get reproduced exactly, but with a delay M that is not necessarily equal to the slope of the pha&e; that is, x[n] = xd [-nL- a-] , n- M = 0, ±L, ±2L, .... Determine whether this imposes any constraint on whether the filter length N is odd or even. 7.52 In this problem we develop the dual to the time-domain sampling theorem, whereby a time-limited signal can be reconstructed fromfrequency-domain samples. To de- velop this result, consider the frequency-domain sampling operation in Figure P7 .52. - +oo X(jw)--~ X 1-----i~X(jw) = X(jw)P(jw) = ~ X0kw0) 3(w-kw0) k = -00 +oo P(jw) = ~ 3(w- kw0) k = -00 (J) P(jw) t t t t l 1 1 1 t (J) Figure P7.52 Chap. 7 Problems 581 (a) Show that x(t) = x(t) * p(t) where x(t), x(t), and p(t) are the inverse Fourier transforms of X(jw ), X(jw ), and P(jw ), respectively. (b) Assuming that x(t) is time-limited so that x(t) = 0 for ltl 2: _!!_,show that x(t) Wo can be obtained from x(t) through a ""low-time windowing"" operation. That is, x(t) = x(t)w(t) where wo, w(t) = { O, (c) Show that x(t) is not recoverable from x(t) if x(t) is not constrained to be zero for It I 2: .!!_. wo 8 CoMMUNICATION SYSTEMS 8.0 INTRODUCTION Communication systems play a key role in our modem world in transmitting information between people, systems, and computers. In general terms, in all communication systems the information at the source is first processed by a transmitter or modulator to change it into a form suitable for transmission over the communication channel. At the receiver, the signal is then recovered through appropriate processing. This processing is required for a variety of reasons. In particular, quite typically, any specific communication channel has associated with it a frequency range over which it is best suited for transmitting a signal and outside of which communication is severely degraded or impossible. For example, the atmosphere will rapidly attenuate signals in the audible frequency range ( 10 Hz to 20 kHz), whereas it will propagate signals at a higher frequency range over longer distances. Thus, in transmitting audio signals such as speech or music over a communication channel that relies on propagation through the atmosphere, the transmitter first embeds the signal through an appropriate process into another, higher frequency signal. Many of the concepts and techniques we have developed in the earlier chapters of this text play a central role in the analysis and design of communication systems. As with any concept that is closely tied to a wide variety of important applications, there are a large number of detailed issues to be considered, and, as indicated in the bibliography, there are many excellent texts on the subject. While a full and detailed analysis of communication systems is well beyond the scope of our discussions here, with the background of the previous chapters we are now in a position to introduce some of the basic principles and issues encountered in the design and analysis of these systems. The general process of embedding an information -bearing signal into a second signal is typically referred to as modulation. Extracting the information -bearing signal 582"
8 Communication Systems,"8 CoMMUNICATION SYSTEMS 8.0 INTRODUCTION Communication systems play a key role in our modem world in transmitting information between people, systems, and computers. In general terms, in all communication systems the information at the source is first processed by a transmitter or modulator to change it into a form suitable for transmission over the communication channel. At the receiver, the signal is then recovered through appropriate processing. This processing is required for a variety of reasons. In particular, quite typically, any specific communication channel has associated with it a frequency range over which it is best suited for transmitting a signal and outside of which communication is severely degraded or impossible. For example, the atmosphere will rapidly attenuate signals in the audible frequency range ( 10 Hz to 20 kHz), whereas it will propagate signals at a higher frequency range over longer distances. Thus, in transmitting audio signals such as speech or music over a communication channel that relies on propagation through the atmosphere, the transmitter first embeds the signal through an appropriate process into another, higher frequency signal. Many of the concepts and techniques we have developed in the earlier chapters of this text play a central role in the analysis and design of communication systems. As with any concept that is closely tied to a wide variety of important applications, there are a large number of detailed issues to be considered, and, as indicated in the bibliography, there are many excellent texts on the subject. While a full and detailed analysis of communication systems is well beyond the scope of our discussions here, with the background of the previous chapters we are now in a position to introduce some of the basic principles and issues encountered in the design and analysis of these systems. The general process of embedding an information -bearing signal into a second signal is typically referred to as modulation. Extracting the information -bearing signal 582"
8.0 Introduction,"8 CoMMUNICATION SYSTEMS 8.0 INTRODUCTION Communication systems play a key role in our modem world in transmitting information between people, systems, and computers. In general terms, in all communication systems the information at the source is first processed by a transmitter or modulator to change it into a form suitable for transmission over the communication channel. At the receiver, the signal is then recovered through appropriate processing. This processing is required for a variety of reasons. In particular, quite typically, any specific communication channel has associated with it a frequency range over which it is best suited for transmitting a signal and outside of which communication is severely degraded or impossible. For example, the atmosphere will rapidly attenuate signals in the audible frequency range ( 10 Hz to 20 kHz), whereas it will propagate signals at a higher frequency range over longer distances. Thus, in transmitting audio signals such as speech or music over a communication channel that relies on propagation through the atmosphere, the transmitter first embeds the signal through an appropriate process into another, higher frequency signal. Many of the concepts and techniques we have developed in the earlier chapters of this text play a central role in the analysis and design of communication systems. As with any concept that is closely tied to a wide variety of important applications, there are a large number of detailed issues to be considered, and, as indicated in the bibliography, there are many excellent texts on the subject. While a full and detailed analysis of communication systems is well beyond the scope of our discussions here, with the background of the previous chapters we are now in a position to introduce some of the basic principles and issues encountered in the design and analysis of these systems. The general process of embedding an information -bearing signal into a second signal is typically referred to as modulation. Extracting the information -bearing signal 582 Sec. 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 583 is known as demodulation. As we will see, modulation techniques not only allow us to embed information into signals that can be transmitted effectively, but also make possible the simultaneous transmission of more than one signal with overlapping spectra over the same channel, through a concept referred to as multiplexing. There are a wide variety of modulation methods used in practice, and in this chapter we examine several of the most important of these. One large class of modulation meth- ods relies on the concept of amplitude modulation or AM in which the signal we wish to transmit is used to modulate the amplitude of another signal. A very common form of am- plitude modulation is sinusoidal amplitude modulation, which we explore in some detail in Sections 8.1-8.4 together with the related concepts of frequency-division multiplexing. Another important class of AM systems involves the modulation of the amplitude of a pulsed signal, and in Sections 8.5 and 8.6 we examine this form of modulation as well as the con~ept of time-division multiplexing. In Section 8.7 we then examine a different form of modulation, namely sinusoidal frequency modulation in which the information -bearing signal is used to vary the frequency of a sinusoidal signal. All of the discussion up through Section 8.7 focuses attention on continuous-time signals, since most transmission media, such as the atmosphere, are best thought of as continuous-time phenomena. Nevertheless, not only is it possible to develop analogous techniques for discrete-time signals, but it is of considerable practical importance to con- sider modulation concepts involving such signals, and in Section 8.8 we examine some of the basic ideas behind the communication of discrete-time signals. 8. 1 COMPLEX EXPONENTIAL AND SINUSOIDAL AMPLITUDE MODULATION Many communication systems rely on the concept of sinusoidal amplitude modulation, in which a complex exponential or sinusoidal signal c(t) has its amplitude multiplied (mod- ulated) by the information -bearing signal x(t). The signal x(t) is typically referred to as the modulating signal and the signal c(t) as the carrier signal. The modulated signal y(t) is then the product of these two signals: y(t) = x(t)c(t) As we discussed in Section 8.0, an important objective in modulation is to produce a signal whose frequency range is suitable for transmission over the communication channel to be used. In telephone transmission systems, for example, long-distance transmission is often accomplished over microwave or satellite links. The individual voice signals are in the frequency range 200 Hz to 4 kHz, whereas a microwave link requires signals in the range 300 megahertz (MHz) to 300 gigahertz (GHz), and communication satellite links operate in the frequency range from a few hundred MHz to over 40 GHz. Thus, for transmission over these channels, the information in a voice signal must be shifted into these higher ranges of frequency. As we will see in this section, sinusoidal amplitude modulation achieves such a shift in frequency in a very simple manner. 8. 1. 1 Amplitude Modulation with a Complex Exponential Carrier There are two common forms of sinusoidal amplitude modulation, one in which the carrier signal is a complex exponential of the form c(t) = ei(w,t+8,) (8.1)"
8.1 Complex Exponential and Sinusoidal Amplitude Modulation,"Sec. 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 583 is known as demodulation. As we will see, modulation techniques not only allow us to embed information into signals that can be transmitted effectively, but also make possible the simultaneous transmission of more than one signal with overlapping spectra over the same channel, through a concept referred to as multiplexing. There are a wide variety of modulation methods used in practice, and in this chapter we examine several of the most important of these. One large class of modulation meth- ods relies on the concept of amplitude modulation or AM in which the signal we wish to transmit is used to modulate the amplitude of another signal. A very common form of am- plitude modulation is sinusoidal amplitude modulation, which we explore in some detail in Sections 8.1-8.4 together with the related concepts of frequency-division multiplexing. Another important class of AM systems involves the modulation of the amplitude of a pulsed signal, and in Sections 8.5 and 8.6 we examine this form of modulation as well as the con~ept of time-division multiplexing. In Section 8.7 we then examine a different form of modulation, namely sinusoidal frequency modulation in which the information -bearing signal is used to vary the frequency of a sinusoidal signal. All of the discussion up through Section 8.7 focuses attention on continuous-time signals, since most transmission media, such as the atmosphere, are best thought of as continuous-time phenomena. Nevertheless, not only is it possible to develop analogous techniques for discrete-time signals, but it is of considerable practical importance to con- sider modulation concepts involving such signals, and in Section 8.8 we examine some of the basic ideas behind the communication of discrete-time signals. 8. 1 COMPLEX EXPONENTIAL AND SINUSOIDAL AMPLITUDE MODULATION Many communication systems rely on the concept of sinusoidal amplitude modulation, in which a complex exponential or sinusoidal signal c(t) has its amplitude multiplied (mod- ulated) by the information -bearing signal x(t). The signal x(t) is typically referred to as the modulating signal and the signal c(t) as the carrier signal. The modulated signal y(t) is then the product of these two signals: y(t) = x(t)c(t) As we discussed in Section 8.0, an important objective in modulation is to produce a signal whose frequency range is suitable for transmission over the communication channel to be used. In telephone transmission systems, for example, long-distance transmission is often accomplished over microwave or satellite links. The individual voice signals are in the frequency range 200 Hz to 4 kHz, whereas a microwave link requires signals in the range 300 megahertz (MHz) to 300 gigahertz (GHz), and communication satellite links operate in the frequency range from a few hundred MHz to over 40 GHz. Thus, for transmission over these channels, the information in a voice signal must be shifted into these higher ranges of frequency. As we will see in this section, sinusoidal amplitude modulation achieves such a shift in frequency in a very simple manner. 8. 1. 1 Amplitude Modulation with a Complex Exponential Carrier There are two common forms of sinusoidal amplitude modulation, one in which the carrier signal is a complex exponential of the form c(t) = ei(w,t+8,) (8.1) 584 Communication Systems Chap. 8 and the second in which the carrier signal is sinusoidal and of the form c(t) = cos( wet + (J c). (8.2) In both cases, the frequency We is referred to as the carrier frequency. Let us consider first the case of a complex exponential carrier, and for convenience, let us choose (J e = 0, so that the modulated signal is (8.3) From the multiplication property (Section 4.5), and with X(jw ), Y(jw ), and C(jw) denoting the Fourier transforms of x(t), y(t), and c(t), respectively, } f+x Y(jw) = 7T _ x X(j(})C(j(w - (}))d(}. (8.4) 2 For c(t) a complex exponential as given in eq. (8.1), C(jw) = 27T8(w -We), (8.5) and hence, Y(jw) = X(jw - jwc). (8.6) Thus, the spectrum of the modulated output y(t) is simply that of the input, shifted in frequency by an amount equal to the carrier frequency We. For example, with X(jw) band limited with highest frequency WM (and bandwidth 2wM), as depicted in Figure 8.1(a), the output spectrum Y(jw) is that shown in Figure 8.l(c). X(jw) ~ -wM WM w (a) C(jw) 27T I t We w (b) Figure 8.1 Effect in the frequency domain of amplitude modulation with a complex exponential carrier: (a) spec- trum of modulating signal x(t); (b) spectrum of carrier c(t) = ejwct; (c) w spectrum of amplitude-modulated sig- (c) nal y(t) = x(t)ejwct. Sec. 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 585 From eq. (8.3), it is clear that x(t) can be recovered from the modulated signal y(t) by multiplying by the complex exponential e- jwct; that is, x(t) = y(t)e-jwct. (8.7) In the frequency domain, this has the effect of shifting the spectrum of the modulated signal back to its original position on the frequency axis. The process of recovering the original signal from the modulated signal is referred to as demodulation, a topic we discuss at more length in Section 8.2. Since ejwct is a complex signal, eq. (8.3) can be rewritten as y(t) = x(t)coswct + jx(t)sinwct. (8.8) Implementation of eq. (8.7) or (8.8) with x(t) real utilizes two separate multipliers and two sinusoidal carrier signals that have a phase difference of 7T/2, as depicted in Figure 8.2 for c(t) given by eq. (8.1). In Section 8.4 we give an example of one of the applications in which there are particular advantages to using a system, such as in Figure 8.2, employing two sinusoidal carriers with a phase difference of 7T/2. x(t) Figure 8.2 Implementation of am- plitude modulation with a complex ex- ponential carrier c(t) = ei(wct+ec). 8. 1 .2 Amplitude Modulation with a Sinusoidal Carrier In many situations, using a sinusoidal carrier of the form of eq. (8.2) is often simpler than and equally as effective as using a complex exponential carrier. In effect, using a sinusoidal carrier corresponds to retaining only the real or imaginary part of the output of Figure 8.2. A system that uses a sinusoidal carrier is depicted in Figure 8.3. x(t) --co.:. ~ ;rl--ec-) ----l·~ y(t) Figure 8.3 Amplitude modulation with a sinusoidal carrier. The effect of amplitude modulation with a sinusoidal carrier in the form of eq. (8.2) can be analyzed in a manner identical to that in the preceding subsection. Again, for 586 Communication Systems Chap.8 convenience we choose () c = 0. In this case, the spectrum of the carrier signal is C(jw) = 7T[8(w - we) + 8(w + We)], (8.9) and thus, from eq. (8.4 ), 1 Y(jw) = 2 [X(jw - jwc) + X(jw + jwe)]. (8.10) With X(jw) as depicted in Figure 8.4(a), the spectrum of y(t) is that shown in Figure 8.4(c). Note that there is now a replication of the spectrum of the original signal, centered around both +we and -we. As a consequence, x(t) is recoverable from y(t) only if we>ww since otherwise the two replications will overlap in frequency. This is in contrast to the case of a complex exponential carrier, for which a replication of the spectrum of the original signal is centered only around We. Specifically, as we saw in Section 8.1.1, in the case of amplitude modulation with a complex exponential carrier, x(t) can always be recovered from y(t) for any choice of We by shifting the spectrum back to its original location by multiplying by e~ jw, t, as in eq. (8.7). With a sinusoidal carrier, on the other hand, as we see from Figure 8.4, if We < WM, then there will be an overlap between the two replications of X(jw ). For example, Figure 8.5 depicts Y(jw) for we = wM/2. Clearly, the spectrum of x(t) is no longer replicated in Y(jw ), and thus, it may no longer be possible to recover x(t) from y(t). X(jw) ch -wM WM w (a) C(jw) f I f -we We w (b) Figure 8.4 Effect in the frequency domain of amplitude modulation with a sinusoidal carrier: (a) spectrum of modulating signal x(t); (b) spectrum w of carrier c(t) = cos wet; (c) spectrum (c) of amplitude-modulated signal. Sec. 8.2 Demodulation for Sinusoidal AM 587 (a) Figure 8.5 Sinusoidal amplitude modulation with carrier cos wet for which we = wM/2: (a) spectrum of (J) modulating signal; (b) spectrum of (b) modulated signal. 8.2 DEMODULATION FOR SINUSOIDAL AM At the receiver in a communication system, the information-bearing signal x(t) is recov- ered through demodulation. In this section, we examine the process of demodulation for sinusoidal amplitude modulation, as introduced in the previous section. There are two com- monly used methods for demodulation, each with its own advantages and disadvantages. In Section 8.2.1 we discuss the first of these, a process referred to as synchronous demod- ulation, in which the transmitter and receiver are synchronized in phase. In Section 8.2.2, we describe an alternative method referred to as asynchronous demodulation. 8.2.1 Synchronous Demodulation Assuming that we > w M, demodulation of a signal that was modulated with a sinusoidal carrier is relatively straightforward. Specifically, consider the signal y(t) = x(t) cos Wet. (8.11) As was suggested in Example 4.21, the original signal can be recovered by modulating y(t) with the same sinusoidal carrier and applying a lowpass filter to the result. To see this, consider w(t) = y(t) cos Wet. (8.12) Figure 8.6 shows the spectra of y(t) and w(t), and we observe that x(t) can be recovered from w(t) by applying an ideallowpass filter with a gain of 2 and a cutoff frequency that is greater than w M and less than 2w e - w M. The frequency response of the lowpass filter is indicated by the dashed line in Figure 8.6(c). The basis for using eq. (8.12) and a lowpass filter to demodulate y(t) can also be seen algebraically. From eqs. (8.11) and (8.12), it follows that w(t) = x(t) cos2 Wet,"
8.2 Demodulation for Sinusoidal AM,"Sec. 8.2 Demodulation for Sinusoidal AM 587 (a) Figure 8.5 Sinusoidal amplitude modulation with carrier cos wet for which we = wM/2: (a) spectrum of (J) modulating signal; (b) spectrum of (b) modulated signal. 8.2 DEMODULATION FOR SINUSOIDAL AM At the receiver in a communication system, the information-bearing signal x(t) is recov- ered through demodulation. In this section, we examine the process of demodulation for sinusoidal amplitude modulation, as introduced in the previous section. There are two com- monly used methods for demodulation, each with its own advantages and disadvantages. In Section 8.2.1 we discuss the first of these, a process referred to as synchronous demod- ulation, in which the transmitter and receiver are synchronized in phase. In Section 8.2.2, we describe an alternative method referred to as asynchronous demodulation. 8.2.1 Synchronous Demodulation Assuming that we > w M, demodulation of a signal that was modulated with a sinusoidal carrier is relatively straightforward. Specifically, consider the signal y(t) = x(t) cos Wet. (8.11) As was suggested in Example 4.21, the original signal can be recovered by modulating y(t) with the same sinusoidal carrier and applying a lowpass filter to the result. To see this, consider w(t) = y(t) cos Wet. (8.12) Figure 8.6 shows the spectra of y(t) and w(t), and we observe that x(t) can be recovered from w(t) by applying an ideallowpass filter with a gain of 2 and a cutoff frequency that is greater than w M and less than 2w e - w M. The frequency response of the lowpass filter is indicated by the dashed line in Figure 8.6(c). The basis for using eq. (8.12) and a lowpass filter to demodulate y(t) can also be seen algebraically. From eqs. (8.11) and (8.12), it follows that w(t) = x(t) cos2 Wet, 588 Communication Systems Chap. 8 Y(jw) A ~ -we (we-wM) We (we+wM) w (a) C(jw) 1T 1T 1 I 1 -we We w (b) - W(jw) I -.1 I 2&. -, I I I I I I I I w (c) Figure 8.6 Demodulation of an amplitude-modulated signal with a sinu- soidal carrier: (a) spectrum of modulated signal; (b) spectrum of carrier signal; (c) spectrum of modulated signal multiplied by the carrier. The dashed line indicates the frequency response of a lowpass filter used to extract the de- modulated signal. or, using the trigonometric identity 1 1 cos2 Wet = 2 + 2 cos 2wet, we can rewrite w(t) as 1 1 w(t) = 2x (t) + 2x (t) cos 2wet. (8.13) Thus, w(t) consists of the sum of two terms, namely one-half the original signal and one- half the original sighal modulated with a sinusoidal carrier at twice the original carrier frequency We. Both of these terms are apparent in the spectrum shown in Figure 8.6(c). Applying the lowpass filter to w(t) corresponds to retaining the first term on the right-hand side of eq. (8.13) and eliminating the second t~rm. The overall system for amplitude modulation and demodulation using a complex exponential carrier is depicted in Figure 8.7, and the overall system for modulation and demodulation using a sinusoidal carrier is depicted in Figure 8.8. In these figures, we have indicated the more general case in which, for both the complex exponential and the sinusoidal carrier, a carrier phase () c is included. The modification of the preceding analysis so as to include ()cis straightforward and is considered in Problem 8.21. Sec. 8.2 Demodulation for Sinusoidal AM 589 x(t) ------~~ X l---~ y(t) x(t)--~0~--~ y(t) l ei(wct + flc) (a) (a) y(t) ~w(t) H(jw) w(t) l y(t) --~ 0 X ~--~-~~ 21 1-----'!~ x(t) e-j(Wcl + flc) l Lowpass filter (b) cos (wet + ec) Figure 8. 7 System for ampli- (b) tude modulation and demodulation using a complex exponential car- Figure 8.8 Amplitude modulation and demodulation with a sinusoidal car- rier: (a) modulation; (b) demodula- rier: (a) modulation system; (b) demodulation system. The lowpass filter cut- tion. off frequency Wco is greater than wM and less than 2wc- wM. In the systems of Figures 8.7 and 8.8, the demodulating signal is assumed to be synchronized in phase with the modulating signal, and consequently the process is referred to as synchronous demodulation. Suppose, however, that the modulator and demodulator are not synchronized in phase. For the case of the complex exponential carrier, with 8 c denoting the phase of the modulating carrier and cf>c the phase of the demodulating carrier, y(t) = ej(wct+8c) x(t), (8.14) w(t) = e- j(wct+4>c) y(t), (8.15) and consequently, w(t) = ej(Oc-4>c) x(t). (8.16) Thus, if 8 c -# cf>c, w(t) will have a complex amplitude factor. For the particular case in which x(t) is positive, x(t) = lw(t)l, and thus x(t) can be recovered by taking the magni- tude of the demodulated signal. For the sinusoidal carrier, again let 8 c and cf>c denote the phases of the modulating and demodulating carriers, respectively, as indicated in Figure 8.9. The input to the lowpass filter is now w(t) = x(t) cos( wet + 8 c) cos(w et+ cf>c), (8.17) or, using the trigonometric identity (8.18) 590 Communication Systems Chap.8 x(t) __. .....,.~ X 1--~ y(t) (a) H(jw) w(t) y(t) --ooo~•~0 X 1------11•~1 21 l (b) Figure 8. 9 Sinusoidal amplitude modulation and demodulation system for which the carrier signals and the modulator and demodulator are not synchro- nized: (a) modulator; (b) demodulator. we have (8.19) and the output of the lowpass filter is then x(t) multiplied by the amplitude factor cos(8c- cf>c). If the oscillators in the modulator and demodulator are in phase, 8 c = cf>c, and the output of the lowpass filter is x(t). On the other hand, if these oscillators have a phase difference of 7r/2, the output will be zero. In general, for a maximum output signal, the os- cillators should be in phase. Of even more importance, the phase relation between the two oscillators must be maintained over time, so that the amplitude factor cos( 8 c - cf>c) does not vary. This requires careful synchronization between the modulator and the demodu- lator, which is often difficult, particularly when they are geographically separated, as is typical in a communication system. The corresponding effects of, and the need for, syn- chronization not only between the phase of the modulator and demodulator, but between the frequencies of the carrier signals used in both, are explored in detail in Problem 8.23. 8.2.2 Asynchronous Demodulation In many systems that employ sinusoidal amplitude modulation, an alternative demod- ulation procedure referred to as asynchronous demodulation is commonly used. Asyn- chronous demodulation avoids the need for synchronization between the modulator and demodulator. In particular, suppose that x(t) is always positive and that the carrier fre- quency w c is much higher than w M, the highest frequency in the modulating signal. The modulated signal y(t) will then have the general form illustrated in Figure 8.10. Sec. 8.2 Demodulation for Sinusoidal AM 591 In particular, the envelope of y(t)-that is, a smooth curve connecting the peaks in y(t)- would appear to be a reasonable approximation to x(t). Thus, x(t) could be approximately recovered through the use of a system that tracks these peaks to extract the envelope. Such a system is referred to as an envelope detector. One example of a simple circuit that acts as an envelope detector is shown in Figure 8.ll(a). This circuit is generally followed by a lowpass filter to reduce the variations at the carrier frequency, which are evident in Figure 8.ll(b) and which will generally be present in the output of an envelope detector of the type indicated in Figure 8.11(a). The two basic assumptions required for asynchronous demodulation are that x(t) be positive and that x(t) vary slowly compared to We, so that the envelope is easily tracked. The second condition is satisfied, for example, in audio transmission over a radio- frequency (RF) channel, where the highest frequency present in x(t) is typically 15 to 20 kHz and wei2TT is in the range 500kHz to 2 MHz. The first condition, that x(t) be positive, can be satisfied by simply adding an appropriate constant value to x(t) or, equivalently, by a simple change in the modulator, as shown in Figure 8.12. The output of the envelope detector then approximates x(t) +A, from which x(t) is easily obtained. To use the envelope detector for demodulation, we require that A be sufficiently large so that x(t) + A is positive. Let K denote the maximum amplitude of x(t); that is, lx(t)l ::::; K. For x(t) +A to be positive, we require that A> K. The ratio KIA is commonly referred to as the modulation index m. Expressed in percent, it is referred to as the percent modulation. An illustration of the output of the modulator of Figure 8.12 for x(t) sinu- soidal and form = 0.5 (50% modulation) and m = 1.0 (100% modulation), is shown in Figure 8.13. In Figure 8.14, we show a comparison of the spectra associated with the modulated signal when synchronous demodulation and when asynchronous demodulation are used. We note in particular that the output of the modulator for the asynchronous system in Figure 8.12 has an additional component A cos wet that is neither present nor necessary in the synchronous system. This is represented in the spectrum of Figure 8.14(c) by the pres- ence of impulses at +we and -we. For a fixed maximum amplitude K of the modulating signal, as A is decreased the relative amount of carrier present in the modulated output decreases. Since the carrier component in the output contains no information, its presence y(t) J( Envelope ______, Figure 8. 1 0 Amplitude-modulated signal for which the modulating signal Envelope is positive. The dashed curve repre- sents the envelope of the modulated signal. 592 Communication Systems Chap.8 + y(t) c R w(t) (a) (b) Figure 8. 11 Demodulation by envelope detection: (a) circuit for envelope detection using half-wave rectification; (b) waveforms associated with the en- velope detector in (a): r(t) is the half-wave rectified signal, x(t) is the true envelope, and w(t) is the envelope obtained from the circuit in (a). The rela- tionship between x(t) and w(t) has been exaggerated in (b) for purposes of illustration. In a practical asynchronous demodulation system, w(t) would typi- cally be a much closer approximation to x(t) than depicted here. x(t)--~ 1---~ y(t)=(A+x(t)) coswct A Figure 8. 12 Modulator for an asynchronous modulation-demodulation system. represents an inefficiency-for example, in the amount of power required to transmit the modulated signal-and thus, in one sense it is desirable to make the ratio Kl A-i.e., the modulation index m-as large as possible. On the other hand, the ability of a simple envelope detector such as that in Figure 8.11 to follow the envelope and thus extract x(t) improves as the modulation index decreases. Hence, there is a trade-off between the effi- Sec. 8.2 Demodulation for Sinusoidal AM 593 (a) Figure 8. 1 3 Output of the am- plitude modulation system of Figure 8.12: (a) modulation index m = 0.5; (b) (b) modulation index m = 1.0. X(jw) & -wM WM w (a) 6 it 6 w Figure 8. 14 Comparison of spec- -we We (b) tra for synchronous and asynchronous sinusoidal amplitude modulation sys- tems: (a) spectrum of modulating signal; (b) spectrum of x(t) cos wet representing modulated signal in a synchronous system; (c) spectrum of [x(t) + A] cos wet representing modu- w lated signal in an asynchronous (c) system. 594 Communication Systems Chap.8 ciency of the system in terms of the power in the output of the modulator and the quality of the demodulated signal. There are a number of advantages and disadvantages to the asynchronous modulation- demodulation system of Figures 8.11 and 8.12, compared with the synchronous system of Figure 8.8. The synchronous system requires a more sophisticated demodulator because the oscillator in the demodulator must be synchronized with the oscillator in the modu- lator, both in phase and in frequency. On the other hand, the asynchronous modulator in general requires transmitting more power than the synchronous modulator, since, for the envelope detector to operate properly, the envelope must be positive, or equivalently, there must be a carrier component present in the transmitted signal. This is often preferable in cases such as that associated with public radio broadcasting, in which it is desirable to mass-produce large numbers of receivers (demodulators) at moderate cost. The additional cost in transmitted power is then offset by the savings in cost for the receiver. On the other hand, in situations in which transmitter power requirements are at a premium, as in satellite communication, the cost of implementing a more sophisticated synchronous receiver is warranted. 8.3 FREQUENCY-DIVISION MULTIPLEXING Many systems used for transmitting signals provide more bandwidth than is required for any one signal. For example, a typical microwave link has a total bandwidth of several gigahertz, which is considerably greater than the bandwidth required for one voice chan- nel. If the individual voice signals, which are overlapping in frequency, have their fre- quency content shifted by means of sinusoidal amplitude modulation so that the spectra of the modulated signals no longer overlap, they can be transmitted simultaneously over a single wide band channel. The resulting concept is referred to as frequency-division multi- plexing (FDM). Frequency-division multiplexing using a sinusoidal carrier is illustrated in Figure 8.15. The_ individual signals to be transmitted are assumed to be band limited and COSWat ~ Ya(t) Xa(t)~ coswbt ~ Yb(t) xb(t) X w(t) Figure 8. 1 5 Frequency-division multiplexing using sinusoidal amplitude , modulation."
8.3 Frequency-Division Multiplexing,"594 Communication Systems Chap.8 ciency of the system in terms of the power in the output of the modulator and the quality of the demodulated signal. There are a number of advantages and disadvantages to the asynchronous modulation- demodulation system of Figures 8.11 and 8.12, compared with the synchronous system of Figure 8.8. The synchronous system requires a more sophisticated demodulator because the oscillator in the demodulator must be synchronized with the oscillator in the modu- lator, both in phase and in frequency. On the other hand, the asynchronous modulator in general requires transmitting more power than the synchronous modulator, since, for the envelope detector to operate properly, the envelope must be positive, or equivalently, there must be a carrier component present in the transmitted signal. This is often preferable in cases such as that associated with public radio broadcasting, in which it is desirable to mass-produce large numbers of receivers (demodulators) at moderate cost. The additional cost in transmitted power is then offset by the savings in cost for the receiver. On the other hand, in situations in which transmitter power requirements are at a premium, as in satellite communication, the cost of implementing a more sophisticated synchronous receiver is warranted. 8.3 FREQUENCY-DIVISION MULTIPLEXING Many systems used for transmitting signals provide more bandwidth than is required for any one signal. For example, a typical microwave link has a total bandwidth of several gigahertz, which is considerably greater than the bandwidth required for one voice chan- nel. If the individual voice signals, which are overlapping in frequency, have their fre- quency content shifted by means of sinusoidal amplitude modulation so that the spectra of the modulated signals no longer overlap, they can be transmitted simultaneously over a single wide band channel. The resulting concept is referred to as frequency-division multi- plexing (FDM). Frequency-division multiplexing using a sinusoidal carrier is illustrated in Figure 8.15. The_ individual signals to be transmitted are assumed to be band limited and COSWat ~ Ya(t) Xa(t)~ coswbt ~ Yb(t) xb(t) X w(t) Figure 8. 1 5 Frequency-division multiplexing using sinusoidal amplitude , modulation. Sec. 8.3 Frequency-Division Multiplexing 595 are modulated with different carrier frequencies. The modulated signals are then summed and transmitted simultaneously over the same communication channel. The spectra of the individual subchannels and the composite multiplexed signal are illustrated in Figure 8.16. Through this multiplexing process, the individual input signals are allocated distinct seg- ments of the frequency band. To recover the individual channels in the demultiplexing process requires two basic steps: bandpass filtering to extract the modulated signal corre- sponding to a specific channel, followed by demodulation to recover the original signal. This is illustrated in Figure 8.17 to recover channel a, where, for purposes of illustration, synchronous demodulation is assumed. Xa(jw) Xb(jw) Xe(jw) __ill_ __rb_ ~ -wM WM w -wM WM w -wM WM w Ya(jw) ~ I ~ -wa -wa w Yb(jw) (I (I I -wb -wb w Ye(jw) 1\ I 1\ -we We W W(jw) /\(I~ ~(II\ Figure 8.16 Spectra associated I with the frequency-division multiplexing -we -wb -wa Wa wb We w system of Figure 8.15. 596 Communication Systems Chap.8 ~+-----Demultiplexing-----f-+--------Demodulation-----~ Bandpass Lowpass filter coswat filter H1(jw) ~ H2 (jw) w(t) Ya(t) X 1,1111,1 I 12 I -wa Wa w -wM wM w Figure 8. 1 7 Demultiplexing and demodulation for a frequency-division multiplexed signal. Telephone communication is one important application of frequency-division multi- plexing. Another is the transmission of signals through the atmosphere in the RF band. In the United States, the use of radio frequencies for transmitting signals over the range 10 kHz to 275 GHz is controlled by the Federal Communications Commission, and different portions of the range are allocated for different purposes. The current allo- cation of frequencies is shown in Figure 8.18. As indicated, the frequency range in the neighborhood of 1 MHz is assigned to the AM broadcast band, where AM refers specifically to the use of sinusoidal amplitude modulation. Individual AM radio sta- tions are assigned specific frequencies within the AM band, and thus, many stations can broadcast simultaneously through this use of frequency-division multiplexing. In principle, at the receiver, an individual radio station can be selected by demultiplex- ing and demodulating, as illustrated in Figure 8.17. The tuning dial on the receiver would then control both the center frequency of the bandpass filter and the frequency of the demodulating oscillator. In fact, for public broadcasting, asynchronous modulation and demodulation are used to simplify the receiver and reduce its cost. Furthermore, the demultiplexing in Figure 8.17 requires a sharp cutoff bandpass filter with variable center frequency. Variable frequency-selective filters are difficult to implement, and consequently, a fixed filter is implemented instead, and an intermediate stage of mod- ulation and filtering [referred to in a radio receiver as the intermediate-frequency (IF) stage] is used. The use of modulation to slide the spectrum of the signal past a fixed bandpass filter replaces the use of a variable bandpass filter in a manner similar to the procedure discussed in Section 4.5 .1. This basic procedure is incorporated into typical home AM radio receivers. Some of the more detailed issues involved are considered in Problem 8.36. As illustrated in Figure 8.16, in the frequency-division multiplexing system of Fig- ure 8.15 the spectrum of each individual signal is replicated at both positive and negative frequencies, and thus the modulated signal occupies twice the bandwidth of the original. This represents an inefficient use of bandwidth. In the next section we consider an al- ternative form of sinusoidal amplitude modulation, which leads to more efficient use of bandwidth at the cost of a more complicated modulation system. Sec. 8.4 Single-Sideband Sinusoidal Amplitude Modulation 597 Frequency Propagation Channel range Designation Typical uses method features 30-300 Hz ELF Macrowave, submarine com- Megametric waves Penetration of conducting (extremely munication earth and seawater low frequency) 0.3-3 kHz VF Data terminals, telephony Copper wire (voice frequency) 3-30kHz VLF Navigation, telephone, tele- Surface ducting Low attenuation, little fading, (very low fre- graph, frequency and timing (ground wave) extremely stable phase and quency) standards frequency, large antennas 30-300 kHz LF Industrial (power line) com- Mostly surface ducting Slight fading, high atmo- (low frequency) munication, aeronautical spheric pulse and maritime long-range navigation, radio beacons 0.3-3 MHz MF Mobile, AM broadcasting, Ducting and ionospheric Increased fading, but reliable (medium frequency) amateur, public safety reflection (sky wave) 3-30MHz HF Military communication, aero- Ionospheric reflecting sky Intermittent and frequency- (high frequency) nautical mobile, interna- wave, 50-400 km layer selective fading, multipath tiona) fixed, amateur and altitudes citizen's band, industrial 30-300 MHz VHF FM and TV broadcast, land Sky wave (ionospheric and Fading, scattering, and multi- (very high transportation (taxis, buses, tropospheric scatter) path frequency) railroad) 0.3-3 GHz UHF UHF TV, space telemetry, Transhorizon tropospheric (ultra high radar, military scatter and line-of-sight frequency) relaying 3-30 GHz SHF Satellite and space commu- Line-of-sight ionosphere Ionospheric penetration, (super high nication. common carrier penetration extraterrestrial noise, frequency) (CC), microwave high directly 30-300 GHz EHF Experimental, government, Line of sight Water vapor and oxygen (extremely high radio astronomy absorption frequency) 103-107 GHz Infrared, visible light, Optical communications Line of sight ultraviolet Figure 8. 18 Allocation of frequencies in the RF spectrum. 8.4 SINGLE-SIDEBAND SINUSOIDAL AMPLITUDE MODULATION For the sinusoidal amplitude modulation systems discussed in Section 8.1, the total bandwidth of the original signal x(t) is 2w M, including both positive and negative frequencies, where WM is the highest frequency present in x(t). With the use of a complex exponential carrier, the spectrum is translated to w c, and the total width of the frequency band over which there is energy from the signal is still 2w M, although the modulated signal is now complex. With a sinusoidal carrier, on the other hand, the spec- trum of the signal is shifted to +we and -we, and thus, twice the bandwidth is required. This suggests that there is a basic redundancy in the modulated signal with a sinusoidal carrier. Using a technique referred to as single-sideband modulation, we can remove the redundancy. The spectrum of x(t) is illustrated in Figure 8.19(a), in which we have shaded the positive and negative frequency components differently to distinguish them. The spectrum"
8.4 Single-Sideband Sinusoidal Amplitude Modulation,"Sec. 8.4 Single-Sideband Sinusoidal Amplitude Modulation 597 Frequency Propagation Channel range Designation Typical uses method features 30-300 Hz ELF Macrowave, submarine com- Megametric waves Penetration of conducting (extremely munication earth and seawater low frequency) 0.3-3 kHz VF Data terminals, telephony Copper wire (voice frequency) 3-30kHz VLF Navigation, telephone, tele- Surface ducting Low attenuation, little fading, (very low fre- graph, frequency and timing (ground wave) extremely stable phase and quency) standards frequency, large antennas 30-300 kHz LF Industrial (power line) com- Mostly surface ducting Slight fading, high atmo- (low frequency) munication, aeronautical spheric pulse and maritime long-range navigation, radio beacons 0.3-3 MHz MF Mobile, AM broadcasting, Ducting and ionospheric Increased fading, but reliable (medium frequency) amateur, public safety reflection (sky wave) 3-30MHz HF Military communication, aero- Ionospheric reflecting sky Intermittent and frequency- (high frequency) nautical mobile, interna- wave, 50-400 km layer selective fading, multipath tiona) fixed, amateur and altitudes citizen's band, industrial 30-300 MHz VHF FM and TV broadcast, land Sky wave (ionospheric and Fading, scattering, and multi- (very high transportation (taxis, buses, tropospheric scatter) path frequency) railroad) 0.3-3 GHz UHF UHF TV, space telemetry, Transhorizon tropospheric (ultra high radar, military scatter and line-of-sight frequency) relaying 3-30 GHz SHF Satellite and space commu- Line-of-sight ionosphere Ionospheric penetration, (super high nication. common carrier penetration extraterrestrial noise, frequency) (CC), microwave high directly 30-300 GHz EHF Experimental, government, Line of sight Water vapor and oxygen (extremely high radio astronomy absorption frequency) 103-107 GHz Infrared, visible light, Optical communications Line of sight ultraviolet Figure 8. 18 Allocation of frequencies in the RF spectrum. 8.4 SINGLE-SIDEBAND SINUSOIDAL AMPLITUDE MODULATION For the sinusoidal amplitude modulation systems discussed in Section 8.1, the total bandwidth of the original signal x(t) is 2w M, including both positive and negative frequencies, where WM is the highest frequency present in x(t). With the use of a complex exponential carrier, the spectrum is translated to w c, and the total width of the frequency band over which there is energy from the signal is still 2w M, although the modulated signal is now complex. With a sinusoidal carrier, on the other hand, the spec- trum of the signal is shifted to +we and -we, and thus, twice the bandwidth is required. This suggests that there is a basic redundancy in the modulated signal with a sinusoidal carrier. Using a technique referred to as single-sideband modulation, we can remove the redundancy. The spectrum of x(t) is illustrated in Figure 8.19(a), in which we have shaded the positive and negative frequency components differently to distinguish them. The spectrum 598 Communication Systems Chap. a in Figure 8.19(b) results from modulation with a sinusoidal carrier, where we identify an upper and lower sideband for the portion of the spectrum centered at +we and that centered at -we. Comparing Figures 8.19(a) and (b), we see that X(jw) can be recovered if only the upper sidebands at positive and negative frequencies are retained, or alternatively, if only the lower sidebands at positive and negative frequencies are retained. The resulting spectrum if only the upper sidebands are retained is shown in Figure 8.19(c), and the resulting spectrum if only the lower sidebands are retained is shown in Figure 8.19(d). The conversion of x(t) to the form corresponding to Figure 8.19(c) or (d) is referred to as single-sideband modulation (SSB), in contrast to the double-sideband modulation (DSB) of Figure 8.19(b), in which both sidebands are retained. There are several methods by which the single-sideband signal can be obtained. One is to apply a sharp cutoff bandpass or high pass filter to the double-sideband signal of Figure 8.19(b ), as illustrated in Figure 8.20, to remove the unwanted sideband. Another is to use a procedure that utilizes phase shifting. Figure 8.21 depicts a system designed X(jw) ~ -wM WM w (a) Y(jw) A t A w sideband sideband (b) sideband sideband Yu(jw) ~ ~l ~ -we We w (c) Yj(jw) t Figure 8.19 Double- and single- sideband modulation: (a) spectrum of ~ ~ modulating signal; (b) spectrum af- ter modulation with a sinusoidal car- rier; (c) spectrum with only the upper -we We w sidebands; (d) spectrum with only the (d) lower sidebands. Sec. 8.4 Single-Sideband Sinusoidal Amplitude Modulation 599 Y(t) H(jw) • Yu(t) ·I Y(jw) A ~t A -we We w H(jw) 1t -we We w Yu(jw) ~ ~1 ~ Figure 8.20 System for retaining the upper sidebands using ideal high- -we We w pass filtering. to retain the lower sidebands. The system H(jw) in the figure is referred to as a ""90° phase-shift network,"" for which the frequency response is of the form H(jw) = { ~ j, w >0 (8.20) ], w <0"" The spectra of x(t), Y1 (t) = x(t) cos Wet, Y2(t) = Xp(t) sin wet, and y(t) are illustrated in Figure 8.22. As is examined in Problem 8.28, to retain the upper sidebands instead of the lower sidebands, the phase characteristic of H(jw) is reversed so that H(jw) = { j, . w >0 (8.21) - ], w <0"" As is explored in Problem 8.29, synchronous demodulation of single-sideband systems can be accomplished in a manner identical to synchronous demodulation of double-sideband systems. The price paid for the increased efficiency of single-sideband systems is added complexity in the modulator. 600 Communication Systems Chap. a Y1 (t) x(t)--.. y(t) t H(jw) <):: H(jw) ± 1T ---~2 w w -¥1---- Figure 8.21 System for single-sideband amplitude modulation, using a goo phase-shift network, in which only the lower sidebands are retained. In summary, in Sections 8.1 through 8.4 we have seen a number of variations of complex exponential and sinusoidal amplitude modulation. With asynchronous demod- ulation, discussed in Section 8.2.2, a constant must be added to the modulating signal so that it is positive. This results in the presence of the carrier signal as a component in the modulated output, requiring more power for transmission, but resulting in a simpler demodulator than is required in a synchronous system. Alternatively, only the upper or lower sidebands in the modulated output may be retained, which makes more efficient use of bandwidth and transmitter power, but requires a more sophisticated modulator. Sinu- soidal amplitude modulation with both sidebands and the presence of a carrier is typically abbreviated as AM-DSB/WC (amplitude modulation, double sideband/with carrier) and, when the carrier is suppressed or absent, as AM-DSB/SC (amplitude modulation, double- sideband/suppressed carrier). The corresponding single-sideband systems are abbreviated AM-SSB/WC and AM-SSB/SC. Sections 8.1 through 8.4 are intended to provide an introduction to many of the basic concepts associated with sinusoidal amplitude modulation. There are many variations in details and implementation, and the reader is referred to the bibliography for an indication of the numerous excellent books that explore this topic further. Sec. 8.5 Amplitude Modulation with a Pulse-Train Carrier 601 X(jw) w Y1(jw) L\, ~1 A -we We w Y2 (jw) 1 2 w Y(jw) 1t Figure 8.22 Spectra associated with the single-sideband system of w Figure 8.21. 8.5 AMPLITUDE MODULATION WITH A PULSE-TRAIN CARRIER 8.5.1 Modulation of a Pulse-Train Carrier In previous sections, we examined amplitude modulation with a sinusoidal carrier. Another important class of amplitude modulation techniques corresponds to the use of a carrier signal that is a pulse train, as illustrated in Figure 8.23; amplitude modulation of this type effectively corresponds to transmitting equally spaced time slices of x(t). In general, we would not expect that an arbitrary signal could be recovered from such a set of time slices. However, our examination of the concept of sampling in Chapter 7 suggests that this should be possible if x(t) is band limited and the pulse repetition frequency is high enough. From Figure 8.23, y(t) = x(t)c(t); (8.22) i.e., the modulated signal y(t) is the product of x(t) and the carrier c(t). With Y(jw ), X(jw ), and C(jw) representing the Fourier transforms of each of these signals, it follows from"
8.5 Amplitude Modulation with a Pulse-Train Carrier,"Sec. 8.5 Amplitude Modulation with a Pulse-Train Carrier 601 X(jw) w Y1(jw) L\, ~1 A -we We w Y2 (jw) 1 2 w Y(jw) 1t Figure 8.22 Spectra associated with the single-sideband system of w Figure 8.21. 8.5 AMPLITUDE MODULATION WITH A PULSE-TRAIN CARRIER 8.5.1 Modulation of a Pulse-Train Carrier In previous sections, we examined amplitude modulation with a sinusoidal carrier. Another important class of amplitude modulation techniques corresponds to the use of a carrier signal that is a pulse train, as illustrated in Figure 8.23; amplitude modulation of this type effectively corresponds to transmitting equally spaced time slices of x(t). In general, we would not expect that an arbitrary signal could be recovered from such a set of time slices. However, our examination of the concept of sampling in Chapter 7 suggests that this should be possible if x(t) is band limited and the pulse repetition frequency is high enough. From Figure 8.23, y(t) = x(t)c(t); (8.22) i.e., the modulated signal y(t) is the product of x(t) and the carrier c(t). With Y(jw ), X(jw ), and C(jw) representing the Fourier transforms of each of these signals, it follows from 602 Communication Systems Chap.8 c(t) ! x(t) ~ y(t) x(t) 0 c(t) 1:~· 1 r-l dJ D D D 0 y(t) ~ b D d Figure 8.23 Amplitude modulation 0 of a pulse train. the multiplication property that Y(jw) }J +x = - X(jO)C(j(w - O))dO. (8.23) 27T -x Since c(t) is periodic with period T, C(jw) consists of impulses in frequency spaced by 27TIT; that is, C(jw) = 27T L akB(w- kwc). (8.24) k=-X where We = 27T/T and the coefficients ak are the Fourier series coefficients of c(t), which, from Example 3.5, are sin(kwc~/2) (8.25) 7Tk Sec. 8.5 Amplitude Modulation With a Pulse Train-Carrier 603 The spectrum of c(t) is shown in Figure 8.24(b). With the spectrum of x(t) as illustrated in Figure 8.24(a), the resulting spectrum of the modulated signal y(t) is shown in Fig- ure 8.24(c). From eqs. (8.23) and (8.24), Y(jw) is a sum of scaled and shifted replicas of X(jw): +QG Y(jw) = .2:= akX(j(w - kwc)). (8.26) k= -'X X(jw) (a) C(jw) / / / / ' ' ' ' w (b) Y(jw) w (c) Figure 8.24 Spectra associated with amplitude modulation of a pulse train: (a) spectrum of a bandlimited-signal x(t); (b) spectrum of the pulse carrier signal c(t) in Figure 8.23; (c) spectrum of the modulated pulse train y(t). 604 Communication Systems Chap.B Comparing eq. (8.26) with eq. (7.6) and Figure 8.24 with Figure 7.3(c), we see that the spectrum of y(t) is very similar in form to the spectrum resulting from sampling with a periodic impulse train, the only difference being the values of the Fourier coefficients of the pulse train. For the periodic impulse train used in Chapter 7, all of the Fourier coefficients are equal to liT in value, while for the pulse train c(t) in Figure 8.23, the Fourier coefficients are given by eq. (8.25). Consequently, the replicas of X(jw) do not overlap as long as We> 2wM, which corresponds to the condition of the Nyquist sampling theorem. If this constraint is satisfied, then, as with impulse-train sampling, x(t) can be recovered from y(t) through the use of a lowpass filter with cutoff frequency greater than WM and less than We- WM· Note that the same conclusion holds for a wide variety of other pulselike carrier waveforms: If c(t) is any periodic signal with Fourier transform as in eq. (8.24) for some set of Fourier coefficients ak. then Y(jw) is given by eq. (8.26). Then, as long as We = 2n'/T > 2w M, the replicas of X(jw) do not overlap, allowing us to recover x(t) by lowpass filtering, provided that the DC Fourier coefficient a0 is nonzero. As shown in Problem 8.11, if a0 is zero or unacceptably small, then, by using a bandpass filter to select one of the shifted replicas of X(jw) with a larger value of ak. we obtain a sinusoidal AM signal with a scaled version of x(t) as the modulating signal. Using the demodulation methods described in Section 8.2, we can then recover x(t). 8.5.2 Time-Division Multiplexing Amplitude modulation with a pulse-train carrier is often used to transmit several signals over a single channel. As indicated in Figure 8.23, the modulated output signal y(t) is nonzero only when the carrier signal c(t) is on (i.e., is nonzero). During the intervals in which c(t) is off, other similarly modulated signals can be transmitted. Two equivalent representations of this process are shown in Figure 8.25. In this technique for transmitting several signals over a single channel, each signal is in effect assigned a set of time slots of duration A that repeat every T seconds and that do not overlap with the slots assigned to other signals. The smaller the ratio AfT, the larger the number of signals that can be transmitted over the channel. This procedure is referred to as time-division multiplexing (TDM). Whereas frequency-division mul~iplexing, as discussed in Section 8.3, assigns different frequency intervals to individual signals, time-division multiplexing assigns dif- ferent time intervals to individual signals. Demultiplexing the individual signals from the composite signal in Figure 8.25 is accomplished by time gating, to select the particular time slots associated with each individual signal. 8.6 PULSE-AMPLITUDE MODUlATION 8.6. 1 Pulse-Amplitude Modulated Signals In Section 8.5 we described a modulation system in which a continuous-time signal x(t) modulates a periodic pulse train, corresponding to transmitting time slices of x(t) of dura- tion A seconds every T seconds. As we saw both in that discussion and in our investigation of sampling in Chapter 7, our ability to recover x(t) f~om these time slices depends not on their duration A, but rather on their frequency 27T/T, which must exceed the Nyquist rate"
8.6 Pulse-Amplitude Modulation,"604 Communication Systems Chap.B Comparing eq. (8.26) with eq. (7.6) and Figure 8.24 with Figure 7.3(c), we see that the spectrum of y(t) is very similar in form to the spectrum resulting from sampling with a periodic impulse train, the only difference being the values of the Fourier coefficients of the pulse train. For the periodic impulse train used in Chapter 7, all of the Fourier coefficients are equal to liT in value, while for the pulse train c(t) in Figure 8.23, the Fourier coefficients are given by eq. (8.25). Consequently, the replicas of X(jw) do not overlap as long as We> 2wM, which corresponds to the condition of the Nyquist sampling theorem. If this constraint is satisfied, then, as with impulse-train sampling, x(t) can be recovered from y(t) through the use of a lowpass filter with cutoff frequency greater than WM and less than We- WM· Note that the same conclusion holds for a wide variety of other pulselike carrier waveforms: If c(t) is any periodic signal with Fourier transform as in eq. (8.24) for some set of Fourier coefficients ak. then Y(jw) is given by eq. (8.26). Then, as long as We = 2n'/T > 2w M, the replicas of X(jw) do not overlap, allowing us to recover x(t) by lowpass filtering, provided that the DC Fourier coefficient a0 is nonzero. As shown in Problem 8.11, if a0 is zero or unacceptably small, then, by using a bandpass filter to select one of the shifted replicas of X(jw) with a larger value of ak. we obtain a sinusoidal AM signal with a scaled version of x(t) as the modulating signal. Using the demodulation methods described in Section 8.2, we can then recover x(t). 8.5.2 Time-Division Multiplexing Amplitude modulation with a pulse-train carrier is often used to transmit several signals over a single channel. As indicated in Figure 8.23, the modulated output signal y(t) is nonzero only when the carrier signal c(t) is on (i.e., is nonzero). During the intervals in which c(t) is off, other similarly modulated signals can be transmitted. Two equivalent representations of this process are shown in Figure 8.25. In this technique for transmitting several signals over a single channel, each signal is in effect assigned a set of time slots of duration A that repeat every T seconds and that do not overlap with the slots assigned to other signals. The smaller the ratio AfT, the larger the number of signals that can be transmitted over the channel. This procedure is referred to as time-division multiplexing (TDM). Whereas frequency-division mul~iplexing, as discussed in Section 8.3, assigns different frequency intervals to individual signals, time-division multiplexing assigns dif- ferent time intervals to individual signals. Demultiplexing the individual signals from the composite signal in Figure 8.25 is accomplished by time gating, to select the particular time slots associated with each individual signal. 8.6 PULSE-AMPLITUDE MODUlATION 8.6. 1 Pulse-Amplitude Modulated Signals In Section 8.5 we described a modulation system in which a continuous-time signal x(t) modulates a periodic pulse train, corresponding to transmitting time slices of x(t) of dura- tion A seconds every T seconds. As we saw both in that discussion and in our investigation of sampling in Chapter 7, our ability to recover x(t) f~om these time slices depends not on their duration A, but rather on their frequency 27T/T, which must exceed the Nyquist rate Sec. 8.6 Pulse-Amplitude Modulation 605 / ' ·k.._ __, _\1r--------+-• y(t) I '( (a) ···D n n x1( t) Y1(t) ···D n n D··· x2(t) ···D n n D··· y(t) x3(t) ···D n D··· x4(t) Figure 8.25 Time-division (b) multiplexing. in order to ensure an alias-free reconstruction of x(t). That is, in principle, we need only transmit the samples x(nT) of the signal x(t). In fact, in modern communication systems, sampled values of the information- bearing signal x(t), rather than time slices are more typically transmitted. For practical reasons, there are limitations on the maximum amplitude that can be transmitted over a communication channel, so that transmitting impulse-sampled versions of x(t) is not practical. Instead, the samples x(nT) are used to modulate the amplitude of a sequence of pulses, resulting in what is referred to as a pulse-amplitude modulation (PAM) system. 606 Communication Systems Chap. 8 The use of rectangular pulses corresponds to a sample-and-hold strategy in which pulses of duration ~ and amplitude proportional to the instantaneous sample values of x(t) are transmitted. The resulting waveform for a single PAM channel of this type is illus- trated in Figure 8.26. In the figure, the dotted curve represents the signal x(t). As with the modulation scheme in Section 8.5, PAM signals can be time multiplexed. This is illus- trated in Figure 8.27, which depicts the transmitted waveform with three time-multiplexed channels. The pulses associated with each channel are distinguished by shading, as well as by the channel number above each pulse. For a given pulse-repetition period T, as the pulse width decreases, more time-multiplexed channels can be transmitted over the same communication channel or medium. However, as the pulse width decreases, it is typically necessary to increase the amplitude of the transmitted pulses so that a reasonable amount of energy is transmitted in each pulse. In addition to energy considerations, a number of other issues must be addressed in designing a PAM signal. In particular, as long as the sampling frequency exceeds the Nyquist rate, we know that x(t) can be reconstructed exactly from its samples, and con- - - -""'!! y(t) / / / / /- / Figure 8.26 Transmitted waveform for a single PAM channel. The dotted curve represents the signal x{t). y(t) Figure 8.27 Transmitted waveform with three time-multiplexed PAM channels. The pulses associated with each channel are distinguished by shading, as well as by the channel number above each pulse. Here, the intersymbol spacing is 7; = T/3. Sec. 8.6 Pulse-Amplitude Modulation 607 sequently we can use these samples to modulate the amplitude of a sequence of pulses of any shape. The choice of pulse shape is dictated by considerations such as the frequency selectivity of the communication medium being used and the problem of intersymbol in- terference, which we discuss next. 8.6.2 lntersymbol Interference in PAM Systems In the TDM pulse-amplitude modulation system just described, the receiver can, in prin- ciple, separate the channels by sampling the time-multiplexed waveform at appropriate times. For example, consider the time-multiplexed signal in Figure 8.27, which consists of pulse-amplitude-modulated versions of three signals x1 (t), x2(t), and x3(t). If we sam- ple y(t) at appropriate times, corresponding, for example, to the midpoints of each pulse, we can separate the samples of the three signals. That is, y(t) = Ax 1 (t), t = 0, ±3T1, ±6T1, ••• , y(t) = Ax2(t), t = T1, T1 :±: 3T1, T1 :±: 6T1, ••• , (8.27) y(t) = Ax3(t), t = 2T1, 2T1 :±: 3T1, T1 :±: 6T1, ••• , where T1 is the intersymbol spacing, here equal to T 13, and where A is the appropriate proportionality constant. In other words, samples of x1 (t), x2(t), and x3(t) can be obtained by appropriate sampling of the received time-multiplexed PAM signal. The strategy indicated in the preceding paragraph assumes that the transmitted pulses remain distinct as they propagate over the communication channel. In transmis- sion through any realistic channel, however, the pulses can be expected to be distorted through effects such as additive noise and filtering. Additive noise in the channel will, of course, introduce amplitude errors at the sampling times. Filtering due to the nonideal frequency response of a channel causes a smearing of the individual pulses that can cause the received pulses to overlap in time. This interference is illustrated in Figure 8.28 and is referred to as intersymbol inteiference. intersymbol interference I t Sampling time for channel 2 ~ Sampling time Sampling time for channel 1 for channel 3 Figure 8.28 lntersymbol interference. 608 Communication Systems Chap. a The smearing over time of the idealized pulses in Figure 8.27 can result from the bandwidth constraints of the channel or from phase dispersion caused by nonconstant group delay, as was discussed in Section 6.2.2. (See in particular, Example 6.1.) If the intersymbol interference is due only to the limited bandwidth of the channel, an approach is to use a pulse shape p(t) that is itself band limited and therefore not affected (or only minimally affected) by the restricted bandwidth of the channel. In particular, if the chan- nel has a frequency response H(jw) that has no distortion over a specified frequency band (e.g., if H(jw) = 1 for lwl < W), then if the pulse that is used is band limited (i.e., if P(jw) = 0 for iw I 2: W), each PAM signal will be received without distortion. On the other hand, by using such a pulse, we no longer have pulses without overlap as in Fig- ure 8.27. Nevertheless, intersymbol interference can be avoided in the time domain, even with a band-limited pulse, if the pulse shape is constrained to have zero-crossings at the other sampling times [so that eq. (8.27) continues to hold]. For example, consider the sine pulse = -Tt -sin-( 7T-t1T-t) p(t ) 7Tt and its corresponding spectrum displayed in Figure 8.29. Since the pulse is zero at integer multiples of the symbol spacing T 1, as indicated in Figure 8.30, there will be no intersym- bol interference at these instants. That is, if we sample the received signal at t = kT1, then the contributions to this sampled value from all of the other pulses, i.e., from p(t- mTt) form =I= k, will be identically zero. Of course, avoiding interference from adjacent symbols p(t) P(jw) ------~----~----~-----w Figure 8.29 A sine pulse and its corresponding spectrum. Sec. 8.6 Pulse-Amplitude Modulation 609 Pulse used to transmit sample of channel 2 Pulse used to transmit sample of channel 1 \ Pulse used to transmit sample of channel 3 Sampling Sampling Sampling time for time for time for channel1 channel2 channel3 Figure 8.30 Absence of intersymbol interference when sine pulses with correctly chosen zero-crossings are used. requires high accuracy in the sampling times, so that sampling occurs at the zero-crossings of the adjacent symbols. The sine pulse is only one of many band-limited pulses with time-domain zero- crossings at ±T1, ±2T1, etc. More generally, consider a pulse p(t) with spectrum of the form l+PI(jw), P(jw) = lP 1( jw ), (8.28) 0, otherwise and with P1 (jw) having odd symmetry around TT!T1, so that P, (- jw + j ~ ) = - P 1 (jw + j ~ ) 0 ,; w ,; (8.29) as illustrated in Figure 8.31. If P 1( jw) = 0, p(t) is the sine pulse itself. More generally, as explored in Problem 8.42, for any P(jw) satisfying the conditions in eqs. (8.28) and (8.29), p(t) will have zero-crossing at ±T1, ±2T1, •••• While signals satisfying eqs. (8.28) and (8.29) allow us to overcome the problem of limited channel bandwidth, other channel distortions may occur that require a differ- ent choice of pulse waveform or some additional processing of the received signal prior to the separation of the different TOM signals. In particular, if jH(jw )j is not constant over the passband, there may be a need to perform channel equalization-i.e., filtering of 610 Communication Systems Chap.8 P(jw) 1T -T1 Figure 8.31 Odd symmetry around 1r!T1 as defined in eq. (8.29). the received signal to correct for the nonconstant channel gain. Also, if the channel has nonlinear phase, distortion can result that leads to intersymbol interference, unless com- pensating signal processing is performed. Problems 8.43 and 8.44 provide illustrations of these effects. 8.6.3 Digital Pulse-Amplitude and Pulse-Code Modulation The PAM system described in the preceding subsections involves the use of a discrete set of samples to modulate a sequence of pulses. This set of samples can be thought of as a discrete-time signal x[n], and in many applications x[n] is in fact stored in or generated by a digital system. In such cases, the limited word length of a digital system implies that x[n] can take on only a finite, quantized set of values, resulting in only a finite set of possible amplitudes for the modulated pulses. In fact, in many cases this quantized form of digital PAM is reduced to a system using only a few-typically, only two-amplitude values. In particular, if each sample of x[n] is represented as a binary number (i.e., a finite string of O's and 1' s), then a pulse with one of two possible values (one value corresponding to a 0 and one value to a 1) can be set for each binary digit, or bit, in the string. More generally, in order to protect against transmission errors or provide secure communication, the sequence of binary digits rep- resenting x[n] might first be transformed or encoded into another sequence of O's and 1' s before transmission. For example, a very simple error detection mechanism is to transmit one additional modulated pulse for each sample of x[n], representing a parity check. That is, this additional bit would be set to 1 if the binary representation of x[n] has an odd num- ber of 1' sin it and to 0 if there is an even number of 1' s. The receiver can then check the received parity bit against the other received bits in order to detect inconsistencies. More complex coding and error correction schemes can certainly be employed, and the design of codes with particular desirable properties is an important component of communication system design. For obvious reasons, a PAM system modulated by an encoded sequence of O's and 1 'sis referred to as a pulse-code modulation (PCM) system. Sec. 8.7 Sinusoidal Frequency Modulation 611 8.7 SINUSOIDAL FREQUENCY MODULATION In the preceding sections, we discussed a number of specific amplitude modulation sys- tems in which the modulating signal was used to vary the amplitude of a sinusoidal or a pulse carrier. As we have seen, such systems are amenable to detailed analysis using the frequency-domain techniques we developed in preceding chapters. In another very important class of modulation techniques referred to as frequency modulation ( FM), the modulating signal is used to control the frequency of a sinusoidal carrier. Modulation sys- tems of this type have a number of advantages over amplitude modulation systems. As suggested by Figure 8.1 0, with sinusoidal amplitude modulation the peak amplitude of the envelope of the carrier is directly dependent on the amplitude of the modulating signal x(t), which can have a large dynamic range-i.e., can vary significantly. With frequency modulation, the envelope of the carrier is constant. Consequently, an FM transmitter can always operate at peak power. In addition, in FM systems, amplitude variations introduced over a transmission channel due to additive disturbances or fading can, to a large extent, be eliminated at the receiver. For this reason, in public broadcasting and a variety of other contexts, FM reception is typically better than AM reception. On the other hand, as we will see, frequency modulation generally requires greater bandwidth than does sinusoidal amplitude modulation. Frequency modulation systems are highly nonlinear and, consequently, are not as straightforward to analyze as are the amplitude modulation systems discussed in the pre- ceding sections. However, the methods we have developed in earlier chapters do allow us to gain some understanding of the nature and operation of these systems. We begin by introducing the general notion of angle modulation. Consider a sinu- soidal carrier expressed in the form c(t) = Acos(wct + Oc) = AcosO(t), (8.30) where O(t) = w ct + (} c and where w cis the frequency and(} c the phase of the carrier. Angle modulation, in general, corresponds to using the modulating signal to change or vary the angle O(t). One form that this sometimes takes is to use the modulating signal x(t) to vary the phase (} c so that the modulated signal takes the form y(t) = A cos[wct + Oc(t)], (8.31) where (} c is now a function of time, specifically of the form (} c(t) = Oo + kpx(t). (8.32) If x(t) is, for example, constant, the phase of y(t) will be constant and proportional to the amplitude of x(t). Angle modulation of the form of eq. (8.31) is referred to as phase modulation. Another form of angle modulation corresponds to varying the derivative of the angle proportionally with the modulating signal; that is, y(t) = A cos O(t), (8.33) where (8.34)"
8.7 Sinusoidal Frequency Modulation,"Sec. 8.7 Sinusoidal Frequency Modulation 611 8.7 SINUSOIDAL FREQUENCY MODULATION In the preceding sections, we discussed a number of specific amplitude modulation sys- tems in which the modulating signal was used to vary the amplitude of a sinusoidal or a pulse carrier. As we have seen, such systems are amenable to detailed analysis using the frequency-domain techniques we developed in preceding chapters. In another very important class of modulation techniques referred to as frequency modulation ( FM), the modulating signal is used to control the frequency of a sinusoidal carrier. Modulation sys- tems of this type have a number of advantages over amplitude modulation systems. As suggested by Figure 8.1 0, with sinusoidal amplitude modulation the peak amplitude of the envelope of the carrier is directly dependent on the amplitude of the modulating signal x(t), which can have a large dynamic range-i.e., can vary significantly. With frequency modulation, the envelope of the carrier is constant. Consequently, an FM transmitter can always operate at peak power. In addition, in FM systems, amplitude variations introduced over a transmission channel due to additive disturbances or fading can, to a large extent, be eliminated at the receiver. For this reason, in public broadcasting and a variety of other contexts, FM reception is typically better than AM reception. On the other hand, as we will see, frequency modulation generally requires greater bandwidth than does sinusoidal amplitude modulation. Frequency modulation systems are highly nonlinear and, consequently, are not as straightforward to analyze as are the amplitude modulation systems discussed in the pre- ceding sections. However, the methods we have developed in earlier chapters do allow us to gain some understanding of the nature and operation of these systems. We begin by introducing the general notion of angle modulation. Consider a sinu- soidal carrier expressed in the form c(t) = Acos(wct + Oc) = AcosO(t), (8.30) where O(t) = w ct + (} c and where w cis the frequency and(} c the phase of the carrier. Angle modulation, in general, corresponds to using the modulating signal to change or vary the angle O(t). One form that this sometimes takes is to use the modulating signal x(t) to vary the phase (} c so that the modulated signal takes the form y(t) = A cos[wct + Oc(t)], (8.31) where (} c is now a function of time, specifically of the form (} c(t) = Oo + kpx(t). (8.32) If x(t) is, for example, constant, the phase of y(t) will be constant and proportional to the amplitude of x(t). Angle modulation of the form of eq. (8.31) is referred to as phase modulation. Another form of angle modulation corresponds to varying the derivative of the angle proportionally with the modulating signal; that is, y(t) = A cos O(t), (8.33) where (8.34) 612 Communication Systems Chap.B For x(t) constant, y(t) is sinusoidal with a frequency that is offset from the carrier fre- quency we by an amount proportional to the amplitude of x(t). For that reason, angle modulation of the form of eqs. (8.33) and (8.34) is commonly referred to as frequency modulation. Although phase modulation and frequency modulation are different forms of angle modulation, they can be easily related. From eqs. (8.31) and (8.32), for phase modulation, dO(t) _ k dx(t) -----;[( - w (' + p -----;[(' (8.35) x(t) x(t) k::/ k::/ y(t) y(t) (a) (b) x(t) y(t) (c) Figure 8.32 Phase modulation, frequency modulation, and their relationship: (a) phase modulation with a ramp as the modulating signal; (b) frequency modulation with a ramp as the modulating signal; (c) frequency modulation with a step (the derivative of a ramp) as the modulating signal. Sec. 8.7 Sinusoidal Frequency Modulation 613 and thus, comparing eqs. (8.34) and (8.35), we see that phase modulating with x(t) is iden- tical to frequency modulating with the derivative of x(t). Likewise, frequency modulating with x(t) is identical to phase modulating with the integral of x(t). An illustration of phase modulation and frequency modulation is shown in Figures 8.32(a) and (b). In both cases, the modulating signal is x(t) = tu(t) (i.e., a ramp signal increasing linearly with time for t > 0). In Figure 8.32(c ), an example of frequency modulation is shown with a step (the derivative of a ramp) as the modulating signal [i.e., x(t) = u(t)]. The correspondence between Figures 8.32(a) and (c) should be evident. Frequency modulation with a step corresponds to the frequency of the sinusoidal carrier changing instantaneously from one value to another when x(t) changes value at t = 0, much as the frequency of a sinusoidal oscillator changes when the frequency setting is switched instantaneously. When the frequency modulation is a ramp, as in Figure 8.32(b) , the frequency changes linearly with time. This notion of a time-varying frequency is often best expressed in terms of the concept of instantaneous frequency. For y(t) = A cos O(t), (8.36) the instantaneous frequency of the sinusoid is defined as ·( ) _ WIt - ddO(ft)' (8.37) Thus, for y(t) truly sinusoidal [i.e., O(t) = (wet + 00)], the instantaneous frequency is we, as we would expect. For phase modulation as expressed in eqs. (8.31) and (8.32), the instantaneous frequency is We+ kp(dx(t)ldt), and for frequency modulation as expressed in eqs. (8.33) and (8.34), the instantaneous frequency is We + k1 x(t). Since frequency modulation and phase modulation are easily related, we will phrase the remaining discussion in terms of frequency modulation alone. To gain some insight into how the spectrum of the frequency-modulated signal is affected by the modulating signal x(t), it is useful to consider two cases in which the modulating signal is sufficiently simple so that some of the essential properties of frequency modulation become evident. 8. 7. 1 Narrowband Frequency Modulation Consider the case of frequency modulation with x(t) = Acoswmt. (8.38) From eqs. (8.34) and (8.37), the instantaneous frequency is Wi(t) = We + kt A COS Wmt, (8.39) which varies sinusoidally between We+ ktA and We- k1A. With we have 614 Communication Systems Chap.8 and y(t) = cos[wet + f x(t)dt] (8.40) = cos (wet+ ~w sinwntf +eo), Wm where 00 is a constant of integration. For convenience we will choose 00 = 0, so that y(t) = COS Wet + -~Sw in. [ Wmt l. (8.41) Wm The factor ~wlw 111 , which we denote by m, is defined as the modulation index for frequency modulation. The properties of FM systems tend to be different, depending on whether the modulation index m is small or large. The case in which m is small is referred to as narrowband FM. In general, we can rewrite eq. (8.41) as y(t) = cos( wet + m sin W111 t) (8.42) or y(t) = coswctcos(msinw 111t)- sinwctsin(msinwmt). (8.43) When m is sufficiently small ( << Tr/2), we can make the approximations cos(m sinwmt) = 1, (8.44) sin(msinwmt) = msinw 111 t, (8.45) so that eq. (8.42) becomes y(t) = cos Wet - m(sin W 111 t)(sin Wet). (8.46) The spectrum of y(t) based on this approximation is shown in Figure 8.33. We note that it has a similarity to AM-DSB/WC in that the carrier frequency is present in the spectrum and there are sidebands representing the spectrum of the modulating signal in eq. (8.38). However, in AM-DSB/WC the additional carrier injected is in phase with the modulated carrier, whereas, as we see in eq. (8.46) for the case of the narrowband FM, the carrier signal has a phase difference of Trl2 in relation to the amplitude-modulated carrier. The waveforms corresponding to AM-DSB/WC and FM are also very different. Figure 8.34(a) illustrates the narrowband FM waveform corresponding to eq. (8.46). For comparison, Figure 8.34(b) shows the AM-DSB/WC signal (8.47) For the narrowband FM signal of eq. (8.46), the bandwidth of the sidebands is equal to the bandwidth of the modulating signal, and in particular, although the approximation in the equation is based on assuming that m << Trl2, the bandwidth of the sidebands is otherwise independent of the modulation index m (i.e., it depends only on the bandwidth of the modulating signal, not on its amplitude). A similar statement applies for narrowband FM with a more general modulating signal. Sec. 8.7 Sinusoidal Frequency Modulation 615 Y(jw) 7T 7T m7T m7T 2 2 (!) Figure 8.33 Approximate spectrum for narrowband FM. (a) Figure 8.34 Comparison of narrowband FM and AM-DSB/WC: (b) (a) narrowband FM; (b) AM-DSB/WC. 8. 7.2 Wideband Frequency Modulation When m is large, the approximation leading to eq. (8.46) no longer applies, and the spec- trum of y(t) depends on both the amplitude and the spectrum of the modulating signal x(t). With y(t) expressed in the form of eq. (8.43), we note that the terms cos[m sinw 111 t] and sin[m sinw 111 t] are periodic signals with fundamental frequency w 111 • Thus, the Fourier transform of each of these signals is an impulse train with impulses at integer multiples of w 111 and amplitudes proportional to the Fourier series coefficients. The coefficients for these two periodic signals involve a class of functions referred to as Bessel functions of the first kind. The first term in eq. (8.43) corresponds to a sinusoidal carrier of the form cos wet amplitude modulated by the periodic signal cos[m sin w 111 t] and the second term to a sinusoidal carrier sin wet amplitude modulated by the periodic signal sin[m sinw 111 t]. Multiplication by the carrier signals has the effect in the frequency domain of translating the spectrum of eq. (8.43) to the carrier frequency, so that it is centered at plus and minus 616 Communication Systems Chap.8 We. In Figures 8.35(a) and (b) we illustrate, for w > 0, the the magnitude of the spectra of the two individual terms in eq. (8.43), and in Figure 8.35(c) the magnitude of the combined spectrum representing the modulated signal y(t). The spectrum of y(t) consists of impulses at frequencies ±we + nwm, n = 0, ± 1, ±2, ... , and is not, strictly speaking, band limited around ±we. However, the behavior of the Fourier series coefficients of cos[m sinwmt] and sin[m sin wmt] are such that the amplitude of the nth harmonic for In I > m can be considered negligible, and thus, the total bandwidth B of each sideband centered around +we and -we is effectively limited to 2mwm. That is, B = 2mwnz, (8.48) or, since m = k1Aiwm = dwlwm, B = 2k;-A = 2dw. (8.49) Comparing eqs. (8.39) and (8.49), we note that the effective bandwidth of each sideband is equal to the total excursion of the instantaneous frequency around the carrier (a) (b) Figure 8.35 Magnitude of spec- trum of wideband frequency modula- tion with m = 12: (a) magnitude of spectrum of cos wcfcos[msin wmt]; (b) magnitude of spectrum of sin wcfsin[msin wmt]; (c) combined spectral magnitude of (c) cos[ wet+ msin wmt]. Sec. 8.7 Sinusoidal Frequency Modulation 617 frequency. Therefore, for wideband FM, since we assume that m is large, the bandwidth of the modulated signal is much larger than the bandwidth of the modulating signal, and in contrast to the narrowband case, the bandwidth of the transmitted signal in wideband FM is directly proportional to amplitude A of the modulating signal and the gain factor k f. 8.7.3 Periodic Square-Wave Modulating Signal Another example that lends insight into the properties of frequency modulation is that of a modulating signal which is a periodic square wave. Referring to eq. (8.39), let k f = 1 so that ~w = A, and let x(t) be given by Figure 8.36. The modulated signal y(t) is illustrated in Figure 8.37. The instantaneous frequency is We+ ~w when x(t) is positive and We- ~w when x(t) is negative. Thus, y(t) can also be written as y(t) = r(t) cos[(w, + Aw )t] + r ~ ~ ~ )cos[(w, ~ Aw )t], (8.50) x(t) A T T T T t -2 -4 T 4 2 -A-r- Figure 8.36 Symmetric periodic square wave. y(t) J Figure 8.37 Frequency modulation u with a periodic square-wave modulat- u ing signal. 618 Communication Systems Chap.8 where r(t) is the symmetric square wave shown in Figure 8.38. Thus, for this particular modulating signal, we are able to recast the problem of determining the spectrum of the FM signal y(t) as the determination of the spectrum of the sum of the two AM signals in eq. (8.50). Specifically, 1 Y(jw) = [R(jw +)we+ jb,.w) + R(jw- )we- jb,.w)] 2 1 + 2[RT(jw +)we - jb,.w) + RT(jW -)we+ jb,.w )], (8.51) where R(jw) is the Fourier transform of the periodic square wave r(t) in Figure 8.38 and RT()w) is the Fourier transform of r(t- T/2). From Example 4.6, with T = 4T1, (8.52) and RT()w) = R(jw )e- JwT/2. (8.53) The magnitude of the spectrum of Y(jw) is illustrated in Figure 8.39. As with wideband FM, the spectrum has the general appearance of two sidebands, centered around We± dw, that decay for w <We - b,.w and w >We+ b,.w. r(t) ...] 11 I I I 3T T T 0 T T 3T -4 -2 -4 T 4 2 4 Figure 8.38 Symmetric square wave r(t) in eq. (8.50). Y(jw) 1 ~~~~1~1~11~11~1~11~1~~~~11~1~11~1~11~11~1~11~1~11~11~1~11~11~1~11~1~11~11~1~11~1~11~11~1~11~1~11~11~1~11~11~'~11 ___ _ (we~ Llw) We (we+ Llw) w Figure 8.39 Magnitude of the spectrum for w > 0 corresponding to fre- quency modulation with a periodic square-wave modulating signal. Each of the vertical lines in the figure represents an impulse of area proportional to the height of the line. Sec. 8.8 Discrete-Time Modulation 619 Systems for the demodulation of FM signals typically are of two types. One type of demodulation system corresponds to converting the FM signal to an AM signal through differentiation, while demodulation systems of the second type directly track the phase or frequency of the modulated signal. The foregoing discussion provides only a brief intro- duction to the characteristics of frequency modulation, and we have again seen how the basic techniques developed in the earlier chapters can be exploited to analyze and gain an insight into an important class of systems. 8.8 DISCRETE-TIME MODUlATION 8.8. 1 Discrete-Time Sinusoidal Amplitude Modulation A discrete-time amplitude modulation system is depicted in Figure 8.40, in which c[n] is the carrier and x[n] the modulating signal. The basis for our analysis of continuous- time amplitude modulation was the multiplication property for Fourier transforms- specifically, the fact that multiplication in the time domain corresponds to convolution in the frequency domain. As we discussed in Section 5.5, there is a corresponding property for discrete-time signals which we can use to analyze discrete-time amplitude modulation. Specifically, consider y[n] = x[n]c[n]. With X(eiw), Y(eiw), and C(eiw) denoting the Fourier transforms of x[n], y[n], and c[n], respectively, Y(eiw) is proportional to the periodic convolution of X(eiw) and C(eiw); that is, Y(eiw) = - 1 I X(eifJ)C(ei(w-fJ))d(). (8.54) 27T 27T Since X(eiw) and C(eiw) are periodic with a period of27T, the integration can be performed over any frequency interval of length 27T. Let us first consider sinusoidal amplitude modulation with a complex exponential carrier, so that (8.55) As we saw in Section 5.2, the Fourier transform of c[n] is a periodic impulse train; that is, +<X) C(eiw) = L 27TO(w -We + k27T), (8.56) k= -00 c[n] l x[n] --~0~--~ y[n] Figure 8.40 Discrete-time ampli- tude modulation."
8.8 Discrete-Time Modulation,"Sec. 8.8 Discrete-Time Modulation 619 Systems for the demodulation of FM signals typically are of two types. One type of demodulation system corresponds to converting the FM signal to an AM signal through differentiation, while demodulation systems of the second type directly track the phase or frequency of the modulated signal. The foregoing discussion provides only a brief intro- duction to the characteristics of frequency modulation, and we have again seen how the basic techniques developed in the earlier chapters can be exploited to analyze and gain an insight into an important class of systems. 8.8 DISCRETE-TIME MODUlATION 8.8. 1 Discrete-Time Sinusoidal Amplitude Modulation A discrete-time amplitude modulation system is depicted in Figure 8.40, in which c[n] is the carrier and x[n] the modulating signal. The basis for our analysis of continuous- time amplitude modulation was the multiplication property for Fourier transforms- specifically, the fact that multiplication in the time domain corresponds to convolution in the frequency domain. As we discussed in Section 5.5, there is a corresponding property for discrete-time signals which we can use to analyze discrete-time amplitude modulation. Specifically, consider y[n] = x[n]c[n]. With X(eiw), Y(eiw), and C(eiw) denoting the Fourier transforms of x[n], y[n], and c[n], respectively, Y(eiw) is proportional to the periodic convolution of X(eiw) and C(eiw); that is, Y(eiw) = - 1 I X(eifJ)C(ei(w-fJ))d(). (8.54) 27T 27T Since X(eiw) and C(eiw) are periodic with a period of27T, the integration can be performed over any frequency interval of length 27T. Let us first consider sinusoidal amplitude modulation with a complex exponential carrier, so that (8.55) As we saw in Section 5.2, the Fourier transform of c[n] is a periodic impulse train; that is, +<X) C(eiw) = L 27TO(w -We + k27T), (8.56) k= -00 c[n] l x[n] --~0~--~ y[n] Figure 8.40 Discrete-time ampli- tude modulation. 620 Communication Systems Chap. a which is sketched in Figure 8.4l(b). With X(eiw) as illustrated in Figure 8.41(a), the spec- trum of the modulated signal is that shown in Figure 8.41(c). In particular, we note that Y(e.iw) = X(ei<w-wc)). This is the discrete-time counterpart to Figure 8.1, and here again, with x[n] real, the modulated signal will be complex. Demodulation is accomplished by multiplying by e- Jw,n to translate the spectrum back to its original location on the fre- quency axis, so that x[n] = y[n]e- Jw,n. (8.57) As explored in Problem 6.43, if We = 7T so that c[n] = ( -l)n, the result of modula- tion in the time domain is to change the algebraic sign of x[ n] for odd values of n, while in the frequency domain the consequence is the interchanging of high and low frequencies. Problem 6.44 explores the use of this type of modulation in utilizing a lowpass filter to achieve highpass filtering and vice versa. As an alternative to a complex exponential carrier, we can use a sinusoidal carrier, in which case, with x[n] real, the modulated signal y[n] will also be real. With c[n] = cos wen, the spectrum of the carrier consists of periodically repeated pairs of impulses at -21T -wM w 0 WM 21T (a) C(eiw) rn 2n I 2n I I -21T -21T+We 0 We 21T 21T+We w (b) Y(eiw) (we-wM) We (we+wM) w (c) Figure 8.41 (a) Spectrum of x[n]; (b) spectrum of c[n] = eiwcn; (c) spec- trum of y[n] = x[n]c[n]. Sec. 8.8 Discrete-Time Modulation 621 w = ±we+ k27T, as illustrated in Figure 8.42(b). With X(ejw) as shown in Figure 8.42(a), the resulting spectrum for the modulated signal is shown in Figure 8.42(c) and corresponds to replicating X(ejw) at the frequencies w = ±we + k27T. In order that the individual replications of X(ejw) do not overlap, we require that (8.58) and or, equivalently, (8.59) The first condition is identical to that in Section 8.2 for continous-time sinusoidal ampli- tude modulation, while the second results from the inherent periodicity of discrete-time X(eiw) 1\ ~ 1\ -2'TT -wM 0 WM 2'TT w (a) C(eiw) I r r I r r I r -2'TT -2'TT+We -we 0 We 2'TT-We 2'TT 2'TT+We w (b) Y(eiw) (2'TT-we-wM) I 1\ 1\ t 1\ \1\ I /\··· -2'TT -2'TT+We -we 0 ( We \ 2'TT-We 2'TT 2'TT+We w we-wM we+WM (c) Figure 8.42 Spectra associated with discrete-time modulation using a sinusoidal carrier: (a) spectrum of a bandlimited-signal x[n]; (b) spectrum of a sinusoidal carrier signal c[n] = cos wen; (c) spectrum of the modulated signal y[n] = x[n]c[n]. 622 Communication Systems Chap. a spectra. Combining eqs. (8.58) and (8.59), we see that for amplitude modulation with a sinusoidal carrier, we must restrict w c so that (8.60) Demodulation can be accomplished in a manner similar to that employed in con- tinuous time. As illustrated in Figure 8.43, multiplication of y[n] with the same carrier used in the modulator results in several replications of the spectrum of the original signal, one of which is centered about w = 0. By lowpass filtering to eliminate the unwanted replications of X(ejw), the demodulated signal is obtained. As should be evident from the foregoing discussion, analysis of discrete-time am- plitude modulation proceeds in a manner similar to that of continuous-time amplitude modulation, with only slight differences. For example, as explored in Problem 8.47, in the synchronous modulation and demodulation system, the effect of a phase difference or a frequency difference between the sinusoidal carriers in the modulator and demodulator is identical in both discrete and continuous time. In addition, just as in continuous time, we can use discrete-time sinusoidal AM as the basis for frequency-division multiplexing in cos wen Lowpass filter ~ H(eiw) y[n] w[n] X Ih x[n] -Wco Wco w Y(eiw) -21·~ !\1iL I L.. I T 1T -w 0 w 1T t 21T w -21T -1T -wco Wco 1T 21T w X(eiw) ~ ! L Figure 8.43 System and associated spectra for discrete-time synchronous -21T 21T w demodulation. Sec. 8.9 Summary 623 discrete time. Furthermore, as explored in Problem 8.48, we can also consider using a discrete-time signal to modulate a pulse train, leading to time-division multiplexing of discrete-time signals. The implementation of discrete-time multiplexing systems provides an excellent ex- ample of the flexibility of discrete-time processing in general and the importance of the operation of upsampling (see Section 7.5.2) in particular. Consider a discrete-time FDM system with M sequences that we wish to frequency-division multiplex. With M channels, it is required that the spectral energy for each input channel xi[n] be band limited; that is, 7T - < lwl < 7T. (8.61) M If the sequences originally occupied the entire frequency band corresponding, for example, to having sampled a set of continuous-time signals at the Nyquist rate, then they would first have to be converted to a higher sampling rate (i.e., upsampled) before frequency-division multiplexing. This idea is explored further in Problem 8.33. 8.8.2 Discrete-Time Transmodulation One context in which discrete-time modulation is widely used, together with the oper- ations of decimation, upsampling, and interpolation introduced in Chapter 7, is digital communication systems. Typically, in such systems continuous-time signals are trans- mitted over communication channels in the form of discrete-time signals, obtained by sampling. The continous-time signals are often in the form of time-division-multiplexed (TDM) or frequency-division-multiplexed (FDM) signals. The signals are then converted to discrete-time sequences whose values are represented digitally, for storage or long- distance transmission. In some systems, because of different constraints or requirements at the transmitting end and the receiving end, or because sets of signals that have been in- dividually multiplexed by different methods are then multiplexed together, there is often the requirement for converting from sequences representing TDM signals to sequences representing FDM signals or vice versa. This conversion from one modulation or multi- plexing scheme to another is referred to as transmodulation or transmultiplexing. In the context of digital communication systems, one obvious way of implementing transmulti- plexing is to convert back to continuous-time signals, demultiplex and demodulate, and then modulate and multiplex as required. However, if the new signal is then to be con- verted back to a discrete-time signal, it is clearly more efficient for the entire process to be carried out directly in the discrete-time domain. Figure 8.44 shows, in block diagram form, the steps involved in converting a discrete-time TDM signal to a discrete-time FDM signal. Note that, after demultiplexing the TDM signal, each channel must be upsampled in preparation for frequency-division multiplexing. 8.9 SUMMARY In this chapter, we have examined a number of the basic concepts associated with com- munication systems. In particular, we have examined the concept of modulation, in which a signal we wish to communicate is used to modulate a second signal referred to as the"
8.9 Summary,"Sec. 8.9 Summary 623 discrete time. Furthermore, as explored in Problem 8.48, we can also consider using a discrete-time signal to modulate a pulse train, leading to time-division multiplexing of discrete-time signals. The implementation of discrete-time multiplexing systems provides an excellent ex- ample of the flexibility of discrete-time processing in general and the importance of the operation of upsampling (see Section 7.5.2) in particular. Consider a discrete-time FDM system with M sequences that we wish to frequency-division multiplex. With M channels, it is required that the spectral energy for each input channel xi[n] be band limited; that is, 7T - < lwl < 7T. (8.61) M If the sequences originally occupied the entire frequency band corresponding, for example, to having sampled a set of continuous-time signals at the Nyquist rate, then they would first have to be converted to a higher sampling rate (i.e., upsampled) before frequency-division multiplexing. This idea is explored further in Problem 8.33. 8.8.2 Discrete-Time Transmodulation One context in which discrete-time modulation is widely used, together with the oper- ations of decimation, upsampling, and interpolation introduced in Chapter 7, is digital communication systems. Typically, in such systems continuous-time signals are trans- mitted over communication channels in the form of discrete-time signals, obtained by sampling. The continous-time signals are often in the form of time-division-multiplexed (TDM) or frequency-division-multiplexed (FDM) signals. The signals are then converted to discrete-time sequences whose values are represented digitally, for storage or long- distance transmission. In some systems, because of different constraints or requirements at the transmitting end and the receiving end, or because sets of signals that have been in- dividually multiplexed by different methods are then multiplexed together, there is often the requirement for converting from sequences representing TDM signals to sequences representing FDM signals or vice versa. This conversion from one modulation or multi- plexing scheme to another is referred to as transmodulation or transmultiplexing. In the context of digital communication systems, one obvious way of implementing transmulti- plexing is to convert back to continuous-time signals, demultiplex and demodulate, and then modulate and multiplex as required. However, if the new signal is then to be con- verted back to a discrete-time signal, it is clearly more efficient for the entire process to be carried out directly in the discrete-time domain. Figure 8.44 shows, in block diagram form, the steps involved in converting a discrete-time TDM signal to a discrete-time FDM signal. Note that, after demultiplexing the TDM signal, each channel must be upsampled in preparation for frequency-division multiplexing. 8.9 SUMMARY In this chapter, we have examined a number of the basic concepts associated with com- munication systems. In particular, we have examined the concept of modulation, in which a signal we wish to communicate is used to modulate a second signal referred to as the r ll TDM signal r I I I I Demultiplex 21T r 1 i 1 o (N channels) cos (N )n l x4[n] diid! Figure 8.44 Block diagram for TDM-to-FDM transmultiplexing. Sec. 8.9 Summary 625 carrier, and we have looked in detail at the concept of amplitude modulation. The proper- ties of amplitude modulation are most easily interpreted in the frequency domain through the multiplication property of the Fourier transform. Amplitude modulation with a com- plex exponential or sinusoidal carrier is typically used to shift the spectrum of a sig- nal in frequency and is applied, for example, in communication systems to place the spectrum in a frequency range suitable for transmission and to permit frequency-division multiplexing. Variations of sinusoidal amplitude modulation, such as the insertion of a carrier signal for asynchronous systems and single- and double-sideband systems, were discussed. We also examined several other forms of modulation -based communication. In this regard, we briefly introduced the concepts of frequency and phase modula- tion. Although these forms of modulation are more difficult to analyze in detail, it is possible to gain significant insight into their characteristics through the frequency domain. We further examined in some detail amplitude modulation of a pulsed signal, which led us to the concepts of time-division multiplexing and pulse-amplitude modulation, in which successive samples of a discrete-time signal are used to mod- ulate the amplitude of a sequence of pulses. This led in tum to an examination of discrete-time modulation and digital communication, in which the flexibility of discrete- time processing facilitates the design and implementation of more sophisticated communication systems involving concepts such as pulse-code modulation and transmodulation. Chapter 8 Probleml The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 8.1. Let x(t) be a signal for which X(jw) = 0 when lwl > WM. Another signal y(t) is specified as having the Fourier transform Y(jw) = 2X(j(w - we)). Determine a signal m(t) such that x(t) = y(t)m(t). 8.2. Let x(t) be a real-valued signal for which X(jw) = 0 when lwl > 1,0007T. Suppos- ing that y(t) = ejwct x(t), answer the following questions: (a) What constraint should be placed on We to ensure that x(t) is recoverable from y(t)? (b) What constraint should be placed on We to ensure that x(t) is recoverable from (Re{y(t)}?"
Problems,"Sec. 8.9 Summary 625 carrier, and we have looked in detail at the concept of amplitude modulation. The proper- ties of amplitude modulation are most easily interpreted in the frequency domain through the multiplication property of the Fourier transform. Amplitude modulation with a com- plex exponential or sinusoidal carrier is typically used to shift the spectrum of a sig- nal in frequency and is applied, for example, in communication systems to place the spectrum in a frequency range suitable for transmission and to permit frequency-division multiplexing. Variations of sinusoidal amplitude modulation, such as the insertion of a carrier signal for asynchronous systems and single- and double-sideband systems, were discussed. We also examined several other forms of modulation -based communication. In this regard, we briefly introduced the concepts of frequency and phase modula- tion. Although these forms of modulation are more difficult to analyze in detail, it is possible to gain significant insight into their characteristics through the frequency domain. We further examined in some detail amplitude modulation of a pulsed signal, which led us to the concepts of time-division multiplexing and pulse-amplitude modulation, in which successive samples of a discrete-time signal are used to mod- ulate the amplitude of a sequence of pulses. This led in tum to an examination of discrete-time modulation and digital communication, in which the flexibility of discrete- time processing facilitates the design and implementation of more sophisticated communication systems involving concepts such as pulse-code modulation and transmodulation. Chapter 8 Probleml The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 8.1. Let x(t) be a signal for which X(jw) = 0 when lwl > WM. Another signal y(t) is specified as having the Fourier transform Y(jw) = 2X(j(w - we)). Determine a signal m(t) such that x(t) = y(t)m(t). 8.2. Let x(t) be a real-valued signal for which X(jw) = 0 when lwl > 1,0007T. Suppos- ing that y(t) = ejwct x(t), answer the following questions: (a) What constraint should be placed on We to ensure that x(t) is recoverable from y(t)? (b) What constraint should be placed on We to ensure that x(t) is recoverable from (Re{y(t)}? 626 Communication Systems Chap.B 8.3. Let x(t) be a real-valued signal for which X(jw) = 0 when lwl > 2,0007T. Ampli- tude modulation is performed to produce the signal g(t) = x(t) sin(2,0007Tt). A proposed demodulation technique is illustrated in Figure P8.3 where g(t) is the input, y(t) is the output, and the ideallowpass filter has cutoff frequency 2,0007T and passband gain of 2. Determine y(t). Ideal g(t) ..,__ _. ..,.~I lowpass 1----...._ y(t) filter cos(2000'1Tt) Figure P8.3 8.4. Suppose x(t) = sin 2007Tt + 2 sin 4007Tt and g(t) = x(t) sin 4007Tt. If the product g(t)(sin 4007Tt) is passed through an ideallowpass filter with cutoff frequency 4007T and passband gain of 2, determine the signal obtained at the output of the lowpass filter. 8.5. Suppose we wish to transmit the signal sin 1,0007Tt x(t ) = 7Tt using a modulator that creates the signal w(t) = (x(t) + A) cos(l0,0007Tt). Determine the largest permissible value of the modulation index m that would allow asynchronous demodulation to be used to recover x(t) from w(t). For this problem, you should assume that the maximum magnitude taken on by a side lobe of a sine function occurs at the instant of time that is exactly halfway between the two zero- crossings enclosing the side lobe. 8.6. Assume that x(t) is a signal whose Fourier transform X(jw) is zero for lwl > WM. The signal g(t) may be expressed in terms of x(t) as where* denotes convolution and We> WM. Determine the value of the constant A such that Chap. 8 Problems 627 AsinwMt x(t) = (g(t) cos Wet)* . 1Tt 8.7. An AM-SSB/SC system is applied to a signal x(t) whose Fourier transform X(jw) is zero for lw I > w M. The carrier frequency w c used in the system is greater than w M. Let g(t) denote the output of the system, assuming that only the upper sidebands are retained. Let q(t) denote the output of the system, assuming that only the lower sidebands are retained. The system in Figure P8.7 is proposed for converting g(t) into q(t). How should the parameter w0 in the figure be related to we? What should be the value of passband gain A? g(t) 1------i~ q (t) coswot Figure P8. 7 8.8. Consider the modulation system shown in Figure P8.8. The input signal x(t) has a Fourier transform X(jw) that is zero for lw I > w M. Assuming that We > w M, answer the following questions: (a) Is y(t) guaranteed to be real if x(t) is real? (b) Can x(t) be recovered from y(t)? -J· W>O H(jw)= { . ' J ,w<O x(t) y(t) Figure P8.8 8.9. Two signals x1 (t) and x2(t), each with a Fourier transform that is zero for lwl > we, are to be combined using frequency-division multiplexing. The AM-SSB/SC technique of Figure 8.21 is applied to each signal in a manner that retains the lower sidebands. The carrier frequencies used for x 1( t) and x2(t) are We and 2we, respec- tively. The two modulated signals are then summed together to obtain the FDM signal y(t). 628 Communication Systems Chap. a (a) For what values of w is Y(jw) guaranteed to be zero? (b) Specify the values of A and w 0 so that XJ (t) = [{ y(t) * sin wot} l A sin Wet ~ COS Wot * Trt , where* denotes convolution. 8.10. A signal x(t) is multiplied by the rectangular pulse train c(t) shown in Figure P8.10. (a) What constraint should be placed on X(jw) to ensure that x(t) can be recovered from the product x(t)c(t) by using an ideallowpass filter? (b) Specify the cutoff frequency w c and the passband gain A of the ideal lowpass filter needed to recover x(t) from x(t)c(t). [Assume that X(jw) satisfies the constraint determined in part (a).] c(t) r 0 25X10-3sec ~ - 1_. ...... - :---- ... 0 t(sec) Figure P8. 1 0 8.11. Let X c(t) = L akejkw,r, k=-X where a0 = 0 and a 1 =rf 0, be a real-valued periodic signal. Also, let x(t) be a signal with X(jw) = 0 for lwl 2: wJ2. The signal x(t) is used to modulate the carrier c(t) to obtain y(t) = x(t)c(t). (a) Specify the passband and the passband gain of an ideal bandpass filter so that, with input y(t), the output of the filter is g(t) = (a I ejwct +a~ e- jw,.t)x(t). (b) If a1 = latlej<ta1, show that g(t) = Acos(wct + cp)x(t), and express A and cp in terms of la1l and <ta1. 8.12. Consider a set of 10 signals xi(t), i = 1, 2, 3, ... , 10. Assume that each Xi(t) has Fourier transform such that Xi(jw) = 0 for lwl 2: 2,00071'. All10 signals are to be time-division multiplexed after each is multiplied by a carrier c(t) shown in Figure P8.12. If the period T of c(t) is chosen to have the maximum allowable value, what is the largest value of~ such that all 10 signals can be time-division multiplexed? Chap. 8 Problems 629 - - - -T 0 T 2T Figure P8. 12 8.13. A class of popularly used pulses in PAM are those which have a raised cosine fre- quency response. The frequency response of one of the members of this class is P(jw) = { i(1 +coswJ'), 0::::; lwl::::; ~~, 0, elsewhere where T1 is the intersymbol spacing. (a) Determine p(O). (b) Determine p(kTI), where k = ± 1, ±2, .... 8.14. Consider the frequency-modulated signal y(t) = cos( wet + m cos Wmt), where we>> Wm and m << 7T/2. Specify an approximation to Y(jw) for w > 0. 8.15. For what values of w 0 in the range -7r < w 0 ::::; 7T is amplitude modulation with carrier ejwon equivalent to amplitude modulation with carrier cos w 0 n? 8.16. Suppose x[n] is a real-valued discrete-time signal whose Fourier transform X(ejw) has the property that . 7T X(elw) = 0 for S ::::; w ::::; 'TT. We use x[n] to modulate a sinusoidal carrier c[n] = sin(57T/2)n to produce y[n] = x[n]c[n]. Determine the values of win the range 0 ::::; w ::::; 7T for which Y(ejw) is guaranteed to be zero. 8.17. Consider an arbitrary finite-duration signal x[n] with Fourier transform X(ejw). We generate a signal g[n] through insertion of zero-valued samples: x[n/4], n = 0, ±4, ±8, ± 12, ... g [ n ] = [ ] X(4) n = { 0, otherwise The signal g[n] is passed through an ideallowpass filter with cutoff frequency 7T/4 and passband gain of unity to produce a signale q:[n ]. Finally, we obtain y[n] = q[n] cos n). For what values of w is Y(ejw) guaranteed to be zero? 630 Communication Systems Chap. 8 8.18. Let x[n] be a real-valued discrete-time signal whose Fourier transform X(ejw) is zero for w 2: 7T/4. We wish to obtain a signal y[n] whose Fourier transform has the property that, in the interval -7r < w :::; 1T, X(ej(w-~)), '!!_ < w < 37T 2 - 4 Y(ejw) = X(ej(w+~)), - 37T < w :::; 7T { 4 2 0, otherwise The system in Figure P8.18 is proposed for obtaining y[n] from x[n]. Determine constraints that the frequency response H ( ejw) of the filter in the figure must satisfy for the proposed system to work. x[n] y[n] Figure PS. 18 8.19. Consider 10 arbitrary real-valued signals Xi[n], i = 1, 2, ... , 10. Suppose each Xi[n] is upsampled by a factor of N, and then sinusoidal amplitude modulation is applied to it with carrier frequency wi = i1TI10. Determine the value of N which would guarantee that all 10 modulated signals can be summed together to yield an FDM signal y[n] from which each Xi[n] can be recovered. 8.20. Let v1 [n] and v2 [n] be two discrete-time signals obtained through the sampling (without aliasing) of continuous-time signals. Let be a TDM signal, where, fori = 1, 2, n = 0, ±2, ±4, ±6, ... otherwise The signal y[n] is processed by the systemS depicted in Figure P8.20 to obtain a signal g[n]. For the two filters used inS, lwl:::; ~ ~<w:::;1T· Chap. 8 Problems 631 Determine the signal p[n] used in S such that g[n] represents frequency-division multiplexing of v1 [n] and v2 [n]. p[n] + y[n] g[n] + p[n-1] Figure P8.20 BASIC PROBLEMS 8.21. In Sections 8.1 and 8.2, we analyzed the sinusoidal amplitude modulation and de- modulation system of Figure 8.8, assuming that the phase 8 c of the carrier signal was zero. (a) For the more general case of arbitrary phase 8 c in the figure, show that the signal in the demodulation system can be expressed as 1 1 w(t) = 2x(t) + 2x(t)cos(2wct + 28c). (b) If x(t) has a spectrum that is zero for lw I 2:: w M, determine the relationships required among w co [the cutoff frequency of the ideallowpass filter in Figure 8.8(b)], We (the carrier frequency), and WM so that the output of the lowpass filter is proportional to x(t). Does your answer depend on the carrier phase 8 c? 8.22. In Figure P8.22(a), a system is shown with input signal x(t) and output signal y(t). The input signal has the Fourier transform X(jw) shown in Figure P8.22(b). Deter- mine and sketch Y(jw ), the spectrum of y(t). 1 1 n 1 n ~ y(t) I I I -5w -3w 3w 5w w -3w 3w w cos(5wt) cos(3wt) (a) Figure P8.22 632 Communication Systems Chap.8 X(jw) ~ (1) -2w 2w (b) Figure P8.22 Continued 8.23. In Section 8.2, we discussed the effect of a loss of synchronization in phase between the carrier signals in the modulator and demodulator in sinusoidal amplitude modu- lation. We showed that the output of the demodulation is attenuated by the cosine of the phase difference, and in particular, when the modulator and demodulator have a phase difference of Tr/2, the demodulator output is zero. As we demonstrate in this problem, it is also important to have frequency synchronization between the modulator and demodulator. Consider the amplitude modulation and demodulation systems in Figure 8.8 with () c = 0 and with a change in the frequency of the demodulator carrier so that w(t) = y(t) cos wc~t, where y(t) = x(t) cos Wet. Let us denote the difference in frequency between the modulator and demodulator as ~w (i.e., wd- We = ~w ). Also, assume that x(t) is band limited with X(jw) = 0 for lw I 2 w M, and assume that the cutoff frequency w co of the lowpass filter in the demodulator satisfies the inequality (a) Show that the output of the lowpass filter in the demodulator is proportional to x(t) cos(~wt). (b) If the spectrum of x(t) is that shown in Figure P8.23, sketch the spectrum of the output of the demodulator. X(jw) ~ wM w Figure P8.23 8.24. Figure P8.24 shows a system to be used for sinusoidal amplitude modulation, where x(t) is band limited with maximum frequency WM, so that X(jw) = 0, lwl > WM. Chap. 8 Problems 633 As indicated, the signal s(t) is a periodic impulse train with period T and with an offset from t = 0 of d. The system H(jw) is a bandpass filter. (a) With Li = 0, WM = 7ri2T, w 1 = 7r!T, and wh = 37r/T, show that y(t) is pro- portional to x(t) cos wet, where We = 27r!T. (b) If WM, w 1, and wh are the same as given in part (a), but dis not necessarily zero, show that y(t) is proportional to x(t) cos(wct + Oc), and determine We and 0 c as a function of T and d. (c) Determine the maximum allowable value of WM relative toT such that y(t) is proportional to x(t) cos( wet + 0 c). x(t) H(jw) y(t) s(t) s(t) 1-T-1 t t t1 I t t 0 Ll (T +Ll) (2T +Ll) H{jw} AI -wh -w.R w.R wh w Figure P8.24 8.25. A commonly used system to maintain privacy in voice communication is a speech scrambler. As illustrated in Figure P8.25(a), the input to the system is a normal speech signal x(t) and the output is the scrambled version y(t). The signal y(t) is transmitted and then unscrambled at the receiver. We assume that all inputs to the scrambler are real and band limited to the frequency w M; that is, X (j w) = 0 for lw I > w M. Given any such input, our pro- posed scrambler permutes different bands of the spectrum of the input signal. In addition, the output signal is real and band limited to the same frequency band; that is, Y(jw) = 0 for lw I > w M. The specific algorithm for the scrambler is Y(jw) = X(j(w- WM)), w > 0, Y(jw) = X(j(w + WM)), w < 0. (a) If X(jw) is given by the spectrum shown in Figure P8.25(b), sketch the spec- trum of the scrambled signal y(t). (b) Using amplifiers, multipliers, adders, oscillators, and whatever ideal filters you find necessary, draw the block diagram for such an ideal scrambler. (c) Again using amplifiers, multipliers, adders, oscillators, and ideal filters, draw a block diagram for the associated unscrambler. 634 Communication Systems Chap.B x(t)~ x(t) (Normal speech) (a) X(jw) & (b) Figure P8.25 8.26. In Section 8.2.2, we discussed the use of an envelope detector for asynchronous demodulation of an AM signal of the form y(t) = [x(t) + A] cos(wct + Oc). An alternative demodulation system, which also does not require phase synchroniza- tion, but does require frequency synchronization, is shown in block diagram form in Figure P8.26. The lowpass filters both have a cutoff frequency of We. The signal y(t) = [x(t) +A] cos(wct + Oc), with Oc constant but unknown. The signal x(t) is band limited with X(jw) = 0, lwl > WM, and with WM <We. As we required for the use of the envelope detector, x(t) +A > 0 for all t. Show that the system in Figure P8.26 can be used to recover x(t) from y(t) without knowledge of the modulator phase ec · cos wet Low pass filter Square y(t) r(t) root Low pass filter sin wet Figure P8.26 8.27. As discussed in Section 8.2.2, asynchronous modulation-demodulation requires the injection of the carrier signal so that the modulated signal is of the form Chap. 8 Problems 635 y(t) = [A + x(t)] cos(wct + Oc), (P8.27-1) where A+ x(t) > 0 for all t. The presence of the carrier means that more transmitter power is required, representing an inefficiency. (a) Let x(t) = cos WMt with WM <We and A+ x(t) > 0. For a periodic signal y(t) with period T, the average power over time is defined asPy = (1/T) JT y2(t) dt. Determine and sketch Py for y(t) in eq. (P8.27-1). Express your answer as a function of the modulation index m, defined as the maximum absolute value of x(t) divided by A. (b) The efficiency of transmission of an amplitude-modulated signal is defined to be the ratio of the power in the sidebands of the signal to the total power in the signal. With x(t) = coswMt, and with WM <We and A+ x(t) > 0, determine and sketch the efficiency of the modulated signal as a function of the modulation index m. 8.28. In Section 8.4 we discussed the implementation of single-sideband modulation using 90° phase-shift networks, and in Figures 8.21 and 8.22 we specifically illustrated the system and associated spectra required to retain the lower sidebands. Figure P8.28(a) shows the corresponding system required to retain the upper sidebands. ,....--~~y,(t) x(t) __{_ +_j _.w_>_O ___. Xp(t) ~~ y(t) H(jw)= . -J ,W< 0 '-- (a) X(jw) w (b) Figure P8.28 636 Communication Systems Chap. 8 (a) With the same X(jw) illustrated in Figure 8.22, sketch Y1( jw ), Y2(jw ), and Y(jw) for the system in Figure P8.28(a), and demonstrate that only the upper sidebands are retained. (b) For X(jw) imaginary, as illustrated in Figure P8.28(b), sketch Y 1(jw), Y2(jw), and Y(jw) for the system in Figure P8.28(a), and demonstrate that, for this case also, only the upper sidebands are retained. 8.29. Single-sideband modulation is commonly used in point-to-point voice communica- tion. It offers many advantages, including effective use of available power, con- servation of bandwidth, and insensitivity to some forms of random fading in the channel. In double-sideband suppressed carrier (DSB/SC) systems the spectrum of the modulating signal appears in its entirety in two places in the transmitted spectrum. Single-sideband modulation eliminates this redundancy, thus conserving bandwidth and increasing the signal-to-noise ratio within the remaining portion of the spectrum that is transmitted. In Figure P8.29(a), two systems for generating an amplitude-modulated single-sideband signal are shown. The system on the top can be used to generate a single-sideband signal for which the lower sideband is retained, and the system on the bottom can produce a single-sideband signal for which the upper sideband is retained. (a) For X(jw) as shown in Figure P8.29(b), determine and sketch S(jw), the Fourier transform of the lower sideband modulated signal, and R(jw ), the Fourier transform of the upper sideband modulated signal. Assume that We> W3. The upper sideband modulation scheme is particularly useful with voice communication, as any real filter has a finite transition region for the cutoff (i.e., near we). This region can be accommodated with negligible distortion, since the voice signal does not have any significant energy near w = 0 (i.e., for jwj < Wt = 27T X 40Hz). (b) Another procedure for generating a single-sideband signal is termed the phase- shift method and is illustrated in Figure P8.29(c). Show that the single- sideband signal generated is proportional to that generated by the lower sideband modulation scheme of Figure P8.29(a) [i.e., p(t) is proportional to s(t)]. (c) All three AM-SSB signals can be demodulated using the scheme shown on the right-hand side of Figure P8.29(a). Show that, whether the received sig- nal is s(t), r(t), or p(t), as long as the oscillator at the receiver is in phase with oscillators at the transmitter, and w = We, the output of the demodulator is x(t). The distortion that results when the oscillator is not in phase with the trans- mitter, called quadrature distortion, can be particularly troublesome in data communication. 8.30. Amplitude modulation with a pulse-train carrier may be modeled as ip Figure P8.30(a). The output of the system is q(t). (a) Let x(t) be a band-limited signal [i.e., X(jw) = 0, jwj 2: 7TIT], as shown in Figure P8.30(b ). Determine and sketch R(jw) and Q(jw ). ~~ 141 I x(t) s(t) - w w (J) _______. x(t) r(t) (a) X(jw) (b) (c) Figure P8.29 637 638 Communication Systems Chap. 8 (b) Find the maximum value of~ such that w(t) = x(t) with an appropriate filter M(jw). (c) Determine and sketch the compensating filter M(jw) such that w(t) = x(t). PAM system h(t) x(t) r(t) _ffi_ ,_....,.._q(_t). .... ~ -ll/2 ll/2 p(t)= L o(t-nT) -----~=~~---------------------1 (a) XOw) cb w .:rr T (b) Figure P8.30 8.31. Let x[n] be a discrete-time signal with spectrumX(ejw), and let p(t) be a continuous- time pulse function with spectrum P(jw ). We form the signal +oc y(t) = L x[n]p(t - n). n= -oc (a) Determine the spectrum Y(jw) in terms of X(ejw) and P(jw ). (b) If p(t) = { cos 81Tt, O:st:sl 0, elsewhere ' determine P(jw ) and Y (j w). 8.32. Consider a discrete-time signal x[n] with Fourier transform shown in Figure P8.32(a). The signal is amplitude modulated by a sinusoidal sequence, as indi- cated in Figure P8.32(b). Chap. 8 Problems 639 (a) Determine and sketch Y(e.iw), the Fourier transform of y[n]. (b) A proposed demodulation system is shown in Figure P8.32(c). For what value of fJ c. w1p, and G will x[n] = x[n]? Are any restrictions on we and w1P necessary to guarantee that x[n] is recoverable from y[n]? 1T (a) x[n]-...,.Ty[n] (b) Figure P8.32 8.33. Let us consider the frequency-division multiplexing of discrete-time signals xi[n], i = 0, 1, 2, 3. Furthermore, each Xi[n] potentially occupies the entire frequency band ( -7r < w < 1r). The sinusoidal modulation of upsampled versions of each of these signals may be carried out by using either double-sideband techniques or single-sideband techniques. (a) Suppose each signal Xi[n] is appropriately upsampled and then modulated with cos[i(1TI4)n]. What is the minimum amount ofupsampling that must be carried out on each xi[n] in order to ensure that the spectrum of the FDM signal does not have any aliasing? (b) If the upsampling of each xi[n] is restricted to be by a factor of 4, how would you use single-sideband techniques to ensure that the FDM signal does not have any aliasing? Hint: See Problem 8.17. 640 Communication Systems Chap.8 ADVANCED PROBLEMS 8.34. In discussing amplitude modulation systems, modulation and demodulation were carried out through the use of a multiplier. Since multipliers are often difficult to implement, many practical systems use a nonlinear element. In this problem, we illustrate the basic concept. In Figure P8.34, we show one such nonlinear system for amplitude modu- lation. The system consists of squaring the sum of the modulating signal and the carrier and then bandpass filtering to obtain the amplitude-modulated signal. Assume that x(t) is band limited, so that X(jw) = 0, lwl > WM. Deter- mine the bandpass filter parameters A, w 1, and wh such that y(t) is an amplitude- modulated version of x(t) [i.e., such that y(t) = x(t) cos wet]. Specify the necessary constraints, if any, on we and w M. y(t) Figure P8.34 8.35. The modulation -demodulation scheme proposed in this problem is similar to sinu- soidal amplitude modulation, except that the demodulation is done with a square wave with the same zero-crossings as cos wet. The system is shown in Figure P8.35(a); the relation between cos wet and p(t) is shown in Figure P8.35(b). Let the input signal x(t) be a band-limited signal with maximum frequency WM <We, as shown in Figure P8.35(c). (a) Sketch and dimension the real and imaginary parts of Z(jw ), P(jw ), and Y(jw ), the Fourier transforms of z(t), p(t), and y(t), respectively. (b) Sketch and dimension a filter H(jw) so that v(t) = x(t). Modulation Demodulation 1--------, ---------------------, I I x(t) ~@-..__z(_t) __- t•~@~o---Y(_t)---!•~~~~~ v(t) l t t L::.f: cos wet p(t) I ---------------------1 (a) Figure P8.35 Chap. 8 Problems 641 (b) Gte {X(jw)} ~m{X(jw)} ,!, (c) Figure P8.35 Continued 8.36. The accurate demultiplexing-demodulation of radio and television signals is gener- ally performed using a system called the superheterodyne receiver, which is equiv- alent to a tunable filter. The basic system is shown in Figure P8.36(a). (a) The input signal y(t) consists of the superposition of many amplitude-modulated signals that have been multiplexed using frequency-division multiplexing, so that each signal occupies a different frequency channel. Let us consider one such channel that contains the amplitude-modulated signal y 1 (t) = x1 (t) cos wet, with spectrum Y1( jw) as depicted at the top of Figure P8.36(b). We want to de- multiplex and demodulate y 1 (t) to recover the modulating signal x1( t), using the system of Figure P8.36(a). The coarse tunable filter has the spectrum H 1( jw) shown at the bottom ofFigureP8.36(b). Determine the spectrumZ(jw) of the in- put signal to the fixed selective filter H2(jw ). Sketch and label Z(jw) for w >0. (b) The fixed frequency-selective filter is a bandpass type centered around the fixed frequency w 1 , as shown in Figure P8.36(c). We would l~ke the output of the filter with spectrum H2(jw) to be r(t) = XJ(t)cosw 1t. In terms of We and WM, what constraint must WT satisfy to guarantee that an undistorted spectrum of x1( t) is centered around w = w f? (c) What must G, a, and {3 be in Figure P8.36(c) so that r(t) = x1 (t) cos w 1t? 8.37. The following scheme has been proposed to perform amplitude modulation: The in- put signal x(t) is added to the carrier signal cos wet and then put through a nonlinear 642 Communication Systems Chap. 8 We Oscilator We cos(we + wt)t It Coarse Fixed tunable z(t) y(t) -----. selective r(t) ~ To X ""? filter filter demodulator H1(jw) H2(jw) (a) Y(jw) (we-wM) We(we+wM) w Input signal K Coarse tunable filter H2(jw) (b) Gl.__________.__ ~I- a Wt J3 w (c) Figure P8.36 Chap. 8 Problems 643 device, so that the output z(t) is related to the input by z(t) = eY<n - 1, y(t) = x(t) + cos Wet. This is illustrated in Figure P8.37(a). Such a nonlinear relation can be implemented through the current-voltage characteristics of a diode, where, with i(t) and v(t) the diode and current and voltage, respectively, i(t) = Ioeav(t) - 1 (a real). To study the effects of nonlinearity, we can examine the spectrum of z(t) and how it relates to X(jw) and we. To accomplish this, we use the power series for eY, which is 1 1 eY = 1 + y + 2 y2 + 6 y3 + .... (a) If the spectrum of x(t) is given by Figure P8.37(b), and if We = 100w 1, sketch and label Z(jw ), the spectrum of z(t), using the first four terms in the power series for eY. (b) The bandpass filter has parameters as shown in Figure P8.37(c). Determine the range of a and the range of f3 such that r(t) is an amplitude-modulated version of x(t). x(t) -TI z=e'-1 1 z(t). ~ r(t) coswct (a) X(jw) 11 -w1 (1)1 (J) (b) H(jw) n 11 n -J3 -a a J3 (J) (c) Figure P8.37 8.38. In Figure P8.38(a), a communication system is shown that transmits a band-limited signal x(t) as periodic bursts of high-frequency energy. Assume that X(jw) = 0 for lwl > WM. Two possible choices, m 1 (t) and m2(t), are considered for the modulating signal m(t). m1( t) is a periodic train of sinusoidal pulses, each of duration D, as shown in Figure P8.38(b). That is, 644 Communication Systems Chap. 8 oc m1 (t) = L, p(t - kT), k= -oc where ( ) _ { COS Wet, ltl < (D/2) p t - 0, ltl > (D/2) . m2(t) is cos Wet periodically blanked or gated; that is, m2(t) = g(t) cos wet, where g(t) is as shown in Figure P8.38(b ). m(t) coswct x(t)-~ ~_d;_ r(t) (a) p(t) :-n-1-""-, -012 ~- _v_ ~ v:=\JD/2 m1(t) _0 12 :-_n_v -~ _ 1_ v-:""JJ-D/2 g(t) I I I ' -D/2 D/2 T m2(t) :-_n~ _v -_ 1_ v-:""=-\J, (b) Figure P8.38 Chap. 8 Problems 645 The following relationships between the parameters T, D, We, and WM are assumed: D<T, 27T We>> D' 27T T >2wM. Also, assume that [sin(x)]/x is negligible for x >> 1. Determine whether, for some choice of w1p, either m1 (t) or m2(t) will result in a demodulated signal x(t). For each case in which your answer is yes, determine an acceptable range for WJp· 8.39. Suppose we wish to communicate one of two possible messages: message m0 or message m1• To do so, we will send a burst of one of two frequencies over a time interval of length T. Note that Tis independent of which message is being trans- mitted. For message m0 we will send cos w 0t, and for message m1 we will send cosw 1t. Thus, a burst b(t) will look as shown in Figure P8.39(a). Such a communi- m 0(t) lr\/\J'J\ m 1(t) ~1VV\fV\f'T (a) cosw 0 t Line ""m0 "" Choose maximum b(t) of the ""m 0 ""or absolute ""m1"" values Line ""m 1"" (b) Figure P8.39 646 Communication Systems Chap. a cation system is called frequency shift keying (FSK). When the burst of frequency b(t) is received, we wish to determine whether it represents message m0 or message m1• To accomplish this, we do as illustrated in Figure P8.39(b). (a) Show that the maximum difference between the absolute values of the two lines in Figure P8.39(b) occurs when cos w 0 t and cos w 1t have the relationship i,T cos wotcos w 1t dt = 0. (b) Is it possible to choose w 0 and w 1 such that there is no interval of length T for which LT cosw0tcosw,tdt = 0? 8.40. In Section 8.3, we discussed the use of sinusoidal modulation for frequency- division multiplexing whereby several signals are shifted into different frequency bands and then summed for simultaneous transmission. In the current problem, we explore another multiplexing concept referred to as quadrature multiplexing. In this multiplexing procedure, two signals can be transmitted simultaneously in the same frequency band if the two carrier signals are 90° out of phase. The multiplexing sys- tem is shown in Figure P8.40(a) and the demultiplexing system in Figure P8.40(b). x 1(t) and x2(t) are both assumed to be band limited with maximum frequency WM, so that X1( jw) = X2(jw) = 0 for lwl > WM. The carrier frequency We is assumed to be greater than WM. Show that Y1 (t) = x 1 (t) and Y2(t) = x2(t). r(t) =multiplexed signal (a) Figure P8.40 8.41. In Problem 8.40, we introduced the concept of quadrature multiplexing, whereby two signals are summed after each has been modulated with carrier signals of iden- tical frequency, but with a phase difference of 90°. The corresponding discrete-time multiplexer and demultiplexer are shown in Figure P8.41. The signals x 1 [n] and x2 [n] are both assumed to be band limited with maximum frequency WM, so that Chap. 8 Problems 647 r(t) } Demultiplexed outputs H(jw) 12 (I) (b) Figure P8.40 Continued r[n] =multiplexed signal (a) r[n] ra----y2[n] Demultiplexed outputs sin wen (b) Figure P8.41 648 Communication Systems Chap. a (a) Determine the range of values for We so that x 1 [n] and x2[n] can be recovered from r[n]. (b) With We satisfying the conditions in part (a), determine H(efw) so that YI [n] = XI [n] and y2[n] = x2[n]. 8.42. In order to avoid intersymbol interference, pulses used in PAM systems are designed to be zero at integer multiples of the symbol spacing TI. In this problem, we develop a class of pulses which are zero at t = kTI, k = ± 1, ±2, ±3, .... Consider a pulse PI (t) that is real and even and that has a Fourier transform PI (jw ). Also, assume that P1 (- jw + j ~ ) = - P1 ~w + j ~ } 0 <; w -; (a) Define a periodic sequence PI (t) with Fourier transform P1(jw) = m~oo P1 (jw- jm ~7} and show that P- I (}.W ) = - p-I (]·W - } .2T7;T ) · (b) Use the result of the previous part to show that for some T PI (t) = 0, t = kT, k = 0, ±2, ±4, .... (c) Use the result of the previous part to show that Pt(kTt) = 0, k = ± 1, ±2, ±3, .... (d) Show that a pulse p(t) with Fourier transform 1 + PI(jw), lwl ~ ¥; P(jw) = Pt(jw), f; ~ lwl ~ ~7 { 0, otherwise also has the property that p(kTI) = 0, k = ±1, ±2, ±3, .... 8.43. The impulse response of a channel used for PAM communication is specified by h(t) = 10,000e-I,OOOtu(t). It is assumed that the phase response of the channel is approximately linear in the bandwidth of the channel. A pulse that is received after passing through the channel is processed using an LTI systemS with impulse response g(t) in order to compen- sate for the nonuniform gain over the channel bandwidth. Chap. 8 Problems 649 (a) Verify that if g(t) has the Fourier transform G(jw) = A+ }Bw, where A and Bare real constants, then g(t) can compensate for the nonuniform gain over the channel bandwidth. Determine the values of A and B. (b) It is proposed that S be implemented with the system shown in Figure P8.43. Determine the values of the gain factors a and f3 in this system. x(t) x(t) y(t) (Received signal (Received signal before compensation) after compensation) Figure P8.43 8.44. In this problem, we explore an equalization method used to avoid intersymbol in- terference caused in PAM ~ystems by the channel having nonlinear phase over its bandwidth. When a PAM pulse with zero-crossings at integer multiples of the symbol spacing T 1 is passed through a channel with nonlinear phase, the received pulse may no longer have zero-crossings at times that are integer multiples ofT 1 • Therefore, in order to avoid intersymbol interference, the received pulse is passed through a zero- forcing equalizer, which forces the pulse to have zero-crossings at integer multiples of T1• This equalizer generates a new pulse y(t) by summing up weighted and shifted versions of the received pulse x(t). The pulse y(t) is given by N y(t) = L GtX(t- lTJ), (P8.44-1) 1=-N where the a1 are all real and are chosen such that ~ 6: k = 0 y(kT1) { k = ±1, ±2, ±3, ... , ±N. (a) Show that the equalizer is a filter and determine its impulse response. (b) To illustrate the selection of the weights a1, let us consider an example. If x(OTJ) = 0.0, x(- TJ) = 0.2, x(TJ) = -0.2, and x(kT1) = 0 for lkl > 1, de- termine the values of a0 , a 1, and a_ 1 such that y(±TJ) = 0. 650 Communication Systems Chap.8 8.45. A band-limited signal x(t) is to be transmitted using narrowband FM techniques. That is, the modulation index m, as defined in Section 8.7, is much less than Tr/2. Before x(t) is transmitted to the modulator, it is processed so that X(jw )iw =O = 0 and lx(t)l < 1. This normalized x(t) is now used to angle-modulate a carrier to form the FM signal (a) Determine the instantaneous frequency w;. (b) Using eqs. (8.44) and (8.45), the narrowband assumption (m << Tr/2), and the preceding normalization conditions, show that (c) What is the relationship among the bandwidth of y(t), the bandwidth of x(t), and the carrier frequency We? 8.46. Consider the complex exponential function of time, s(t) = ei8(t)' (P8.46-1) where O(t) = w t2 0 /2. Since the instantaneous frequency w; = d8/dt is also a function of time, the signal s(t) may be regarded as an FM signal. In particular, since the signal sweeps linearly through the frequency spectrum with time, it is often called a frequency ""chirp"" or ""chirp signal."" (a) Determine the instantaneous frequency. (b) Determine and sketch the magnitude and phase of the Fourier transform of the ""chirp signal."" To evaluate the Fourier transform integral, you may find it help- ful to complete the square in the exponent in the integrand and to use the relation +oc . 2 1T e12 J dz = H-(1 + j). -oc 2 LTI x(t) ------~~@-------+-1 h (t) = s (t) 11----l•~@-------+- y(t) t t s*(t) s*(t) Figure P8.46 Chap. 8 Problems 651 (c) Consider the system in Figure P8.46, in which s(t) is the ""chirp signal"" in eq. (P8.46-1). Show that y(t) = X(jw 0 t), where X(jw) is the Fourier trans- form of x(t). (Note: The system in Figure P8.46 is referred to as the ""chirp"" transform algo- rithm and is often used in practice to obtain the Fourier transform of a signal.) 8.47. In Section 8.8 we considered synchronous discrete-time modulation and demod- ulation with a sinusoidal carrier. In this problem we want to consider the effect of a loss in synchronization in phase and/or frequency. The modulation and demod- ulation systems are shown in Figure P8.47(a), where both a phase and frequency difference between the modulator and demodulator carriers is indicated. Let the frequency difference w d - w c be denoted as ~w and the phase difference () d - () c as ~e. (a) If the spectrum of x[n] is that shown in Figure P8.47(b), sketch the spectrum of w[n], assuming ~w = 0. (b) If ~w = 0, show that w can be chosen so that the output r[ n] is r[ n] = x[n] cos~(). In particular, what is r[n] if~() = n/2? (c) For~() = 0, and w = WM + ~w, show that the output r[n] = x[n] cos[~wn] (assume that ~w is small). ~ x[n]~~y[n] 1---~ r[n] -w w w (a) (b) Figure P8.47 8.48. In this problem, we consider the analysis of discrete-time amplitude modulation of a pulse-train carrier. The system to be considered is shown in Figure P8.48(a). 652 Communication Systems Chap. a (a) Determine and sketch the discrete-time Fourier transform of the periodic square-wave signal p[n] in Figure P8.48(a). (b) Assume that x[ n] has the spectrum shown in Figure P8.48(b ). With wM = 7ri2N and with M = 1 in Figure P8.48(a), sketch Y(ejw), the Fourier transform of y[n]. (c) Now assume that X(ejw) is known to be band limited with X(ejw) = 0, WM < w < 27r- WM, but is otherwise unspecified. For the system of Figure P8.48(a), determine, as a function of N, the maximum allowable value of w M that will permit x[n] to be recovered from y[n]. Indicate whether your result depends onM. (d) With w M and N satisfying the condition determined in part (c), state or show in block diagram form how to recover x[n] from y[n]. x[n] ---.~ y[n] t p[n] p[n] IIIIII ••••••••••• IIIIIt ••••••••••• IIIIII ••••••••••• I 01• • •M N (N+M) n (a) 1T w (b) Figure P8.48 8.49. In practice it is often very difficult to build an amplifier at very low frequencies. Consequently~ low-frequency amplifiers typically exploit the principles of ampli- tude modulation to shift the signal into a higher-frequency band. Such an amplifier is referred to as a chopper amplifier and is illustrated in the block-diagram form in Figure P8.49. x(t)--y--8 y(t) s(t) s(t) Figure P8.49 Chap. 8 Problems 653 s(t) D 1c6 D -T 0 T T T 42 H1(jw) At -=1rT 1T 31T w T T H2(jw) 11 I -=1rT 1T w T Figure P8.49 Continued (a) Determine in terms ofT the highest allowable frequency present in x(t), if y(t) is to be proportional to x(t) (i.e., if the overall system is to be equivalent to an amplifier). (b) With x(t) bandlimited as specified in part (a), determine the gain of the overall system in Figure P8.49 in terms of A and T. 9 THE LAPLACE TRANSFORM 9.0 INTRODUCTION In the preceding chapters, we have seen that the tools of Fourier analysis are extremely useful in the study of many problems of practical importance involving signals and LTI systems. This is due in large part to the fact that broad classes of signals can be represented as linear combinations of periodic complex exponentials and that complex exponentials are eigenfunctions of LTI systems. The continuous-time Fourier transform provides us with a representation for signals as linear combinations of complex exponentials of the form est with s = jw. However the eigenfunction property introduced in Section 3.2 and many of its consequences apply as well for arbitrary values of s and not only those values that are purely imaginary. This observation leads to a generalization of the continuous-time Fourier transform, known as the Laplace transform, which we develop in this chapter. In the next chapter we develop the corresponding discrete-time generalization known as the z-transform. As we will see, the Laplace and z-transforms have many of the properties that make Fourier analysis so useful. Moreover, not only do these transforms provide additional tools and insights for signals and systems that can be analyzed using the Fourier transform, but they also can be applied in some very important contexts in which Fourier transforms cannot. For example Laplace and z-transforms can be applied to the analysis of many un- stable systems and consequently play an important role in the investigation of the stability or instability of systems. This fact, combined with the algebraic properties that Laplace and z-transforms share with Fourier transforms, leads to a very important set of tools for system analysis and in particular for the analysis of feedback systems, which we develop in Chapter 11. 654"
9 The Laplace Transform,"9 THE LAPLACE TRANSFORM 9.0 INTRODUCTION In the preceding chapters, we have seen that the tools of Fourier analysis are extremely useful in the study of many problems of practical importance involving signals and LTI systems. This is due in large part to the fact that broad classes of signals can be represented as linear combinations of periodic complex exponentials and that complex exponentials are eigenfunctions of LTI systems. The continuous-time Fourier transform provides us with a representation for signals as linear combinations of complex exponentials of the form est with s = jw. However the eigenfunction property introduced in Section 3.2 and many of its consequences apply as well for arbitrary values of s and not only those values that are purely imaginary. This observation leads to a generalization of the continuous-time Fourier transform, known as the Laplace transform, which we develop in this chapter. In the next chapter we develop the corresponding discrete-time generalization known as the z-transform. As we will see, the Laplace and z-transforms have many of the properties that make Fourier analysis so useful. Moreover, not only do these transforms provide additional tools and insights for signals and systems that can be analyzed using the Fourier transform, but they also can be applied in some very important contexts in which Fourier transforms cannot. For example Laplace and z-transforms can be applied to the analysis of many un- stable systems and consequently play an important role in the investigation of the stability or instability of systems. This fact, combined with the algebraic properties that Laplace and z-transforms share with Fourier transforms, leads to a very important set of tools for system analysis and in particular for the analysis of feedback systems, which we develop in Chapter 11. 654"
9.0 Introduction,"9 THE LAPLACE TRANSFORM 9.0 INTRODUCTION In the preceding chapters, we have seen that the tools of Fourier analysis are extremely useful in the study of many problems of practical importance involving signals and LTI systems. This is due in large part to the fact that broad classes of signals can be represented as linear combinations of periodic complex exponentials and that complex exponentials are eigenfunctions of LTI systems. The continuous-time Fourier transform provides us with a representation for signals as linear combinations of complex exponentials of the form est with s = jw. However the eigenfunction property introduced in Section 3.2 and many of its consequences apply as well for arbitrary values of s and not only those values that are purely imaginary. This observation leads to a generalization of the continuous-time Fourier transform, known as the Laplace transform, which we develop in this chapter. In the next chapter we develop the corresponding discrete-time generalization known as the z-transform. As we will see, the Laplace and z-transforms have many of the properties that make Fourier analysis so useful. Moreover, not only do these transforms provide additional tools and insights for signals and systems that can be analyzed using the Fourier transform, but they also can be applied in some very important contexts in which Fourier transforms cannot. For example Laplace and z-transforms can be applied to the analysis of many un- stable systems and consequently play an important role in the investigation of the stability or instability of systems. This fact, combined with the algebraic properties that Laplace and z-transforms share with Fourier transforms, leads to a very important set of tools for system analysis and in particular for the analysis of feedback systems, which we develop in Chapter 11. 654 Sec. 9.1 The Laplace Transform 655 9.1 THE lAPlACE TRANSFORM In Chapter 3, we saw that the response of a linear time-invariant system with impulse response h(t) to a complex exponential input of the form est is y(t) = H(s)est, (9.1) where H(s) = I:oo h(t)e-st dt. (9.2) For s imaginary (i.e., s = jw ), the integral in eq. (9.2) corresponds to the Fourier trans- form of h(t). For general values of the complex variables, it is referred to as the Laplace transform of the impulse response h(t). The Laplace transform of a general signal x(t) is defined as 1 +oo X(s) ~ I-o o x(t)e-st dt, (9.3) and we note in particular that it is a function of the independent variable s corresponding to the complex variable in the exponent of e-st. The complex variables can be written as s = (j' + jw, with (j' and w the real and imaginary parts, respectively. For convenience, we will sometimes denote the Laplace transform in operator form as cC{x(t)} and denote the transform relationship between x(t) and X(s) as oC x(t) ~ X(s). (9.4) Whens jw, eq. (9.3) becomes +oo X(jw) = I-o o x(t)e- jwt dt, (9.5) which corresponds to the Fourier transform of x(t); that is, X(s)ls= jw = 5={x(t)}. (9.6) The Laplace transform also bears a straightforward relationship to the Fourier trans- form when the complex variables is not purely imaginary. To see this relationship, consider X(s) as specified in eq. (9.3) with s expressed ass = (j' + jw, so that +oo X((J + jw) = I-o o x(t)e-(O""+ jw)t dt, (9.7) 1 The transform defined by eq. (9.3) is often called the bilateral Laplace transform, to distinguish it from the unilateral Laplace transform, which we discuss in Section 9.9. The bilateral transform in eq. (9.3) involves an integration from -oo to +oc, while the unilateral transform has a form similar to that in eq. (9.3), but with limits of integration from 0 to +oo. As we are primarily concerned with the bilateral transform, we will omit the word ""bilateral,"" except where it is needed in Section 9.9 to avoid ambiguity."
9.1 The Laplace Transform,"Sec. 9.1 The Laplace Transform 655 9.1 THE lAPlACE TRANSFORM In Chapter 3, we saw that the response of a linear time-invariant system with impulse response h(t) to a complex exponential input of the form est is y(t) = H(s)est, (9.1) where H(s) = I:oo h(t)e-st dt. (9.2) For s imaginary (i.e., s = jw ), the integral in eq. (9.2) corresponds to the Fourier trans- form of h(t). For general values of the complex variables, it is referred to as the Laplace transform of the impulse response h(t). The Laplace transform of a general signal x(t) is defined as 1 +oo X(s) ~ I-o o x(t)e-st dt, (9.3) and we note in particular that it is a function of the independent variable s corresponding to the complex variable in the exponent of e-st. The complex variables can be written as s = (j' + jw, with (j' and w the real and imaginary parts, respectively. For convenience, we will sometimes denote the Laplace transform in operator form as cC{x(t)} and denote the transform relationship between x(t) and X(s) as oC x(t) ~ X(s). (9.4) Whens jw, eq. (9.3) becomes +oo X(jw) = I-o o x(t)e- jwt dt, (9.5) which corresponds to the Fourier transform of x(t); that is, X(s)ls= jw = 5={x(t)}. (9.6) The Laplace transform also bears a straightforward relationship to the Fourier trans- form when the complex variables is not purely imaginary. To see this relationship, consider X(s) as specified in eq. (9.3) with s expressed ass = (j' + jw, so that +oo X((J + jw) = I-o o x(t)e-(O""+ jw)t dt, (9.7) 1 The transform defined by eq. (9.3) is often called the bilateral Laplace transform, to distinguish it from the unilateral Laplace transform, which we discuss in Section 9.9. The bilateral transform in eq. (9.3) involves an integration from -oo to +oc, while the unilateral transform has a form similar to that in eq. (9.3), but with limits of integration from 0 to +oo. As we are primarily concerned with the bilateral transform, we will omit the word ""bilateral,"" except where it is needed in Section 9.9 to avoid ambiguity. 656 The Laplace Transform Chap.9 or +x X(u + jw) = f-oo [x(t)e-(]'1]e- jwt dt. (9.8) We recognize the right-hand side of eq. (9.8) as the Fourier transform of x(t)e-(]'1; that is, the Laplace transform of x(t) can be interpreted as the Fourier transform of x(t) after multiplication by a real exponential signal. The real exponential e-m may be decaying or growing in time, depending on whether u is positive or negative. To illustrate the Laplace transform and its relationship to the Fourier transform, let us consider the following example: Example 9.1 Let the signal x(t) = e-aru(t). From Example 4.1, the Fourier transform X(jw) con- verges for a > 0 and is given by a> 0. (9.9) From eq. (9.3), the Laplace transform is X(s) = J"""" e-aru(t)e-s1dt = ("""" e-<s + a)tdt, (9.10) -x Jo or, withs = u + jw, X(u + jw) = L""' e-(a+a)te-jwt dt. (9.11) By comparison with eq. (9.9) we recognize eq. (9.11) as the Fourier transform of e -(a+a)t u(t), and thus, X(u + jw) = u +a> 0, (9.12) (u +a)+ jw' or equivalently, since s = u + jw and u = (Jl.e{s}, 1 X(s) = --, (Jl.e{s} > -a. (9.13) s+a That is, .c 1 e-aru(t) ~ --, CRe{s} >-a. (9.14) s+a For example, for a 0, x(t) is the unit step with Laplace transform X(s) = lis, (Jl.e{s} > 0. We note, in particular, that just as the Fourier transform does not converge for all signals, the Laplace transform may converge for some values of CRe{s} and not for others. In eq. (9.13), the Laplace transform converges only for u = ffi-e{s} > -a. If a is positive, Sec. 9.1 The Laplace Transform 657 then X(s) can be evaluated at u = 0 to obtain 1 X(O+jw)= (9.15) jw +a' As indicated in eq. (9.6), for u = 0 the Laplace transform is equal to the Fourier transform, as is evident in the preceding example by comparing eqs. (9.9) and (9.15). If a is negative or zero, the Laplace transform still exists, but the Fourier transform does not. Example 9.2 For comparison with Example 9.1, let us consider as a second example the signal x(t) = -e-at u( -t). (9.16) Then X(s) = - J:oo e-ate-stu(-t)dt (9.17) = - J:oo e-(s+a)t dt, or 1 X(s) = --. (9.18) s+a For convergence in this example, we require that ffi-e{s +a} < 0, or ffi-e{s} < -a; that is, J3 1 -e-atu(-t) ~ --, ffi-e{s} < -a. (9.19) s+a Comparing eqs. (9.14) and (9.19), we see that the algebraic expression for the Laplace transform is identical for both of the signals considered in Examples 9.1 and 9 .2. However, from the same equations, we also see that the set of values of s for which the expression is valid is very different in the two examples. This serves to illustrate the fact that, in specifying the Laplace transform of a signal, both the algebraic expression and the range of values of s for which this expression is valid are required. In general, the range of values of s for which the integral in eq.(9.3) converges is referred to as the region of convergence (which we abbreviate as ROC) of the Laplace transform. That is, the ROC consists of those values of s = u + jw for which the Fourier transform of x(t)e-ut converges. We will have more to say about the ROC as we develop some insight into the properties of the Laplace transform. A convenient way to display the ROC is shown in Figure 9 .1. The variable s is a complex number, and in the figure we display the complex plane, generally referred to as the s-plane, associated with this complex variable. The coordinate axes are ffi-e{s} along the horizontal axis and dm{s} along the vertical axis. The horizontal and vertical axes are sometimes referred to as the u-axis and the jw-axis, respectively. The shaded region in Figure 9.1 (a) represents the set of points in the s-plane corresponding to the region of convergence for Example 9 .1. The shaded region in Figure 9.1 (b) indicates the region of convergence for Example 9. 2. 658 The Laplace Transform Chap.9 9m 9m s-plane s-plane -a 1 ffi-e ,-a ffi-e I I I I I I I I I I I I I I (a) (b) Figure 9.1 (a) ROC for Example 9.1; (b) ROC for Example 9.2. Example 9.3 In this example, we consider a signal that is the sum of two real exponentials: (9.20) The algebraic expression for the Laplace transform is then X(s) = I~x [3e- 21 u(t)- 2e- 1u(t)]e-I1 dt (9.21) = 3 J:x e-'2te- 11 U(t)dt- 2 J:""' e- 1e-11 u(t)dt. Each of the integrals in eq. (9.21) is of the same form as the integral in eq. (9.10), and consequently, we can use the result in Example 9.1 to obtain 3 X(s) = -- -- -2- . (9.22) s+2 s+l To determine the ROC we note that x(t) is a sum of two real exponentials, and from eq. (9.21) we see that X(s) is the sum of the Laplace transforms of each of the individual terms. The first term is the Laplace transform of 3e- 21 u(t) and the second term the Laplace transform of -2e- 1u(t). From Example 9.1, we know that £ 1 e-r u(t) ~ --1' ffi-e{s} > -1, s+ 1 1 £ 1 e-- u(t) ~ -- ffi-e{s} > -2. s + 2' The set of values of ffi-e{s} for which the Laplace transforms of both terms converge is ffi-e{s} > -1, and thus, combining the two terms on the right-hand side of eq. (9.22), we obtain 21 1 £ s- 1 3e- u(t)-2e- u(t)~ . ffi£{s}>-l. (9.23) s-1 + 3_s + 2 Sec. 9.1 The Laplace Transform 659 Example 9.4 In this example, we consider a signal that is the sum of a real and a complex exponential: x(t) = e-21 u(t) + e-1(cos 3t)u(t). (9.24) Using Euler's relation, we can write x(t) = [e-2t + ~e-(l-3j)l + ~e-(1+3j)t] u(t), and the Laplace transform of x(t) then can be expressed as + -1 Joo e-0 - 31.) 1 u(t)e-st dt (9.25) 2 -00 +-1 Joo e-(1+31.l 1u(t)e-s1 dt. 2 -00 Each of the integrals in eq. (9.25) represents a Laplace transform of the type en- countered in Example 9.1. It follows that e-21 J3 1 u(t) ~ s + , CRe{s} > -2, (9.26) 2 1 e-0-3jltu(t) ~ CRe{s} > -1, + (9.27) s (1- 3j)' 1 e-(1+3jltu(t) ~ CRe{s} > -1. (9.28) s + (1 + 3j)' For all three Laplace transforms to converge simultaneously, we must have CRe{s} > -1. Consequently, the Laplace transform of x(t) is 1 1( 1 ) 1( 1 ) CRe{s} > -1, s + (9.29) 2 + 2 s + ( 1 - 3 j) + 2 s + ( 1 + 3 j) ' or, with terms combined over a common denominator, 2 -21 -t £ 2s + 5s + 12 ro e u(t) + e (cos 3t)u(t) ~ (s2 + 25 + 10)(s + 2)' IJ\-e{s} > -1. (9.30) In each of the four preceding examples, the Laplace transform is rational, i.e., it is a ratio of polynomials in the complex variables, so that N(s) X(s) = D(s)' (9.31) where N(s) and D(s) are the numerator polynomial and denominator polynomial, respec- tively. As suggested by Examples 9.3 and 9.4, X(s) will be rational whenever x(t) is a linear combination of real or complex exponentials. As we will see in Section 9.7, rational 660 The Laplace Transform Chap.9 transforms also arise when we consider LTI systems specified in terms of linear constant- coefficient differential equations. Except for a scale factor, the numerator and denominator polynomials in a rational Laplace transform can be specified by their roots; thus, mark- ing the locations of the roots of N(s) and D(s) in the s-plane and indicating the ROC provides a convenient pictorial way of describing the Laplace transform. For example, in Figure 9.2(a) we show the s-plane representation of the Laplace transform of Example 9.3, with the location of each root of the denominator polynomial in eq. (9.23) indicated with ""X"" and the location of the root of the numerator polynomial in eq. (9.23) indicated with ""o."" The corresponding plot of the roots of the numerator and denominator polynomials for the Laplace transform in Example 9.4 is given in Figure 9.2(b). The region of convergence for each of these examples is shaded in the corresponding plot. !Jm I I I s-plane I I I I )( ~ -2 -11 I I I I I I (a) !Jm o*l s-plane I I I I I Figure 9.2 s-plane representation -2 -11 ffi-e of the Laplace transforms for (a) Ex- I I ample 9.3 and (b) Example 9.4. Each I x in these figures marks the location 01 of a pole of the corresponding Laplace I )I( transform-i.e., a root of the denomi- I nator. Similarly, each o marks a zero- i.e., a root of the the numerator. The (b) shaded regions indicate the ROCs. For rational Laplace transforms, the roots of the numerator polynomial are com- monly referred to as the zeros of X(s), since, for those values of s, X(s) = 0. The roots of the denominator polynomial are referred to as the poles of X(s), and for those values of s, X(s) is infinite. The poles and zeros of X(s) in the finite s-plane completely char- acterize the algebraic expression for X(s) to within a scale factor. The representation of X(s) through its poles and zeros in the s-plane is referred to as the pole-zero plot of X(s). Sec. 9.1 The Laplace Transform 661 However, as we saw in Examples 9.1 and 9 .2, know ledge of the algebraic form of X (s) does not by itself identify the ROC for the Laplace transform. That is, a complete specification, to within a scale factor, of a rational Laplace transform consists of the pole-zero plot of the transform, together with its ROC (which is commonly shown as a shaded region in the s-plane, as in Figures 9.1 and 9.2). Also, while they are not needed to specify the algebraic form of a rational transform X(s), it is sometimes convenient to refer to poles or zeros of X(s) at infinity. Specifically, if the order of the denominator polynomial is greater than the order of the numerator poly- nomial, then X(s) will become zero ass approaches infinity. Conversely, if the order of the numerator polynomial is greater than the order of the denominator, then X(s) will become unbounded as s approaches infinity. This behavior can be interpreted as zeros or poles at infinity. For example, the Laplace transform in eq. (9.23) has a denominator of order 2 and a numerator of order only 1, so in this case X(s) has one zero at infinity. The same is true for the transform in eq. (9.30), in which the numerator is of order 2 and the denominator is of order 3. In general, if the order of the denominator exceeds the order of the numerator by k, X(s) will have k zeros at infinity. Similarly, if the order of the numerator exceeds the order of the denominator by k, X(s) will have k poles at infinity. Example 9.5 Let (9.32) The Laplace transform of the second and third terms on the right-hand side of eq. (9.32) can be evaluated from Example 9.1. The Laplace transform of the unit impulse can be evaluated directly as (9.33) which is valid for any value of s. That is, the ROC of £{8(t)} is the entire s-plane. Using this result, together with the Laplace transforms of the other two terms in eq. (9.32), we obtain 4 1 1 1 X ( s) = 1 - 3 s + 1 + 3 s - 2, ffi-e{ s} > 2, (9.34) or X (s- 1)2 (s) (s + l)(s- 2)' ffi-e{s} > 2, (9.35) = where the ROC is the set of values of s for which the Laplace transforms of all three terms in x(t) converge. The pole-zero plot for this example is shown in Figure 9.3, together with the ROC. Also, since the degrees of the numerator and denominator of X(s) are equal, X(s) has neither poles nor zeros at infinity. 662 The Laplace Transform Chap.9 9m s-plane I I I I X X I -1 +1 +2 (Jl.e I I I I I I I I I Figure 9.3 Pole-zero plot and ROC for Example 9.5. Recall from eq. (9.6) that, for s = jw, the Laplace transform corresponds to the Fourier transform. However, if the ROC of the Laplace transform does not include the jw-axis, (i.e., if CRe{s} = 0), then the Fourier transform does not converge. As we see from Figure 9.3, this, in fact, is the case for Example 9.5, which is consistent with the fact that the term (1/3)e2 t u(t) in x(t) does not have a Fourier transform. Note also in this example that the two zeros in eq. (9.35) occur at the same value of s. In general, we will refer to the order of a pole or zero as the number of times it is repeated at a given location. In Example 9.5 there is a second-order zero at s = 1 and two first-order poles, one at s = - 1, the other at s = 2. In this example the ROC lies to the right of the rightmost pole. In general, for rational Laplace transforms, there is a close relationship between the locations of the poles and the possible ROCs that can be associated with a given pole-zero plot. Specific constraints on the ROC are closely associated with time-domain properties of x(t). In the next section, we explore some of these constraints and properties. 9.2 THE REGION OF CONVERGENCE FOR lAPlACE TRANSFORMS In the preceding section, we saw that a complete specification of the Laplace transform re- quires not only the algebraic expression for X(s), but also the associated region of conver- gence. As evidenced by Examples 9.1 and 9 .2, two very different signals can have identical algebraic expressions for X(s), so that their Laplace transforms are distinguishable only by the region of convergence. In this section, we explore some specific constraints on the ROC for various classes of signals. As we will see, an understanding of these constraints often permits us to specify implicitly or to reconstruct the ROC from knowledge of only the al- gebraic expression for X(s) and certain general characteristics of x(t) in the time domain. Property 1: The ROC of X(s) consists of strips parallel to the jw-axis in the s-plane. The validity of this property stems from the fact that the ROC of X(s) consists of those values of s = (J' + jw for which the Fourier transform of x(t)e-at converges. That"
9.2 The Region of Convergence for Laplace Transforms,"662 The Laplace Transform Chap.9 9m s-plane I I I I X X I -1 +1 +2 (Jl.e I I I I I I I I I Figure 9.3 Pole-zero plot and ROC for Example 9.5. Recall from eq. (9.6) that, for s = jw, the Laplace transform corresponds to the Fourier transform. However, if the ROC of the Laplace transform does not include the jw-axis, (i.e., if CRe{s} = 0), then the Fourier transform does not converge. As we see from Figure 9.3, this, in fact, is the case for Example 9.5, which is consistent with the fact that the term (1/3)e2 t u(t) in x(t) does not have a Fourier transform. Note also in this example that the two zeros in eq. (9.35) occur at the same value of s. In general, we will refer to the order of a pole or zero as the number of times it is repeated at a given location. In Example 9.5 there is a second-order zero at s = 1 and two first-order poles, one at s = - 1, the other at s = 2. In this example the ROC lies to the right of the rightmost pole. In general, for rational Laplace transforms, there is a close relationship between the locations of the poles and the possible ROCs that can be associated with a given pole-zero plot. Specific constraints on the ROC are closely associated with time-domain properties of x(t). In the next section, we explore some of these constraints and properties. 9.2 THE REGION OF CONVERGENCE FOR lAPlACE TRANSFORMS In the preceding section, we saw that a complete specification of the Laplace transform re- quires not only the algebraic expression for X(s), but also the associated region of conver- gence. As evidenced by Examples 9.1 and 9 .2, two very different signals can have identical algebraic expressions for X(s), so that their Laplace transforms are distinguishable only by the region of convergence. In this section, we explore some specific constraints on the ROC for various classes of signals. As we will see, an understanding of these constraints often permits us to specify implicitly or to reconstruct the ROC from knowledge of only the al- gebraic expression for X(s) and certain general characteristics of x(t) in the time domain. Property 1: The ROC of X(s) consists of strips parallel to the jw-axis in the s-plane. The validity of this property stems from the fact that the ROC of X(s) consists of those values of s = (J' + jw for which the Fourier transform of x(t)e-at converges. That Sec. 9.2 The Region of Convergence for Laplace Transforms 663 is, the ROC of the Laplace transform of x(t) consists ofthose values of s for which x(t)e-crt is absolutely integrable:2 r: lx(t)le -m dt < 00. (9.36) Property 1 then follows, since this condition depends only on a, the real part of s. Property 2: For rational Laplace transforms, the ROC does not contain any poles. Property 2 is easily observed in all the examples studied thus far. Since X (s) is infinite at a pole, the integral in eq. (9.3) clearly does not converge at a pole, and thus the ROC cannot contain values of s that are poles. Property 3: If x(t) is of finite duration and is absolutely integrable, then the ROC is the entire s-plane. The intuition behind this result is suggested in Figures 9.4 and 9.5. Specifically, a finite-duration signal has the property that it is zero outside an interval of finite duration, as illustrated in Figure 9.4. In Figure 9.5(a), we have shown x(t) of Figure 9.4 multiplied by a decaying exponential, and in Figure 9.5(b) the same signal multiplied by a growing Figure 9.4 Finite-duration signal. """"./Decaying exponential '- Growing exponential '...... --~-- =-=- (a) (b) Figure 9.5 (a) Finite-duration signal of Figure 9.4 multiplied by a decaying exponen- tial; (b) finite-duration signal of Figure 9.4 multiplied by a growing exponential. 2For a more thorough and formal treatment of Laplace transforms and their mathematical properties, including convergence, see E. D. Rainville, The Laplace Transform: An Introduction (New York: Macmil- lan, 1963), and R. V. Churchill and J. W. Brown, Complex Variables and Applications (5th ed.) (New York: McGraw-Hill, 1990). Note that the condition of absolute integrability is one of the Dirichlet conditions intro- duced in Section 4.1 in the context of our discussion of the convergence of Fourier transforms. 664 The Laplace Transform Chap.9 exponential. Since the interval over which x(t) is nonzero is finite, the exponential weight- ing is never unbounded, and consequently, it is reasonable that the integrability of x(t) not be destroyed by this exponential weighting. A more formal verification of Property 3 is as follows: Suppose that x(t) is absolutely integrable, so that To I /x(t)/ dt < oo. (9.37) Tl For s = if + jw to be in the ROC, we require that x(t)e-crt be absolutely integrable, i.e., To I /x(t)/e -crt dt < oo. (9.38) Tl Eq. (9.37) verifies that sis in the ROC when ffi-e{s} = if = 0. For if > 0, the maximum value of e-(rt over the interval on which x(t) is nonzero is e-aT1 , and thus we can write (9.39) Since the right-hand side of eq.(9.39) is bounded, so is the left-hand side; therefore, the s-plane for ffi-e{s} > 0 must also be in the ROC. By a similar argument, if if < 0, then To < JTo -/x(t)/e-m dt e-aT2 -/x(t)/ dt, (9.40) JTl Tl and again, x(t)e-crt is absolutely integrable. Thus, the ROC includes the entire s-plane. Example 9.6 Let x(t) = { e-at, 0 < t < T (9.41) 0, otherwise Then X(s) = (Te-at e-st dt = _I_ [1 - e-(s+a)T]. Jo s (9.42) +a Since in this example x(t) is of finite length, it follows from Property 3 that the ROC is the entire s-plane. In the form of eq. (9.42), X(s) would appear to have a pole at s = -a, which, from Property 2, would be inconsistent with an ROC that consists of the entire s-plane. In fact, however, in the algebraic expression in eq. (9.42), both numerator and denominator are zero at s = -a, and thus, to determine X(s) at s = -a, we can use L'hopital's rule to obtain !L(l - e-(s+a)T) l lim X(s) = lim ds = lim Te-aT e-sT, \'->-a s->-a [ !L(s +a) s->-a ds so that X(-a) = T. (9.43) Sec. 9.2 The Region of Convergence for Laplace Transforms 665 It is important to recognize that, to ensure that the exponential weighting is bounded over the interval in which x(t) is nonzero, the preceding discussion relies heavily on the fact that x(t) is of finite duration. In the next two properties, we consider modifications of the result in Property 3 when x(t) is of finite extent in only the positive-time or negative- time direction. Property 4: If x(t) is right sided, and if the line CR-€{s} = cr0 is in the ROC, then all values of s for which CR-€{s} > cr0 will also be in the ROC. A right-sided signal is a signal for which x(t) = 0 prior to some finite time T1, as illustrated in Figure 9.6. It is possible that, for such a signal, there is no value of s for which 2 the Laplace transform will converge. One example is the signal x(t) = et u(t). However, suppose that the Laplace transform converges for some value of cr, which we denote by cro. Then (9.44) or equivalently, since x(t) is right sided, (9.45) x(t) Figure 9.6 Right-sided signal. Then if cr 1 > cr0 , it must also be true that x(t)e-a1t is absolutely integrable, since e-a1t decays faster than e-aot as t ~ +oo, as illustrated in Figure 9.7. Formally, we can say that with c:TJ > cro, 'YJ lx(t)le-att dt = f'YJ lx(t)le-aote-(at-ao)t dt f Tt Tt (9.46) :::::; e-(at-ao)Tt f'YJ lx(t)le-aot dt. Tt Since T1 is finite, it follows from eq. (9.45) that the right side of the inequality in eq. (9.46) is finite, and hence, x(t)e-a 1t is absolutely integrable. Note that in the preceding argument we explicitly rely on the fact that x(t) is right sided, so that, although with cr1 > cr0 , e-a 1t diverges faster than e-aot as t ~ -oo, x(t)e-a 1t cannot grow without bound in the negative-time direction, since x(t) = 0 for 666 The Laplace Transform Chap.9 Figure 9.7 If x(t) is right sided and x(t)e-'rot is absolutely integrable, then x(t)e-'r1t, u1 > u0, will also be absolutely integrable. t < T1• Also, in this case, if a points is in the ROC, then all the points to the right of s, i.e., all points with larger real parts, are in the ROC. For this reason, the ROC in this case is commonly referred to as a right-half plane. Property 5: If x(t) is left sided, and if the line CRe{s} = u 0 is in the ROC, then all values of s for which CRe{s} < u 0 will also be in the ROC. A left-sided signal is a signal for which x(t) = 0 after some finite time T2, as illus- trated in Figure 9.8. The argument and intuition behind this property are exactly analogous to the argument and intuition behind Property 4. Also, for a left-sided signal, the ROC is commonly referred to as a left-half plane, as if a point s is in the ROC, then all points to the left of s are in the ROC. x(t) T2 Figure 9.8 Left-sided signal. Property 6: If x(t) is two sided, and if the line ffi-£{s} = u 0 is in the ROC, then the ROC will consist of a strip in the s-plane that includes the line ffi-£{s} = u 0 . A two-sided signal is a signal that is of infinite extent for both t > 0 and t < 0, as illustrated in Figure 9.9(a). For such a signal, the ROC can be examined by choosing an arbitrary time To and dividing x(t) into the sum of a right-sided signal xR(t) and a left- sided signal XL(t), as indicated in Figures 9.9(b) and 9.9(c). The Laplace transform of x(t) converges for values of s for which the transforms of both XR(t) and XL(t) converge. From Property 4, the ROC of £{xR(t)} consists of a half-plane ffi-£{s} > uR for some value uR, and from Property 5, the ROC of £{xL(t)} consists of a half-plane ffi-£{s} < uL for some value uL. The ROC of £{x(t)} is then the overlap of these two half-planes, as indicated in Figure 9.1 0. This assumes, of course, that u R < u L, so that there is some overlap. If this is not the case, then even if the Laplace transforms of XR(t) and xL(t) individually exist, the Laplace transform of x(t) does not. Sec. 9.2 The Region of Convergence for Laplace Transforms 667 x(t) ""---- (a) """"'----- J (b) (c) Figure 9. 9 Two-sided signal divided into the sum of a right-sided and left-sided sig- nal: (a) two-sided signal x{t); (b) the right-sided signal equal to x{t) for t > To and equal to 0 for t < T0; (c) the left-sided signal equal to x{t) for t < To and equal to 0 for t >To. 9m <TRI ffi..e I I I I I I I (a) 9m 9m I al ffi..e aRI <TL ffi..e I I I I I I I I I I I I I I (b) (c) Figure 9.10 (a) ROC for xR(t) in Figure 9.9; (b) ROC for xL(t) in Figure 9.9; (c) the ROC for x(t) = xR(t) + xL(t), assuming that the ROCs in (a) and (b) overlap. 668 The Laplace Transform Chap.9 Example 9.7 Let (9.47) as illustrated in Figure 9.11 for both b > 0 and b < 0. Since this is a two-sided signal, let us divide it into the sum of a right-sided and left-sided signal; that is, (9.48) Figure 9.11 Signal x(t) = e-bltl for both b > 0 and b < 0. From Example 9.1, .c 1 e-br u(t) ~ s + b' CRe{s} > -b, (9.49) and from Example 9.2, .c -1 e+b1u(-t) ~ --, CRe{s} <+b. (9.50) s-b Although the Laplace transforms of each of the individual terms in eq. (9.48) have a region of convergence, there is no common region of convergence if b s; 0, and thus, for those values of b, x(t) has no Laplace transform. If b > 0, the Laplace transform of x(t) is -bltl .c 1 1 -2b e ~ s + b - s- b = s2- b2' -b < (Jl.e{s} < +b. (9.51) The corresponding pole-zero plot is shown in Figure 9.12, with the shading indicating the ROC. Sec. 9.2 The Region of Convergence for Laplace Transforms 669 9m I I : : s-plane I I I I I I -----bX I : ----+----I¥ I b- ----CR-e I I I I I I I I I I Figure 9.12 Pole-zero plot and ROC for Example 9.7. A signal either does not have a Laplace transform or falls into one of the four cate- gories covered by Properties 3 through 6. Thus, for any signal with a Laplace transform, the ROC must be the entire s-plane (for finite-length signals), a left-half plane (for left- sided signals), a right-half plane (for right-sided signals), or a single strip (for two-sided signals). In all the examples that we have considered, the ROC has the additional property that in each direction (i.e., CR-e{s} increasing and CR-e{s} decreasing) it is bounded by poles or extends to infinity. In fact, this is always true for rational Laplace transforms: Property 7: If the Laplace transform X(s) of x(t) is rational, then its ROC is bounded by poles or extends to infinity. In addition, no poles of X(s) are contained in the ROC. A formal argument establishing this property is somewhat involved, but its validity is essentially a consequence of the facts that a signal with a rational Laplace transform consists of a linear combination of exponentials and, from Examples 9.1 and 9.2, that the ROC for the transform of individual terms in this linear combination must have the property. As a consequence of Property 7, together with Properties 4 and 5, we have Property 8: If the Laplace transform X(s) of x(t) is rational, then if x(t) is right sided, the ROC is the region in the s-plane to the right of the rightmost pole. If x(t) is left sided, the ROC is the region in the s-plane to the left of the leftmost pole. To illustrate how different ROCs can be associated with the same pole-zero pattern, let us consider the following example: Example 9.8 Let 1 X(s) = (s + l)(s + 2)' (9.52) with the associated pole-zero pattern in Figure 9.13(a). As indicated in Figures 9.13(b)- ( d), there are three possible ROCs that can be associated with this algebraic expression, corresponding to three distinct signals. The signal associated with the pole-zero pat- tern in Figure 9.13(b) is right sided. Since the ROC includes the jw-axis, the Fourier 670 The Laplace Transform Chap.9 9m 9m I s-plane I s-plane I I I ---x-x~--ffi-£ ---x-x~--------ffi--£ (a) (b) 9m 9m I s-plane I s-plane I I I I I --~-X-+---ffi-£ --x-x~------ 1 I ffi£ I I I I I I I I I I I I (c) (d) Figure 9.13 (a) Pole-zero pattern for Example 9.8; (b) ROC corresponding to a right-sided sequence; (c) ROC corresponding to a left-sided sequence; (d) ROC corresponding to a two-sided sequence. transform of this signal converges. Figure 9 .13( c) corresponds to a left -sided signal and Figure 9.13(d) to a two-sided signal. Neither of these two signals have Fourier trans- forms, since their ROCs do not include the jw-axis. 9.3 THE INVERSE LAPLACE TRANSFORM In Section 9.1 we discussed the interpretation of the Laplace transform of a signal as the Fourier transform of an exponentially weighted version of the signal; that is, with s ex- pressed ass = u + jw, the Laplace transform of a signal x(t) is X(<T + jw) = :J{x(l)e -m) = [,, x(t)e -me- Jwt dt (9.53) for values of s = u + jw in the ROC. We can invert this relationship using the inverse Fourier transform as given in eq. (4.9). We have x(t)e-m = ~- 1 {X(u + jw)} = - 1 f +x X(u + jw)e1. w 1 dw, (9.54) 27T -X or, multiplying both sides by em, we obtain x(t) = - 1 f +oc X(u + jw )e(if+ . ;w)t dw. (9.55) 27T -X"
9.3 The Inverse Laplace Transform,"670 The Laplace Transform Chap.9 9m 9m I s-plane I s-plane I I I ---x-x~--ffi-£ ---x-x~--------ffi--£ (a) (b) 9m 9m I s-plane I s-plane I I I I I --~-X-+---ffi-£ --x-x~------ 1 I ffi£ I I I I I I I I I I I I (c) (d) Figure 9.13 (a) Pole-zero pattern for Example 9.8; (b) ROC corresponding to a right-sided sequence; (c) ROC corresponding to a left-sided sequence; (d) ROC corresponding to a two-sided sequence. transform of this signal converges. Figure 9 .13( c) corresponds to a left -sided signal and Figure 9.13(d) to a two-sided signal. Neither of these two signals have Fourier trans- forms, since their ROCs do not include the jw-axis. 9.3 THE INVERSE LAPLACE TRANSFORM In Section 9.1 we discussed the interpretation of the Laplace transform of a signal as the Fourier transform of an exponentially weighted version of the signal; that is, with s ex- pressed ass = u + jw, the Laplace transform of a signal x(t) is X(<T + jw) = :J{x(l)e -m) = [,, x(t)e -me- Jwt dt (9.53) for values of s = u + jw in the ROC. We can invert this relationship using the inverse Fourier transform as given in eq. (4.9). We have x(t)e-m = ~- 1 {X(u + jw)} = - 1 f +x X(u + jw)e1. w 1 dw, (9.54) 27T -X or, multiplying both sides by em, we obtain x(t) = - 1 f +oc X(u + jw )e(if+ . ;w)t dw. (9.55) 27T -X Sec. 9.3 The Inverse Laplace Transform 671 That is, we can recover x(t) from its Laplace transform evaluated for a set of values of s = u + jw in the ROC, with u fixed and w varying from -oo to +oo. We can highlight this and gain additional insight into recovering x(t) from X(s) by changing the variable of integration in eq. (9.55) from w to sand using the fact that u is constant, so that ds = j dw. The result is the basic inverse Laplace transform equation: 1 fer+ jx x(t) = - . X(s)est ds. (9.56) 21 1') cr-j':fj This equation states that x(t) can be represented as a weighted integral of complex exponentials. The contour of integration in eq. (9.56) is the straight line in the s-plane corresponding to all points s satisfying CRc{s} = u. This line is parallel to the jw-axis. Furthermore, we can choose any such line in the ROC-i.e., we can choose any value of u such that X(u + jw) converges. The formal evaluation of the integral for a general X(s) requires the use of contour integration in the complex plane, a topic that we will not consider here. However, for the class of rational transforms, the inverse Laplace transform can be determined without directly evaluating eq. (9.56) by using the technique of partial- fraction expansion in a manner similar to that used in Chapter 4 to determine the inverse Fourier transform. Basically, the procedure consists of expanding the rational algebraic expression into a linear combination of lower order terms. For example, assuming no multiple-order poles, and assuming that the order of the denominator polynomial is greater than the order of the numerator polynomial, we can expand X(s) in the form m X(s) = L,A-· 1 -. (9.57) i=l s + ai From the ROC of X(s), the ROC of each of the individual terms in eq. (9.57) can be inferred, and then, from Examples 9.1 and 9 .2, the inverse Laplace transform of each of these terms can be determined. There are two possible choices for the inverse transform of each term AJ(s + ai) in the equation. If the ROC is to the right of the pole at s = - ai, then the inverse transform of this term is Aie-aJ u(t), a right-sided signal. If the ROC is to the left of the pole at s = -ai, then the inverse transform of the term is -Aie-aJu(-t), a left-sided signal. Adding the inverse transforms of the individual terms in eq. (9.57) then yields the inverse transform of X(s). The details of this procedure are best presented through a number of examples. Example 9.9 Let 1 X(s) (s + 1)(s + 2)' CRe{s} > -1. (9.58) = To obtain the inverse Laplace transform, we first perform a partial-fraction expansion to obtain 1 A B X(s) = (s + l)(s + 2) --+--. (9.59) s+l s+2 672 The Laplace Transform Chap.9 As discussed in the appendix, we can evaluate the coefficients A and B by multiplying both sides of eq. (9.59) by (s + 1)(s + 2) and then equating coefficients of equal powers of son both sides. Alternatively, we can use the relation A = [(s + l)X(s)lls= -I = 1, (9.60) B = [(s + 2)X(s)]l.l·= -2 = -1. (9.61) Thus, the partial-fraction expansion for X(s) is 1 1 X(s) = --1 - --2. (9.62) s + s + From Examples 9.1 and 9.2, we know that there are two possible inverse trans- forms for a transform of the form ll(s + a), depending on whether the ROC is to the left or the right of the pole. Consequently, we need to determine which ROC to associate with each of the individual first-order terms in eq. (9.62). This is done by reference to the properties of the ROC developed in Section 9.2. Since the ROC for X(s) is CRe{s} > -1, the ROC for the individual terms in the partial-fraction expansion of eq. (9.62) includes (Jl.e{s} > -1. The ROC for each term can then be extended to the left or right (or both) to be bounded by a pole or infinity. This is illustrated in Figure 9.14. Figure 9.14(a) shows the pole-zero plot and ROC for X(s), as specified in eq. (9.58). Figure 9.14(b) and 9.14(c) represent the individual terms in the partial-fraction expansion in eq. (9.62). The ROC for the sum is indicated with lighter shading. For the term represented by Figure 9.14(c), the ROC for the sum can be extended to the left as shown, so that it is bounded by a pole. I I s-plane I I I I -X-X--+----- -2 -1 I I I I I (a) 9m I I I s-plane I s-plane I I I I I I I ---x--~------ ->k---+---- -1 -2 I I I I I (b) (c) Figure 9.14 Construction of the ROCs for the individual terms in the partial-fraction expansion of X(s) in Example 9.8: (a) pole-zero plot and ROC for X(s); (b) pole at s = -1 and its ROC; (c) pole at s = -2 and its ROC. Sec. 9.3 The Inverse Laplace Transform 673 Since the ROC is to the right of both poles, the same is true for each of the indi- vidual terms, as can be seen in Figures 9.14(b) and (c). Consequently, from Property 8 in the preceding section, we know that each of these terms corresponds to a right-sided signal. The inverse transform of the individual terms in eq. (9.62) can then be obtained by reference to Example 9.1: £ 1 e-tu(t) ~ --1, CRe{s} > -1, (9.63) s+ £ 1 e- 21 u(t) ~ -- CR.e{s} > -2. (9.64) s + 2' We thus obtain -t -21 £ 1 [e - e ]u(t) ~ (s + 1)(s + 2)' (R.e{s} > -1. (9.65) Example 9.10 Let us now suppose that the algebraic expression for X(s) is again given by eq. (9.58), but that the ROC is now the left-half plane ffi-e{s} < -2. The partial-fraction expansion for X(s) relates only to the algebraic expression, so eq. (9.62) is still valid. With this new ROC, however, the ROC is to the left of both poles and thus, the same must be true for each of the two terms in the equation. That is, the ROC for the term corresponding to the pole at s = -1 is ffi-e{s} < -1, while the ROC for the term with pole at s = -2 is ffi-e{s} < -2. Then, from Example 9.2, I £ 1 -e- u( -t) ~ s + , ffi-e{s} < -1, (9.66) 1 21 £ 1 -e- u( -t) ~ s + , ffi-e{s} < -2, (9.67) 2 so that £ 1 x(t) = [ -e-1 + e- 21 ]u( -t) ~ (s + )(s + ), ffi-e{s} < -2. (9.68) 1 2 Example 9. 1 1 Finally, suppose that the ROC of X(s) in eq. (9.58) is -2 < ffi-e{s} < -1. In this case, the ROC is to the left of the pole at s = -1 so that this term corresponds to the left-sided signal in eq. (9.66), while the ROC is to the right of the pole at s = -2 so that this term corresponds to the right-sided signal in eq. (9.64). Combining these, we find that £ 1 x(t) = -e-1 u( -t)- e- 21 u(t) ~ (s + )(s + ), -2 < ffi-e{s} < -1. (9.69) 1 2 As discussed in the appendix, when X(s) has multiple-order poles, or when the de- nominator is not of higher degree than the numerator, the partial-fraction expansion of X(s) will include other terms in addition to the first-order terms considered in Examples 9.9- 9 .11. In Section 9.5, after discussing properties of the Laplace transform, we develop some other Laplace transform pairs that, in conjunction with the properties, allow us to extend the inverse transform method outlined in Example 9.9 to arbitrary rational transforms. 674 The Laplace Transform Chap.9 9.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT As we saw in Section 9.1, the Fourier transform of a signal is the Laplace transform evalu- ated on the jw-axis. In this section we discuss a procedure for geometrically evaluating the Fourier transform and, more generally, the Laplace transform at any set of values from the pole-zero pattern associated with a rational Laplace transform. To develop the procedure, let us first consider a Laplace transform with a single zero [i.e., X(s) = s - a], which we evaluate at a specific value of s, say, s = s1• The algebraic expression s 1 - a is the sum of two complex numbers, s1 and -a, each of which can be represented as a vector in the complex plane, as illustrated in Figure 9.15. The vector representing the complex number s1 -a is then the vector sum of s1 and -a, which we see in the figure to be a vector from the zero at s = a to the point s1• The value of X(s1) then has a magnitude that is the length of this vector and an angle that is the angle of the vector relative to the real axis. If X(s) instead has a single pole at s = a [i.e., X(s) = 1/(s- a)], then the denominator would be represented by the same vector sum of s1 and -a, and the value of X(st) would have a magnitude that is the reciprocal of the length of the vector from the pole to s = s 1 and an angle that is the negative of the angle of the vector with the real axis. s-plane a ffi-£ Figure 9.15 Complex plane rep- resentation of the vectors s1, a, and s1 - a representing the complex num- bers St, a, and St - a, respectively. A more general rational Laplace transform consists of a product of pole and zero terms of the form discussed in the preceding paragraph; that is, it can be factored into the form (9.70) To evaluate X(s) at s = s1, each term in the product is represented by a vector from the zero or pole to the point s1• The magnitude of X(st) is then the magnitude of the scale factor M, times the product of the lengths of the zero vectors (i.e., the vectors from the zeros to s1) divided by the product of the lengths of the pole vectors (i.e., the vectors from the poles to s1) . The angle of the complex number X (s1) is the sum of the angles of the zero vectors minus the sum of the angles of the pole vectors. If the scale factor Min eq. (9.70) is negative, an additional angle of 1T would be included. If X(s) has a multiple pole or zero"
9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot,"674 The Laplace Transform Chap.9 9.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT As we saw in Section 9.1, the Fourier transform of a signal is the Laplace transform evalu- ated on the jw-axis. In this section we discuss a procedure for geometrically evaluating the Fourier transform and, more generally, the Laplace transform at any set of values from the pole-zero pattern associated with a rational Laplace transform. To develop the procedure, let us first consider a Laplace transform with a single zero [i.e., X(s) = s - a], which we evaluate at a specific value of s, say, s = s1• The algebraic expression s 1 - a is the sum of two complex numbers, s1 and -a, each of which can be represented as a vector in the complex plane, as illustrated in Figure 9.15. The vector representing the complex number s1 -a is then the vector sum of s1 and -a, which we see in the figure to be a vector from the zero at s = a to the point s1• The value of X(s1) then has a magnitude that is the length of this vector and an angle that is the angle of the vector relative to the real axis. If X(s) instead has a single pole at s = a [i.e., X(s) = 1/(s- a)], then the denominator would be represented by the same vector sum of s1 and -a, and the value of X(st) would have a magnitude that is the reciprocal of the length of the vector from the pole to s = s 1 and an angle that is the negative of the angle of the vector with the real axis. s-plane a ffi-£ Figure 9.15 Complex plane rep- resentation of the vectors s1, a, and s1 - a representing the complex num- bers St, a, and St - a, respectively. A more general rational Laplace transform consists of a product of pole and zero terms of the form discussed in the preceding paragraph; that is, it can be factored into the form (9.70) To evaluate X(s) at s = s1, each term in the product is represented by a vector from the zero or pole to the point s1• The magnitude of X(st) is then the magnitude of the scale factor M, times the product of the lengths of the zero vectors (i.e., the vectors from the zeros to s1) divided by the product of the lengths of the pole vectors (i.e., the vectors from the poles to s1) . The angle of the complex number X (s1) is the sum of the angles of the zero vectors minus the sum of the angles of the pole vectors. If the scale factor Min eq. (9.70) is negative, an additional angle of 1T would be included. If X(s) has a multiple pole or zero Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 675 (or both), corresponding to some of the a /s being equal to each other or some of the {3/s being equal to each other (or both), the lengths and angles of the vectors from each of these poles or zeros must be included a number of times equal to the order of the pole or zero. Example 9.12 Let 1 1 X(s) = --, ffi-e{s} > - s+! 2. (9.71) 2 The Fourier transform is X(s)ls= jw. For this example, then, the Fourier transform is X(jw) = jw ~ 112 (9.72) The pole-zero plot for X(s) is shown in Figure 9.16. To determine the Fourier transform graphically, we construct the pole vector as indicated. The magnitude of the Fourier transform at frequency w is the reciprocal of the length of the vector from the pole to the point jw on the imaginary axis. The phase of the Fourier transform is the negative of the angle of the vector. Geometrically, from Figure 9.16, we can write (9.73) and <f:X(jw) = -tan -I 2w. (9.74) s-plane w jw +j_ 2 Figure 9.16 Pole-zero plot for Example 9.12. IX(jw)l is the reciprocal of the length of the vector shown, and <r:X(jw) is the negative of the angle of the vector. Often, part of the value of the geometric determination of the Fourier transform lies in its usefulness in obtaining an approximate view of the overall characteristics of the transform. For example, in Figure 9 .16, it is readily evident that the length of the pole vector monotonically increases with increasing w, and thus, the magnitude of the Fourier 676 The Laplace Transform Chap.9 transform will monotonically decrease with increasing w. The ability to draw general con- elusions about the behavior of the Fourier transform from the pole-zero plot is further il- lustrated by a consideration of general first- and second-order systems. 9 .4. 1 First-Order Systems As a generalization of Example 9.12, let us consider the class of first-order systems that was discussed in some detail in Section 6.5.1. The impulse response for such a system is h(t) = -1e - tiT u(t), (9.75) T and its Laplace transform is = -- > --1 H(s) ffi-e{s} . (9.76) ST + 1' T The pole-zero plot is shown in Figure 9.17. Note from the figure that the length of the pole vector is minimal for w = 0 and increases monotonically as w increases. Also, the angle of the pole in~reases monotonically from 0 to 7r/2 as w increases from 0 to oo. s-plane w _1 T Figure 9.17 Pole-zero plot for first- order system of eq. (9.76). From the behavior of the pole vector as w varies, it is clear that the magnitude of the frequency response H(jw) monotonically decreases as w increases, while <t.H(jw) monotonically decreases from 0 to -'TT'/2, as shown in the Bode plots for this system in Figure 9.18. Note also that when w = liT, the real and imaginary parts of the pole vector are equal, yielding a value of the magnitude of the frequency response that is reduced by a factor of J2, or approximately 3 dB, from its maximum at w = 0 and a value of 7r/4 for the angle of the frequency response. This is consistent with our examination of first-order systems in Section 6.5.1, where we noted that w = liT is often referred to as the 3-dB point or the break frequency-i.e., the frequency at which the straight-line approximation of the Bode plot of IH (jw )I has a break in its slope. As we also saw in Section 6.5 .1, the time constant T controls the speed of response of first-order systems, and we now see that Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 677 20 3 dB _i 0 dB .... Asymptotic ]: ~approximation I 0 -20 ci) .2 0 C\J -40 -60 0.1/T 1/T 1 0/T 100/T w 7T/4 0 3 :C -7T/4 >.f -7T/2 -37T/4 0.1/T 1/T 10/T 100/T Figure 9.18 Frequency response w for a first-order system. the pole of such a system at s = -liT is on the negative real axis, at a distance to the origin that is the reciprocal of the time constant. From our graphical interpretation, we can also see how changing the time constant or, equivalently, the position of the pole of H(s) changes the characteristics of a first-order system. In particular, as the pole moves farther into the left-halfplane, the break frequency and, hence, the effective cutoff frequency of the system increases. Also, from eq. (9.75) and from Figure 6.19, we see that this same movement of the pole to the left corresponds to a decrease in the time constant T, resulting in a faster decay of the impulse response and a correspondingly faster rise time in the step response. This relationship between the real part of the pole locations and the speed of the system response holds more generally; that is, poles farther away from the jw-axis are associated with faster response terms in the impulse response. 9.4.2 Second-Order Systems Let us next consider the class of second-order systems, which was discussed in some detail in Section 6.5.2. The impulse response and frequency response for the system, originally 678 The Laplace Transform Chap.9 given in eqs. (6.37) and (6.33), respectively, are h(t) = M[ec 1t - ec2']u(t), (9.77) where Ct = -{wn + Wn~, c2 = -{wn-Wn~' M = __w _n_ _ 2~' and w~ (9.78) H(jw) = (jw)2 + 2{wn(jw) + wr The Laplace transform of the impulse response is (9.79) For { > 1, c1 and c2 are real and thus both poles lie on the real axis, as indicated in Figure 9.19(a). The case of { > 1 is essentially a product of two first-order terms, as in Section 9.4.1. Consequently, in this case iH(jw)i decreases monotonically as lwl in- creases, while <r:H(jw) varies from 0 at w = 0 to -7r as w ~ oo. This can be verified from Figure 9.19(a) by observing that the length of the vector from each of the two poles to the points = jw increases monotonically as w increases from 0, and the angle of each of these vectors increases from 0 to 7T/2 as w increases from 0 to oo. Note also that as { increases, one pole moves closer to the jw-axis, indicative of a term in the impulse re- sponse that decays more slowly, and the other pole moves farther into the left-half plane, indicative of a term in the impulse response that decays more rapidly. Thus, for large val- ues of{, it is the pole close to the jw-axis that dominates the system response for large time. Similarly, from a consideration of the pole vectors for { >> 1, as indicated in Fig- ure 9.19(b), for low frequencies the length and angle of the vector for the pole close to the jw-axis are much more sensitive to changes in w than the length and angle of the vector for the pole far from the jw-axis. Hence, we see that for low frequencies, the char- acteristics of the frequency response are influenced principally by the pole close to the jw-axis. For 0 < { < 1, c1 and c2 are complex, so that the pole-zero plot is that shown in Figure 9.19(c). Correspondingly, the impulse response and step response have oscilla- tory parts. We note that the two poles occur in complex conjugate locations. In fact, as we discuss in Section 9.5.5, the complex poles (and zeros) for a real-valued signal al- ways occur in complex conjugate pairs. From the figure-partiJularly when { is small, so that the poles are close to the jw-axis-as w approaches Wn 1 - {2 , the behavior of the frequency response is dominated by the pole vector in the second quadrant, and in Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 679 gm gm s-plane s-plane (a) (b) gm gm s-plane cos e = ~ X (c) (d) Figure 9.19 (a) Pole-zero plot for a second-order system with ( > 1; (b) pole vec- tors for ( >> 1; (c) pole-zero plot for a second-order system with 0 < ( < 1; (d) pole vectors for 0 < ( < 1 and for w = wn~ and w = wn~ :±: (wn. 680 The Laplace Transform Chap.9 particular, the length of that pole vector has a minimum at w = wn~· Thus, qual- itatively, we would expect the magnitude of the frequency response to exhibit a peak in the vicinity of that frequency. Because of the presence of the other pole, the peak will occur not exactly at w = Wn~, but at a frequency slightly less than this. A careful sketch of the magnitude of the frequency response is shown in Figure 9.20(a) for w n = 1 and several values of ( where the expected behavior in the vicinity of the poles is clearly evident. This is consistent with our analysis of second-order systems in Section 6.5.2. JH(jw)l -1 w (a) <l:H(jw) w (b) Figure 9.20 (a) Magnitude and (b) phase of the frequency response for a second-order system with 0 < ~ < 1. Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 681 Thus, for 0 < ( < 1, the second-order system is a nonideal bandpass filter, with the parameter ( controlling the sharpness and width of the peak in the frequency response. In particular, from the geometry in Figure 9 .19( d), we see that the length of the pole vector from the second-quadrant pole increases by a factor of j2 from its minimum at w = w n ~when w increases or decreases from this value by ( w n. Consequently, for small (,and neglecting the effect of the distant third-quadrant pole, IH(jw )I is within a factor of J2 of its peak value over the frequency range Wn~- (wn < W < Wn~ + (wn. If we define the relative bandwidth Bas the length of this frequency interval divided by the undamped natural frequency w n, we see that B = 2(. Thus, the closer ( is to zero, the sharper and narrower the peak in the frequency response is. Note also that B is the reciprocal of the quality measure Q for second-order systems defined in Section 6.5.2. Thus, as the quality increases, the relative bandwidth decreases and the filter becomes increasingly frequency selective. An analogous picture can be developed for <r:H (w ), which is plotted in Figure 9 .20(b) for wn = 1 and several values of(. As can be seen from Figure 9.19(d), the angle of the second-quadrant pole vector changes from -11'14 to 0 to 11'14 as w changes from Wn~- (wn toWn~ toWn~+ (wn. For small values of(, the angle for the third-quadrant pole changes very little over this frequency interval, resulting in a rapid change in <r:H(jw) of approximately 71'/2 over the interval, as captured in the figure. Varying wn with ( fixed only changes the frequency scale in the preceding discussion-i.e., IH(w)l and <r:H(w) depend only on wlwn. From Figure 9.19(c), we also can readily determine how the poles and system characteristics change as we vary (, keeping w n constant. Since cos (} = (, the poles move along a semicircle with fixed radius w n. For ( = 0, the two poles are on the imaginary axis. Correspondingly, in the time domain, the impulse response is sinusoidal with no damping. As ( increases from 0 to 1, the two poles remain complex and move into the left-half plane, and the vectors from the origin to the poles maintain a constant overall magnitude w n. As the real part of the poles becomes more negative, the associated time response will decay more quickly as t ~ oo. Also, as we have seen, as (increases from 0 toward 1, the relative bandwidth of the frequency response increases, and the frequency response becomes less sharp and less frequency selective. 9.4.3 All-Pass Systems As a final illustration of the geometric evaluation of the frequency response, let us con- sider a system for which the Laplace transform of the impulse response has the pole-zero plot shown in Figure 9.21(a). From this figure, it is evident that for any point along the jw-axis, the pole and zero vectors have equal length, and consequently, the magnitude of the frequency response is constant and independent of frequency. Such a system is com- 682 The Laplace Transform Chap.9 s-plane w -a a (a) <tH(jw) IH(jw)l w (b) Figure 9.21 (a) Pole-zero plot for an all-pass system; (b) magnitude and phase of an all-pass frequency response. monly referred to as an all-pass system, since it passes all frequencies with equal gain (or attenuation). The phase of the frequency response is 8 I - 82, or, since 8 I = 7T - 82, 1:H(jw) = 7T- 282. (9.80) From Figure 9.21(a), 82 = tan- 1( wla), and thus, -1-H(jw) 1 = 7r- 2tan- (~). (9.81) The magnitude and phase of H(jw) are illustrated in Figure 9.2l(b). 9.5 PROPERTIES OF THE LAPLACE TRANSFORM In exploiting the Fourier transform, we relied heavily on the set of properties developed in Section 4.3. In the current section, we consider the corresponding set of properties for"
9.5 Properties of the Laplace Transform,"682 The Laplace Transform Chap.9 s-plane w -a a (a) <tH(jw) IH(jw)l w (b) Figure 9.21 (a) Pole-zero plot for an all-pass system; (b) magnitude and phase of an all-pass frequency response. monly referred to as an all-pass system, since it passes all frequencies with equal gain (or attenuation). The phase of the frequency response is 8 I - 82, or, since 8 I = 7T - 82, 1:H(jw) = 7T- 282. (9.80) From Figure 9.21(a), 82 = tan- 1( wla), and thus, -1-H(jw) 1 = 7r- 2tan- (~). (9.81) The magnitude and phase of H(jw) are illustrated in Figure 9.2l(b). 9.5 PROPERTIES OF THE LAPLACE TRANSFORM In exploiting the Fourier transform, we relied heavily on the set of properties developed in Section 4.3. In the current section, we consider the corresponding set of properties for Sec. 9.5 Properties of the Laplace Transform 683 the Laplace transform. The derivations of many of these results are analogous to those of the corresponding properties for the Fourier transform. Consequently, we will not present the derivations in detail, some of which are left as exercises at the end of the chapter. (See Problems 9.52-9.54.) 9.5.1 Linearity of the laplace Transform If with a region of convergence that will be denoted as R1 and x (s) with a region of convergence that 2 will be denoted as R2, then £ ax1 (t) + bx2(t) ~ aX1 (s) + bX2(s), with ROC (9.82) containing R 1 n R 2. As indicated, the region of convergence of X(s) is at least the intersection of R1 and R2, which could be empty, in which case X(s) has no region of convergence-i.e., x(t) has no Laplace transform. For example, for x(t) as in eq. (9.47) of Example 9.7, with b > 0 the ROC for X(s) is the intersection of the ROCs for the two terms in the sum. If b < 0, there are no common points in R1 and R2 ; that is, the intersection is empty, and thus, x(t) has no Laplace transform. The ROC can also be larger than the intersection. As a simple example, for x1 (t) = x2(t) and a = -bin eq. (9.82), x(t) = 0, and thus, X(s) = 0. The ROC of X(s) is then the entire s-plane. The ROC associated with a linear combination of terms can always be constructed by using the properties of the ROC developed in Section 9 .2. Specifically, from the inter- section of the ROCs for the individual terms (assuming that it is not empty), we can find a line or strip that is in the ROC of the linear combination. We then extend this to the right ((Re{s} increasing) and to the left (ffi-e{s} decreasing) to the nearest poles (which may be at infinity). Example 9. 1 3 In this example, we illustrate the fact that the ROC for the Laplace transform of a linear combination of signals can sometimes extend beyond the intersection of the ROCs for the individual terms. Consider X(t) = Xt (t) - X2(t), (9.83) 684 The Laplace Transform Chap.9 where the Laplace transforms of x 1 (t) and x 2(t) are, respectively, 1 X,(s) = s + 1' CRe{s} > -1, (9.84) and 1 X2(s) (s + 1)(s + 2)' CR..e{s} > -1. (9.85) = The pole-zero plot, including the ROCs for X1( s) and X2(s), is shown in Figures 9.22(a) and (b). From eq. (9.82), 1 1 s + 1 X(s) = -s- +-1 - -(s_+_1_)-(s_+_2_) (9.86) (s + l)(s + 2) - s + 2 · Thus, in the linear combination of x 1( t) and x 2(t), the pole at s = -1 is canceled by a zero at s = -1. The pole-zero plot for X(s) = X1( s)- X2(s) is shown in Figure 9.22(c). The intersection of the ROCs for X1( s) and X2(s) is ffi-e{s} > -1. However, since the ROC is always bounded by a pole or infinity, for this example the ROC for X(s) can be extended to the left to be bounded by the pole at s = -2, as a result of the pole-zero cancellation at s = - 1. !1m !1m !1m I I I I s-plane I s-plane I s-plane I I I I R1 I f\ I R I I I I I I -~ -X-X ffi-e -2 -1 ffi-e -~ I I I ffi-e I I I I I I I I I I I I (a) (b) (c) Figure 9.22 Pole-zero plots and ROCs for Example 9.13: (a) X1(s); (b) X2(s); (c) X1 (s) - X2(s). The ROC for X1 (s) - X2(s) includes the inter- section of R1 and R2, which can then be extended to be bounded by the pole at s = -2. 9.5.2 Time Shifting If .c x(t) ~ X(s), withROC = R, then .c x(t- to) ~ e-stox(s), with ROC = R. (9.87) Sec. 9.5 Properties of the Laplace Transform 685 9.5.3 Shifting in the s-Domain If £ x(t) ~ X(s), with ROC= R, then £ esot x(t) ~ X(s - s0), with ROC = R + CRe{so}. (9.88) That is, the ROC associated with X(s- s0) is that of X(s), shifted by CRe{s0}. Thus, for any values that is in R, the values + CR.e{s0} will be in R1. This is illustrated in Figure 9.23. Note that if X(s) has a pole or zero at s = a, thenX(s-s0) has a pole or zero ats-s0 = a- i.e., s = a + so. An important special case of eq. (9.88) is when s0 = jw0-i.e., when a signal x(t) is used to modulate a periodic complex exponential ejwot. In this case, eq. (9.88) becomes . £ eJwot x(t) ~ X(s - jw0 ), with ROC = R. (9.89) The right-hand side of eq. (9.89) can be interpreted as a shift in the s-plane parallel to the jw-axis. That is, if the Laplace transform of x(t) has a pole or zero at s = a, then the Laplace transform of ejwot x(t) has a pole or zero at s = a+ jw0 . 9.5.4 Time Scaling If £ x(t) ~ X(s), withROC = R, l I 1 s-plane I s-plane Rl I I I I I l l r1 I I I I r2 + CR-e (so) I l I I I I I 1 I (a) (b) Figure 9.23 Effect on the ROC of shifting in the s-domain: (a) the ROC of X(s); (b) the ROC of X(s- s0). 686 The Laplace Transform Chap.9 then x(at) ....:.._. !x(n with ROC R1 ~ aR. (9.90) 1 1 That is, for any value s in R [which is illustrated in Figure 9.24(a)], the value a/s will be in Rt. as illustrated in Figure 9.24(b) for a positive value of a < 1. Note that, for 0 <a< 1, there is a compression in the size of the ROC of X(s) by a factor of a, as depicted in Figure 9.24(b), while for a> 1, the ROC is expanded by a factor of a. Also, eq. (9.90) implies that if a is negative, the ROC undergoes a reversal plus a scaling. In particular, as depicted in Figure 9.24(c), the ROC of lliaiX(sla) for 0 >a > -1 involves s-plane s-plane R .!1 ~ a a (a) (b) 9m s-plane ~ ~ a a Figure 9.24 Effect on the ROC of time scaling: (a) ROC of X(s); (b) ROC of (1/lai}X(s/a) for 0 <a< 1; (c) ROC of (c) (1/lai}X(s/a) for 0 >a> -1. Sec. 9.5 Properties of the Laplace Transform 687 a reversal about the jw-axis, together with a change in the size of the ROC by a factor of lal· Thus, time reversal of x(t) results in a reversal of the ROC. That is, £ x(-t) ~ X(-s), with ROC= -R. (9.91) 9.5.5 Conjugation If £ x(t) ~ X(s), with ROC = R, (9.92) then x *( t) ~£ X *( s *) , with ROC = R. (9.93) Therefore, X(s) = X*(s*) when x(t) is real. (9.94) Consequently, if x(t) is real and if X(s) has a pole or zero at s = s0 (i.e., if X(s) is un- bounded or zero at s = s0), then X(s) also has a pole or zero at the complex conjugate points = s~. For example, the transform X(s) for the real signal x(t) in Example 9.4 has poles at s = 1 ± 3j and zeros at s = ( -5 ± .ifil)/2. 9. 5. 6 Convolution Property If with ROC = R1, and with ROC = R2, then (9.95) In a manner similar to the linearity property set forth in Section 9.5.1, the ROC of X1( s)X2(s) includes the intersection of the ROCs of X1( s) and X2(s) and may be larger if pole-zero cancellation occurs in the product. For example, if s + 1 XJ(S) = s + 2' CR-t:?{s} > -2, (9.96) 688 The Laplace Transform Chap. 9 and s+2 X2(s) = s + , CRe{s} > -1, (9.97) 1 then X1 (s)X2(s) = 1, and its ROC is the entire s-plane. As we saw in Chapter 4, the convolution property in the context of the Fourier transform plays an important role in the analysis of linear time-invariant systems. In Sec- tions 9.7 and 9.8 we will exploit in some detail the convolution property for Laplace trans- forms for the analysis of LTI systems in general and, more specifically, for the class of systems represented by linear constant-coefficient differential equations. 9.5.7 Differentiation in the Time Domain If £ x(t) ~ X(s), with ROC= R, then dx(t) £ ~ sX(s), with ROC containing R. (9.98) dt This property follows by differentiating both sides of the inverse Laplace transform as expressed in equation (9.56). Specifically, let 1 f<T+ jx x(t) = -. X(s)est ds. 21T 1 (T- joo Then dx(t) 1 J<T+joo -- - - . sX(s)e5tds. (9.99) dt 2 1T 1 (T- joo Consequently, dx(t)ldt is the inverse Laplace transform of sX(s). The ROC of sX(s) in- cludes the ROC of X(s) and may be larger if X(s) has a first-order pole at s = 0 that is canceled by the multiplication by s. For example, if x(t) = u(t), then X(s) = 1/s, with an ROC that is CRe{s} > 0. The derivative of x(t) is an impulse with an associated Laplace transform that is unity and an ROC that is the entire s-plane. 9.5.8 Differentiation in the s-Domain Differentiating both sides of the Laplace transform equation (9.3), i.e., X(s) ~ r~x x(t)e-""dt, we obtain dX(s) +oo -oo (-t)x(t)e-stdt. ds r Sec. 9.5 Properties of the Laplace Transform 689 Consequently, if £ x(t) ~ X(s), with ROC= R, then £ dX(s) -tx(t) ~ withROC = R. (9.100) ds ' The next two examples illustrate the use of this property. Example 9.14 Let us find the Laplace transform of x(t) = te-at u(t). (9.101) Since CRe{s} > -a, s +a' it follows from eq. (9.100) that te-atu(t) ~ _!!__ [-1-] = __1 _, CRe{s} >-a. (9.102) . ds s +a (s + a)2 In fact, by repeated application of eq. (9.100), we obtain t2 £ 1 -e-at u(t) ~--- CRe{s} >-a, (9.103) 2 (s+a)3' and, more generally, tn-I (n- 1)! e-at u(t) ffi-e{s} > -a. (9.104) (s+a)n' As the next example illustrates, this specific Laplace transform pair is particularly use- ful when applying partial-fraction expansion to the determination of the inverse Laplace transform of a rational function with multiple-order poles. Example 9. 1 5 Consider the Laplace transform 2s2 + 5s + 5 X(s) = (s + 1)2(s + 2)' ffi-e{s} > -1. Applying the partial-fraction expansion method described in the appendix, we can write 2 1 3 X(s) = (s + 1)2 - (s + 1) + s + 2' ffi-e{s} > -1. (9.105) 690 The Laplace Transform Chap.9 Since the ROC is to the right of the poles at s = -1 and -2, the inverse transform of each of the terms is a right-sided signal, and, applying eqs. (9.14) and (9.104), we obtain the inverse transform 9.5.9 Integration in the Time Domain If £ x(t) ~ X(s), with ROC= R, then L 1 x (T)dT -X(s), with ROC containing (9.106) s R n {(Re{s} > 0}. This property is the inverse of the differentiation property set forth in Section 9.5. 7. It can be derived using the convolution property presented in Section 9.5.6. Specifically, fx x(T )dT = u(t) * x(t). (9.107) From Example 9.1, with a = 0, £ u(t) ~­ ffi-e{s} > 0, (9.108) s' and thus, from the convolution property, £ * 1 u(t) x(t) ~ -X(s), (9.109) s with an ROC that contains the intersection of the ROC of X(s) and the ROC of the Laplace transform of u(t) in eq. (9.108), which results in the ROC given in eq. (9.106). 9.5.1 0 The Initial- and Final-Value Theorems Under the specific constraints that x(t) = 0 for t < 0 and that x(t) contains no impulses or higher order singularities at the origin, one can directly calculate, from the Laplace transform, the initial value x(O+)-i.e., x(t) as t approaches zero from positive values of t. Specifically the initial-value theorem states that x(O+) = lim sX(s), (9.110) s~x Also, if x(t) = 0 fort< 0 and, in addition, x(t) has a finite limit as t ~ x, then the final- value theorem says that lim x(t) = lim sX(s). (9.111) t---+x s---->0 The derivation of these results is considered in Problem 9.53. Sec. 9.5 Properties of the Laplace Transform 691 Example 9.16 The initial- and final-value theorems can be useful in checking the correctness of the Laplace transform calculations for a signal. For example, consider the signal x(t) in Example 9.4. From eq. (9.24), we see that x(O+) = 2. Also, using eq. (9.29), we find that . = . 2s3 + 5s2 + 12s hm sX(s) hm = 2, 3 2 S->OC S-->00 S + 4 S + 14 S + 20 which is consistent with the initial-value theorem in eq. (9.110). 9.5.11 Table of Properties In Table 9.1, we summarize the properties developed in this section. In Section 9. 7, many of these properties are used in applying the Laplace transform to the analysis and characterization of linear time-invariant systems. As we have illustrated in several exam- ples, the various properties of Laplace transforms and their ROCs can provide us with TABLE 9.1 PROPERTIES OF THE LAPLACE TRANSFORM Laplace Section Property Signal Transform ROC x(t) X(s) R x, (t) X1(s) R, x2(t) X2(s) R2 ---------- ----------- ------------------ 9.5.1 Linearity ax1 (t) + bx2(t) aX1 (s) + bX2(s) At least R1 n R2 9.5.2 Time shifting x(t- to) e-sto X(s) R 9.5.3 Shifting in the s-Domain esot x(t) X(s- s0 ) Shifted version of R (i.e., s is in the ROC if s - s0 is in R) 9.5.4 Time scaling x(at) Ia1! x(sa) Scaled ROC (i.e., s is in the ROC if s/a is in R) 9.5.5 Conjugation x*(t) X*(s*) R 9.5.6 Convolution x 1( t) * x2(t) X1(s)X2(s) At least R1 n R2 d 9.5. 7 Differentiation in the dix(t) sX(s) At least R Time Domain d 9.5.8 Differentiation in the -tx(t) dsX(s) R s-Domain 1 9.5.9 Integration in the Time foo X(T)d(T) -X(s) At least R n {<Re{s} > 0} Domain s Initial- and Final-Value Theorems 9.5.1 0 If x(t) = 0 for t < 0 and x(t) contains no impulses or higher-order singularities at t = 0, then x(O+) = lim sX(s) s~oo If x(t) = 0 fort< 0 and x(t) has a finite limit as t -7 oc, then lim x(t) = lim sX(s) 1-+-:xo S--t-:xo 692 The Laplace Transform Chap.9 considerable information about a signal and its transform that can be useful either in char- acterizing the signal or in checking a calculation. In Sections 9.7 and 9.8 and in some of the problems at the end of this chapter, we give several other examples of the uses of these properties. 9.6 SOME LAPLACE TRANSFORM PAIRS As we indicated in Section 9.3, the inverse Laplace transform can often be easily evaluated by decomposing X (s) into a linear combination of simpler terms, the inverse transform of each of which can be recognized. Listed in Table 9.2 are a number of useful Laplace TABLE 9.2 LAPLACE TRANSFORMS OF ELEMENTARY FUNCTIONS Transform pair Signal Transform ROC 1 8(t) 1 Ails 1 2 u(t) - ffi-c{s} > 0 s 1 3 -u( -t) - ffi-c{s} < 0 s rn-1 1 4 (n- 1)! u(t) - ffi-c{s} > 0 sn fn-1 1 5 - (n- 1)! u(-t) - ffi-c{s} < 0 sn 6 e-at u(t) 1 -- ffi-c{s} > -a s+a 7 -e-a1 1 u(-t) -- ffi-c{s} < -a s+a fn-1 1 8 (n- 1)! e-atu(t) -+- - ffi-c{s} > -a (s a)n fn-1 9 - (n _ )! e -at u ( -t) 1 --- ffi-c{s} < -a 1 (s + a)n 10 8(t- T) e-sT Ails s 11 [cosw0 t]u(t) -2 - ffi-c{s} > 0 s + w5 12 Wo [sin w0 t]u(t) ffi-c{s} > 0 s2 + w5 s+a 13 [e-at coswot]u(t) 2 ffi-c{s} > -a (s+a) +w5 14 [e-at sinwot]u(t) wo 2 ffi-c{s} > -a (s+a) +w5 15 u (t) = dno(t) sn n dtn All s 1 16 U-n(t) = u(t) * · · · * u(t) - ffi-c{s} > 0 '--.r----1 sn n times"
9.6 Some Laplace Transform Pairs,"692 The Laplace Transform Chap.9 considerable information about a signal and its transform that can be useful either in char- acterizing the signal or in checking a calculation. In Sections 9.7 and 9.8 and in some of the problems at the end of this chapter, we give several other examples of the uses of these properties. 9.6 SOME LAPLACE TRANSFORM PAIRS As we indicated in Section 9.3, the inverse Laplace transform can often be easily evaluated by decomposing X (s) into a linear combination of simpler terms, the inverse transform of each of which can be recognized. Listed in Table 9.2 are a number of useful Laplace TABLE 9.2 LAPLACE TRANSFORMS OF ELEMENTARY FUNCTIONS Transform pair Signal Transform ROC 1 8(t) 1 Ails 1 2 u(t) - ffi-c{s} > 0 s 1 3 -u( -t) - ffi-c{s} < 0 s rn-1 1 4 (n- 1)! u(t) - ffi-c{s} > 0 sn fn-1 1 5 - (n- 1)! u(-t) - ffi-c{s} < 0 sn 6 e-at u(t) 1 -- ffi-c{s} > -a s+a 7 -e-a1 1 u(-t) -- ffi-c{s} < -a s+a fn-1 1 8 (n- 1)! e-atu(t) -+- - ffi-c{s} > -a (s a)n fn-1 9 - (n _ )! e -at u ( -t) 1 --- ffi-c{s} < -a 1 (s + a)n 10 8(t- T) e-sT Ails s 11 [cosw0 t]u(t) -2 - ffi-c{s} > 0 s + w5 12 Wo [sin w0 t]u(t) ffi-c{s} > 0 s2 + w5 s+a 13 [e-at coswot]u(t) 2 ffi-c{s} > -a (s+a) +w5 14 [e-at sinwot]u(t) wo 2 ffi-c{s} > -a (s+a) +w5 15 u (t) = dno(t) sn n dtn All s 1 16 U-n(t) = u(t) * · · · * u(t) - ffi-c{s} > 0 '--.r----1 sn n times Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 693 transform pairs. Transform pair 1 follows directly from eq. (9.3). Transform pairs 2 and 6 follow directly from Example 9.1 with a = 0 and a = a, respectively. Transform pair 4 was developed in Example 9.14 using the differentiation property. Transform pair 8 follows from transform pair 4 using the property set forth in Section 9.5.3. Transform pairs 3, 5, 7, and 9 are based on transform pairs 2, 4, 6 and 8, respectively, together with the time- scaling property of section 9.5.4 with a = -1. Similarly, transform pairs 10 through 16 can all be obtained from earlier ones in the table using appropriate properties in Table 9.1 (see Problem 9.55). 9.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING THE LAPLACE TRANSFORM One of the important applications of the Laplace transform is in the analysis and character- ization of LTI systems. Its role for this class of systems stems directly from the convolution property (Section 9.5.6). Specifically, the Laplace transforms of the input and output of an LTI system are related through multiplication by the Laplace transform of the impulse response of the system. Thus, Y(s) = H(s )X(s ). (9.112) where X(s), Y(s), and H(s) are the Laplace transforms of the input, output, and impulse response of the system, respectively. Equation (9 .112) is the counterpart, in the context of Laplace transforms, of eq. (4.56) for Fourier transform. Also, from our discussion in Section 3.2 on the response of LTI systems to complex exponentials, if the input to an LTI system is x(t) = es1 , with sin the ROC of H(s), then the output will be H(s)est; i.e., est is an eigenfunction of the system with eigenvalue equal to the Laplace transform of the impulse response. If the ROC of H(s) includes the imaginary axis, then for s = jw, H(s) is the frequency response of the LTI system. In the broader context of the Laplace transform, H(s) is commonly referred to as the system function or, alternatively, the transfer function. Many properties of LTI systems can be closely associated with the characteristics of the system function in the s-plane. We illustrate this next by examining several important properties and classes of systems. 9. 7. 1 Causality For a causal LTI system, the impulse response is zero for t < 0 and thus is right sided. Consequently, from the discussion in Section 9.2, we see that The ROC associated with the system function for a causal system is a right-half plane. It should be stressed, however, that the converse of this statement is not necessarily true. That is, as illustrated in Example 9.19 to follow, an ROC to the right of the rightmost"
9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform,"Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 693 transform pairs. Transform pair 1 follows directly from eq. (9.3). Transform pairs 2 and 6 follow directly from Example 9.1 with a = 0 and a = a, respectively. Transform pair 4 was developed in Example 9.14 using the differentiation property. Transform pair 8 follows from transform pair 4 using the property set forth in Section 9.5.3. Transform pairs 3, 5, 7, and 9 are based on transform pairs 2, 4, 6 and 8, respectively, together with the time- scaling property of section 9.5.4 with a = -1. Similarly, transform pairs 10 through 16 can all be obtained from earlier ones in the table using appropriate properties in Table 9.1 (see Problem 9.55). 9.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING THE LAPLACE TRANSFORM One of the important applications of the Laplace transform is in the analysis and character- ization of LTI systems. Its role for this class of systems stems directly from the convolution property (Section 9.5.6). Specifically, the Laplace transforms of the input and output of an LTI system are related through multiplication by the Laplace transform of the impulse response of the system. Thus, Y(s) = H(s )X(s ). (9.112) where X(s), Y(s), and H(s) are the Laplace transforms of the input, output, and impulse response of the system, respectively. Equation (9 .112) is the counterpart, in the context of Laplace transforms, of eq. (4.56) for Fourier transform. Also, from our discussion in Section 3.2 on the response of LTI systems to complex exponentials, if the input to an LTI system is x(t) = es1 , with sin the ROC of H(s), then the output will be H(s)est; i.e., est is an eigenfunction of the system with eigenvalue equal to the Laplace transform of the impulse response. If the ROC of H(s) includes the imaginary axis, then for s = jw, H(s) is the frequency response of the LTI system. In the broader context of the Laplace transform, H(s) is commonly referred to as the system function or, alternatively, the transfer function. Many properties of LTI systems can be closely associated with the characteristics of the system function in the s-plane. We illustrate this next by examining several important properties and classes of systems. 9. 7. 1 Causality For a causal LTI system, the impulse response is zero for t < 0 and thus is right sided. Consequently, from the discussion in Section 9.2, we see that The ROC associated with the system function for a causal system is a right-half plane. It should be stressed, however, that the converse of this statement is not necessarily true. That is, as illustrated in Example 9.19 to follow, an ROC to the right of the rightmost 694 The Laplace Transform Chap.9 pole does not guarantee that a system is causal; rather, it guarantees only that the impulse response is right sided. However, if H (s) is rational, then, as illustrated in Examples 9.17 and 9.18 to follow, we can determine whether the system is causal simply by checking to see if its ROC is a right-half plane. Specifically, For a system with a rational system function, causality of the system is equivalent to the ROC being the right-half plane to the right of the rightmost pole. Example 9. 1 7 Consider a system with impulse response h(t) = e-tu(t). (9.113) Since h(t) = 0 fort < 0, this system is causal. Also, the system function can be obtained from Example 9.1: H(s) = s + 1' CRe{s} > -1. (9.114) In this case, the system function is rational and the ROC in eq. (9 .114) is to the right of the rightmost pole, consistent with our statement that causality for systems with rational system functions is equivalent to the ROC being to the right of the rightmost pole. Example 9. 1 8 Consider a system with impulse response Since h(t) =I= 0 fort < 0, this system is not causal. Also, from Example 9.7, the system function is -2 H(s) = s _ , -1 < CRe{s} < +1. 2 1 Thus, H(s) is rational and has an ROC that is not to the right of the the rightmost pole, consistent with the fact that the system is not causal. Example 9.19 Consider the system function es H(s) = s + 1' CRe{s} > -1. (9.115) For this system, the ROC is to the right of the rightmost pole. Therefore, the impulse response must be right sided. To determine the impulse response, we first use the result Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 695 of Example 9.1: e-r u(t) CRe{s} > -1. (9.116) s + 1' Next, from the time-shifting property of Section 9.5.2 [eq. (9.87)], the factor e' in eq. (9.115) can be accounted for by a shift in the time function in eq. (9.116). Then CRe{s} > -1, (9.117) so that the impulse response associated with the system is h(t) = e-u+ 1 )u(t + 1), (9.118) which is nonzero for - 1 < t < 0. Hence, the system is not causal. This example serves as a reminder that causality implies that the ROC is to the right of the rightmost pole, but the converse is not in general true, unless the system function is rational. In an exactly analogous manner, we can deal with the concept of anticausality. A system is anticausal if its impulse response h(t) = 0 for t > 0. Since in that case h(t) would be left sided, we know from Section 9.2 that the ROC of the system function H(s) would have to be a left-half plane. Again, in general, the converse is not true. That is, if the ROC of H(s) is a left-half plane, all we know is that h(t) is left sided. However, if H(s) is rational, then having an ROC to the left of the leftmost pole is equivalent to the system being anticausal. 9.7.2 Stability The ROC of H(s) can also be related to the stability of a system. As mentioned in Sec- tion 2.3.7, the stability of an LTI system is equivalent to its impulse response being abso- lutely integrable, in which case (Section 4.4) the Fourier transform ofthe impulse response converges. Since the Fourier transform of a signal equals the Laplace transform evaluated along the jw-axis, we have the following: An LTI system is stable if and only if the ROC of its system function H(s) includes the entire jw-axis [i.e., ~e(s) = 0]. Example 9.20 Let us consider an LTI system with system function s - 1 H (s) = -(s_+_1) -(s---2-) (9.119) Since the ROC has not been specified, we know from our discussion in Section 9.2 that there are several different ROCs and, consequently, several different system impulse re- sponses that can be associated with the algebraic expression for H(s) given in eq. (9.119). 696 The Laplace Transform Chap.9 If, however. we have information about the causality or stability of the system, the ap- propriate ROC can be identified. For example, if the system is known to be causal, the ROC will be that indicated in Figure 9.25(a), with impulse response lz(t) = (3 2 e r + 31 e-,.,r ) u(t). (9.120) Note that this particular choice of ROC does not include the jw-axis, and consequently, the corresponding system is unstable (as can be checked by observing that h(t) is not absolutely integrable). On the other hand, if the system is known to be stable, the ROC is that given in Figure 9.25(b). and the corresponding impulse response is 2 . 1 ,., lz(t) = -e -ru(t)- - e-1u(-t), 3 3 which is absolutely integrable. Finally, for the ROC in Figure 9.25(c), the system is anticausal and unstable, with 9m 9m I I I I s-plane I s-plane I I I I I I I I X X X X -1 2 <R-c -1 2 <R-c I I I I I I I I I I I I I I I I I I (a) (b) 9m I I s-plane I I I xI X ~1 2 <R-c (c) Figure 9.25 Possible ROCs for the system function of Example 9.20 with poles at s = -1 and s = 2 and a zero at s = 1: (a) causal, unstable system; (b) noncausal, stable system; (c) anticausal, unstable system. Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 697 It is perfectly possible, of course, for a system to be stable (or unstable) and have a system function that is not rational. For example, the system function in eq. (9.115) is not rational, and its impulse response in eq. (9 .118) is absolutely integrable, indicating that the system is stable. However, for systems with rational system functions, stability is easily interpreted in terms of the poles of the system. For example, for the pole-zero plot in Figure 9.25, stability corresponds to the choice of an ROC that is between the two poles, so that the jw-axis is contained in the ROC. For one particular and very important class of systems, stability can be characterized very simply in terms of the locations of the poles. Specifically, consider a causal LTI system with a rational system function H(s). Since the system is causal, the ROC is to the right of the rightmost pole. Consequently, for this system to be stable (i.e., for the ROC to include the jw-axis), the rightmost pole of H(s) must be to the left of the jw-axis. That is, A causal system with rational system function H(s) is stable if and only if all of the poles of H(s) lie in the left-half of the s-plane-i.e., all of the poles have negative real parts. Example 9.21 Consider again the causal system in Example 9.17. The impulse response in eq. (9 .113) is absolutely integrable, and thus the system is stable. Consistent with this, we see that the pole of H(s) in eq. (9.114) is at s = -1, which is in the left-half of the s-plane. In contrast, the causal system with impulse response h(t) = e21 u(t) is unstable, since h(t) is not absolutely integrable. Also, in this case 1 H(s) = --, ffi-R{s} > 2, s-2 so the system has a pole at s = 2 in the right half of the s-plane. Example 9.22 Let us consider the class of causal second-order systems previously discussed in Sec- tions 9.4.2 and 6.5.2. The impulse response and system function are, respectively, (9.121) and (9.122) where CJ = -~wn +wn~• (9.123) C2 = -~wn - Wn~' (9.124) M = Wn (9.125) 2~· 698 The Laplace Transform Chap. 9 9m I I s-plane I I I ~ I I I I I I I Figure 9.26 Pole locations and ROC for a causal second-order system with?< 0. In Figure 9.19, we illustrated the pole locations for ( > 0. In Figure 9.26, we illustrate the pole locations for ( < 0. As is evident from the latter figure and from eqs. (9.124) and (9.125), for ( < 0 both poles have positive real parts. Consequently, for ( < 0, the causal second-order system cannot be stable. This is also evident in eq. (9.121), since, with (Jl.e{c 1} > 0 and <Re{c2} > 0, each term grows exponentially as t increases, and thus h(t) cannot be absolutely integrable. 9. 7.3 LTI Systems Characterized by Linear Constant-Coefficient Differential Equations In Section 4. 7, we discussed the use of the Fourier transform to obtain the frequency re- sponse of an LTI system characterized by a linear constant -coefficient differential equation without first obtaining the impulse response or time-domain solution. In an exactly anal- ogous manner, the properties of the Laplace transform can be exploited to directly obtain the system function for an LTI system characterized by a linear constant-coefficient dif- ferential equation. We illustrate this procedure in the next example. Example 9.23 Consider an LTI system for which the input x(t) and output y(t) satisfy the linear constant -coefficient differential equation dy(t) -----;[( + 3y(t) = x(t). (9.126) Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 699 Applying the Laplace transform to both sides of eq. (9.126), and using the linearity and differentiation properties set forth in Sections 9.5.1 and 9.5.7, respectively [(eqs. (9.82) and (9.98)], we obtain the algebraic equation sY(s) + 3Y(s) = X(s). (9.127) Since, from eq. (9.112), the system function is H( ) = Y(s) s X(s)' we obtain, for this system, H(s) = s + 3"" (9.128) This, then, provides the algebraic expression for the system function, but not the region of convergence. In fact, as we discussed in Section 2.4, the differential equation itself is not a complete specification of the LTI system, and there are, in general, differ- ent impulse responses, all consistent with the differential equation. If, in addition to the differential equation, we know that the system is causal, then the ROC can be inferred to be to the right of the rightmost pole, which in this case corresponds to CRe{s} > -3. If the system were known to be anticausal, then the ROC associated with H(s) would be (Jl.e{s} < -3. The corresponding impulse response in the causal case is h(t) = e-3r u(t), (9.129) whereas in the anticausal case it is h(t) = -e-3t u( -t). (9.130) The same procedure used to obtain H (s) from the differential equation in Exam- ple 9.23 can be applied more generally. Consider a general linear constant-coefficient dif- ferential equation of the form ~ dky(t) _ ~ b dk x(t) Lak-dk - L k-dk · (9.131) k=O t k=O t Applying the Laplace transform to both sides and using the linearity and differenti- ation properties repeatedly, we obtain (9.132) or H(s) = (9.133) 700 The Laplace Transform Chap.9 Thus, the system function for a system specified by a differential equation is always ratio- nal, with zeros at the solutions of (9.134) and poles at the solutions of (9.135) Consistently with our previous discussion, eq. (9.133) does not include a specification of the region of convergence of H(s), since the linear constant-coefficient differential equa- tion by itself does not constrain the region of convergence. However, with additional in- formation, such as know ledge about the stability or causality of the system, the region of convergtmce can be inferred. For example, if we impose the condition of initial rest on the system, so that it is causal, the ROC will be to the right of the rightmost pole. Example 9.24 t An RLC circuit whose capacitor voltage and inductor current are initially zero constitutes an LTI system describable by a linear constant-coefficient differential equation. Consider the series RLC circuit in Figure 9.27. Let the voltage across the voltage source be the input signal x(t), and let the voltage measured across the capacitor be the output signal y(t). Equating the sum of the voltages across the resistor, inductor, and capacitor with the source 'voltage, we obtain 2 Rc dy(t) + LCd y(t) dt dt2 + () = () y t X t . (9.136) Applying eq. (9.133), we obtain l!LC H(s) = s2 + (RIL)s + (1/LC)' (9.137) As shown in Problem 9.64, if the values of R, L, and C are all positive, the poles of this system fupction will have negative real parts, and consequently, the system will be stable. R L Figure 9.27 A series RLC circuit. Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 701 9.7.4 Examples Relating System Behavior to the System Function As we have seen, system properties such as causality and stability can be directly related to the system function and its characteristics. In fact, each of the properties of Laplace transforms that we have described can be used in this way to relate the behavior of the system to the system function. In this section, we give several examples illustrating this. Example 9.25 Suppose we know that if the input to an LTI system is x(t) = e-3' u(t), then the output is As we now show, from this knowledge we can determine the system function for this system and from this can immediately deduce a number of other properties of the system. Taking Laplace transforms of x(t) and y(t), we get 1 X(s) = -- , ffi-e{s} > -3, s+ 3 and 1 Y(s) ffi-e{s} > -1. = (s + l)(s + 2)' From eq. (9.112), we can then conclude that H(s) = Y(s) = s + 3 s+3 X(s) (s + l)(s + 2) s2 + 3s + 2· Furthermore, we can also determine the ROC for this system. In particular, we know from the convolution property set forth in Section 9.5.6 that the ROC of Y(s) must include at least the intersections of the ROCs of X(s) and H(s). Examining the three possible choices for the ROC of H(s) (i.e., to the left of the pole at s = -2, between the poles at -2 and -1, and to the right of the pole at s = -1), we see that the only choice that is consistent with the ROCs of X(s) and Y(s) is ffi-e{s} > -1. Since this is to the right of the rightmost pole of H (s ), we conclude that H (s) is causal, and since both poles of H(s) have negative real parts, it follows that the system is stable. Moreover, from the relationship between eqs. (9.131) and (9.133), we can specify the differential equation that, together with the condition of initial rest, characterizes the system: d 2y(t) dy(t) 2 () _ dx(t) () d t-? + 3 d t + yt- d + 3 xt. t Example 9.26 Suppose that we are given the following information about an LTI system: 1. The system is causal. 2. The system function is rational and has only two poles, at s = -2 and s = 4. 702 The Laplace Transform Chap.9 3. If x(t) = 1, then y(t) = 0. 4. The value of the impulse response at t = o+ is 4. From this information we would like to determine the system function of the system. From the first two facts, we know that the system is unstable (since it is causal and has a pole at s = 4 with positive real part) and that the system function is of the form p(s) p(s) H(s) = (s + 2)(s- 4) s2 - 2s- 8' where p(s) is a polynomial ins. Because the response y(t) to the input x(t) = 1 = e0·r must equal H(O) · e0'' = H(O), we conclude, from fact 3, that p(O) = 0-i.e., that p(s) must have a root at s = 0 and thus is of the form p(s) = sq(s), where q(s) is another polynomial ins. Finally, from fact 4 and the initial-value theorem in Section 9.5.10, we see that 1' H( 1' s2q(s) 4 s~ s s) = s~ s2 - 2s- 8 = · (9.138) Ass ~ oo, the terms of highest power ins in both the numerator and the denominator of sH(s) dominate and thus are the only ones of importance in evaluating eq. (9.138). Furthermore, if the numerator has higher degree than the denominator, the limit will diverge. Consequently, we can obtain a finite nonzero value for the limit only if the degree of the numerator of sH(s) is the same as the degree of the denominator. Since the degree of the denominator is 2, we conclude that, for eq. (9.138) to hold, q(s) must be a constant-i.e., q(s) = K. We can evaluate this constant by evaluating (9.139) Equating eqs. (9.138) and (9.139), we see that K = 4, and thus, 4s H(s) = (s + 2)(s- 4) Example 9.27 Consider a stable and causal system with impulse response h(t) and system function H(s). Suppose H(s) is rational, contains a pole at s = -2, and does not have a zero at the origin. The location of all other poles and zeros is unknown. For each of the following statements let us determine whether we can definitely say that it is true, whether we can definitely say that it is false, or whether there is insufficient information to ascertain the statement's truth: (a) ~ { h(t)e3'} converges. (b) L+: h(t) dt = o. (c) th(t) is the impulse response of a causal and stable system. Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 703 (d) d h(t)ldt contains at least one pole in its Laplace transform. (e) h(t) has finite duration. (f) H(s) = H( -s). (g) lim.1 _,x H(s) = 2. Statement (a) is false, since ty{h(t)e3t} corresponds to the value of the Laplace transform of h(t) at s = -3. If this converges, it implies that s = -3 is in the ROC. A causal and stable system must always have its ROC to the right of all of its poles. However, s = -3 is not to the right of the pole at s = -2. Statement (b) is false, because it is equivalent to stating that H(O) = 0. This con- tradicts the fact that H(s) does not have a zero at the origin. Statement (c) is true. According to Table 9.1, the property set forth in Section 9.5.8, the Laplace transform of th(t) has the same ROC as that of H(s). This ROC includes the jw-axis, and therefore, the corresponding system is stable. Also, h(t) = 0 fort < 0 implies that th(t) = 0 fort < 0. Thus, th(t) represents the impulse response of a causal system. Statement (d) is true. According to Table 9.1, dh(t)ldt has the Laplace transform sH(s). The multiplication by s does not eliminate the pole at s = -2. Statement (e) is false. If h(t) is of finite duration, then if its Laplace transform has any points in its ROC, the ROC must be the entire s-plane. However, this is not consistent with H(s) having a pole at s = -2. Statement (f) is false. If it were true, then, since H(s) has a pole at s = -2, it must also have a pole at s = 2. This is inconsistent with the fact that all the poles of a causal and stable system must be in the left half of the s-plane. The truth of statement (g) cannot be ascertained with the information given. The statement requires that the degree of the numerator and denominator of H(s) be equal, and we have insufficient information about H(s) to determine whether this is the case. 9. 7. 5 Butterworth Filters In Example 6.3 we briefly introduced the widely-used class of LTI systems known as Butterworth filters. The filters in this class have a number of properties, including the characteristics of the magnitude of the frequency response of each of these filters in the passband, that make them attractive for practical implementation. As a further illustration of the usefulness of Laplace transforms, in this section we use Laplace transform tech- niques to determine the system function of a Butterworth filter from the specification of its frequency response magnitude. An Nth-order lowpass Butterworth filter has a frequency response the square of whose magnitude is given by 1 + (9.140) (jwl jwc)2N' where N is the order of the filter. From eq. (9.140), we would like to determine the system function B(s) that gives rise to IB(Jw )1 2. We first note that, by definition, IB(Jw )1 2 = B(jw )B*(jw ). (9.141) 704 The Laplace Transform Chap.9 If we restrict the impulse response of the Butterworth filter to be real, then from the prop- erty of conjugate symmetry for Fourier transforms, B*(jw) = B(- jw), (9.142) so that 1 B(jw )B(- jw) = 1 + (jwl jwc)2N. (9.143) Next, we note that B(s)ls=jw = B(jw), and consequently, from eq. (9.143), 1 B(s)B( -s) = 1 + ( I . )2N. (9.144) S ]We The roots of the denominator polynomial corresponding to the combined poles of B(s)B( -s) are at s = ( -1)112N (jwc). (9.145) Equation (9 .145) is satisfied for any value s = s P for which lspl = We (9.146) and 7T(2k + 1) 7T <.sp = 2N + 2' k an integer; (9.147) that is, s = w exp 1. [7T(2kN + 1) P c ( + 7T 12 ]) . (9.148) 2 In Figure 9.28 we illustrate the positions of the poles of B(s)B( -s) for N 1, 2, 3, and 6. In general, the following observations can be made about these poles: 1. There are 2N poles equally spaced in angle on a circle of radius We in the s-plane. 2. A pole never lies on the jw-axis and occurs on the 0'-axis for N odd, but not for N even. 3. The angular spacing between the poles of B(s)B( -s) is 7T/N radians. To determine the poles of B(s) given the poles of B(s)B( -s), we observe that the poles of B(s)B( -s) occur in pairs, so that if there is a pole at s = sp, then there is also a pole at s = -sp. Consequently, to construct B(s), we choose one pole from each pair. If we restrict the system to be stable and causal, then the poles that we associate with B(s) are the poles along the semicircle in the left-half plane. The pole locations specify B(s) only to within a scale factor. However, from eq. (9.144), we see that B2(s)ls=O = 1, or equivalently, from eq. (9 .140), the scale factor is chosen so that the square of the magnitude of the frequency response has unity gain at w = 0. To illustrate the determination of B(s), let us consider the cases N = 1, N = 2, and N = 3. In Figure 9.28 we showed the poles of B(s)B( -s), as obtained from eq. (9.148). Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 705 9m N=1 .... ..... / "" ' \ I \ ----X X------ \ 'W \ I c ' ..... _ .... ""/ 9m Figure 9.28 Position of the poles of B(s)B( -s) for N = 1, 2, 3, and 6. In Figure 9.29 we show the poles associated with B(s) for each of these values of N. The corresponding transfer functions are: N = 1: We . B(s) = (9.149) S +We' N = 2: (9.150) N = 3: (9.151) (s + wc)(s2 + wcs + w~) w~ Based on the discussion in Section 9.7.3, from B(s) we can determine the associated linear constant-coefficient differential equation. Specifically, for the foregoing three values 706 The Laplace Transform Chap. 9 N=1 N=2 ; ,. iwc / I I --X--+------ - we\ CRo£ -we\ CRo£ \ ' ' X .... .. .... x- iwc / I ---w--c *\ --r--------CR-o£ \ 'x_ Figure 9.29 Position of the poles of B(s) for N = 1, 2, and 3. of N, the corresponding differential equations are: N = 1: (9.152) N = 2: (9.153) N = 3: 9.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS The use of the Laplace transform allows us to replace time-domain operations such as differentiation, convolution, time shifting, and so on, with algebraic operations. We have already seen many of the benefits of this in terms of analyzing LTI systems, and in this section we take a look at another important use of system function algebra, namely, in analyzing interconnections of LTI systems and synthesizing systems as interconnections of elementary system building blocks."
9.8 System Function Algebra and Block Diagram Representations,"706 The Laplace Transform Chap. 9 N=1 N=2 ; ,. iwc / I I --X--+------ - we\ CRo£ -we\ CRo£ \ ' ' X .... .. .... x- iwc / I ---w--c *\ --r--------CR-o£ \ 'x_ Figure 9.29 Position of the poles of B(s) for N = 1, 2, and 3. of N, the corresponding differential equations are: N = 1: (9.152) N = 2: (9.153) N = 3: 9.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS The use of the Laplace transform allows us to replace time-domain operations such as differentiation, convolution, time shifting, and so on, with algebraic operations. We have already seen many of the benefits of this in terms of analyzing LTI systems, and in this section we take a look at another important use of system function algebra, namely, in analyzing interconnections of LTI systems and synthesizing systems as interconnections of elementary system building blocks. Sec. 9.8 System Function Algebra and Block Diagram Representations 707 9.8.1 System Functions for Interconnections of LTI Systems Consider the parallel interconnection of two systems, as shown in Figure 9.30(a). The impulse response of the overall system is (9.155) and from the linearity of the Laplace transform, H(s) = H1 (s) + H2(s). (9.156) Similarly, the impulse response of the series interconnection in Figure 9 .30(b) is (9.157) and the associated system function is (9.158) (a) Figure 9.30 (a) Parallel intercon- nection of two LTI systems; (b) series (b) combination of two LTI systems. The utility of the Laplace transform in representing combinations of linear systems through algebraic operations extends to far more complex interconnections than the simple parallel and series combinations in Figure 9.30. To illustrate this, consider the feedback interconnection of two systems, as indicated in Figure 9 .31. The design, applications, and analysis of such interconnections are treated in detail in Chapter 11. While analysis of the system in the time domain is not particularly simple, determining the overall system func- tion from input x(t) to output y(t) is a straightforward algebraic manipulation. Specifically, from Figure 9.31, Y(s) = H 1 (s)E(s), (9.159) E(s) = X(s) - Z(s), (9.160) 708 The Laplace Transform Chap.9 + x(t)--~ 1---------.... y(t) Figure 9.31 Feedback interconnec- tion of two LTI systems. and Z(s) = H2(s)Y(s), (9.161) from which we obtain the relation Y(s) = H1 (s)[X(s) - H2(s)Y(s)], (9.162) or Y(s) = H(s) = H 1( s) + (9.163) X(s) 1 H1 (s)H2(s) · 9.8.2 Block Diagram Representations for Causal LTI Systems Described by Differential Equations and Rational System Functions In Section 2.4.3, we illustrated the block diagram representation of an LTI system de- scribed by a first-order differential equation using the basic operations of addition, multi- plication by a coefficient, and integration. These same three operations can also be used to build block diagrams for higher order systems, and in this section we illustrate this in several examples. Example 9.28 Consider the causal LTI system with system function 1 H(s) = --. s+3 From Section 9.7.3, we know that this system can also be described by the differential equation dy(t) ---;[( + 3y(t) = x(t), together with the condition of initial rest. In Section 2.4.3 we constructed a block diagram representation, shown in Figure 2.32, for a first-order system such as this. An equiva- lent block diagram (corresponding to Figure 2.32 with a = 3 and b = 1) is shown in Sec. 9.8 System Function Algebra and Block Diagram Representations 709 x(t) 1----.......- ~ y(t) (a) (b) Figure 9.32 (a) Block diagram representation of the causal LTI system in Example 9.28; (b) equivalent block diagram representation. Figure 9.32(a). Here, lis is the system function of a system with impulse response u(t), i.e., it is the system function of an integrator. Also, the system function -3 in the feed- back path in Figure 9.32(a) corresponds to multiplication by the coefficient -3. The block diagram in the figure involves a feedback loop much as we considered in the pre- vious subsection and as pictured in Figure 9.31, the sole difference being that the two signals that are the inputs to the adder in Figure 9.32(a) are added, rather than sub- tracted as in Figure 9.31. However, as illustrated in Figure 9.32(b), by changing the sign of the coefficient in the multiplication in the feedback path, we obtain a block diagram representation of exactly the same form as Figure 9.31. Consequently, we can apply eq. (9.163) to verify that lis H(s) = 1 + 3/s s + 3' Example 9.29 Consider now the causal LTI system with system function H(s) = -s+-2 = ( -1- ) (s + 2). (9.164) s+3 s+3 As suggested by eq. (9.164), this system can be thought of as a cascade of a system with system function ll(s + 3) followed by a system with system functions + 2, and 710 The Laplace Transform Chap. 9 we have illustrated this in Figure 9.33(a), in which we have used the block diagram in Figure 9.32(a) to represent 1/(s + 3). It is also possible to obtain an alternative block diagram representation for the system in eq. (9.164). Using the linearity and differentiation properties of the Laplace transform, we know that y(t) and z(t) in Figure 9.33 (a) are related by dz(t) y(t) = --;[( + 2z(t). However, the input e(t) to the integrator is exactly the derivative of the output z(t), so that y(t) = e(t) + 2z(t), which leads directly to the alternative block diagram representation shown in Fig- ure 9.33(b). Note that the block diagram in Figure 9.33(a) requires the differentiation of z(t), since dz(t) y(t) = --;[( + 2z(t) In contrast, the block diagram in Figure 9.33(b) does not involve the explicit differenti- ation of any signal. r---------------------------------------1 I I I _x_(t.-) ---~ + 1----e(;..;.t)_-+-1 z(t) y(t) (a) r---........ y(t} __x( -t)--!~ + l---e(;..;.t). .......... __- +-1 (b) Figure 9.33 (a) Block diagram representations for the system in Exam- ple 9.29; (b) equivalent block diagram representation. Sec. 9.8 System Function Algebra and Block Diagram Representations 711 Example 9.30 Consider next a causal second-order system with system function 1 H(s) = (s + 1)(s + 2) + + (9.165) s2 3s 2· The input x(t) and output y(t) for this system satisfy the differential equation 2 d y(t) 3dy(t) 2 ()- () '""' + d (9.166) d t~ t + yt -xt. By employing similar ideas to those used in the preceding examples, we obtain the block diagram representation for this system shown in Figure 9.34(a). Specifically, since the y(t) (a) x(t) ·t y(t) t ·~ ·~ (b) x(t) y(t) (c) Figure 9.34 Block diagram representations for the system in Exam- ple 9.30: (a) direct form; (b) cascade form; (c) parallel form. 712 The Laplace Transform Chap. 9 input to an integrator is the derivative of the output of the integrator, the signals in the block diagram are related by f(t) = d;~t), e(t) = df(t) dt Also, eq. (9.166) can be rewritten as dy(t) -3----;[t - 2y(t) + x(t), or e(t) = -3f(t)- 2y(t) + x(t), which is exactly what is represented in Figure 9.34(a). The block diagram in this figure is sometimes referred to as a direct-form repre- sentation, since the coefficients appearing in the diagram can be directly identified with the coefficients appearing in the system function or, equivalently, the differential equa- tion. Other block diagram representations of practical importance also can be obtained after a modest amount of system function algebra. Specifically, H(s) in eq. (9.165) can be rewritten as H(s) ~ (s ~ 1) (s ~ 2). which suggests that this system can be represented as the cascade of two first-order sys- tems. The cascade-form representation corresponding to H (s) is shown in Figure 9 .34(b ). Alternatively, by performing a partial-fraction expansion of H(s), we obtain 1 1 H(s) = --1 - --2, s + s + which leads to the parallel-form representation depicted in Figure 9.34(c). Example 9.31 As a final example, consider the system function 2 H(s) = 2s + 4s- 6. (9.167) s2 + 3s + 2 Once again, using system function algebra, we can write H(s) in several different forms, each of which suggests a block diagram representation. In particular, we can write H(s) ~ (sz + !s + 2 2 )(2s + 4s- 6), which suggests the representation of H(s) as the cascade of the system depicted in Fig- ure 9.34(a) and the system with system function 2s2 + 4s- 6. However, exactly as we Sec. 9.8 System Function Algebra and Block Diagram Representations 713 did in Example 9.29, we can extract the derivatives required for this second system by ""tapping"" the signals appearing as the inputs to the integrators in the first system. The details qf this construction are examined in Problem 9.36, and the result is the direct- form block diagram shown in Figure 9.35. Once again, in the direct-form representation the coefficients appearing in the block diagram can be determined by inspection from the coefficients in the system function in eq. (9.167). y(t) x(t) Figure 9.35 Direct-form representation for the system in Example 9.31. Alternatively, we can write H(s) in the form H(s) = (2(s- 1))(~) (9.168) s+2 s+1 or 6 8 H(s) = 2+ -----. (9.169) s+2 s+1 The first of these suggests a cascade-form representation, while the second leads to a parallel-form block diagram. These are also considered in Problem 9.36. The methods for constructing block diagram representations for causal LTI systems described by differential equations and rational system functions can be applied equally well to higher order systems. In addition, there is often considerable flexibility in how this is done. For example, by reversing the numerators in eq. (9.168), we can write H(s) = (~)(2(s ~ 1) ), s+2 s+2 which suggests a different cascade form. Also, as illustrated in Problem 9.38, a fourth- order system function can be written as the product of two second-order system functions, each of which can be represented in a number of ways (e.g., direct form, cascade, or par- allel), and it can also be written as the sum of lower order terms, each of which has sev- eral different representations. In this way, simple low-order systems can serve as building blocks for the implementation of more complex, higher order systems. 714 The Laplace Transform Chap.9 9.9 THE UNilATERAl lAPlACE TRANSFORM In the preceding sections of this chapter, we have dealt with what is commonly called the bilateral Laplace transform. In this section, we introduce and examine a somewhat different transform, the unilateral Laplace transform, which is of considerable value in analyzing causal systems and, particularly, systems specified by linear constant-coefficient differential equations with nonzero initial conditions (i.e., systems that are not initially at rest). The unilateral Laplace transform of a continuous-time signal x(t) is defined as ~(s) ~ r(IO x(t)e-st dt, Jo- (9.170) where the lower limit of integration, o-, signifies that we include in the interval of integration any impulses or higher order singularity functions concentrated at t = 0. Once again we adopt a convenient shorthand notation for a signal and its unilateral Laplace transform: 'U£ x(t) ~ ~(s) = 11£{ x(t)}. (9.171) Comparing eqs. (9.170) and (9.3), we see that the difference in the definitions of the unilateral and bilateral Laplace transform lies in the lower limit on the integral. The bilateral transform depends on the entire signal from t = -oo tot = +oo, whereas the uni- lateral transform depends only on the signal from t = o- to oo. Consequently, two signals that differ for t < 0, but that are identical for t ~ 0, will have different bilateral Laplace transforms, but identical unilateral transforms. Similarly, any signal that is identically zero fort < 0 has identical bilateral and unilateral transforms. Since the unilateral transform of x(t) is identical to the bilateral transform of the signal obtained from x(t) by setting its value to 0 for all t < 0, many of the insights, concepts, and results pertaining to bilateral transforms can be directly adapted to the unilateral case. For example, using Property 4 in Section 9.2 for right-sided signals, we see that the ROC for eq. (9.170) is always a right-half plane. The evaluation of the inverse unilateral Laplace transforms is also the same as for bilateral transforms, with the constraint that the ROC for a unilateral transform must always be a right-half plane. 9. 9. 1 Examples of Unilateral Laplace Transforms To illustrate the unilateral Laplace transform, let us consider the following examples: Example 9. 32 Consider the signal t""-1 x(t) = (n _ l)! e-ar u(t). (9.172)"
9.9 The Unilateral Laplace Transform,"714 The Laplace Transform Chap.9 9.9 THE UNilATERAl lAPlACE TRANSFORM In the preceding sections of this chapter, we have dealt with what is commonly called the bilateral Laplace transform. In this section, we introduce and examine a somewhat different transform, the unilateral Laplace transform, which is of considerable value in analyzing causal systems and, particularly, systems specified by linear constant-coefficient differential equations with nonzero initial conditions (i.e., systems that are not initially at rest). The unilateral Laplace transform of a continuous-time signal x(t) is defined as ~(s) ~ r(IO x(t)e-st dt, Jo- (9.170) where the lower limit of integration, o-, signifies that we include in the interval of integration any impulses or higher order singularity functions concentrated at t = 0. Once again we adopt a convenient shorthand notation for a signal and its unilateral Laplace transform: 'U£ x(t) ~ ~(s) = 11£{ x(t)}. (9.171) Comparing eqs. (9.170) and (9.3), we see that the difference in the definitions of the unilateral and bilateral Laplace transform lies in the lower limit on the integral. The bilateral transform depends on the entire signal from t = -oo tot = +oo, whereas the uni- lateral transform depends only on the signal from t = o- to oo. Consequently, two signals that differ for t < 0, but that are identical for t ~ 0, will have different bilateral Laplace transforms, but identical unilateral transforms. Similarly, any signal that is identically zero fort < 0 has identical bilateral and unilateral transforms. Since the unilateral transform of x(t) is identical to the bilateral transform of the signal obtained from x(t) by setting its value to 0 for all t < 0, many of the insights, concepts, and results pertaining to bilateral transforms can be directly adapted to the unilateral case. For example, using Property 4 in Section 9.2 for right-sided signals, we see that the ROC for eq. (9.170) is always a right-half plane. The evaluation of the inverse unilateral Laplace transforms is also the same as for bilateral transforms, with the constraint that the ROC for a unilateral transform must always be a right-half plane. 9. 9. 1 Examples of Unilateral Laplace Transforms To illustrate the unilateral Laplace transform, let us consider the following examples: Example 9. 32 Consider the signal t""-1 x(t) = (n _ l)! e-ar u(t). (9.172) Sec. 9.9 The Unilateral Laplace Transform 715 Since x(t) = 0 fort < 0, the unilateral and bilateral transforms are identical. Thus, from Table 9.2, X(s) = ar' (Jl.e{s} > -a. (9.173) (s + Example 9.33 Consider next x(t) = e-a(t+ l)u(t + 1). (9.174) The bilateral transform X (s) for this example can be obtained from Example 9.1 and the time-shifting property (Section 9.5.2): es X(s) = --, (Jl.e{s} >-a. (9.175) s+a By contrast, the unilateral transform is X(s) = L~ e-a(l+ l)u(t + 1)e-s1 dt (9.176) -a 1 e s +a' ffic{s} > -a. = Thus, in this example, the unilateral and bilateral Laplace transforms are clearly dif- ferent. In fact, we should recognize X(s) as the bilateral transform not of x(t), but of x(t)u(t), consistent with our earlier comment that the unilateral transform is the bilateral transform of a signal whose values for t < o- have been set to zero. Example 9.34 Consider the signal x(t) = o(t) + 2u1 (t) + e1 u(t). (9.177) Since x(t) = 0 for t < 0, and since singularities at the origin are included in the interval of integration, the unilateral transform for x(t) is the same as the bilateral transform. Specifically, using the fact (transform pair 15 in Table 9.2) that the bilateral transform of Un(t) is sn, we have 1 s(2s- 1) X(s) = X(s) = 1 + 2s + s _ ffic{s} > 1. (9.178) 1 s- 1 ' Example 9.35 Consider the unilateral Laplace transform X - 1 (s) - (s + 1)(s + 2) (9.179) 716 The Laplace Transform Chap.9 In Example 9.9, we considered the inverse transform for a bilateral Laplace transform of the exact form as that in eq. (9.179) and for several ROCs. For the unilateral transform, the ROC must be the right-half plane to the right of the rightmost pole of X(s); i.e., in this case, the ROC consists of all points s with CR.e{s} > -1. We can then invert this unilateral transform exactly as in Example 9.9 to obtain x(t) = [e-t- e-2t]u(t) for t > o-, (9.180) where we have emphasized the fact that unilateral Laplace transforms provide us with information about signals only for t > o-. Example 9.36 Consider the unilateral transform s2 - 3 X(s) = --. (9.181) s+2 Since the degree of the numerator of X(s) is not strictly less than the degree of the de- nominator, we expand X(s) as + + -c- X(s) = A Bs . (9.182) s+ 2 Equating eqs. (9.181) and (9.182), and clearing denominators, we obtain s2 - 3 = (A+ Bs)(s + 2) + C, (9.183) and equating coefficients for each power of s yields 1 X(s) = -2 + s + --, (9.184) s+ 2 with an ROC of CRe{s} > -2. Taking inverse transforms of each term results in X(t) = -28(t) + UJ(t) + e-Ztu(t) for t > 0-. (9.185) 9.9.2 Properties of the Unilateral Laplace Transform As with the bilateral Laplace transform, the unilateral Laplace transform has a number of important properties, many of which are the same as their bilateral counterparts and sev- eral of which differ in significant ways. Table 9.3 summarizes these properties. Note that we have not included a column explicitly identifying the ROC for the unilateral Laplace transform for each signal, since the ROC of any unilateral Laplace transform is always a right-half plane. For example the ROC for a rational unilateral Laplace transform is always to the right of the rightmost pole. Contrasting Table 9.3 with Table 9.1 for the bilateral transform, we see that, with the caveat that ROCs for unilateral Laplace transforms are always right-half planes, the linearity, s-domain shifting, time-scaling, conjugation and differentiation in the s-domain Sec. 9.9 The Unilateral Laplace Transform 717 TABLE 9.3 PROPERTIES OF THE UNILATERAL LAPLACE TRANSFORM Property Signal Unilateral Laplace Transform x(t) a::(s) x,(t) a::,(s) X2(t) a::2 (s) ---------------- ----------- ------------------ Linearity ax1( t) + bx2(t) aa::,(s) + ha::2(s) Shifting in the s-domain esot x(t) a::cs - so) Time scaling x(at), a>O ~a:(~) Conjugation X* (t) x * (s) Convolution (assuming X1 (t) * X2(t) a:, (s) a::z(s) that x 1( t) and x 2(t) are identically zero for t < 0) d Differentiation in the time dt x(t) s a::cs) - xco-) domain d Differentiation in the -tx(t) ds a:(s) s-domain 1 Integration in the time L- X(T)dT - a::(s) domain s ---------------- ------------------------------ Initial- and Final-Value Theorems If x(t) contains no impulses or higher-order singularities at t = 0, then x(O+) = E.n.. : !sa::(s) limx(t) = limsa::(s) f~'X· s~o properties are identical to their bilateral counterparts. Similarly, the initial- and final-value theorems stated in Section 9.5.10 also hold for unilateral Laplace transforms.3 The deriva- tion of each of these properties is identical to that of its bilateral counterpart. The convolution property for unilateral transforms also is quite similar to the corre- sponding property for bilateral transforms. This property states that if Xt (t) = x2(t) = 0 for all t < 0, (9.186) 3In fact, the initial- and final-value theorems are basically unilateral transform properties, as they apply only to signals x(t) that are identically 0 fort < 0. 718 The Laplace Transform Chap.9 then (9.187) Equation (9 .187) follows immediately from the bilateral convolution property, since, under the conditions of eq. (9 .186), the unilateral and bilateral transforms are identical for each of the signals x 1( t) and x2(t). Thus, the system analysis tools and system function algebra developed and used in this chapter apply without change to unilateral transforms, as long as we deal with causal LTI systems (for which the system function is both the bilateral and the unilateral transform of the impulse response) with inputs that are identically zero fort< 0. An example of this is the integration property in Table 9.3: If x(t) = 0 fort< 0, then t V£ 1 i x(T) dT = x(t) * u(t) ~ X(s)'U(s) = -X(s) (9.188) o- s As a second case in point, consider the following example: Example 9.37 Suppose a causal LTI system is described by the differential equation d 2 _v.(- t) + d v(t) 3 _._ + 2 (t) = (t) dt (9.189) 2 dt )' X ' together with the condition of initial rest. Using eq. (9.133), we find that the system function for this system is J{(s) = ----=----- s2 + (9.190) 3s + 2' Let the input to this system be x(t) = a u(t). In this case, the unilateral (and bilateral) Laplace transform of the output y(t) is 'Y(s) = J{(s) X(s) = t s(s + )(s + 2) a/2 a a/2 = ----+--. (9.191) s s+1 s+2 Applying Example 9.32 to each term ofeq. (9.191) yields =a[~- 1 + ~e- 21 y(t) e- ]u(t). (9.192) It is important to note that the convolution property for unilateral Laplace transforms applies only if the signals x 1( t) and x2(t) in eq. (9.187) are both zero fort < 0. That is, while we have seen that the bilateral Laplace transform of x 1( t) * x2(t) always equals the product of the bilateral transforms of x 1( t) and x2(t), the unilateral transform of x 1( t)*x2(t) in general does not equal the product of the unilateral transforms if either x 1( t) or x2(t) is nonzero fort< 0. (See, for example, Problem 9.39). Sec. 9.9 The Unilateral Laplace Transform 719 A particularly important difference between the properties of the unilateral and bi- lateral transforms is the differentiation property. Consider a signal x(t) with unilateral Laplace transform ~(s). Then, integrating by parts, we find that the unilateral transform of dx(t)ldt is given by oo d x(t) J, --e-st dt = x(t)e-st loo + s J, oo x(t)e-st dt 0- dt 0- o- (9.193) = s~(s) - x(O-). Similarly, a second application of this would yield the unilateral Laplace transform of d2 x(t)ldt2 , i.e., (9.194) where x'(O-) denotes the derivative of x(t) evaluated at t = o-. Clearly, we can continue the procedure to obtain the unilateral transform of higher derivatives. 9. 9. 3 Solving Differential Equations Using the Unilateral laplace Transform A primary use of the unilateral Laplace transform is in obtaining the solution of linear constant-coefficient differential equations with nonzero initial conditions. We illustrate this in the following example: Example 9.38 Consider the system characterized by the differential equation (9.189) with initial con- ditions (9.195) Let x(t) = au(t). Then, applying the unilateral transform to both sides of eq. (9.189), we obtain s2'Y(s) - f3s - y + 3s'Y(s) - 3{3 + 2'Y(s) = ~. (9.196) s or C)'( ) {3(s + 3) vS = + + + y a (9.197) (s 1)(s 2) (s + 1)(s + +- 2) s(s +- ---1)(s + 2)' where 'Y(s) is the unilateral Laplace transform of y(t). Referring to Example 9.37 and, in particular, to eq. (9.191), we see that the last term on the right-hand side of eq. (9.197) is precisely the unilateral Laplace transform of the response of the system when the initial conditions in eq. (9.195) are both zero ({3 = y = 0). That is, the last term represents the response of the causal LTI system described by eq. (9.189) and the condition of initial rest. This response is often referred 720 The Laplace Transform Chap.9 to as the zero-state response-i.e., the response when the initial state (the set of initial conditions in eq. (9.195)) is zero. An analogous interpretation applies to the first two terms on the right-hand side of eq. (9.197). These terms represent the unilateral transform of the response of the system when the input is zero (a = 0). This response is commonly referred to as the zero- input response. Note that the zero-input response is a linear function of the values of the initial conditions (e.g., doubling the values of both f3 andy doubles the zero-input response). Furthermore, eq. (9.197) illustrates an important fact about the solution of linear constant-coefficient differential equations with nonzero initial conditions, namely, that the overall response is simply the superposition of the zero-state and the zero-input responses. The zero-state response is the response obtained by setting the initial condi- tions to zero-i.e., it is the response of an LTI system defined by the differential equa- tion and the condition of initial rest. The zero-input response is the response to the initial conditions with the input set to zero. Other examples illustrating this can be found in Problems 9.20, 9.40, and 9.66. Finally, for any values of a, {3, andy, we can, of course, expand 'Y(s) in eq. (9.197) in a partial-fraction expansion and invert to obtain y(t). For example, if a = 2, f3 = 3, andy = -5, then performing a partial-fraction expansion for eq. (9.197) we find that 1 1 3 'Y(s) = - - -- + -- . (9.198) s s+1 s+2 Applying Example 9.32 to each term then yields y(t) = [1 - e- 1 + 3e- 21 ]u(t) for t > 0. (9.199) 9.10 SUMMARY In this chapter, we have developed and studied the Laplace transform, which can be viewed as a generalization of the Fourier transform. It is particularly useful as an analytical tool in the analysis and study ofLTI systems. Because of the properties of Laplace transforms, LTI systems, including those represented by linear constant- coefficient differential equations, can be characterized and analyzed in the transform domain by algebraic manipulations. In addition, system function algebra provides a convenient tool both for analyzing intercon- nections of LTI systems and for constructing block diagram representations of LTI systems described by differential equations. For signals and systems with rational Laplace transforms, the transform is often con- veniently represented in the complex plane (s-plane) by marking the locations of the poles and zeros and indicating the region of convergence. From the pole-zero plot, the Fourier transform can be geometrically obtained, within a scaling factor. Causality, stability, and other characteristics are also easily identified from knowledge of the pole locations and the region of convergence. In this chapter, we have been concerned primarily with the bilateral Laplace trans- form. However, we also introduced a somewhat different form of the Laplace transform known as the unilateral Laplace transform. The unilateral transform can be interpreted as the bilateral transform of a signal whose values prior to t = o- have been set to zero. This form of the Laplace transform is especially useful for analyzing systems described by linear constant-coefficient differential equations with nonzero initial conditions."
9.10 Summary,"720 The Laplace Transform Chap.9 to as the zero-state response-i.e., the response when the initial state (the set of initial conditions in eq. (9.195)) is zero. An analogous interpretation applies to the first two terms on the right-hand side of eq. (9.197). These terms represent the unilateral transform of the response of the system when the input is zero (a = 0). This response is commonly referred to as the zero- input response. Note that the zero-input response is a linear function of the values of the initial conditions (e.g., doubling the values of both f3 andy doubles the zero-input response). Furthermore, eq. (9.197) illustrates an important fact about the solution of linear constant-coefficient differential equations with nonzero initial conditions, namely, that the overall response is simply the superposition of the zero-state and the zero-input responses. The zero-state response is the response obtained by setting the initial condi- tions to zero-i.e., it is the response of an LTI system defined by the differential equa- tion and the condition of initial rest. The zero-input response is the response to the initial conditions with the input set to zero. Other examples illustrating this can be found in Problems 9.20, 9.40, and 9.66. Finally, for any values of a, {3, andy, we can, of course, expand 'Y(s) in eq. (9.197) in a partial-fraction expansion and invert to obtain y(t). For example, if a = 2, f3 = 3, andy = -5, then performing a partial-fraction expansion for eq. (9.197) we find that 1 1 3 'Y(s) = - - -- + -- . (9.198) s s+1 s+2 Applying Example 9.32 to each term then yields y(t) = [1 - e- 1 + 3e- 21 ]u(t) for t > 0. (9.199) 9.10 SUMMARY In this chapter, we have developed and studied the Laplace transform, which can be viewed as a generalization of the Fourier transform. It is particularly useful as an analytical tool in the analysis and study ofLTI systems. Because of the properties of Laplace transforms, LTI systems, including those represented by linear constant- coefficient differential equations, can be characterized and analyzed in the transform domain by algebraic manipulations. In addition, system function algebra provides a convenient tool both for analyzing intercon- nections of LTI systems and for constructing block diagram representations of LTI systems described by differential equations. For signals and systems with rational Laplace transforms, the transform is often con- veniently represented in the complex plane (s-plane) by marking the locations of the poles and zeros and indicating the region of convergence. From the pole-zero plot, the Fourier transform can be geometrically obtained, within a scaling factor. Causality, stability, and other characteristics are also easily identified from knowledge of the pole locations and the region of convergence. In this chapter, we have been concerned primarily with the bilateral Laplace trans- form. However, we also introduced a somewhat different form of the Laplace transform known as the unilateral Laplace transform. The unilateral transform can be interpreted as the bilateral transform of a signal whose values prior to t = o- have been set to zero. This form of the Laplace transform is especially useful for analyzing systems described by linear constant-coefficient differential equations with nonzero initial conditions. Chap.9 Problems 721 Chapter 9 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 9.1. For each of the following integrals, specify the values of the real parameter u which ensure that the integral converges: (a) Jooo e-5t e-(a+ jw)t dt (b) J~ ooe-5t e-(a+ jw)t dt (c) J~ 5e-5t e-(a+ jw)t dt (d) J~ ooe-5t e-(a+ jw)t dt (e) J- ooooe-5ltle-(a+ jw)t dt (f) J~ ooe-51tle-(a+ jw)t dt 9.2. Consider the signal x(t) = e-5tu(t -1), and denote its Laplace transform by X(s). (a) Using eq. (9.3), evaluate X(s) and specify its region of convergence. (b) Determine the values of the finite numbers A and to such that the Laplace trans- form G(s) of g(t) = Ae-51u(-t-to) has the same algebraic form as X(s). What is the region of convergence corre- sponding to G(s)? 9.3. Consider the signal x(t) = e- 51 u(t)+e-f31u(t), and denote its Laplace transform by X(s). What are the constraints placed on the real and imaginary parts of {3 if the region of convergence of X(s) is CRe{s} > - 3? 9.4. For the Laplace transform of x(t) = { et sin 2t, t :::; 0 0, t>O' indicate the location of its poles and its region of convergence. 9.5. For each of the following algebraic expressions for the Laplace transform of a signal, determine the number of zeros located in the finite s-plane and the number of zeros located at infinity: 1 1 (a) s + 1 + s + 3 s + 1 (b)-- s2- 1 s3 - 1 (c) s2 + s + 1"
Problems,"Chap.9 Problems 721 Chapter 9 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 9.1. For each of the following integrals, specify the values of the real parameter u which ensure that the integral converges: (a) Jooo e-5t e-(a+ jw)t dt (b) J~ ooe-5t e-(a+ jw)t dt (c) J~ 5e-5t e-(a+ jw)t dt (d) J~ ooe-5t e-(a+ jw)t dt (e) J- ooooe-5ltle-(a+ jw)t dt (f) J~ ooe-51tle-(a+ jw)t dt 9.2. Consider the signal x(t) = e-5tu(t -1), and denote its Laplace transform by X(s). (a) Using eq. (9.3), evaluate X(s) and specify its region of convergence. (b) Determine the values of the finite numbers A and to such that the Laplace trans- form G(s) of g(t) = Ae-51u(-t-to) has the same algebraic form as X(s). What is the region of convergence corre- sponding to G(s)? 9.3. Consider the signal x(t) = e- 51 u(t)+e-f31u(t), and denote its Laplace transform by X(s). What are the constraints placed on the real and imaginary parts of {3 if the region of convergence of X(s) is CRe{s} > - 3? 9.4. For the Laplace transform of x(t) = { et sin 2t, t :::; 0 0, t>O' indicate the location of its poles and its region of convergence. 9.5. For each of the following algebraic expressions for the Laplace transform of a signal, determine the number of zeros located in the finite s-plane and the number of zeros located at infinity: 1 1 (a) s + 1 + s + 3 s + 1 (b)-- s2- 1 s3 - 1 (c) s2 + s + 1 722 The Laplace Transform Chap.9 9.6. An absolutely integrable signal x(t) is known to have a pole at s = 2. Answer the following questions: (a) Could x(t) be of finite duration? (b) Could x(t) be left sided? (c) Could x(t) be right sided? (d) Could x(t) be two sided? 9.7. How many signals have a Laplace transform that may be expressed as (s- 1) (s + 2)(s + 3)(s2 + s + 1) in its region of convergence? 9.8. Let x(t) be a signal that has a rational Laplace transform with exactly two poles, located at s = -1 and s = -3. If g(t) = e2 t x(t) and G(jw) [the Fourier transform of g(t)] converges, determine whether x(t) is left sided, right sided, or two sided. 9.9. Given that oC 1 e-atu(t) ~ --, ffi-e{ s} > ffi-e{- a}, s+a determine the inverse Laplace transform of 2(s + 2) X(s) = s 2 + 7 ffi-e{s} > -3. s + 12' 9.10. Using geometric evaluation of the magnitude of the Fourier transform from the cor- responding pole-zero plot, determine, for each of the following Laplace transforms, whether the magnitude of the corresponding Fourier transform is approximately lowpass, highpass, or bandpass: 1 (a) H 1( s) = (s + 1)(s + 3), ffi-e{s} > -1 (b) H 2(s) = s2 +: +1 , ffi-e{s} > - i s2 (c) H 3(s) = s2 + s + 1, ffi-e{s} > -1 2 9.11. Use geometric evaluation from the pole-zero plot to determine the magnitude of the Fourier transform of the signal whose Laplace transform is specified as s2 - s + 1 1 X(s) = , ffi-e{s} > -- . s 2 + s + 1 2 9.12. Suppose we are given the following three facts about the signal x(t): 1. x(t) = 0 for t < 0. 2. x(k/80) = 0 fork = 1, 2, 3, .... 3. x(l/160) = e- 120 . Let X(s) denote the Laplace transform of x(t), and determine which of the following statements is consistent with the given information about x(t): (a) X(s) has only one pole in the finites-plane. (b) X(s) has only two poles in the finites-plane. (c) X(s) has more than two poles in the finites-plane. Chap.9 Problems 723 9.13. Let g(t) = x(t) +ax( -t), where x(t) = {3 e -t u(t) and the Laplace transform of g(t) is s G(s) = s2- 1' -1 < (ft..e{s} < 1. Determine the values of the constants a and {3. 9.14. Suppose the following facts are given about the signal x(t) with Laplace transform X(s): 1. x(t) is real and even. 2. X(s) has four poles and no zeros in the finites-plane. 3. X(s) has a pole at s = (112)ej7T/4 . 4. f~ CX)x(t) dt = 4. Determine X(s) and its ROC. 9.15. Consider two right-sided signals x(t) and y(t) related through the differential equa- tions dx(t) ---;[[ = -2y(t) + 8(t) and d~;t) = 2x(t). Determine Y(s) and X(s), along with their regions of convergence. 9.16. A causal LTI systemS with impulse response h(t) has its input x(t) and output y(t) related through a linear constant -coefficient differential equation of the form (a) If dh(t) g(t) = ---;[[ + h(t), how many poles does G(s) have? (b) For what real values of the parameter a is S guaranteed to be stable? 9.17. A causal LTI system S has the block diagram representation shown in Figure P9 .17. Determine a differential equation relating the input x(t) to the output y(t) of this system. 724 The Laplace Transform Chap.9 Figure P9.17 9.18. Consider the causal LTI system represented by the RLC circuit examined in Prob- lem 3.20. (a) Determine H(s) and specify its region of convergence. Your answer should be consistent with the fact that the system is causal and stable. (b) Using the pole-zero plot of H(s) and geometric evaluation of the magnitude of the Fourier transform, determine whether the magnitude of the corresponding Fourier transform has an approximately lowpass, highpass, or bandpass char- acteristic. (c) If the value of R is now changed to 10-3 n, determine H(s) and specify its region of convergence. (d) Using the pole-zero plot of H(s) obtained in part (c) and geometric evaluation of the magnitude of the Fourier transform, determine whether the magnitude of the corresponding Fourier transform has an approximately lowpass, highpass, or bandpass characteristic. 9.19. Determine the unilateral Laplace transform of each of the following signals, and specify the corresponding regions of convergence: (a) x(t) = e- 21 u(t + 1) (b) x(t) = o(t + 1) + o(t) + e-2 3 (1+ lu(t + 1) (c) x(t) = e- 21 u(t) + e-41 u(t) 9.20. Consider the RL circuit of Problem 3.19. (a) Determine the zero-state response of this circuit when the input current is x(t) = e- 21 u(t). (b) Determine the zero-input response of the circuit fort > o-, given that y(O-) = 1. (c) Determine the output of the circuit when the input current is x(t) = e- 21 u(t) and the initial condition is the same as the one specified in part (b). BASIC PROBLEMS 9.21. Determine the Laplace transform and the associated region of convergence and pole- zero plot for each of the following functions of time: (a) x(t) = e- 21 u(t) + e- 31 u(t) (b) x(t) = e-41 u(t) + e-51(sin5t)u(t) (c) x(t) = e21u(-t)+e31 u(-t) (d) x(t) = te-21tl Chap.9 Problems 725 (e) x(t) = ltle-2ltl (0 x(t) = ltle2 t u( -t) 1 O:St:S t 0:St:S1 (g) x(t) = { o: elsewhere (h) x(t) = { 2- t, 1 :S t :S 2 (i) x(t) = o(t) + u(t) (j) x(t) = o(3t) + u(3t) 9.22. Determine the function of time, x(t), for each of the following Laplace transforms and their associated regions of convergence: (a) s2~9' (Jl.e{s} > 0 (b) s2·:9 , CJk{s} < 0 s+ I ( C ) (s+ 1)2+9' CRe{s} < -1 (d) s+2 2 12' -4 < CR.e{s} < -3 s +7s+ (e) s+ I 2 -3 < CRe{s} < -2 s +5s+6' 2 (0 s~~}l 1' (Re{s} > 4 2 (g) s -s+l (s+ 1)2 ' CRe{s} > -1 9.23. For each of the following statements about x(t), and for each of the four pole-zero plots in Figure P9.23, determine the corresponding constraint on the ROC: 1. x(t)e- 3 t is absolutely integrable. 2. x(t) * (e-t u(t)) is absolutely integrable. 3. x(t) = 0, t > 1. 4. x(t) = 0, t < -1. X 2j X X 2j 0 -2 2 CRe -2 2 CRe X -2j X X -2j 0 0 2j X 0 2j 0 -2 -2 2 CRe 0 -2j X 0 -2j 0 Figure P9.23 726 The Laplace Transform Chap. 9 9.24. Throughout this problem, we will consider the region of convergence of the Laplace transforms always to include the jw-axis. (a) Consider a signal x(t) with Fourier transform X(jw) and Laplace transform X(s) = s + 112. Draw the pole-zero plot for X(s). Also, draw the vector whose length represents IX(jw )I and whose angle with respect to the real axis repre- sents <r_X(jw) for a given w. (b) By examining the pole-zero plot and vector diagram in part (a), determine a different Laplace transform X1 (s) corresponding to the function of time, x 1( t), so that but x 1 (t) =I= x(t). Show the pole-zero plot and associated vectors that represent X1 (jw ). (c) For your answer in part (b), determine, again by examining the related vector diagrams, the relationship between <r.X(jw) and 1:X1( jw ). (d) Determine a Laplace transform X2(s) such that but x2(t) is not proportional to x(t). Show the pole-zero plot for X2(s) and the associated vectors that represent X2(jw ). (e) For your answer in part (d), determine the relationship between IX2(jw )I and IX(jw)l. (f) Consider a signal x(t) with Laplace transform X(s) for which the pole-zero plot is shown in Figure P9.24. Determine X1( s) such that IX(jw )I = IX1( jw )I and all poles and zeros of X1 (s) are in the left-half of the s-plane [i.e., CRe{s} < 0]. Also, determine X2(s) such that <r.X(jw) = <r.X2(jw) and all poles and zeros of X2(s) are in the left-half of the s-plane. --x----~---+~·~x---------- -2 -1 1 1 CRe 2 Figure P9.24 9.25. By considering the geometric determination of the Fourier transform, as developed in Section 9.4, sketch, for each of the pole-zero plots in Figure P9.25, the magnitude of the associated Fourier transform. Chap.9 Problems 727 !1m !1m --X--+------ (Jl.e 0 -iwo X -jw 0 (a) (b) !1m !1m --X--o-~------ --X-X--+----(:r--c>---- -b -a a b (c) (d) !1m !1m 0 iwo --X--+----<>---- 0 -jw 0 (e) (f) Figure P9.25 9.26. Consider a signal y(t) which is related to two signals x 1( t) and x 2 (t) by y( t) = X 1( t - 2) * X 2 (- t + 3) where x, (t) = e-2t u(t) and 728 The Laplace Transform Chap.9 Given that .r 1 e-ar u(t) ~ --, (Jl.e{s} >a, s+a use properties of the Laplace transform to determine the Laplace transform Y(s) of y(t). 9.27. We are given the following five facts about a real signal x(t) with Laplace transform X(s): 1. X(s) has exactly two poles. 2. X (s) has no zeros in the finite s-plane. 3. X(s) has a pole at s = -1 + j. 4. e2' x(t) is not absolutely integrable. 5. X(O) = 8. Determine X(s) and specify its region of convergence. 9.28. Consider an LTI system for which the system function H(s) has the pole-zero pattern shown in Figure P9.28. --x--x--~----x---<r---- - 2 - 1 + 1 + 2 ffi-e Figure P9.28 (a) Indicate all possible ROCs that can be associated with this pole-zero pattern. (b) For each ROC identified in part (a), specify whether the associated system is stable and/or causal. 9.29. Consider an LTI system with input x(t) = e-t u(t) and impulse response h(t) = e-2t u(t). (a) Determine the Laplace transforms of x(t) and h(t). (b) Using the convolution property, determine the Laplace transform Y(s) of the output y(t). (c) From the Laplace transform of y(t) as obtained in part (b), determine y(t). (d) Verify your result in part (c) by explicitly convolving x(t) and h(t). 9.30. A pressure gauge that can be modeled as an LTI system has a time response to a unit step input given by (1 - e-r - te-t)u(t). For a certain input x(t), the output is observed to be (2 - 3e-r + e-3')u(t). For this observed measurement, determine the true pressure input to the gauge as a function of time. Chap.9 Problems 729 9.31. Consider a continuous-time LTI system for which the input x(t) and output y(t) are related by the differential equation 2 d y(t) - dy(t) - 2 ( ) = ( ) dt2 dt y t X t. Let X(s) and Y(s) denote Laplace transforms of x(t) and y(t), respectively, and let H(s) denote the Laplace transform of h(t), the system impulse response. (a) Determine H(s) as a ratio of two polynomials ins. Sketch the pole-zero pattern of H(s). (b) Determine h(t) for each of the following cases: 1. The system is stable. 2. The system is causal. 3. The system is neither stable nor causal. 9.32. A causal LTI system with impulse response h(t) has the following properties: 1. When the input to the system is x(t) = e2t for all t, the output is y(t) = (116)e2t for all t. 2. The impulse response h(t) satisfies the differential equation -d-h;([t() + . 4 2h(t) = (e- t)u(t) + bu(t), where b is an unknown constant. Determine the system function H (s) of the system, consistent with the information above. There should be no unknown constants in your answer; that is, the constant b should not appear in the answer. 9.33. The system function of a causal LTI system is s + 1 H(s) = s 2 + 2 s + 2. Determine and sketch the response y(t) when the input is -00 < t < oo. 9.34. Suppose we are given the following information about a causal and stable LTI sys- temS with impulse response h(t) and a rational system function H(s): 1. H(l) = 0.2. 2. When the input is u(t), the output is absolutely integrable. 3. When the input is tu(t), the output is not absolutely integrable. 4. The signal d2h(t)ldt2 + 2 dh(t)ldt + 2h(t) is of finite duration. 5. H(s) has exactly one zero at infinity. Determine H (s) and its region of convergence. 9.35. The input x(t) and output y(t) of a causal LTI system are related through the block- diagram representation shown in Figure P9.35. (a) Determine a differential equation relating y(t) and x(t). (b) Is this system stable? 730 The Laplace Transform Chap.9 - ........- -{ + ,___...,.._r-_.....,._----1 + )---.......- - x(t) y(t) Figure P9.35 9.36. In this problem, we consider the construction of various types of block diagram representations for a causal LTI systemS with input x(t), output y(t), and system function 2 H(s) = 2s + 4s- 6. s2 + 3s + 2 To derive the direct-form block diagram representation of S, we first consider a causal LTI system S1 that has the same input x(t) asS, but whose system function is 1 HI(s) = s2 + 3s + 2' With the output of S1 denoted by y 1 (t), the direct-form block diagram representation of S1 is shown in Figure P9.36. The signals e(t) and f(t) indicated in the figure represent respective inputs into the two integrators. (a) Express y(t) (the output of S) as a linear combination of y1 (t), dy1 (t)ldt, and d 2yl (t)ldt2 . (b) How is dy1 (t)ldt related to f(t)? (c) How is d 2y (t)ldt2 1 related to e(t)? (d) Express y(t) as a linear combination of e(t), f(t), and y1 (t). f(t) x(t) Figure P9.36 Chap. 9 Problems 731 (e) Use the result from the previous part to extend the direct-form block diagram representation of S I and create a block diagram representation of S. (0 Observing that H(s) = (2(s - 1) )(~), s+2 s+l draw a block diagram representation for S as a cascade combination of two subsystems. (g) Observing that 6 H(s) = 2 + -- - -8- s+2 s+l' draw a block-diagram representation for S as a parallel combination of three subsystems. 9.37. Draw a direct-form representation for the causal LTI systems with the following system functions: 2 ( a ) H s+ I (b) H ( ) s -5s+6 s I (s ) = s2+5s+6 2 s = s2+ 7s+ 10 (c) H3(s) = (s+2)2 9.38. Consider a fourth-order causal LTI systemS whose system function is specified as 1 H(s) = (s2 - s + l)(s2 + 2s + 1) · (a) Show that a block diagram representation for S consisting of a cascade of four first -order sections will contain multiplications by coefficients that are not purely real. (b) Draw a block diagram representation for S as a cascade interconnection of two second-order systems, each of which is represented in direct form. There should be no multiplications by nonreal coefficients in the resulting block diagram. (c) Draw a block diagram representation for S as a parallel interconnection of two second-order systems, each of which is represented in direct form. There should be no multiplications by nonreal coefficients in the resulting block diagram. 9.39. Let XJ (t) = e-2t u(t) and x2(t) = e-3(t+I)u(t + 1). (a) Determine the unilateral Laplace transform XI (s) and the bilateral Laplace transform XI (s) for the signal x1 (t). (b) Determine the unilateral Laplace transform X 2(s) and the bilateral Laplace transform X2(s) for the signal x2(t). (c) Take the inverse bilateral Laplace transform of the product X 1 (s)X2(s) to deter- mine the signal g(t) = x1( t) * x2(t). (d) Show that the inverse unilateral Laplace transform of the product X 1 (s)X2(s) is not the same as g(t) fort> o-. 9.40. Consider the systemS characterized by the differential equation 3 2 d y(t) 6d y(t) 11 dy(t) 6 () = () dt3 + dt2 + dt + )' t X t . 732 The Laplace Transform Chap.9 (a) Determine the zero-state response of this system for the input x(t) = e-4 t u(t). (b) Determine the zero-input response of the system fort > o-, given that dy(t) I = -1 d2y(t) I = 1 dt t=O- , dt2 • t=O- (c) Determine t4e output of S when the input is x(t) = e-4 t u(t) and the initial con- ditions are the same as those specified in part (b). ADVANCED PROBLEMS 9.41. (a) Show that, if x(t) is an even function, so that x(t) = x( -t), then X(s) = X( -s). (b) Show that, if x(t) is an odd function, so that x(t) = - x( -t), then X(s) = -X(-s). (c) Determine which, if any, ofthe pole-zero plots in Figure P9.41 could correspond to an even f4nction of time. For those that could, indicate the required ROC. ----x--~~--x------ -1 1 (a) (b) ----X ------1f----X------ ----X----+------<->------- -1 1 -1 -j (c) (d) Figure P9.41 Chap.9 Problems 733 9.42. Determine whether each of the following statements is true or false. If a statement is true, construct a convincing argument for it. If it is false, give a counterexample. (a) The Laplace transform of t2u(t) does not converge anywhere on the s-plane. 2 (b) The Laplace transform of et u(t) does not converge anywhere on the s-plane. (c) The Laplace transform of ejwot does not converge anywhere on the s-plane. (d) The Laplace transform of ejwot u(t) does not converge anywhere on the s-plane. (e) The Laplace transform of ltl does not converge anywhere on the s-plane. 9.43. Let h(t) be the impulse response of a causal and stable LTI system with a rational system function. (a) Is the system with impulse response dh(t)ldt guaranteed to be causal and stable? (b) Is the system with impulse response f~oc h(T)dT guaranteed to be causal and unstable? 9.44. Let x(t) be the sampled signal specified as x(t) = Le-nT 8(t - nT), n=O where T > 0. (a) Determine X(s), including its region of convergence. (b) Sketch the pole-zero plot for X(s). (c) Use geometric interpretation of the pole-zero plot to argue that X(jw) is peri- odic. 9.45. Consider the LTI system shown in Figure P9.45(a) for which we are given the fol- lowing information: s+2 X(s) = s- 2' x(t) = 0, t > 0, and [See Figure P9.45(b ).] (a) Determine H(s) and its region of convergence. (b) Determine h(t). y(t) ___.._~ x(t)~y(t) (a) (b) Figure P9.45 734 The Laplace Transform Chap. 9 (c) Using the system function H(s) found in part (a), determine the output y(t) if the input is -oo < t < +oo. 9.46. Let H(s) represent the system function for a causal, stable system. The input to the system consists of the sum of three terms, one of which is an impulse 8(t) and another a complex exponential of the form esot, where s0 is a complex constant. The output is y(t) = -6e-t u(t) + : e4t cos 3t + ~! e4t sin 3t + 8(t). 3 Determine H(s), consistently with this information. 9.47. The signal y(t) = e-2 t u(t) is the output of a causal all-pass system for which the system function is s- 1 H(s) = s + 1' (a) Find and sketch at least two possible inputs x(t) that could produce y(t). (b) What is the input x(t) if it is known that roo lx(tll dt < oo? (c) What is the input x(t) if it is known that a stable (but not necessarily causal) system exists that will have x(t) as an output if y(t) is the input? Find the im- pulse response h(t) of this filter, and show by direct convolution that it has the property claimed [i.e., that y(t) * h(t) = x(t)]. 9.48. The inverse of an LTI system H(s) is defined as a system that, when cascaded with H(s), results in an overall transfer function of unity or, equivalently, an overall im- pulse response that is an impulse. (a) If H 1( s) denotes the transfer function of an inverse system for H(s), determine the general algebraic relationship between H(s) and H1 (s). (b) Shown in Figure P9.48 is the pole-zero plot for a stable, causal system H(s). Determine the pole-zero plot for the associated inverse system. !1m -------X----r-~---------- -1 1 2 Figure P9.48 Chap. 9 Problems 735 9.49. A class of systems, referred to as minimum-delay or minimum-phase systems, is sometimes defined through the statement that these systems are causal and stable and that the inverse systems are also causal and stable. Based on the preceding definition, develop an argument to demonstrate that all poles and zeros of the transfer function of a minimum-delay system must be in the left half of the s-plane [i.e., CRc{s} < 0]. 9.50. Determine whether or not each of the following statements about LTI systems is true. If a statement is true, construct a convincing argument for it. If it is false, give a counterexample. (a) A stable continuous-time system must have all its poles in the left half of the s-plane [i.e., CRc{s} < 0]. (b) If a system function has more poles than zeros, and the system is causal, the step response will be continuous at t = 0. (c) If a system function has more poles than zeros, and the system is not restricted to be causal, the step response can be discontinuous at t = 0. (d) A stable, causal system must have all its poles and zeros in the left half of the s-plane. 9.51. Consider a stable and causal system with a real impulse response h(t) and system function H(s). It is known that H(s) is rational, one of its poles is at -1 + j, one of its zeros is at 3 + j, and it has exactly two zeros at infinity. For each of the following statements, determine whether it is true, whether it is false, or whether there is insufficient information to determine the statement's truth. (a) h(t)e- 3t is absolutely integrable. (b) The ROC for H(s) is ffi,c{s} > -1. (c) The differential equation relating inputs x(t) and outputs y(t) for S may be writ- ten in a form having only real coefficients. (d) lim.1·~ryoH(s) = 1. (e) H(s) does not have fewer than four poles. (f) H(jw) = 0 for at least one finite value of w. (g) If the input to Sis e3t sin t, the output is e3t cost. 9.52. As indicated in Section 9.5, many of the properties of the Laplace transform and their derivation are analogous to corresponding properties of the Fourier transform and their derivation, as developed in Chapter 4. In this problem, you are asked to outline the derivation of a number of the Laplace transform properties. Observing the derivation for the corresponding property in Chapter 4 for the Fourier transform, derive each of the following Laplace transform properties. Your derivation must include a consideration of the region of convergence. (a) Time shifting (Section 9.5.2) (b) Shifting in the s-domain (Section 9.5.3) (c) Time scaling (Section 9.5.4) (d) Convolution property (Section 9.5.6) 9.53. As presented in Section 9.5.10, the initial-value theorem states that, for a signal x(t) with Laplace transform X(s) and for which x(t) = 0 fort < 0, the initial value of x(t) [i.e., x(O+ )] can be obtained from X(s) through the relation x(O+) = lim sX(s). [eq. (9.11 0)] s~x 736 The Laplace Transform Chap.9 First, we note that, since x(t) = 0 for t < 0, x(t) = x(t)u(t). Next, expanding x(t) as a Taylor series at t = 0+, we obtain x(t) = [x (O+) + x(ll(O+ )I+ · · · + x<n)(O+) :~ + .. ·] u(t), (P9.53-1) where x(n)(O+) denotes the nth derivative of x(t) evaluated at t = 0+. (a) Determine the Laplace transform of an arbitrary term x(n)(O+ )(tn/n!)u(t) on the right-hand side of eq. (P9.53-1). (You may find it helpful to review Example 9.14.) (b) From your result in part (a) and the expansion in eq. (P9.53-1), show that X(s) can be expressed as 00 1 X(s) = L~ x(n)(O+)-. sn+I n=O (c) Demonstrate that eq. (9.110) follows from the result of part (b). (d) By first determining x(t), verify the initial-value theorem for each of the fol- lowing examples: (1) X(s) = - 1 s+2 (2) X(s) = (s+~;~+ 3) (e) A more general form of the initial-value theorem states that if x(n)(O+) = 0 for n < N, then x(N)(O+) = lims~oosN+I X(s). Demonstrate that this more general statement also follows from the result in part (b). 9.54. Consider a real-valued signal x(t) with Laplace transform X(s). (a) By applying complex conjugation to both sides of eq. (9.56), show that X(s) = X*(s*). (b) From your result in (a), show that if X(s) has a pole (zero) at s = s0 , it must also have a pole (zero) at s = s0; i.e., for x(t) real, the poles and zeros of X(s) that are not on the real axis must occur in complex conjugate pairs. 9.55. In Section 9.6, Table 9.2, we listed a number of Laplace transform pairs, and we indicated specifically how transform pairs 1 through 9 follow from Examples 9.1 and 9.14, together with various properties from Table 9.1. By exploiting properties from Table 9.1, show how transform pairs 10 through 16 follow from transform pairs 1 through 9 in Table 9.2. 9.56. The Laplace transform is said to exist for a specific complex s if the magnitude of the transform is finite-that is, if IX(s)l < oo. Show that a sufficient condition for the existence of the transform X(s) at s = so = o-o + }wo is that +oo J-o o lx(t)ie-O""ot dt < oo. Chap.9 Problems 737 In other words, show that x(t) exponentially weighted by e-CYot is absolutely inte- grable. You will need to use the result that, for a complex function f(t), rf (t)dtl ~ r/J (t)/ dt. (P9.56-l) 1 Without rigorously proving eq. (P9.56-1), argue its plausibility. 9.57. The Laplace transform X(s) of a signal x(t) has four poles and an unknown number of zeros. The signal x(t) is known to have an impulse at t = 0. Determine what information, if any, this provides about the number of zeros and their locations. 9.58. Let h(t) be the impulse response of a causal and stable LTI system with rational system function H(s). Show that g(t) = CRe{h(t)} is also the impulse response of a causal and stable system. 9.59. IfX(s) denotes the unilateral Laplace transform of x(t), determine, in terms ofX(s), the unilateral Laplace transform of: (a) x(t - 1) (b) x(t + 1) (c) J_ :x( r) dr (d) dd;~t) EXTENSION PROBLEMS 9.60. In long-distance telephone communication, an echo is sometimes encountered due to the transmitted signal being reflected at the receiver, sent back down the line, re- flected again at the transmitter, and returned to the receiver. The impulse response for a system that models this effect is shown in Figure P9.60, where we have as- sumed that only one echo is received. The parameter T corresponds to the one-way travel time along the communication channel, and the parameter a represents the attenuation in amplitude between transmitter and receiver. h(t) a a3 I t 1 0 T 3T Figure P9.60 (a) Determine the system function H(s) and associated region of convergence for the system. (b) From your result in part (a), you should observe that H(s) does not consist of a ratio of polynomials. Nevertheless, it is useful to represent it in terms of poles and zeros, where, as usual, the zeros are the values of s for which H (s) = 0 738 The Laplace Transform Chap.9 and the poles are the values of s for which 11 H (s) = 0. For the system function found in part (a), determine the zeros and demonstrate that there are no poles. (c) From your result in part (b), sketch the pole-zero plot for H(s). (d) By considering the appropriate vectors in the s-plane, sketch the magnitude of the frequency response of the system. 9.61. The autocorrelation function of a signal x(t) is defined as cf>xx(T) = J:oo X(t)x(t+T)dt. (a) Determine, in terms of x(t), the impulse response h(t) of an LTI system for which, when the input is x(t), the output is cf>xAt) [Figure P9.61(a)]. (b) From your answer in part (a), determine <l>xx(s), the Laplace transform of cf>xA T) in terms of X(s). Also, express <l>xxUw), the Fourier transform of cf>xx(T), in terms of X(jw ). (c) If X(s) has the pole-zero pattern and ROC shown in Figure P9.61(b), sketch the pole-zero pattern and indicate the ROC for <I> xx<s). 9m I I I I x(t)-a-. I --X~X+--------- -3 -2 -~ ffi€ <!>,(!) (a) (b) Figure P9.61 9.62. In a number of applications in signal design and analysis, the class of signals cf>n(t) = e -t/2L n(t)u(t), n = 0, 1, 2, ... , (P9.62-1) where = -et dn ( n -t) Ln(t) -d t e , (P9.62-2) n.1 tn is encountered. (a) The functions Ln(t) are referred to as Laguerre polynomials. To verify that they in fact have the form of polynomials, determine Lo(t), L1 (t), and L2(t) explicitly. (b) Using the properties of the Laplace transform in Table 9.1 and Laplace trans- form pairs in Table 9.2, determine the Laplace transform <l>n(s) of cf>n(t). (c) The set of signals cf>n(t) can be generated by exciting a network of the form in Figure P9.62 with an impulse. From your result in part (b), determine H 1( s) and H2(s) so that the impulse responses along the cascade chain are the signals cf>n(t) as indicated. Chap. 9 Problems 739 8(t) ···-Br··· </>i(t) Figure P9.62 9.63. In filter design, it is often possible and convenient to transform a lowpass filter to a high pass filter and vice versa. With H (s) denoting the transfer function of the original filter and G(s) that of the transformed filter, one such commonly used trans- formation consists of replacing s by 11 s; that is, G(s) = HG). (a) For H(s) = 11(s + 112), sketch IH(jw )I and IG(jw )1. (b) Determine the linear constant-coefficient differential equation associated with H(s) and with G(s). (c) Now consider a more general case in which H(s) is the transfer function asso- ciated with the linear constant -coefficient differential equation in the general form ~ dky(t) _ ~ b dkx(t) Lak-dk -L k-dk. (P9.63-l) k=O t k=O t Without any loss of generality, we have assumed that the number of derivatives N is the same on both sides of the equation, although in any particular case, some of the coefficients may be zero. Determine H(s) and G(s). (d) From your result in part (c), determine, in terms of the coefficients in eq. (P9.63-l), the linear constant-coefficient differential equation associated with G(s). 9.64. Consider the RLC circuit shown in Figure 9.27 with input x(t) and output y(t). (a) Show that if R, L, and Care all positive, then this LTI system is stable. (b) How should R, L, and C be related to each other so that the system represents a second-order Butterworth filter? 9.65. (a) Determine the differential equation relating vi(t) and v0 (t) for the RLC circuit of Figure P9.65. 3!1 1h v0(0+) = 1 dvo(t) I= 2 dt t = 0+ Figure P9.65 740 The Laplace Transform Chap.9 (b) Suppose that v;(t) = e- 31 u(t). Using the unilateral Laplace transform, deter- mine v0 (t) fort > 0. 9.66. Consider the RL circuit shown in Figure P9.66. Assume that the current i(t) has reached a steady state with the switch at position A. At time t = 0, the switch is moved from position A to position B. i(t) L=1H Figure P9 .66 (a) Find the differential equation relating i(t) and V2 fort > o-. Specify the initial condition (i.e., the value of i(O-)) for this differential equation in terms of v1• (b) Using the properties of the unilateral Laplace transform in Table 9.3, determine and plot the current i(t) for each of the following values of VJ and v2: (i) v1 = 0 V, v2 = 2 V (ii) v1 = 4 V, v2 = 0 V (iii) v1 = 4 V, v2 = 2 V Using your answers for (i), (ii), and (iii), argue that the current i(t) may be expressed as a sum of the circuit's zero-state response and zero-input response. 10 THE Z-TRANSFORM 10.0 INTRODUCTION In Chapter 9, we developed the Laplace transform as an extension of the continuous-time Fourier transform. This extension was motivated in part by the fact that it can be applied to a broader class of signals than the Fourier transform can, since there are many sig- nals for which the Fourier transform does not converge but the Laplace transform does. The Laplace transform allowed us, for example, to perform transform analysis of unstable systems and to develop additional insights and tools for LTI system analysis. In this chapter, we use the same approach for discrete time as we develop the z- transform, which is the discrete-time counterpart of the Laplace transform. As we will see, the motivations for and properties of the z-transform closely parallel those of the Laplace transform. Just as with the relationship between continuous-time and discrete- time Fourier transforms, however, we will encounter some important distinctions between the z-transform and the Laplace transform that arise from the fundamental differences between continuous-time and discrete-time signals and systems. 10.1 THE z-TRANSFORM As we saw in Section 3.2, for a discrete-time linear time-invariant system with impulse response h[n], the response y[n] of the system to a complex exponential input of the form zn is y[n] = H(z)zn, (10.1) 741"
10 The z-Transform,"10 THE Z-TRANSFORM 10.0 INTRODUCTION In Chapter 9, we developed the Laplace transform as an extension of the continuous-time Fourier transform. This extension was motivated in part by the fact that it can be applied to a broader class of signals than the Fourier transform can, since there are many sig- nals for which the Fourier transform does not converge but the Laplace transform does. The Laplace transform allowed us, for example, to perform transform analysis of unstable systems and to develop additional insights and tools for LTI system analysis. In this chapter, we use the same approach for discrete time as we develop the z- transform, which is the discrete-time counterpart of the Laplace transform. As we will see, the motivations for and properties of the z-transform closely parallel those of the Laplace transform. Just as with the relationship between continuous-time and discrete- time Fourier transforms, however, we will encounter some important distinctions between the z-transform and the Laplace transform that arise from the fundamental differences between continuous-time and discrete-time signals and systems. 10.1 THE z-TRANSFORM As we saw in Section 3.2, for a discrete-time linear time-invariant system with impulse response h[n], the response y[n] of the system to a complex exponential input of the form zn is y[n] = H(z)zn, (10.1) 741"
10.0 Introduction,"10 THE Z-TRANSFORM 10.0 INTRODUCTION In Chapter 9, we developed the Laplace transform as an extension of the continuous-time Fourier transform. This extension was motivated in part by the fact that it can be applied to a broader class of signals than the Fourier transform can, since there are many sig- nals for which the Fourier transform does not converge but the Laplace transform does. The Laplace transform allowed us, for example, to perform transform analysis of unstable systems and to develop additional insights and tools for LTI system analysis. In this chapter, we use the same approach for discrete time as we develop the z- transform, which is the discrete-time counterpart of the Laplace transform. As we will see, the motivations for and properties of the z-transform closely parallel those of the Laplace transform. Just as with the relationship between continuous-time and discrete- time Fourier transforms, however, we will encounter some important distinctions between the z-transform and the Laplace transform that arise from the fundamental differences between continuous-time and discrete-time signals and systems. 10.1 THE z-TRANSFORM As we saw in Section 3.2, for a discrete-time linear time-invariant system with impulse response h[n], the response y[n] of the system to a complex exponential input of the form zn is y[n] = H(z)zn, (10.1) 741"
10.1 The z-Transform,"10 THE Z-TRANSFORM 10.0 INTRODUCTION In Chapter 9, we developed the Laplace transform as an extension of the continuous-time Fourier transform. This extension was motivated in part by the fact that it can be applied to a broader class of signals than the Fourier transform can, since there are many sig- nals for which the Fourier transform does not converge but the Laplace transform does. The Laplace transform allowed us, for example, to perform transform analysis of unstable systems and to develop additional insights and tools for LTI system analysis. In this chapter, we use the same approach for discrete time as we develop the z- transform, which is the discrete-time counterpart of the Laplace transform. As we will see, the motivations for and properties of the z-transform closely parallel those of the Laplace transform. Just as with the relationship between continuous-time and discrete- time Fourier transforms, however, we will encounter some important distinctions between the z-transform and the Laplace transform that arise from the fundamental differences between continuous-time and discrete-time signals and systems. 10.1 THE z-TRANSFORM As we saw in Section 3.2, for a discrete-time linear time-invariant system with impulse response h[n], the response y[n] of the system to a complex exponential input of the form zn is y[n] = H(z)zn, (10.1) 741 742 The z-Transform Chap. 10 where +oo H(z) = L h[n]z-n. (10.2) n= -oo For z = eiw with w real (i.e., with lzl = 1), the summation in eq. (10.2) corresponds to the discrete-time Fourier transform of h[n]. More generally, when lzl is not restricted to unity, the summation is referred to as the z-transform of h[n]. The z-transform of a general discrete-time signal x[n] is defined as 1 +oo X(z) ~ L x[n]z-n, (10.3) n= -oo where z is a complex variable. For convenience, the z-transform of x[n] will sometimes be denoted as Z{x[n]} and the relationship between x[n] and its z-transform indicated as z x[n] ~ X(z). (10.4) In Chapter 9, we considered a number of important relationships between the Laplace transform and the Fourier transform for continuous-time signals. In a similar, but not identical, way, there are a number of important relationships between the z-transform and the discrete-time Fourier transform. To explore these relationships, we express the complex variable z in polar form as z = reiw, (10.5) with r as the magnitude of z and was the angle of z. In terms of rand w, eq. (10.3) becomes +oo X(reiw) = L x[n](refw)-n, n= -oo or equivalently, +oo X(reiw) = L {x[n]r-n}e- Jwn. (10.6) n= -oo From eq. (10.6), we see that X(reiw) is the Fourier transform of the sequence x[n] multiplied by a real exponential r-n; that is, X(reiw) = ~{x[n]r-n}. (10.7) The exponential weighting r-n may be decaying or growing with increasing n, depending on whether r is greater than or less than unity. We note in particular that, for r = 1, or 1T he z-transform defined in eq. (1 0.3) is often referred to as the bilateral z-transform, to distinguish it from the unilateral z-transform, which we develop in Section 10.9. The bilateral z-transform involves a summation from -oo to +oo, while the unilateral transform has a form similar to eq. (10.3), but with summation limits from 0 to +oo. Since we are mostly concerned with the bilateral z-transform, we will refer to X(z) as defined in eq. (10.3) simply as the z-transform, except in Section 10.9, in which we use the words ""unilateral"" and ""bilateral"" to avoid ambiguity. Sec. 10.1 The z-Transform 743 equivalently, lzl = 1, eq. (10.3) reduces to the Fourier transform; that is, (10.8) The relationship between the z-transform and Fourier transform for discrete-time signals parallels closely the corresponding discussion in Section 9.1 for continuous-time signals, but with some important differences. In the continuous-time case, the Laplace transform reduces to the Fourier transform when the real part of the transform variable is zero. Interpreted in terms of the s-plane, this means that the Laplace transform reduces to the Fourier transform on the imaginary axis (i.e., for s = jw ). In contrast, the z-transform reduces to the Fourier transform when the magnitude of the transform variable z is unity (i.e, for z = ejw). Thus, the z-transform reduces to the Fourier transform on the contour in the complex z-plane corresponding to a circle with a radius of unity, as indicated in Figure 10 .1. This circle in the z-plane is referred to as the unit circle and plays a role in the discussion of the z-transform similar to the role of the imaginary axis in the s-plane for the Laplace transform. z-plane Figure 1 0. 1 Complex z-plane. The z-transform reduces to the Fourier transform for values of z on the unit circle. From eq. (10.7), we see that, for convergence of the z-transform, we require that the Fourier transform of x[n]r-n converge. For any specific sequence x[n], we would expect this convergence for some values of r and not for others. In general, the z-transform of a sequence has associated with it a range of values of z for which X(z) converges. As with the Laplace transform, this range of values is referred to as the region of convergence (ROC). If the ROC includes the unit circle, then the Fourier transform also converges. To illustrate the z-transform and the associated region of convergence, let us consider several examples. Example 1 0. 1 Consider the signal x[n] = anu[n]. Then, from eq. (10.3), +x X(z) = L a11 u[n]z-n = L(az- 1t. n=-x n=O For convergence of X(z), we require that L~=o laz- 1 11 1 < oo. Consequently, the region of convergence is the range of values of z for which laz- 1 1 < 1, or equivalently, lzl > lal. 744 The z-Transform Chap. 10 Then X(z) = ~(az- 1 )"" z- a' lzl > lal. (10.9) 11=0 Thus, the z-transform for this signal is well-defined for any value of a, with an ROC determined by the magnitude of a according to eq. (10.9). For example, for a = 1, x[n] is the unit step sequence with z-transform 1 X(z) = ~· lzl > 1. We see that the z-transform in eq. (10.9) is a rational function. Consequently, just as with rational Laplace transforms, the z-transform can be characterized by its zeros (the roots of the numerator polynomial) and its poles (the roots of the denominator polyno- mial). For this example, there is one zero, at z = 0, and one pole, at z = a. The pole-zero plot and the region of convergence for Example 10.1 are shown in Figure 10.2 for a value of a between 0 and 1. For Ia I > 1, the ROC does not include the unit circle, consistent with the fact that, for these values of a, the Fourier transform of a"" u[n] does not converge. 9m ....----- Unit Circle z-plane CR.e Figure 1 0.2 Pole-zero plot and region of convergence for Example 10.1 for 0 <a< 1. Example 1 0.2 Now let x[n] = -a""u[ -n- 1]. Then +·/. -I X(z) = - ~ a""u[ -n- l]z-"" ~ a""z-"" 11= --% ll = -'l~ (10.10) - ~a-""z"" I- ~(a- 1 z)"". II= I 11=0 If la- 1 zl < 1, or equivalently, lzl < Ia I, the sum in eq. ( 10.1 0) converges and 1 1 z X(z) = 1 - a- z = az- = -z - ,a lzl < lal. (10.11) 1 - 1 1 - 1 The pole-zero plot and region of convergence for this example are shown in Fig- ure 10.3 for a value of a between 0 and 1. Sec. 10.1 The z-Transform 745 9m Unit Circle z-plane Figure 1 0.3 Pole-zero plot and region of convergence for Example 10.2 for 0 <a< 1. Comparing eqs. (10.9) and (10.11), and Figures 10.2 and 10.3, we see that the al- gebraic expression for X(z) and the corresponding pole-zero plot are identical in Exam- ples 10.1 and 10.2, and the z-transforms differ only in their regions of convergence. Thus, as with the Laplace transform, specification of the z.,transform requires both the algebraic expression and the region of convergence. Also, in both examples, the sequences were exponentials and the resulting z-transforrns were rational. In fact, as further suggested by the following examples, X(z) will be rational whenever x[n] is a linear combination of real or complex exponentials: Example 10.3 Let us consider a signal that is the sum of two real exponentials: x[n] ~ 7 GJu [n]- 6G )"" u[n]. (10.12) The z-transform is then (10.13) 7 6 (10.14) 1 - .!. z-1 1 - .!. z- 1 3 2 z(z- ~) 1 1 . (10.15) (z - 3 )(z - 2) For convergence of X(z), both sums in eq. (10.13) must converge, which requires that both I0/3)z- 1 1 1 < 1 and IC112)z- 1 < 1, or equivalently, lzl > 113 and lzl > 112. Thus, the region of convergence is lzl > 112. 746 The z-Transform Chap. 10 The z-transform for this example can also be obtained using the results of Exam- ple 10.1. Specifically, from the definition of the z-transform in eq. (1 0.3), we see that the z-transform is linear; that is, if x[n] is the sum of two terms, then X(z) will be the sum of the z-transforms of the individual terms and will converge when both z-transforms converge. From Example 10.1, 1 )n z ( - u[n] ~ \z\ > ~ (10.16) 3 1 - !z- 1 ' - :l and (-1 )n u[n] z ~ \z\ > ~. (10.17) 2 l- !z-1' 2 and consequently, 11 11 7 (1) 3 u[n] - 6 (1) 2 u[n] ~z (10.18) 7 6 --,- 1 • \z\> i· 1- -3 c' 1- -2 z- 1 as we determined before. The pole-zero plot and ROC for the z-transform of each of the individual terms and for the combined signal are shown in Figure 10.4. gm gm z-plane z-plane CRe (a) (b) 9m z-plane CRe (c) Figure 1 0.4 Pole-zero plot and region of convergence for the individual terms and the sum in Example 10.3: (a)1/(1 - tr1 ), \z\ > t; (b)1/(1 - ~r 1 ), \z\ > ~; (c)?/(1 - tr1)- 6/(1 - ~z- 1 ), \z\ > ~- Sec. 10.1 The z-Transform 747 Example 1 0.4 Let us consider the signal The z-transform of this signal is (10.19) or equivalently, I X 3hz (z) = ----,-------,---- (10.20) (z- lei7TI4)(z _ le-J7TI4) 3 3 For convergence of X(z), both sums in eq. (10.19) must converge, which requires that l(l/3)ei7T/4z-'l < 1 and l(l/3)e-J7T/4z- 1 1 < 1, or equivalently, lzl > 113. The pole- zero plot and ROC for this example are shown in Figure 10.5. 9m z-plane <Re Figure 1 o.s Pole-zero plot and ROC for the z-transform in Example 10 .4. In each of the preceding four examples, we expressed the z-transform both as a ratio of polynomials in z and as a ratio of polynomials in z- 1• From the definition of the z-transform as given in eq. (10.3), we see that, for sequences which are zero for n < 0, X(z) involves only negative powers of z. Thus, for this class of signals, it is partic- ularly convenient for X(z) to be expressed in terms of polynomials in z- 1 rather than z, and 748 The z-Transform Chap. 10 when appropriate, we will use that form in our discussion. However, reference to the poles and zeros is always in terms of the roots of the numerator and denominator expressed as polynomials in z. Also, it is sometimes convenient to refer to X(z), written as a ratio of polynomials in z, as having poles at infinity if the degree of the numerator exceeds the degree of the denominator or zeros at infinity if the numerator is of smaller degree than the denominator. 10.2 THE REGION OF CONVERGENCE FOR THE z-TRANSFORM In Chapter 9, we saw that there were specific properties of the region of convergence of the Laplace transform for different classes of signals and that understanding these properties led to further insights about the transform. In a similar manner, we explore a number of properties of the region of convergence for the z-transform. Each of the following properties and its justification closely parallel the corresponding property in Section 9 .2. Property 1: The ROC of X(z) consists of a ring in the z-plane centered about the origin. This property is illustrated in Figure 10.6 and follows from the fact that the ROC consists of those values of z = rejw for which x[n]r-n has a Fourier transform that con- verges. That is, the ROC of the z-transform of x[n] consists of the values of z for which x[n]r-n is absolutely summable:2 +x L Jx[n]Jr- 11 < x. (10.21) n=-x 9m / "" ' ' ' z-plane / I .... I / ' I ' ' I ' I <R.e Figure 1 0.6 ROC as a ring in the z-plane. In some cases, the inner boundary can extend inward to the ori- gin, in which case the ROC becomes a disc. In other cases, the outer bound- ary can extend outward to infinity. 2For a thorough treatment of the mathematical properties of z-transforms, see R.V. Churchill and J.W. Brown, Complex Variables and Applications (5th ed.) (New York: McGraw-Hill, 1990), and E. I. Jury, Theory and Application of the z-Transform Method (Malabar, FL: R. E. Krieger Pub. Co., 1982)."
10.2 The Region of Convergence for the z-Transform,"748 The z-Transform Chap. 10 when appropriate, we will use that form in our discussion. However, reference to the poles and zeros is always in terms of the roots of the numerator and denominator expressed as polynomials in z. Also, it is sometimes convenient to refer to X(z), written as a ratio of polynomials in z, as having poles at infinity if the degree of the numerator exceeds the degree of the denominator or zeros at infinity if the numerator is of smaller degree than the denominator. 10.2 THE REGION OF CONVERGENCE FOR THE z-TRANSFORM In Chapter 9, we saw that there were specific properties of the region of convergence of the Laplace transform for different classes of signals and that understanding these properties led to further insights about the transform. In a similar manner, we explore a number of properties of the region of convergence for the z-transform. Each of the following properties and its justification closely parallel the corresponding property in Section 9 .2. Property 1: The ROC of X(z) consists of a ring in the z-plane centered about the origin. This property is illustrated in Figure 10.6 and follows from the fact that the ROC consists of those values of z = rejw for which x[n]r-n has a Fourier transform that con- verges. That is, the ROC of the z-transform of x[n] consists of the values of z for which x[n]r-n is absolutely summable:2 +x L Jx[n]Jr- 11 < x. (10.21) n=-x 9m / "" ' ' ' z-plane / I .... I / ' I ' ' I ' I <R.e Figure 1 0.6 ROC as a ring in the z-plane. In some cases, the inner boundary can extend inward to the ori- gin, in which case the ROC becomes a disc. In other cases, the outer bound- ary can extend outward to infinity. 2For a thorough treatment of the mathematical properties of z-transforms, see R.V. Churchill and J.W. Brown, Complex Variables and Applications (5th ed.) (New York: McGraw-Hill, 1990), and E. I. Jury, Theory and Application of the z-Transform Method (Malabar, FL: R. E. Krieger Pub. Co., 1982). Sec. 10.2 The Region of Convergence for the z-Transform 749 Thus, convergence is dependent only on r = lzl and not on w. Consequently, if a specific value of z is in the ROC, then all values of z on the same circle (i.e., with the same magnitude) will be in the ROC. This by itself guarantees that the ROC will con- sist of concentric rings. As we will see when we discuss Property 6, the ROC must in fact consist of only a single ring. In some cases the inner boundary of the ROC may ex- tend inward to the origin, and in some cases the outer boundary may extend outward to infinity. Property 2: The ROC does not contain any poles. As with the Laplace transform, this property is simply a consequence of the fact that at a pole X(z) is infinite and therefore, by definition, does not converge. Property 3: If x[n] is of finite duration, then the ROC is the entire z-plane, except possibly z = 0 and/or z = oo. A finite-duration sequence has only a finite number of nonzero values, extending, say, from n = N 1 ton = N 2 , where N 1 and N 2 are finite. Thus, the z-transform is the sum of a finite number of terms; that is, NJ X(z) = 2:_ x[n]z- 11 • (10.22) n=N1 For z not equal to zero or infinity, each term in the sum will be finite, and conse- quently, X(z) will converge. If N 1 is negative and N2 positive, so that x[n] has nonzero values both for n < 0 and n > 0, then the summation includes terms with both positive powers of z and negative powers of z. As lzl ~ 0, terms involving negative powers of z become unbounded, and as lzl ~ oo, terms involving positive powers of z become un- bounded. Consequently, for N1 negative and N2 positive, the ROC does not include z = 0 or z = oo. If N1 is zero or positive, there are only negative powers of z in eq. (10.22), and consequently, the ROC includes z = oo. If N2 is zero or negative, there are only positive powers of z in eq. (10.22), and consequently, the ROC includes z = 0. Example 10.5 Consider the unit impulse signalo[n]. Its z-transform is given by Z +x o[n] ~ L o[n]z-n = 1, (10.23) n=-x with an ROC consisting of the entire z-plane, including z = 0 and z = oo. On the other hand, consider the delayed unit impulse 8 [ n - 1], for which Z +oo o[n- 11 ~ L o[n- 1]z-n = z- 1• (10.24) n=-x 750 The z-Transform Chap. 10 This z-transform is well defined except at z = 0. where there is a pole. Thus, the ROC consists of the entire z-plane, including z = w but excluding z = 0. Similarly, consider an impulse advanced in time, namely, 8[n + 1]. In this case, z t X 8 [ n + 1] ~ ~ 8[ n + 1] z 11 = z, (10.25) L.......,., n=- -z which is well defined for all finite values of z. Thus, the ROC consists of the entire finite z-plane (including z = 0). but there is a pole at infinity. Property 4: If x[ n] is a right -sided sequence, and if the circle \z \ = r0 is in the ROC, then all finite values of z for which \z\ > r0 will also be in the ROC. The justification for this property follows in a manner identical to that of Property 4 in Section 9.2. A right-sided sequence is zero prior to some value of n, say, N 1 • If the circle \z\ = r0 is in the ROC, then x[n]rc)"" is absolutely summable. Now consider \z\ = r 1 with r 1 > r0 , so that r("" decays more quickly than rc)"" for increasing n. As illustrated in Figure 10.7, this more rapid exponential decay will further attenuate sequence values x[n] n Illiiiiiiiirrttttrrrr'''ITtttt~· n lllll Figure 10.7 With r1 > r0, x[n]r1- n decays faster with increasing n than 1 1 0 r r >r does x[n]r0-n. Since x[n] = 0, n < N1, "", IlllllttttttTTttttttttttt''' this implies that if x[n]r0-n is abso- lutely summable, then x[n]r1 n will be n also. Sec. 10.2 The Region of Convergence for the z-Transform 751 for positive values of n and cannot cause sequence values for negative values of n to be- come unbounded, since x[n] is right sided and, in particular, x[n]z-n = 0 for n < N1• Consequently, x[n]rj 11 is absolutely summable. For right-sided sequences in general, eq. (10.3) takes the form X(z) = ~ x[n]z-n, (10.26) n=N1 where N1 is finite and may be positive or negative. If N1 is negative, then the summation in eq. (10.26) includes terms with positive powers of z, which become unbounded as lzl ~ oo. Consequently, for right-sided sequences in general, the ROC will not include infinity. However, for the particular class of causal sequences, i.e., sequences that are zero for n < 0, N1 will be nonnegative, and consequently, the ROC will include z = oo. Property 5: If x[n] is a left-sided sequence, and if the circle lzl = r0 is in the ROC, then all values of z for which 0 < lzl < r0 will also be in the ROC. Again, this property closely parallels the corresponding property for Laplace trans- forms, and the proof of it and its basis in intuition are similar to the proof and intuition for Property 4. In general, for left-sided sequences, from eq. (10.3), the summation for the z-transform will be of the form :N X(z) = Zo x[n]z- 11 , (10.27) n=-x where N2 may be positive or negative. If N2 is positive, then eq. (10.27) includes negative powers of z, which become unbounded as lzl ~ 0. Consequently, for left-sided sequences, the ROC will not in general include z = 0. However, if N2 ::; 0 (so that x[n] = 0 for all n > 0), the ROC will include z = 0. Property 6: If x[n] is two sided, and if the circle lzl = r0 is in the ROC, then the ROC will consist of a ring in the z-plane that includes the circle lzl = r0 . As with Property 6 in Section 9 .2, the ROC for a two-sided signal can be examined by expressing x[n] as the sum of a right-sided and a left-sided signal. The ROC for the right-sided component is a region bounded on the inside by a circle and extending outward to (and possibly including) infinity. The ROC for the left-sided component is a region bounded on the outside by a circle and extending inward to, and possibly including, the origin. The ROC for the composite signal includes the intersection of the ROCs of the components. As illustrated in Figure 10.8, the overlap (assuming that there is one) is a ring in the z-plane. We illustrate the foregoing properties with examples that closely parallel Exam- ples 9.6 and 9.7. 752 The z-Transform Chap. 10 9m / ' / ' / z-plane I / ' ' \ z-plane I \ I \ \ I CR-e \ ' / :' ' / (a) (b) 9m z-plane (c) Figure 10.8 (a) ROC for right-sided sequence; (b) ROC for left-sided sequence; (c) intersection of the ROCs in (a) and (b), representing the ROC for a two-sided se- quence that is the sum of the right-sided and the left-sided sequence. Example 1 0.6 Consider the signal a11 0 :::; n :::; N - 1, a > 0 x[n] = { O,' otherwise Then N-l X(z) = L anz-n n=O N-l (10.28) = L(az-'t n=O 1- (az-l)N 1 ZN- aN 1 - az- 1 - zN -t z - a · Sec. 10.2 The Region of Convergence for the z-Transform 753 Since x[h] is of finite length, it follows from Property 3 that the ROC includes the entire z- plane except possibly the origin and/or infinity. In fact, from our discussion of Property 3, since x[n] is zero for n < 0, the ROC will extend to infinity. However, since x[n] is nonzero for some positive values of n, the ROC will not include the origin. This is evident from eq. (10.28), from which we see that there is a pole of order N- 1 at z = 0. TheN roots of the numerator polynomial are at Zk = aej(27rkiN>, k = 0, 1, ... , N - 1. (10.29) The root for k = 0 cancels the pole at z = a. Consequently, there are no poles other than at the origin. The remaining zeros are at Zk = aej(27rk!N), k = 1, ... , N- 1. (10.30) The pole-zero pattern is shown in Figure 10.9. 9m z-plane (N-1 )st order pole Unit circle a Figure 1 o. 9 Pole-zero pattern for Example 10.6 with N = 16 and 0 < a < 1. The region of convergence for this example consists of all values of z except z = 0. Example 1 0. 7 Let x[n] = bini, b > 0. (10.31) This two-sided sequence is illustrated in Figure 10.10, for both b < 1 and b > 1. The z-transform for the sequence can be obtained by expressing it as the sum of a right-sided and a left-sided sequence. We have (10.32) From Example 10.1, (10.33) and from Example 10 .2, 1 lzl <b. (10.34) 754 The z-Transform Chap. 10 x[n] = bini O<b<1 n (a) x[n] =bini b>1 n (b) Figure 10.10 Sequence x[n] = bini for 0 < b < 1 and forb> 1: (a) b = 0.95; (b) b = 1.05. In Figures 10.11(a)-(d) we show the pole-zero pattern and ROC for eqs. (10.33) and (10.34), for values of b > 1 and 0 < b < 1. Forb> 1, there is no common ROC, and thus the sequence in eq. (10.31) will not have a z-transform, even though the right- sided and left-sided components do individually. Forb< 1, the ROCs in eqs. (10.33) and (10.34) overlap, and thus the z-transform for the composite sequence is 1 1 X(z) = 1-bz-I- 1-b-Iz-1' b < lzl < b' (10.35) or equivalently, b2 - 1 z 1 X(z) = -b- (z- b)(z- b- 1)' b < lzl <b. (10.36) The corresponding pole-zero pattern and ROC are shown in Figure 10.1l(e). (c) (d) ~m Unit circle "" "" / f z-plane I I 1 b I (Jl.e \ \ \ ' / "" / (e) Figure 1 o. 11 Pole-zero plots and ROCs for Example 10.7: (a) eq. (1 0.33) for b > 1; (b) eq. (10.34) forb> 1; (c) eq. (10.33) forO< b < 1; (d) eq. (10.34) for 0 < b < 1; (e) pole-zero plot and ROC for eq. (1 0.36) with 0 < b < 1. Forb> 1, the z-transform of x[n] in eq. (1 0.31) does not converge for any value of z. 756 The z-Transform Chap. 10 In discussing the Laplace transform in Chapter 9, we remarked that for a rational Laplace transform, the ROC is always bounded by poles or infinity. We observe that in the foregoing examples a similar statement applies to the z-transform, and in fact, this is always true: Property 7: If the z-transform X(z) of x[n] is rational, then its ROC is bounded by poles or extends to infinity. Combining Property 7 with Properties 4 and 5, we have Property 8: If the z-transform X(z) of x[n] is rational, and if x[n] is right sided, then the ROC is the region in the z-plane outside the outermost pole-i.e., outside the circle of radius equal to the largest magnitude of the poles of X(z). Furthermore, if x[n] is causal (i.e., if it is right sided and equal to 0 for n < 0), then the ROC also includes z = oo. Thus, for right-sided sequences with rational transforms, the poles are all closer to the origin than is any point in the ROC. Property 9: If the z-transform X(z) of x[n] is rational, and if x[n] is left sided, then the ROC is the region in the z-plane inside the innermost nonzero pole-i.e., inside the circle of radius equal to the smallest magnitude of the poles of X(z) other than any at z = 0 and extending inward to and possibly including z = 0. In particular, if x[n] is anticausal (i.e., if it is left sided and equal to 0 for n > 0), then the ROC also includes z = 0. Thus, for left-sided sequences, the poles of X(z) other than any at z = 0 are farther from the origin than is any point in the ROC. For a given pole-zero pattern, or equivalently, a given rational algebraic expression X(z), there are a limited number of different ROCs that are consistent with the preceding properties. To illustrate how different ROCs can be associated with the same pole-zero pattern, we present the following example, which closely parallels Example 9.8. Example 1 0.8 Let us consider all of the possible ROCs that can be connected with the function 1 X(z) = . (10.37) (1 - ~ z- 1 )( 1 - 2c 1) The associated pole-zero pattern is shown in Figure 10.12(a). Based on our discussion in this section, there are three possible ROCs that can be associated with this algebraic expression for the z-transform. These ROCs are indicated in Figure 10.12(b)-(d). Each corresponds to a different sequence. Figure 10.12(b) is associated with a right-sided sequence, Figure 10.12(c) with a left-sided sequence, and Figure 10.12(d) with a two- sided sequence. Since Figure 10.12(d) is the only one for which the ROC includes the unit circle, the sequence corresponding to this choice of ROC is the only one of the three for which the Fourier transform converges. Sec. 10.3 The Inverse z-Transform 757 9m 9m ' / ' "" (a) (b) 9m 9m Unit circle / / ' I ' ' z-plane I \ I CRe eRe \ I \ I / ' ' ... __ / / --' (c) (d) Figure 1 o. 12 The three possible ROCs that can be connected with the expression for the z-transform in Example 10.8: (a) pole-zero pattern for X(z); (b) pole-zero pattern and ROC if x[n] is right sided; (c) pole-zero pattern and ROC if x[n] is left sided; (d) pole-zero pattern and ROC if x[n] is two sided. In each case, the zero at the origin is a second-order zero. 10.3 THE INVERSE z-TRANSFORM In this section, we consider several procedures for determining a sequence when its z- transform is known. To begin, let us consider the formal relation expressing a sequence in terms of its z-transform. This expression can be obtained on the basis of the interpretation, developed in Section 10.1, of the z-transform as the Fourier transform of an exponentially weighted sequence. Specifically, as expressed in eq. (10.7), (10.38) for any value of r so that z = rejw is inside the ROC. Applying the inverse Fourier trans- form to both sides of eq. (10.38) yields x[n]r-n = g:-I {X(rejw)},"
10.3 The Inverse z-Transform,"Sec. 10.3 The Inverse z-Transform 757 9m 9m ' / ' "" (a) (b) 9m 9m Unit circle / / ' I ' ' z-plane I \ I CRe eRe \ I \ I / ' ' ... __ / / --' (c) (d) Figure 1 o. 12 The three possible ROCs that can be connected with the expression for the z-transform in Example 10.8: (a) pole-zero pattern for X(z); (b) pole-zero pattern and ROC if x[n] is right sided; (c) pole-zero pattern and ROC if x[n] is left sided; (d) pole-zero pattern and ROC if x[n] is two sided. In each case, the zero at the origin is a second-order zero. 10.3 THE INVERSE z-TRANSFORM In this section, we consider several procedures for determining a sequence when its z- transform is known. To begin, let us consider the formal relation expressing a sequence in terms of its z-transform. This expression can be obtained on the basis of the interpretation, developed in Section 10.1, of the z-transform as the Fourier transform of an exponentially weighted sequence. Specifically, as expressed in eq. (10.7), (10.38) for any value of r so that z = rejw is inside the ROC. Applying the inverse Fourier trans- form to both sides of eq. (10.38) yields x[n]r-n = g:-I {X(rejw)}, 758 The z-Transform Chap. 10 or x[n] = rn~-l [X(reiw)]. (10.39) Using the inverse Fourier transform expression in eq. (5.8), we have or, moving the exponential factor r"" inside the integral and combining it with the term eJwn, x[n] = -1 J X(relw. )(rel.w )'1dw. (10.40) 27T 27T That is, we can recover x[n] from its z-transform evaluated along a contour z = reiw in the ROC, with r fixed and w varying over a 27T interval. Let us now change the variable of integration from w to z. With z = reiw and r fixed, dz = jreiwdw = jzdw, or dw = (1/ j)z- 1d z. The integration in eq. (10.40) is over a 27T interval in w, which, in terms of z, corresponds to one traversal around the circle lzl = r. Consequently, in terms of an integration in the z-plane, eq. (10.40) can be rewritten as = ! '1 1 x[n] j X(z)z""- dz, (10.41) 2 where the symbol 0 denotes integration around a counterclockwise closed circular contour centered at the origin and with radius r. The value of r can be chosen as any value for which X(z) converges-i.e., any value such that the circular contour of integration lzl = r is in the ROC. Equation (10.41) is the formal expression for the inverse z-transform and is the discrete-time counterpart of eq. (9.56) for the inverse Laplace transform. As with eq. (9.56), formal evaluation of the inverse transform equation (10.41) requires the use of contour integration in the complex plane. There are, however, a number of alternative procedures for obtaining a sequence from its z-transform. As with Laplace transforms, one particularly useful procedure for rational z-transforms consists of expanding the algebraic expression into a partial-fraction expansion and recognizing the sequences associated with the individual terms. In the following examples, we illustrate the procedure. Example 1 0. 9 Consider the z-transform 3- ~z- 1 6 X(z) = lzl > *· (10.42) (1 __1 2-1 )(1 __1 z-1) , 4 3 There are two poles, one at z = 1/3 and one at z = 114, and the ROC lies outside the outermost pole. That is, the ROC consists of all points with magnitude greater than that of the pole with the larger magnitude, namely the pole at z = 1/3. From Property 4 in Section 10.2, we then know that the inverse transform is a right-sided sequence. As described in the appendix, X(z) can be expanded by the method of partial fractions. For Sec. 10.3 The Inverse z-Transform 759 this example, the partial-fraction expansion, expressed in polynomials in z- 1 , is 1 2 X(z) = 1- lz-1 + 1 - lz-1. (10.43) 4 3 Thus, x[n] is the sum of two terms, one with z-transform 11[1 - (1/4)z- 1 ] and the other with z-transform 2/[1 - (1/3)z- 1 ]. In order to determine the inverse z-transform of each of these individual terms, we must specify the ROC associated with each. Since the ROC for X(z) is outside the outermost pole, the ROC for each individual term in eq. (10.43) must also be outside the pole associated with that term. That is, the ROC for each term consists of all points with magnitude greater than the magnitude of the corresponding pole. Thus, x[n] = x1 [n] + x2[n], (10.44) where (10.45) (10.46) From Example 10.1, we can identify by inspection that x 1[ n] 1 )II = (4 u[n] (10.47) and x [n] = 2 1 )II 2 (3 u[n], (10.48) and thus, 1)11 11 x[n] = (4 u[n] + 2 (13) u[n]. (10.49) Example 1 0. 1 0 Now let us consider the same algebraic expression for X(z) as in eq. (10.42), but with the ROC for X(z) as 114 < lzl < 1/3. Equation (10.43) is still a valid partial-fraction ex- pansion of the algebraic expression for X(z), but the ROC associated with the individual terms will change. In particular, since the ROC for X(z) is outside the pole at z = 1/4, the ROC corresponding to this term in eq. (10.43) is also outside the pole and consists of all points with magnitude greater than 114, as it did in the previous example. However, since in this example the ROC for X(z) is inside the pole at z = 113, that is, since the points in the ROC all have magnitude less than 113, the ROC corresponding to this term must also lie inside this pole. Thus, the z-transform pairs for the individual components in eq. (1 0.44) are (10.50) 760 The z-Transform Chap. 10 and z 2 x,_[n] <E------;> ------=-- (10.51) 1 - !z- 1 ' 3 The signal x 1 [n] remains as in eq. (10.47), while from Example 10.2, we can identify x2[n] 1 )II = -2 (3 u[ -n- 1], (10.52) so that 11 11 x[n] = (41) u[n] - 2 (13) u[ -n- 1]. (10.53) Example 1 0. 11 Finally, consider X(z) as in eq. (10.42), but now with the ROC lzl < 1/4. In this case the ROC is inside both poles, i.e., the points in the ROC all have magnitude smaller than either of the poles at z = 1/3 or z = 1/4. Consequently the ROC for each term in the partial-fraction expansion in eq. (10.43) must also lie inside the corresponding pole. As a result, the z-transform pair for x 1 [n] is given by 1 lzl < 4' (10.54) while the z-transform pair for x2 [n] is given by eq. (10.51). Applying the result of Ex- ample 10.2 to eq. (10.54), we find that x [n] 1 )n 1 = - (4 u[ -n- 1], so that 11 11 x[n] =- (41) u[-n-1] -2 (13) u[-n-1]. The foregoing examples illustrate the basic procedure of using partial-fraction ex- pansions to determine inverse z-transforms. As with the corresponding method for the Laplace transform, the procedure relies on expressing the z-transform as a linear com- bination of simpler terms. The inverse transform of each term can then be obtained by inspection. In particular, suppose that the partial-fraction expansion of X(z) is of the form m A· X(z) = ~ 1- t -t, (10.55) i=I a1z so that the inverse transform of X(z) equals the sum of the inverse transforms of the individ- ual terms in the equation. If the ROC of X(z) is outside the pole at z = ai, the inverse trans- form of the corresponding term in eq. (10.55) is Aia?u[n]. On the other hand, if the ROC of X(z) is inside the pole at z = ai, the inverse transform of this term is -Aia?u[ -n- 1]. In general, the partial-fraction expansion of a rational transform may include terms in Sec. 10.3 The Inverse z-Transform 761 addition to the first-order terms in eq. (10.55). In Section 10.6, we list a number of other z-transform pairs that can be used in conjunction with the z-transform properties to be developed in Section 10.5 to extend the inverse transform method outlined in the preceding example to arbitrary rational z-transforms. Another very useful procedure for determining the inverse z-transform relies on a power-series expansion of X(z). This procedure is motivated by the observation that the definition of the z-transform given in eq. (10.3) can be interpreted as a power series in- volving both positive and negative powers of z. The coefficients in this power series are, in fact, the sequence values x[n]. To illustrate how a power-series expansion can be used to obtain the inverse z-transform, let us consider three examples. Example 1 0. 1 2 Consider the z-transform X(z) = 4z2 + 2 + 3z- 1 , 0 < lzl < oo. (10.56) From the power-series definition of the z-transform in eq. (10.3), we can determine the inverse transform of X(z) by inspection: 4, n = -2 i: n = 0 x[n] = n = 1 \ 0, otherwise That is, x[n] = 48[n + 2] + 28[n] + 38[n- 1]. (10.57) Comparing eqs. ( 10 .56) and ( 10 .57), we see that different powers of z serve as placehold- ers for sequence values at different points in time; i.e., if we simply use the transform pair we can immediately pass from eq. (10.56) to (10.57) and vice versa. Example 1 0. 1 3 Consider X(z) = 1 _ az-I, lzl > lal. This expression can be expanded in a power series by long division: 1 + az- 1 + a2z - 2 + · · · 1- az- 1 1 1 - az- 1 az- 1 az- 1 - a2z-2 a2z-2 762 The z-Transform Chap. 10 or --------=- = 1 + az -I + a2z - 2 + .... (10.58) 1- az- 1 The series expansion of eq. (1 0.58) converges, since lzl > Ia I, or equivalently, laz- 1 1 < 1. Comparing this equation with the definition of the z-transform in equation (10.3), we see, by matching terms in powers of z, that x[n] = 0, n < 0; x[O] = 1; x[l] = a; x[2] = a2; and in general, x[n] = anu[n], which is consistent with Example 10.1. If, instead, the ROC of X(z) is specified as lzl < lal or, equivalently, laz- 1 1 > 1, then the power-series expansion for 11(1- az- 1) in eq. (10.58) does not converge. How- ever, we can obtain a convergent power series by long division again: - az- 1 + 1 or -~---a-z-_---,-1 = -a-! z- a-2z2- .... (10.59) In this case, then, x[n] = 0, n ::::: 0; and x[ -1] = -a- 1 , x[ -2] = -a-2, ... ; that is, x[n] = -anu[ -n- 1]. This is consistent with Example 10.2. The power-series expansion method for obtaining the inverse z-transform is particu- larly useful for nonrational z-transforms, which we illustrate with the following example: Example 1 0. 14 Consider the z-transform X(z) = log(l + az- 1 ), lzl > Ia I. (10.60) With lzl > Ia I, or, equivalently, laz- 1 1 < 1, eq. ( 10 .60) can be expanded in a power series using the Taylor's series expansion Lx (-l)n+lvn log(l + v) = , Iv i < 1. (10.61) n= I n Applying this to eq. (10.60), we have (10.62) from which we can identify x[n] = { ( -l)n+ I a:, n ::::: 1 (10.63) 0, n::::; 0 Sec. 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 763 or equivalently, -(-a)n x[n] = ---u[n- 1]. n In Problem 10.63 we consider a related example with region of convergence lzl < lal. 1 0.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT In Section 10.1 we noted that the z-transform reduces to the Fourier transform for lzl = (i.e., for the contour in the z-plane corresponding to the unit circle), provided that the ROC of the z-transform includes the unit circle, so that the Fourier transform converges. In a similar manner, we saw in Chapter 9 that, for continuous-time signals, the Laplace transform reduces to the Fourier transform on the jw-axis in the s-plane. In Section 9.4, we also discussed the geometric evaluation of the continuous-time Fourier transform from the pole-zero plot. In the discrete-time case, the Fourier transform can again be evaluated geometrically by considering the pole and zero vectors in the z-plane. However, since in this case the rational function is to be evaluated on the contour lzl = 1, we consider the vectors from the poles and zeros to the unit circle rather than to the imaginary axis. To illustrate the procedure, let us consider first-order and second-order systems, as discussed in Section 6.6. 1 0.4. 1 First-Order Systems The impulse response of a first-order causal discrete-time system is of the general form (10.64) and from Example 10.1, its z-transform is 1 z H(z) = 1 - az -I z- a' lzl > lal. (10.65) For Ia I < 1, the ROC includes the unit circle, and consequently, the Fourier transform of h[n] converges and is equal to H(z) for z = ejw. Thus, the frequency response for the first-order system is (10.66) Figure 10.13(a) depicts the pole-zero plot for H(z) in eq. (10.65), including the vectors from the pole (at z = a) and zero (at z = 0) to the unit circle. With this plot, the geometric evaluation of H(z) can be carried out using the same procedure as described in Section 9.4. In particular, if we wish to evaluate the frequency response in eq. (10.65), we perform the evaluation for values of z of the form z = ejw. The magnitude of the frequency response at frequency w is the ratio of the length of the vector v 1 to the length of the vector v2 shown in Figure 10.13(a). The phase of the frequency response is the an- gle ofv1 with respect to the real axis minus the angle ofv2. Furthermore, the vector v1 from"
10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot,"Sec. 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 763 or equivalently, -(-a)n x[n] = ---u[n- 1]. n In Problem 10.63 we consider a related example with region of convergence lzl < lal. 1 0.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT In Section 10.1 we noted that the z-transform reduces to the Fourier transform for lzl = (i.e., for the contour in the z-plane corresponding to the unit circle), provided that the ROC of the z-transform includes the unit circle, so that the Fourier transform converges. In a similar manner, we saw in Chapter 9 that, for continuous-time signals, the Laplace transform reduces to the Fourier transform on the jw-axis in the s-plane. In Section 9.4, we also discussed the geometric evaluation of the continuous-time Fourier transform from the pole-zero plot. In the discrete-time case, the Fourier transform can again be evaluated geometrically by considering the pole and zero vectors in the z-plane. However, since in this case the rational function is to be evaluated on the contour lzl = 1, we consider the vectors from the poles and zeros to the unit circle rather than to the imaginary axis. To illustrate the procedure, let us consider first-order and second-order systems, as discussed in Section 6.6. 1 0.4. 1 First-Order Systems The impulse response of a first-order causal discrete-time system is of the general form (10.64) and from Example 10.1, its z-transform is 1 z H(z) = 1 - az -I z- a' lzl > lal. (10.65) For Ia I < 1, the ROC includes the unit circle, and consequently, the Fourier transform of h[n] converges and is equal to H(z) for z = ejw. Thus, the frequency response for the first-order system is (10.66) Figure 10.13(a) depicts the pole-zero plot for H(z) in eq. (10.65), including the vectors from the pole (at z = a) and zero (at z = 0) to the unit circle. With this plot, the geometric evaluation of H(z) can be carried out using the same procedure as described in Section 9.4. In particular, if we wish to evaluate the frequency response in eq. (10.65), we perform the evaluation for values of z of the form z = ejw. The magnitude of the frequency response at frequency w is the ratio of the length of the vector v 1 to the length of the vector v2 shown in Figure 10.13(a). The phase of the frequency response is the an- gle ofv1 with respect to the real axis minus the angle ofv2. Furthermore, the vector v1 from 764 The z-Transform Chap. 10 20 z-plane 10 ffi£ a= 0.5 -'IT 0 'IT w (a) (b) <tH(eiw) w Figure 1 o. 13 (a) Pole and zero vectors for the geometric determina- tion of the frequency response for a first-order system for a value of a be- tween 0 and 1; (b) magnitude of the frequency response for a = 0.95 and a = 0.5; (c) phase of the frequency (c) response for a = 0.95 and a = 0.5. the zero at the origin to the unit circle has a constant length of unity and thus has no effect on the magnitude of H(ejw). The phase contributed to H(ejw) by the zero is the angle of the zero vector with respect to the real axis, which we see is equal to w. For 0 <a< 1, the pole vector has minimum length at w = 0 and monotonically increases in l~ngth as w increases from zero to 1T. Thus, the magnitude of the frequency response will be Sec. 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 765 maximum at w = 0 and will decrease monotonically as w increases from 0 to 7T. The angle of the pole vector begins at zero and increases monotonically as w increases from zero to 7T. The resulting magnitude and phase of H(e.iw) are shown in Figures 10.13(b) and (c), respectively, for two values of a. The magnitude of the parameter a in the discrete-time first-order system plays a role similar to that of the time constant T for the continuous-time first-order system of Section 9 .4.1. Note first that, as illustrated in Figure 10 .13, the magnitude of the peak of H(e.iw) at w = 0 decreases as lal decreases toward 0. Also, as was discussed in Sec- tion 6.6.1 and illustrated in Figures 6.26 and 6.27, as Ia I decreases, the impulse response decays more sharply and the step response settles more quickly. With multiple poles, the speed of response associated with each pole is related to its distance from the origin, with those closest to the origin contributing the most rapidly decaying terms in the impulse re- sponse. This is further illustrated in the case of second-order systems, which we consider next. 1 0.4.2 Second-Order Systems Next, let us consider the class of second-order systems as discussed in Section 6.6.2, with impulse response and frequency response given in eqs. (6.64) and (6.60), which were- spectively repeat here as h[ ] = 11 Sin(n + 1)(} [ ] n r . (} un (10.67) Sill and H(ejw) = (10.68) 1 - 2r cos oe-Jw + r2e-J2w' where 0 < r < 1 and 0 ::=::: (} ::=::: 7T. Since H(e.iw) = H(z)l~=eiw, we can infer from eq. (10.68) that the system function, corresponding to the z-transform of the system impulse response, is H(z) = 1 - (2r cos O)z- 1 + r2 2 · (10.69) z- The poles of H(z) are located at (10.70) and there is a double zero at z = 0. The pole-zero plot and the pole and zero vectors with 0 < (} < 7T/2 are illustrated in Figure 10.14(a). In this case, the magnitude of the frequency response equals the square of the magnitude ofv1 (since there is a double zero at the origin) divided by the product of the magnitudes of v2 and v3• Because the length of the vector v 1 from the zero at the origin is 1 for all values of w, the magnitude of the frequency response equals the reciprocal of the product of the lengths of the two pole vectors v2 and v3 . Also, the phase of the frequency response equals twice the angle of v 1 with respect to the real axis minus the sums of the angles ofv2 and v3. In Figure 10.14(b) we show the magnitude of the frequency response for r = 0.95 and r = 0.75, while in Figure 10.14(c) we display the phase of H ( e.iw) for the same two values of r. We note in particular that, as we move 766 The z-Transform Chap. 10 z-plane Unit circle 10 r = 0.95 CRe r = 0.75 0 w (a) (b) w Figure 1 o. 14 (a) Zero vectorv1 and pole vectors v2 and v3 used in the geometric cal- culation of the frequency responses for a second-order system; (b) magnitude of the frequency response correspond- ing to the reciprocal of the product of the lengths of the pole vectors for r = 0.95 and r = 0.75; (c) phase of the frequency response for r = 0.95 (c) and r = 0.75. along the unit circle from w = 0 toward w = 7T, the length of the vector v2 first decreases and then increases, with a minimum length in the vicinity of the pole location, at w = (J. This is consistent with the fact that the magnitude of the frequency response peaks for w near 0 when the length of the vector v2 is small. Based on the behavior of the pole vectors, it is also evident that as r increases toward unity, the minimum length of the pole vectors will decrease, causing the frequency response to peak more sharply with increasing r. Also, for r near unity, the angle of the vector v2 changes sharply for w in the vicinity of e. Furthermore, from the form of the impulse response [eq. (10.67) and Figure 6.29] or the Sec. 10.5 Properties of the z-Transform 767 step response [eq. (6.67) and Figure 6.30], we see, as we did with the first-order system, that as the poles move closer to the origin, corresponding to r decreasing, the impulse response decays more rapidly and the step response settles more quickly. 10.5 PROPERTIES OF THE z-TRANSFORM As with the other transforms we have developed, the z-transform possesses a number of properties that make it an extremely valuable tool in the study of discrete-time signals and systems. In this section, we summarize many of these properties. Their derivations are analogous to the derivations of properties for the other transforms, and thus, many are left as exercises at the end of the chapter. (See Problems 10.43 and 10.51-10.54.) 1 0.5. 1 linearity If z x 1 [n] ~ X1( z), with ROC = R1, and then z ax 1 [n] + bx2[n] ~ aX1 (z) + bX2(z), with ROC containing R1 n R2. (10.71) As indicated, the ROC of the linear combination is at least the intersection of R1 and R2 . For sequences with rational z-transforms, if the poles of aX1( z) + bX2(z) consist of all of the poles of X1( z) and X2(z) (i.e., if there is no pole-zero cancellation), then the region of convergence will be exactly equal to the overlap of the individual regions of convergence. If the linear combination is such that some zeros are introduced that cancel poles, then the region of convergence may be larger. A simple example of this occurs when x1 [n] and x2 [n] are both of infinite duration, but the linear combination is of finite duration. In this case the region of convergence of the linear combination is the entire z-plane, with the possible exception of zero and/or infinity. For example, the sequences a11 u[n] and a'1u[n- 1] both have a region of convergence defined by lzl > lal, but the sequence corresponding to the difference ( a 11 u [ n] - a 11 u [ n - I]) = 8 [n ] has a region of convergence that is the entire z-plane. 1 0.5.2 Time Shifting If z x[n] ~ X(z), with ROC = R,"
10.5 Properties of the z-Transform,"Sec. 10.5 Properties of the z-Transform 767 step response [eq. (6.67) and Figure 6.30], we see, as we did with the first-order system, that as the poles move closer to the origin, corresponding to r decreasing, the impulse response decays more rapidly and the step response settles more quickly. 10.5 PROPERTIES OF THE z-TRANSFORM As with the other transforms we have developed, the z-transform possesses a number of properties that make it an extremely valuable tool in the study of discrete-time signals and systems. In this section, we summarize many of these properties. Their derivations are analogous to the derivations of properties for the other transforms, and thus, many are left as exercises at the end of the chapter. (See Problems 10.43 and 10.51-10.54.) 1 0.5. 1 linearity If z x 1 [n] ~ X1( z), with ROC = R1, and then z ax 1 [n] + bx2[n] ~ aX1 (z) + bX2(z), with ROC containing R1 n R2. (10.71) As indicated, the ROC of the linear combination is at least the intersection of R1 and R2 . For sequences with rational z-transforms, if the poles of aX1( z) + bX2(z) consist of all of the poles of X1( z) and X2(z) (i.e., if there is no pole-zero cancellation), then the region of convergence will be exactly equal to the overlap of the individual regions of convergence. If the linear combination is such that some zeros are introduced that cancel poles, then the region of convergence may be larger. A simple example of this occurs when x1 [n] and x2 [n] are both of infinite duration, but the linear combination is of finite duration. In this case the region of convergence of the linear combination is the entire z-plane, with the possible exception of zero and/or infinity. For example, the sequences a11 u[n] and a'1u[n- 1] both have a region of convergence defined by lzl > lal, but the sequence corresponding to the difference ( a 11 u [ n] - a 11 u [ n - I]) = 8 [n ] has a region of convergence that is the entire z-plane. 1 0.5.2 Time Shifting If z x[n] ~ X(z), with ROC = R, 768 The z-Transform Chap. 10 then z x[n - n0 ] ~ z -no X(z), with ROC = R, except for the possible addition or dele- (10.72) tion of the origin or infinity. Because of the multiplication by z-no, for no > 0 poles will be introduced at z = 0, which may cancel corresponding zeros of X(z) at z = 0. Consequently, z = 0 may be a pole of z-no X(z) while it may not be a pole of X(z). In this case the ROC for z-no X(z) equals the ROC of X(z) but with the origin deleted. Similarly, if n0 < 0, zeros will be introduced at z = 0, which may cancel corresponding poles of X(z) at z = 0. Consequently, z = 0 may be a zero of z-no X(z) while it may not be a pole of X(z). In this case z = oo is a pole of z-no X(z), and thus the ROC for z-no X(z) equals the ROC of X(z) but with the z = oo deleted. 1 0.5.3 Scaling in the z-Domain If z x[n] ~ X(z), with ROC= R, then zQx[n] ~ x(z:). with ROC= IZoiR. (10.73) where lzoiR is the scaled version of R. That is, if z is a point in the ROC of X(z), then the point lzolz is in the ROC of X(zizo). Also, if X(z) has a pole (or zero) at z = a, then X(zlzo) has a pole (or zero) at z = z0a. An important special case of eq. (10.73) is when zo = eiwo. In this case, lzoiR = R and (10.74) The left-hand side of eq. (10.74) corresponds to multiplication by a complex exponential sequence. The right-hand side can be interpreted as a rotation in the z-plane; that is, all pole-zero locations rotate in the z-plane by an angle of w 0 , as illustrated in Figure 10.15. This can be seen by noting that if X(z) has a factor of the form 1 - az- 1, then X(e- Jwo z) will have a factor 1- aeiwo z- 1, and thus, a pole or zero at z = a in X(z) will become a pole or zero at z = aeiwo in X(e- Jwo z). The behavior of the z-transform on the unit circle will then also shift by an angle of w 0 . This is consistent with the frequency-shifting property set forth in Section 5.3.3, where multiplication with a complex exponential in the time domain was shown to correspond to a shift in frequency of the Fourier transform. Also, in the more general case when zo = r0 eiwo in eq. (10.73), the pole and zero locations are rotated by w 0 and scaled in magnitude by a factor of r0 . Sec. 10.5 Properties of the z-Transform 769 z-plane CR-e (a) (b) Figure 1 o. 1 s Effect on the pole-zero plot of time-domain multiplica- tion by a complex exponential sequence eiwon: (a) pole-zero pattern for the z-transform for a signal x[n]; (b) pole-zero pattern for the z-transform of x[n]eiwon. 1 0.5.4 Time Reversal If z x[n] ~ X(z), with ROC = R, then z I I x[ -n] ~X(), with ROC = f?· (10.75) That is, if zo is in the ROC for x[n], then 1/zo is in the ROC for x[ -n]. 1 0.5.5 Time Expansion As we discussed in Section 5.3.7, the continuous-time concept of time scaling does not directly extend to discrete time, since the discrete-time index is defined only for integer values. However, the discrete-time concept of time expansion -i.e., of inserting a number of zeros between successive values of a discrete-time sequence x[n]--can be defined and does play an important role in discrete-time signal and system analysis. Specifically, the sequence X(k)[n], introduced in Section 5.3.7 and defined as _ { x[nlk], if n is a multiple of k X(k) [ n ] - 0, (10.76) if n is not a multiple of k has k - 1 zeros inserted between successive values of the original signal. In this case, if z x[n] ~ X(z), with ROC = R, 770 The z-Transform Chap. 10 then z k x(k)[n] ~ X(z ), with ROC = R 11 (10.77) k. That is, if z is in the ROC of X(z), then the point z11k is in the ROC of X(l). Also, if X(z) has a pole (or zero) at z = a, then X(zk) has a pole (or zero) at z = a 11k. The interpretation of this result follows from the power-series form of the z- transform, from which we see that the coefficient of the term z- n equals the value of the signal at time n. That is, with +oo X(z) = .:Z x[n]z-n, n= -oo it follows that +oo +oo X(l) = .:Z x[n](l)-n = .:Z x[n]z-kn_ (10.78) n= -oo n= -oo Examining the right-hand side of eq. (10.78), we see that the only terms that appear are of the form z-kn. In other words, the coefficient of the term z-m in this power series equals 0 if m is not a multiple of k and equals x[ml k] if m is a multiple of k. Thus, the inverse transform of eq. (10.78) is x(k)[n]. 1 0.5.6 Conjugation If z x[n] ~ X(z), with ROC = R, (10.79) then x * [n] ~z X* (z * ), with ROC= R. (10.80) Consequently, if x[n] is real, we can conclude from eq. (10.80) that X(z) = X*(z*). Thus, if X(z) has a pole (or zero) at z = z0 , it must also have a pole (or zero) at the com- plex conjugate point z = z0. For example, the transform X(z) for the real signal x[n] in Example 10.4 has poles at z = (113)e±f7TI4 . 1 0.5.7 The Convolution Property If z x 1[n] ~ X1(z), with ROC= R1, Sec. 10.5 Properties of the z-Transform 771 and then z x 1 [n] * x2[n] ~ X1 (z)X2(z), with ROC containing R1 n R2. (10.81) Just as with the convolution property for the Laplace transform, the ROC of X1( z)X2(z) includes the intersection of R1 and R2 and may be larger if pole-zero can- cellation occurs in the product. The convolution property for the z-transform can be derived in a variety of different ways. A formal derivation is developed in Problem 10.56. A derivation can also be carried out analogous to that used for the convolution property for the continuous-time Fourier transform in Section 4.4, which relied on the interpretation of the Fourier transform as the change in amplitude of a complex exponential through an LTI system. For the z-transform, there is another often useful interpretation of the convolution property. From the definition in eq. (10.3), we recognize the z-transform as a series in z- 1 where the coefficient of z-n is the sequence value x[n]. In essence, the convolution property equation (10.81) states that when two polynomials or power series X1( z) and X2(z) are multiplied, the coefficients in the polynomial representing the product are the convolution of the coefficients in the polynomials X1( z) and X2(z). (See Problem 10.57). Example 1 0. 1 5 Consider an LTI system for which y[n] = h[n] * x[n], (10.82) where h[n] = 8[n] - 8[n- 1]. Note that z 8[n]- 8[n- 1] ~ 1 - z- 1, (10.83) with ROC equal to the entire z-plane except the origin. Also, the z-transform in eq. (10.83) has a zero at z = 1. From eq. (10.81), we see that if z x[n] ~ X(z), with ROC = R, then z y[n] ~ (1- z- 1)X(z), (10.84) with ROC equal to R, with the possible deletion of z = 0 and/or addition of z = 1. Note that for this system y[n] = [o[n] - o[n- 1]] * x[n] = x[n] - x[n- 1]. 772 The z-Transform Chap. 10 That is, y[n] is the first difference ofthe sequence x[n]. Since the first-difference opera- tion is commonly thought of as a discrete-time counterpart to differentiation, eq. (10.83) can be thought of as the z-transfoi'IlJ. counterpart of the Laplace transform differentiation property presented in Section 9.5.7. Example 1 0. 16 Suppose we now consider the inverse of first differencing, namely, accumulation or sum- mation. Specifically, let w[n] be the running sum of x[n]: n w[n] = L x[k] = u[n] * x[n]. (10.85) k=-00 Then, using eq. (10.81) together with the z-transform of the unit step in Example 10.1, we see that n Z 1 w[n] = L x[k] ~ _ z-l X(z), (10.86) 1 k=-00 with ROC including at least the intersection of R with lzl > 1. Eq. (10.86) is the discrete- time z-transform counterpart of the integration property in Section 9.5.9. 10 .5.8 Differentiation in the z-Domain If z x[n] ~ X(z), with ROC = R, then z nx[n] ~ - zdX(z) with ROC = R. (10.87) dz ' This property follows in a straightforward manner by differentiating both sides of the expression for the z-transform given in eq. ( 10.3). As an example of the use of this property, let us apply it to determining the inverse z-transform considered in Example 10.14. Example 1 0. 1 7 If X(z) = log(l + az- 1 ), lzl > lal, (10.88) then z dX(z) az- 1 nx[n] ~ -z-- = + 1 lzl > lal. (10.89) dz 1 az- ' By differentiating, we have converted the z-transform to a rational expression. The inverse z-transform of the right-hand side of eq. (10.~9) can be obtained by using Exam- ple 10.1 together with the time-shifting property, eq: (~0.72), set forth in Section 10.5.2. Sec. 10.5 Properties of the z-Transform 773 Specifically, from Example 10.1 and the linearity property, z a a( -atu[n] ~ _1 , lzl > lal. (10.90) 1 + az Combining this with the time-shifting property yields 1 z az- 1 a(-at- u[n- 1] ~ _ , lzl > lal. 1 + az 1 Consequently, -( -a)n x[n] = --u[n- 1]. (10.91) n Example 1 0. 18 As another example of the use of the differentiation property, consider determining the inverse z-transform for az- 1 X(z) = (1 _ az-l )2 , lzl > lal. (10.92) From Example 10.1, z an u[n] ~ (10.93) 1- az-I' lzl > lal, and hence, (10.94) 1 0.5.9 The Initial-Value Theorem If x[n] = 0, n < 0, then x[O] = lim X(z). (10.95) z~cc This property follows by considering the limit of each term individually in the ex- pression for the z-transform, with x[n] zero for n < 0. With this constraint, X(z) = L x[n]z-n. n=O As z ~ oo, z-n ~ 0 for n > 0, whereas for n = 0, z-n = 1. Thus, eq. (10.95) follows. As one consequence of the initial-value theorem, for a causal sequence, if x[O] is finite, then limz__.oo X(z) is finite. Consequently, with X(z) expressed as a ratio of polyno- mials in z, the order of the numerator polynomial cannot be greater than the order of the denominator polynomial; or, equivalently, the number of finite zeros of X(z) cannot be greater than the number of finite poles. 774 The z-Transform Chap. 10 Example 1 0. 19 The initial-value theorem can also be useful in checking the correctness of the z- transform calculation for a signal. For example, consider the signal x[n] in Example 10.3. From eq. (10.12), we see that x[O] = 1. Also, from eq. (10.14), 1- 3 -1 limX(z) = lim 2z = 1, z->oc z->""' (1- ~z-1)(1- ~z-1) which is consistent with the initial-value theorem. 1 0.5.1 0 Summary of Properties In Table 10.1, we summarize the properties of the z-transform. 1 0.6 SOME COMMON z-TRANSFORM PAIRS As with the inverse Laplace transform, the inverse z-transform can often be easily evalu- ated by expressing X(z) as a linear combination of simpler terms, the inverse transforms of which are recognizable. In Table 10 .2, we have listed a number of useful z-transform pairs. Each of these can be developed from previous examples in combination with the proper- ties of the z-transform listed in Table 10.1. For example, transform pairs 2 and 5 follow directly from Example 10.1, and transform pair 7 is developed in Example 10.18. These, together with the time-reversal and time-shifting properties set forth in Sections 10.5.4 and 10.5.2, respectively, then lead to transform pairs 3, 6, and 8. Transform pairs 9 and 10 can be developed using transform pair 2 together with the linearity and scaling properties developed in Sections 10.5.1 and 10.5.3, respectively. 10.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING z-TRANSFORMS The z-transform plays a particularly important role in the analysis and representation of discrete-time LTI systems. From the convolution property presented in Section 10.5.7, Y(z) = H(z)X(z), (10.96) where X(z), Y(z), and H(z) are the z-transforms of the system input, output, and impulse response, respectively. H (z) is referred to as the system function or transfer function of the system. For z evaluated on the unit circle (i.e., for z = ejw), H(z) reduces to the frequency response of the system, provided that the unit circle is in the ROC for H(z). Also, from our discussion in Section 3.2, we know that if the input to an LTI system is the complex exponential signal x[n] = zn, then the output will be H(z)zn. That is, zn is an eigenfunction of the system with eigenvalue given by H(z), the z-transform of the impulse response. Many properties of a system can be tied directly to characteristics of the poles, zeros, and region of convergence of the system function, and in this section we illustrate some of these relationships by examining several important system properties and an important class of systems."
10.6 Some Common z-Transform Pairs,"774 The z-Transform Chap. 10 Example 1 0. 19 The initial-value theorem can also be useful in checking the correctness of the z- transform calculation for a signal. For example, consider the signal x[n] in Example 10.3. From eq. (10.12), we see that x[O] = 1. Also, from eq. (10.14), 1- 3 -1 limX(z) = lim 2z = 1, z->oc z->""' (1- ~z-1)(1- ~z-1) which is consistent with the initial-value theorem. 1 0.5.1 0 Summary of Properties In Table 10.1, we summarize the properties of the z-transform. 1 0.6 SOME COMMON z-TRANSFORM PAIRS As with the inverse Laplace transform, the inverse z-transform can often be easily evalu- ated by expressing X(z) as a linear combination of simpler terms, the inverse transforms of which are recognizable. In Table 10 .2, we have listed a number of useful z-transform pairs. Each of these can be developed from previous examples in combination with the proper- ties of the z-transform listed in Table 10.1. For example, transform pairs 2 and 5 follow directly from Example 10.1, and transform pair 7 is developed in Example 10.18. These, together with the time-reversal and time-shifting properties set forth in Sections 10.5.4 and 10.5.2, respectively, then lead to transform pairs 3, 6, and 8. Transform pairs 9 and 10 can be developed using transform pair 2 together with the linearity and scaling properties developed in Sections 10.5.1 and 10.5.3, respectively. 10.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING z-TRANSFORMS The z-transform plays a particularly important role in the analysis and representation of discrete-time LTI systems. From the convolution property presented in Section 10.5.7, Y(z) = H(z)X(z), (10.96) where X(z), Y(z), and H(z) are the z-transforms of the system input, output, and impulse response, respectively. H (z) is referred to as the system function or transfer function of the system. For z evaluated on the unit circle (i.e., for z = ejw), H(z) reduces to the frequency response of the system, provided that the unit circle is in the ROC for H(z). Also, from our discussion in Section 3.2, we know that if the input to an LTI system is the complex exponential signal x[n] = zn, then the output will be H(z)zn. That is, zn is an eigenfunction of the system with eigenvalue given by H(z), the z-transform of the impulse response. Many properties of a system can be tied directly to characteristics of the poles, zeros, and region of convergence of the system function, and in this section we illustrate some of these relationships by examining several important system properties and an important class of systems."
10.7 Analysis and Characterization of LTI Systems Using z-Transforms,"774 The z-Transform Chap. 10 Example 1 0. 19 The initial-value theorem can also be useful in checking the correctness of the z- transform calculation for a signal. For example, consider the signal x[n] in Example 10.3. From eq. (10.12), we see that x[O] = 1. Also, from eq. (10.14), 1- 3 -1 limX(z) = lim 2z = 1, z->oc z->""' (1- ~z-1)(1- ~z-1) which is consistent with the initial-value theorem. 1 0.5.1 0 Summary of Properties In Table 10.1, we summarize the properties of the z-transform. 1 0.6 SOME COMMON z-TRANSFORM PAIRS As with the inverse Laplace transform, the inverse z-transform can often be easily evalu- ated by expressing X(z) as a linear combination of simpler terms, the inverse transforms of which are recognizable. In Table 10 .2, we have listed a number of useful z-transform pairs. Each of these can be developed from previous examples in combination with the proper- ties of the z-transform listed in Table 10.1. For example, transform pairs 2 and 5 follow directly from Example 10.1, and transform pair 7 is developed in Example 10.18. These, together with the time-reversal and time-shifting properties set forth in Sections 10.5.4 and 10.5.2, respectively, then lead to transform pairs 3, 6, and 8. Transform pairs 9 and 10 can be developed using transform pair 2 together with the linearity and scaling properties developed in Sections 10.5.1 and 10.5.3, respectively. 10.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING z-TRANSFORMS The z-transform plays a particularly important role in the analysis and representation of discrete-time LTI systems. From the convolution property presented in Section 10.5.7, Y(z) = H(z)X(z), (10.96) where X(z), Y(z), and H(z) are the z-transforms of the system input, output, and impulse response, respectively. H (z) is referred to as the system function or transfer function of the system. For z evaluated on the unit circle (i.e., for z = ejw), H(z) reduces to the frequency response of the system, provided that the unit circle is in the ROC for H(z). Also, from our discussion in Section 3.2, we know that if the input to an LTI system is the complex exponential signal x[n] = zn, then the output will be H(z)zn. That is, zn is an eigenfunction of the system with eigenvalue given by H(z), the z-transform of the impulse response. Many properties of a system can be tied directly to characteristics of the poles, zeros, and region of convergence of the system function, and in this section we illustrate some of these relationships by examining several important system properties and an important class of systems. TABLE 10.1 PROPERTIES OF THE z-TRANSFORM Section Property Signal z-Transform ROC X(z) XJ(Z) X2(z) 10.5.1 Linearity ax1 [n] + bx2[n] aX1 (z) + bXz(z) At least the intersection of R 1 and R 2 10.5.2 Time shifting x[n- no] z-no X(z) R, except for the possible addition or deletion of the origin 10.5.3 Scaling in the z-domain ejwon x[n] X(e- jwo z) R z0x[n] x(~) zoR a 11 x[n] X(a- 1 z) Scaled version of R (i.e., [a[R = the set of points {[a[z} for z in R) 10.5.4 Time reversal x[ -n] Inverted R (i.e., R- 1 = the set of points z- 1, where z is in R) x[r], n = rk 10.5.5 Time expansion X(kl[n] = { for some integer r R 11k (i.e., the set of points z11k, where 0, n #- rk z is in R) 10.5.6 Conjugation x*[n] X*(z*) R 10.5.7 Convolution x 1[ n] * Xz[n] X1 (z)Xz(z) At least the intersection of R 1 and R2 10.5.7 First difference x[n] - x[n - 1] (1 - z- 1 )X(z) At least the intersection of R and lzl > 0 1 10.5.7 Accumulation 1 - z-1 X(z) At least the intersection of R and lzl > 1 dX(z) 10.5.8 Differentiation nx[n] -zdZ R in the z-domain 10.5.9 Initial Value Theorem If x[n] = 0 for n < 0, then x[O] = limX(z) z->x 776 The z-Transform Chap. 10 TABLE 10.2 SOME COMMON z-TRANSFORM PAIRS Signal Transform ROC 1. 8[n] Allz 2. u[n] I- z lzl >I I 1 3.-u[-n-I] I - z lzl <I I 4. 8[n- m] All z, except 0 (if m > 0) or x (if m < 0) 5. a 11 u[n] 1 - az- 1 lzl > lal 6. -a11 U[ -n- I] lzl < lal 7. na 11 u[n] lzl > lal 8. -na""u[ -n- I] (l- az-1)2 lzl < lal 1 - [cos w 1 9. [cos w n]u[n] 0]z- 0 1- [2cosw ]z- 1 + z- 2 lzl >I 0 [sinwo]z- 1 10. [sinw0 n]u[n] 1 - [2 cos wo]z- 1 + z-2 lzl >I I - [rcos w0]z 1 11. [r11 coswon]u[n] 1- [2rcoswo]z- 1 + 2z- 2 lzl > r r [r sin w ]z- 1 12. [r11 sinw0 n]u[n] 0 1- [2rcosw ]z- 1 + r2z- 2 lzl > r 0 1 0. 7. 1 Causality A causal LTI system has an impulse response h[n] that is zero for n < 0, and therefore is right-sided. From Property 4 in Section 10.2 we then know that the ROC of H(z) is the exterior of a circle in the z-plane. For some systems, e.g., if h[n] = o[n], so that H(z) = 1, the ROC can extend all the way in to and possibly include the origin. Also, in general, for a right-sided impulse response, the ROC may or may not include infinity. For example, if h[n] = o[n + 1], then H(z) = z, which has a pole at infinity. However, as we saw in Property 8 in Section 10 .2, for a causal system the power series H(z) = L h[n]z-n n=O does not include any positive powers of z. Consequently, the ROC includes infinity. Sum- marizing, we have the follow principle: A discrete-time LTI system is causal if and only if the ROC of its system function is the exterior of a circle, including infinity. Sec. 10.7 Analysis and Characterization of LTI z-Transforms 777 If H(z) is rational, then, from Property 8 in Section 10.2, for the system to be causal, the ROC must be outside the outermost pole and infinity must be in the ROC. Equivalently, the limit of H(z) as z ~ oo must be finite. As we discussed in Section 10.5.9, this is equivalent to the numerator of H(z) having degree no larger than the denominator when both are expressed as polynomials in z. That is: A discrete-time LTI system with rational system function H(z) is causal if and only if: (a) the ROC is the exterior of a circle outside the outermost pole; and (b) with H(z) expressed as a ratio of polynomials in z, the order of the numerator cannot be greater than the order of the denominator. Example 1 0.20 Consider a system with system function whose algebraic expression is = 3 2 2 + H(z) z - z z. z2 + lz + l 4 8 Without even knowing the ROC for this system, we can conclude that the system is not causal, because the numerator of H(z) is of higher order than the denominator. Example 1 0.21 Consider a system with system function 1 1 H(z) = 1 - lz-t + 1 - 2z-t, lzl > 2 (10.97) 2 Since the ROC for this system function is the exterior of a circle outside the outermost pole, we know that the impulse response is right-sided. To determine if the system is causal, we then need only check the other condition required for causality, namely that H (z), when expressed as a ratio of polynomials in z, has numerator degree no larger than the denominator. For this example, (10.98) z2 - ~z + 1' 2 so that the numerator and denominator of H(z) are both of degree two, and consequently we can conclude that the system is causal. This can also be verified by calculating the inverse transform of H(z). In particular, using transform pair 5 in Table 10.2, we find that the impulse response of this system is h[n] ~ [ (U + 2""] u[n]. (10.99) Since h[n] = 0 for n < 0, we can confirm that the system is causal. 10.7 .2 Stability As we discussed iJ1 Section 2.3.7, the stability of a discrete-time LTI system is equivalent to its impulse response being absolutely summable. In this case the Fourier transform of h[n] 778 The z-Transform Chap. 10 converges, and consequently, the ROC of H(z) must include the unit circle. Summarizing, we obtain the following result: An LTI system is stable if and only if the ROC of its system function H(z) includes the unit circle, lzl = 1. Example 1 0.22 Consider again the system function in eq. (10.97). Since the associated ROC is the region \z\ > 2, which does not include the unit circle, the system is not stable. This can also be seen by noting that the impulse response in eq. (10.99) is not absolutely summable. If, however, we consider a system whose system function has the same algebraic expression as in eq. (10.97) but whose ROC is the region 112 < \z\ < 2, then the ROC does contain the unit circle, so that the corresponding system is noncausal but stable. In this case, using transform pairs 5 and 6 from Table 10 .2, we find that the corresponding impulse response is 11 h[n] = 1 ) u[n] - 211 (2 u[ -n- 1], (10.100) which is absolutely summable. Also, for the third possible choice of ROC associated with the algebraic expression for H(z) in eq. (10.97), namely, \z\ < 112, the corresponding system is neither causal (since the ROC is not outside the outermost pole) nor stable (since the ROC does not include the unit circle). This can also be seen from the impulse response, which (using transform pair 6 in Table 10.2) is h[nj ~ - [ GJ + 2'}[ -n- 1]. As Example 10.22 illustrates, it is perfectly possible for a system to be stable but not causal. However, if we focus on causal systems, stability can easily be checked by examining the locations of the poles. Specifically, for a causal system with rational system function, the ROC is outside the outermost pole. For this ROC to include the unit circle, lzl = 1, all of the poles of the system must be inside the unit circle. That is: A causal LTI system with rational system function H(z) is stable if and only if all of the poles of H(z) lie inside the unit circle-i.e., they must all have magnitude smaller than 1. Example 1 0.23 Consider a causal system with system function 1 H(z) = 1 - az -1, which has a pole at z = a. For this system to be stable, its pole must be inside the unit circle, i.e., we must have \a\ < 1. This is consistent with the condition for the absolute summability of the corresponding impulse response h[n] = a11 u[n]. Sec. 10.7 Analysis and Characterization of LTI z-Transforms 779 Example 1 0.24 The system function for a second-order system with complex poles was given in eq. (10.69), specifically, H(z) = 1- (2rcos8)z~ 1 + (10.101) r2z-2' with poles located at z1 = rei8 and z2 = re~ JO. Assuming causality, we see that the ROC is outside the outermost pole (i.e., lzl > lrl). The pole-zero plot and ROC for this system are shown in Figure 10.16 for r < 1 and r > 1. For r < 1, the poles are inside the unit circle, the ROC includes the unit circle, and therefore, the system is stable. For r > 1, the poles are outside the unit circle, the ROC does not include the unit circle, and the system is unstable. !lm !lm Unit circle Unit circle z-plane - j z-plane ,. .. ,~-- --~X I / ' I \ I \ I I I I \ 1 : ffi..e (a) (b) Figure 1 o. 16 Pole-zero plot for a second-order system with complex poles: (a) r < 1; (b) r > 1. 1 0. 7. 3 LTI Systems Characterized by linear Constant-Coefficient Difference Equations For systems characterized by linear constant-coefficient difference equations, the proper- ties of the z-transform provide a particularly convenient procedure for obtaining the system function, frequency response, or time-domain response of the system. Let us illustrate this with an example. Example 10.25 Consider an LTI system for which the input x[n] and output y[n] satisfy the linear constant-coefficient difference equation 1 1 y[n]- ly[n- 1] = x[n] + x[n- 1]. (10.102) 3 780 The z-Transform Chap. 10 Applying the z-transform to both sides of eq. ( 10.1 02), and using the linearity property set forth in Section 10.5.1 and the time-shifting property presented in Section 10.5.2, we obtain or 1+.!3. z- 1 ] Y(z) = X(z) . (10.103) [ 1- .!.z- 1 2 From eq. (10.96), then, Y(z) 1 + .!.z- 1 H(z) = X( ) = ~ _ . (10.104) z 1 - 2z 1 This provides the algebraic expression for H(z), but not the region of convergence. In fact, there are two distinct impulse responses that are consistent with the difference equation (10.102), one right sided and the other left sided. Correspondingly, there are two different choices for the ROC associated with the algebraic expression (10.104). One, lzl > 112, is associated with the assumption that h[n] is right sided, and the other, lzl < 112, is associated with the assumption that h[n] is left sided. Consider first the choice of ROC equal to lzl > 1. Writing H(z) = ( 1 + 1 3z -1) 1 1 _ , 1- 2z I we can use transform pair 5 in Table 10.2, together with the linearity and time-shifting properties, to find the corresponding impulse response 11 h[n] = 1) u[n] + 1 (1)n-l (2 3 2 u[n- 1]. For the other choice of ROC, namely, lzl < 1, we can use transform pair 6 in Table 10.2 and the linearity and time-shifting properties, yielding 11 h[n] =- 1) (2 u[-n- 1]- 1 (1)n-l 3 2 u[-n]. In this case, the system is anticausal (h[n] = 0 for n > 0) and unstable. For the more general case of an Nth-order difference equation, we proceed in a man- ner similar to that in Example 10.25, applying the z-transform to both sides of the equation and using the linearity and time-shifting properties. In particular, consider an LTI system for which the input and output satisfy a linear constant-coefficient difference equation of the form N M L aky[n - k] = L bkx[n - k]. (10.105) k=O k=O Sec. 10.7 Analysis and Characterization of LTI z-Transforms 781 Then taking z-transforms of both sides of eq. (10.105) and using the linearity and time- shifting properties, we obtain N M .L, akz-kY(z) = .L, bkz-kX(z), k=O k=O or N M Y(z) .L, akz-k = X(z) .L, bkz-k, k=O k=O so that H( 2 ) = Y(z) (10.106) X(z) We note in particular that the system function for a system satisfying a linear constant- coefficient difference equation is always rational. Consistent with our previous example and with the related discussion for the Laplace transform, the difference equation by itself does not provide information about which ROC to associate with the algebraic expression H(z). An additional constraint, such as the causality or stability of the system, however, serves to specify the region of convergence. For example, if we know in addition that the system is causal, the ROC will be outside the outermost pole. If the system is stable, the ROC must include the unit circle. 1 0.7.4 Examples Relating System Behavior to the System Function As the previous subsections illustrate, many properties of discrete-time LTI systems can be directly related to the system function and its characteristics. In this section, we give several additional examples to show how z-transform properties can be used in analyzing systems. Example 1 0.26 Suppose that we are given the following information about an LTI system: 1. If the input to the system is x1 [n] = (116r u[n], then the output is y,[n] = HU + w(U]u[n], where a is a real number. 2. If x2[n] = ( -l)n, then the output is Y2[n] = ~( -l)n. As we now show, from these two pieces of information, we can determine the system function H(z) for this system, including the value of the number a, and can also immediately deduce a number of other properties of the system. The z-transforms of the signals specified in the first piece of information are 782 The z-Transform Chap. 10 1 X 1 ( z) = -----,---- 1 - !z-1' lzl > 6' (10.107) 6 a 10 YI(Z) = + --.,.--- 1 - ! z- 1 1 - ! z- 1 2 3 (10.108) (a+ 10)- (5 + })z- 1 1 (1 _ ! z- I )(1 _ ! z- I ) ' lzl > 2· 2 3 From eq. (10.96), it follows that the algebraic expression for the system function is Y1(z) [(a+ 10)- (5 + ~)z- 1 ][1- -1z - 1] H(z) = -- = - 6 (10.109) X1(z) (1- !z-1)(1- !z-1) 2 3 Furthermore, we know that the response to x2 [n] = ( -l)n must equal ( -l)n multiplied by the system function H(z) evaluated at z = -1. Thus from the second piece of infor- mation given, we see that 7 _ H _ _ [(a+ 10) + 5 + }][~] 1 (10.110) 4 - ( ) - ( ~ )( ~) Solving eq. (10.110), we find that a = -9, so that (1- 2z- 1)(1- !z- 1) H(z) = 6 4z- (10.111) (1- 1)(1- ~z- 1 )' or (10.112) or, finally, z2 - .!lz + ! H(z) = 6 3 (10.113) z2 - ~z + ! · 6 6 Also, from the convolution property, we know that the ROC of Y1 (z) must include at least the intersections of the ROCs of X1 (z) and H(z). Examining the three possible ROCs for H(z) (namely, lzl < 113, 113 < lzl < 112, and lzl > 112), we find that the only choice that is consistent with the ROCs of X1 (z) and Y1( z) is lzl > 112. Since the ROC for the system includes the unit circle, we know that the system is stable. Furthermore, from eq. (10.113) with H(z) viewed as a ratio of polynomials in z, the order of the numerator does not exceed that of the denominator, and thus we can conclude that the LTI system is causal. Also, using eqs. (10.112) and (10.106), we can write the difference equation that, together with the condition of initial rest, characterizes the system: 5 1 13 1 y[n]- 6y[n- 1] + 6y[n- 2] = x[n]- 6 x[n- 1] + 3x[n- 2]. Example 1 0.27 Consider a stable and causal system with impulse response h[n] and rational system function H(z). Suppose it is known that H(z) contains a pole at z = 112 and a zero somewhere on the unit circle. The precise number and locations of all of the other poles Sec. 10.8 System Function Algebra and Block Diagram Representations 783 and zeros are unknown. For each of the following statements, let us determine whether we can definitely say that it is true, whether we can definitely say that it is false, or whether there is insufficient information given to determine if it is true or not: (a) ~ { (112)Hh[nl} converges. (b) H(ejw) = 0 for some w. (c) h[n] has finite duration. (d) h[n] is real. (e) g[n] = n[h[n] * h[n]] is the impulse response of a stable system. Statement (a) is true.~ { (112)"" h[nl} corresponds to the value ofthe z-transform of h[n] at z = 2. Thus, its convergence is equivalent to the point z = 2 being in the ROC. Since the system is stable and causal, all of the poles of H(z) are inside the unit circle, and the ROC includes all the points outside the unit circle, including z = 2. Statement (b) is true because there is a zero on the unit circle. Statement (c) is false because a finite-duration sequence must have an ROC that includes the entire z-plane, except possibly z = 0 and/or z = oo. This is not consistent with having a pole at z = 112. Statement (d) requires that H(z) = H*(z*). This in tum implies that if there is a pole (zero) at a nonreallocation z = z0 , there must also be a pole (zero) at z = z~. Insufficient information is given to validate such a conclusion. Statement (e) is true. Since the system is causal, h[n] = 0 for n < 0. Conse- quently, h[n] * h[n] = 0 for n < 0; i.e., the system with h[n] * h[n] as its impulse re- sponse is causal. The same is then true for g[n] = n[h[n] * h[n]]. Furthermore, by the convolution property set forth in Section 10.5.7, the system function corresponding to the impulse response h[n] * h[n] is H 2(z), and by the differentiation property presented in Section 10.5.8, the system function corresponding to g[n] is (10.114) From eq. (10.114), we can conclude that the poles of G(z) are at the same locations as those of H (z), with the possible exception of the origin. Therefore, since H (z) has all its poles inside the unit circle, so must G(z). It follows that g[n] is the impulse response of a causal and stable system. 1 0.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS Just as with the Laplace transform in continuous time, the z-transform in discrete time allows us to replace time-domain operations such as convolution and time shifting with algebraic operations. This was exploited in Section 10.7.3, where we were able to replace the difference-equation description of an LTI system with an algebraic description. The use of the z-transform to convert system descriptions to algebraic equations is also helpful in analyzing interconnections ofLTI systems and in representing and synthesizing systems as interconnections of basic system building blocks."
10.8 System Function Algebra and Block Diagram Representations,"Sec. 10.8 System Function Algebra and Block Diagram Representations 783 and zeros are unknown. For each of the following statements, let us determine whether we can definitely say that it is true, whether we can definitely say that it is false, or whether there is insufficient information given to determine if it is true or not: (a) ~ { (112)Hh[nl} converges. (b) H(ejw) = 0 for some w. (c) h[n] has finite duration. (d) h[n] is real. (e) g[n] = n[h[n] * h[n]] is the impulse response of a stable system. Statement (a) is true.~ { (112)"" h[nl} corresponds to the value ofthe z-transform of h[n] at z = 2. Thus, its convergence is equivalent to the point z = 2 being in the ROC. Since the system is stable and causal, all of the poles of H(z) are inside the unit circle, and the ROC includes all the points outside the unit circle, including z = 2. Statement (b) is true because there is a zero on the unit circle. Statement (c) is false because a finite-duration sequence must have an ROC that includes the entire z-plane, except possibly z = 0 and/or z = oo. This is not consistent with having a pole at z = 112. Statement (d) requires that H(z) = H*(z*). This in tum implies that if there is a pole (zero) at a nonreallocation z = z0 , there must also be a pole (zero) at z = z~. Insufficient information is given to validate such a conclusion. Statement (e) is true. Since the system is causal, h[n] = 0 for n < 0. Conse- quently, h[n] * h[n] = 0 for n < 0; i.e., the system with h[n] * h[n] as its impulse re- sponse is causal. The same is then true for g[n] = n[h[n] * h[n]]. Furthermore, by the convolution property set forth in Section 10.5.7, the system function corresponding to the impulse response h[n] * h[n] is H 2(z), and by the differentiation property presented in Section 10.5.8, the system function corresponding to g[n] is (10.114) From eq. (10.114), we can conclude that the poles of G(z) are at the same locations as those of H (z), with the possible exception of the origin. Therefore, since H (z) has all its poles inside the unit circle, so must G(z). It follows that g[n] is the impulse response of a causal and stable system. 1 0.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS Just as with the Laplace transform in continuous time, the z-transform in discrete time allows us to replace time-domain operations such as convolution and time shifting with algebraic operations. This was exploited in Section 10.7.3, where we were able to replace the difference-equation description of an LTI system with an algebraic description. The use of the z-transform to convert system descriptions to algebraic equations is also helpful in analyzing interconnections ofLTI systems and in representing and synthesizing systems as interconnections of basic system building blocks. 784 The z-Transform Chap. 10 1 0.8.1 System Functions for Interconnections of LTI Systems The system function algebra for analyzing discrete-time block diagrams such as series, parallel, and feedback interconnections is exactly the same as that for the corresponding continuous-time systems in Section 9 .8.1. For example, the system function for the cascade of two discrete-time LTI systems is the product of the system functions for the individual systems in the cascade. Also, consider the feedback interconnection of two systems, as shown in Figure 10.17. It is relatively involved to determine the difference equation or im- pulse response for the overall system working directly in the time domain. However, with the systems and sequences expressed in terms of their z-transforms, the analysis involves only algebraic equations. The specific equations for the interconnection of Figure 10.17 exactly parallel eqs. (9.159)-(9.163), with the final result that the overall system function for the feedback system of Figure 10.17 is Y(z) = H(z) = H,(z) (10.115) X(z) 1 + H, (z)H2(Z). + e[n] H1(z) x[n] "" + h y[n] - 1[n] ,"" H2(z) h2[n] - Figure 10.17 Feedback intercon- nection of two systems. 1 0.8.2 Block Diagram Representations for Causal LTI Systems Described by Difference Equations and Rational System Functions As in Section 9.8.2, we can represent causal LTI systems described by difference equations using block diagrams involving three basic operations-in this case, addition, multiplica- tion by a coefficient, and a unit delay. In Section 2.4.3, we described such a block diagram for a first-order difference equation. We first revisit that example, this time using system function algebra, and then consider several slightly more complex examples to illustrate the basic ideas in constructing block diagram representations. Example 1 0.28 Consider the causal LTI system with system function (10.116) Using the results in Section 10.7.3, we find that this system can also be described by the difference equation 1 y[n] - 4 y[n - 1] = x[n], Sec. 10.8 System Function Algebra and Block Diagram Representations 785 together with the. condition of initial rest. In Section 2.4.3 we constructed a block diagram representation for a first-order system of this form, and an equivalent block diagram (corresponding to Figure 2.28 with a = -114 and b = 1) is shown in Figure 10.18(a). Here, z- 1 is the system function of a unit delay. That is, from the time-shifting property, the input and output of this system are related by w[n] = y[n- 1]. The block diagram in Figure 10.18(a) contains a feedback loop much as for the sys- tem considered in the previous subsection and pictured in Figure 10.17. In fact, with some minor modifications, we can obtain the equivalent block diagram shown in Fig- ure 10.18(b), which is exactly in the form shown in Figure 10.17, with H 1(z) = 1 and H2(z) = -114z- 1• Then, applying eq. (10.115), we can verify that the system function of the system in Figure 10.18 is given by eq. (10.116). • y[n] (a) + x[n] __. ...,.. . , + 1---------~--.- y[n] (b) Figure 1 o. 18 (a) Block diagram representations of the causal LTI system in Example 10 .28; (b) equivalent block diagram representation. Example 1 0.29 Suppose we now consider the causal LTI system with system function H(z) = 1 - 2z-I = ( 1 )(1 - 2z-1). (10.117) 1 - ! z- 1 1 - ! z- 1 4 4 As eq. (1 0.117) suggests, we can think of this system as the cascade of a system with system function 1/[1 - (1/4)z-1 ] and one with system function 1 - 2z-1• We have il- lustrated the cascade in Figure 10 .19(a), in which we have used the block diagram in Figure 10.18(a) to represent 1/[1 - (1/4)z-1 ]. We have also represented 1 - 2z-1 using a unit delay, an adder, and a coefficient multiplier. Using the time-shifting property, we then see that the input v[n] and output y[ n] of the system with the system function 1 - 2z-1 786 The z-Transform Chap. 10 are related by y[n] = v[n] - 2v[n- 1]. While the block diagram in Figure 10.19(a) is certainly a valid representation of the system in eq. ( 10 .117), it has an inefficiency whose elimination leads to an alternative block-diagram representation. To see this, note that the input to both unit delay elements in Figure 10 .19(a) is v[n], so that the outputs of these elements are identical; i.e., w[n] = s[n] = v[n- 1]. Consequently, we need not keep both of these delay elements, and we can simply use the output of one of them as the signal to be fed to both coefficient multipliers. The result is the block diagram representation in Figure 10 .19(b ). Since each unit delay element requires a memory register to store the preceding value of its input, the representation in Figure 10 .19(b) requires less memory than that in Figure 10.19(a). -----------------1 f:\ v[n] x[n] -......:-----o~~~f---------,l~:;....;:..~___,l------•~1 + 1-T-----!~ y[n] k_2 ,____..... (a) x[n] -----~1 + 1------------------~1 + 1------1~ y[n] 1 4 (b) Figure 1 o. 19 (a) Block-diagram representations for the system in Exam- ple 10 .29; (b) equivalent block-diagram representation using only one unit de- lay element. Example 1 0.30 Next, consider the second-order system function (10.118) 1 + !z- 1 - !s z-2 ' 4 which is also described by the difference equation 1 1 y[n] + 4y[n- 1]- sy[n- 2] = x[n]. (10.119) Sec. 10.8 System Function Algebra and Block Diagram Representations 787 Using the same ideas as in Example 10.28, we obtain the block-diagram representation for this system shown in Figure 10.20(a). Specifically, since the two system function blocks in this figure with system function z- 1 are unit delays, we have f[n] = y[n- 1], e[n] = f[n- 1] = y[n- 2], so that eq. (10.119) can be rewritten as 1 1 y[n] = - 4y[n- 1] + gy[n- 2] + x[n], (a) x[n] y[n] -2 ~3 (b) y[n] x[n]~ Figure 1 0.20 Block-diagram representations for the system in Exam- ple 10.30: (a) direct form; (b) cascade form; (c) parallel form. 788 The z-Transform Chap. 10 or y[n] = -41 1 f[n] + se[n] + x[n], which is exactly what the figure represents. The block diagram in Figure 10.20(a) is commonly referred to as a direct-form representation, since the coefficients appearing in the diagram can be determined by inspection from the coefficients appearing in the difference equation or, equivalently, the system function. Alternatively, as in continuous time, we can obtain both cascade- form and parallel-form block diagrams with the aid of a bit of system function algebra. Specifically, we can rewrite eq. (10.118) as H(~ = ( 1 )( 1 ) , (10.120) 1 + ~z- 1 1- ~z-1 which suggests the cascade-form representation depicted in Figure 10.20(b) in which the system is represented as the cascade of two systems corresponding to the two factors in eq. (10.120). Also, by performing a partial-fraction expansion, we obtain ~ I H(z) = 3 + 3 1 + _!_ z- I 1 - _!_ z- I ' 2 4 which leads to the parallel-form representation depicted in Figure 10.20(c). Example 10.31 Finally, consider the system function (10.121) Writing 1+~z-11 -~z-2 )( 7 - 1 1 -2) H(z)= 1-4z -2z (10.122) ( suggests representing the system as the cascade of the system in Figure 10.20(a) and the system with system function 1 - ~z- 1 - ~z- 2 • However, as in Example 10.29, the unit delay elements needed to implement the first term in eq. (10.122) also produce the de- layed signals needed in computing the output of the second system. The result is the direct-form block diagram shown in Figure 10.21, the details of the construction of which are examined in Problem 10.38. The coefficients in the direct-form representation can be determined by inspection from the coefficients in the system function of eq. (10.121). We can also write H (z) in the forms 1 H(z) = (1+ 1 l4 z- )(1- 22 - ) (10.123) 1 + ! z- 1 1 - ! z- 1 2 4 Sec. 10.9 The Unilateral z-Transform 789 y[n] Figure 1 0.21 Direct-form representation for the system in Example 10 .31. and 5/3 14/3 H(z) = 4+ -+- - (10.124) 1 _!_ z- 1 1 - _!_ z- 1 • 2 4 Eq. (10.123) suggests a cascade-form representation, while eq. (10.124) leads to a parallel-form block diagram. These are also considered in Problem 10.38. The concepts used in constructing block-diagram representations in the preceding examples can be applied directly to higher order systems, and several examples are con- sidered in Problem 10.39. As in continuous time, there is typically considerable flexibility in doing this-e.g., in how numerator and denominator factors are paired in a product rep- resentation as in eq. (10.123), in the way in which each factor is implemented, and in the order in which the factors are cascaded. While all of these variations lead to representa- tions of the same system, in practice there are differences in the behavior of the different block diagrams. Specifically, each block-diagram representation of a system can be trans- lated directly into a computer algorithm for the implementation of the system. However, because the finite word length of a computer necessitates quantizing the coefficients in the block diagram and because there is numerical roundoff as the algorithm operates, each of these representations will lead to an algorithm that only approximates the behavior of the original system. Moreover, the errors in each of these approximations will be somewhat different. Because of these differences, considerable effort has been put into examining the relative merits of the various block-diagram representations in terms of their accuracy and sensitivity to quantization effects. For discussions of this subject, the reader may tum to the references on digital signal processing in the bibliography at the end of the book. 10.9 THE UNILATERAl z-TRANSFORM The form of the z-transform considered thus far in this chapter is often referred to as the bilateral z-transform. As was the case with the Laplace transform, there is an alterna- tive form, referred to as the unilateral z-transform, that is particularly useful in analyzing causal systems specified by linear constant-coefficient difference equations with nonzero initial conditions (i.e., systems that are not initially at rest). In this section, we introduce the unilateral z-transform and illustrate some of its properties and uses, paralleling our discussion of the unilateral Laplace transform in Section 9.9."
10.9 The Unilateral z-Transform,"Sec. 10.9 The Unilateral z-Transform 789 y[n] Figure 1 0.21 Direct-form representation for the system in Example 10 .31. and 5/3 14/3 H(z) = 4+ -+- - (10.124) 1 _!_ z- 1 1 - _!_ z- 1 • 2 4 Eq. (10.123) suggests a cascade-form representation, while eq. (10.124) leads to a parallel-form block diagram. These are also considered in Problem 10.38. The concepts used in constructing block-diagram representations in the preceding examples can be applied directly to higher order systems, and several examples are con- sidered in Problem 10.39. As in continuous time, there is typically considerable flexibility in doing this-e.g., in how numerator and denominator factors are paired in a product rep- resentation as in eq. (10.123), in the way in which each factor is implemented, and in the order in which the factors are cascaded. While all of these variations lead to representa- tions of the same system, in practice there are differences in the behavior of the different block diagrams. Specifically, each block-diagram representation of a system can be trans- lated directly into a computer algorithm for the implementation of the system. However, because the finite word length of a computer necessitates quantizing the coefficients in the block diagram and because there is numerical roundoff as the algorithm operates, each of these representations will lead to an algorithm that only approximates the behavior of the original system. Moreover, the errors in each of these approximations will be somewhat different. Because of these differences, considerable effort has been put into examining the relative merits of the various block-diagram representations in terms of their accuracy and sensitivity to quantization effects. For discussions of this subject, the reader may tum to the references on digital signal processing in the bibliography at the end of the book. 10.9 THE UNILATERAl z-TRANSFORM The form of the z-transform considered thus far in this chapter is often referred to as the bilateral z-transform. As was the case with the Laplace transform, there is an alterna- tive form, referred to as the unilateral z-transform, that is particularly useful in analyzing causal systems specified by linear constant-coefficient difference equations with nonzero initial conditions (i.e., systems that are not initially at rest). In this section, we introduce the unilateral z-transform and illustrate some of its properties and uses, paralleling our discussion of the unilateral Laplace transform in Section 9.9. 790 The z-Transform Chap. 10 The unilateral z-transform of a sequence x[ n] is defined as ~(z) = L x[n]z-n. (10.125) n=O As in previous chapters, we adopt a convenient shorthand notation for a signal and its unilateral z-transform: 'UZ x[n] ~ ~(z) = 11Z{ x[nJ}. (10.126) The unilateral z-transform differs from the bilateral transform in that the summation is carried out only over nonnegative values of n, whether or not x[n] is zero for n < 0. Thus the unilateral z-transform of x[n] can be thought of as the bilateral transform of x[n]u[n] (i.e., x[n] multiplied by a unit step). In particular, then, for any sequence that is zero for n < 0, the unilateral and bilateral z-transforms will be identical. Referring to the discussion of regions of convergence in Section 10.2, we also see that, since x[n]u[n] is always a right-sided sequence, the region of convergence of ~(z) is always the exterior of a circle. Because of the close connection between bilateral and unilateral z-transforms, the calculation of unilateral transforms proceeds much as for bilateral transforms, with the caveat that we must take care to limit the range of summation in the transform to n ~ 0. Similarly, the calculation of inverse unilateral transforms is basically the same as for bilat- eral transforms, once we take into account the fact that the ROC for a unilateral transform is always the exterior of a circle. 1 0.9.1 Examples of Unilateral z-Transforms and Inverse Transforms Example 1 0.32 Consider the signal x[n] = anu[n]. (10.127) Since x[ n] = 0, n < 0, the unilateral and bilateral transforms are equal for this example, and thus, in particular, X(z) = 1 - art, lzl > lal. (10.128) Example 1 0.33 Let x[n] = an+ 1 u[n + 1]. (10.129) In this case the unilateral and bilateral transforms are not equal, since x[ -1] = 1 ¥- 0. The bilateral transform is obtained from Example 10.1 and the time-shifting property set forth in Section 10.5.2. Specifically, z X(z) = 1 - az -t, lzl > lal. (10.130) Sec. 10.9 The Unilateral z-Transform 791 In contrast, the unilateral transform is ~(z) = L x[n]z-n n=O X = Lan+lz-"", n=O or ~(z) = a 1- az- 1 lzl > lal. (10.131) ' Example 1 0.34 Consider the unilateral z-transform (10.132) In Example 10.9, we considered the inverse transform for a bilateral z-transform X(z) of the same form as in eq. (10.132) and for several different ROCs. In the case of the unilateral transform, the ROC must be the exterior of the circle of radius equal to the largest magnitude of the poles of ~(z)-in this instance, all points z with lzl > 1/3. We can then invert the unilateral transform exactly as in Example 10.9, yielding x[n] = (41) "" u[n] + 2 (13) "" u[n] for n 2: 0. (10.133) In eq. (10.133), we have emphasized the fact that inverse unilateral z-transforms provide us with information about x[n] only for n 2: 0. Another approach to inverse transforms introduced in Section 10.3, namely, iden- tifying the inverse transforms from the coefficients in the power-series expansion of the z-transform, also can be used for unilateral transforms. However, in the unilateral case, a constraint which must be satisfied is that, as a consequence of eq. (10.125), the power- series expansion for the transform cannot contain terms with positive powers of z. For instance, in Example 10.13 we performed long division on the bilateral transform 1 X(z) = 1 -t (10.134) - az in two ways, corresponding to the two possible ROCs for X(z). Only one of these choices, namely, that corresponding to the ROC lzl > Ia!, led to a series expansion without positive powers of z, i.e., (10.135) 1 - az-I 792 The z-Transform Chap. 10 and this is the only choice for the expansion if eq. (1 0.134) represents a unilateral trans- form. Note that the requirement that X(z) have a power-series expansion with no terms with positive powers of z implies that not every function of z can be a unilateral z-transform. In particular, if we consider a rational function of z written as a ratio of polynomials in z (not in z~ 1) , i.e., p(z) (10.136) q(z)' then for this to be a unilateral transform (with the appropriately chosen ROC as the ex- terior of a circle), the degree of the numerator must be no bigger than the degree of the denominator. Example 1 0.35 A simple example illustrating the preceding point is given by the rational function in eq. (10.130), which we can write as a ratio of polynomials in z: z2 (10.137) z-a There are two possible bilateral transforms that can be associated with this function, namely those corresponding to the two possible ROCs, lzl < lal and lzl > lal. The choice lzl > Ia Ic orresponds to a right-sided sequence, but not to a signal that is zero for all n < 0, since its inverse transform, which is given by eq. (10.129), is nonzero for n = -1. More generally, if we associate eq. (10.136) with the bilateral transform with the ROC that is the exterior of the circle with radius given by the magnitude of the largest root of q(z), then the inverse transform will certainly be right sided. However, for it to be zero for all n < 0, it must also be the case that degree(p(z)) ::::; degree(q(z)). 1 0.9.2 Properties of the Unilateral z-Transform The unilateral z-transform has many important properties, some of which are identical to their bilateral counterparts and several of which differ in significant ways. Table 10.3 sum- marizes these properties. Note that we have not included a column explicitly identifying the ROC for the unilateral z-transform for each signal, since the ROC of any unilateral z- transform is always the exterior of a circle. For example, the ROC for a rational unilateral z-transform is always outside the outermost pole. By contrasting this table with the corresponding Table 10.1 for bilateral z-transforms, we can gain considerable insight into the nature of the unilateral transform. In particular, several properties-namely, linearity, scaling in the z-domain, time expansion, conjuga- tion, and differentiation in the z-domain -are identical to their bilateral counterparts, as is the initial-value theorem stated in Section 10.5.9, which is fundamentally a unilateral transform property, since it requires x[n] = 0 for n < 0. One bilateral property, namely, the time-reversal property set forth in Section 10.5.4, obviously has no meaningful coun- terpart for the unilateral transform, while the remaining properties differ in important ways between the bilateral and unilateral cases. Sec. 10.9 The Unilateral z-Transform 793 TABLE 10.3 PROPERTIES OF THE UNILATERAL z-TRANSFORM Property Signal Unilateral z-Transform x[n] x 1[n] x2[n] Linearity ax1 [n] + bx2[n] a~, (z) + b~2(z) Time delay x[n- 1] z-'~(z) + x[ -1] Time advance x[n + 1] z~(z) - zx[O] Scaling in the z-domain e1w 011 x[n] ~(e- Jwo z) z0x[n] ~(z/zo) 11 Q X[n] ~(a- 1 z) Time expansion xk[n] = { x[m], n = mk ~(zk) 0, n =I= mk for any m Conjugation x*[n] ~*(z*) Convolution (assuming x 1[ n] * x2[n] ~,(z)~2(z) that x 1 [n] and x2 [n] are identically zero for n < 0) First difference x[n] - x[n- 1] (1- z- 1 )~(z)- x[-1] 1 Accumulation 1 - z-I ~(z) d~(z) Differentiation in the nx[n] - z-----;IZ z-domain Initial Value Theorem x[O] = lim ~(z) 7--'+X Let us examine the difference in the convolution property first. Table 10.3 states that if XI [n] = x2 [n] = 0 for all n < 0, then (10.138) Since in this case the unilateral and bilateral transforms are identical for each of these signals, eq. (10.138) follows from the bilateral convolution property. Thus, the system analysis and system function algebra developed and used in this chapter apply without change to unilateral transforms, as long as we are considering causal LTI systems (for which the system function is both the bilateral and the unilateral transform of the impulse response) with inputs that are identically zero for n < 0. An exa~ple of such application is to the accumulation or summation property in Table 10.3. Specifically, if x[n] = 0 for n < 0, then Ln 'UZ · 1 x[k] = x[n] * u[n] ~ X(z)'U(z) = X(z) __ . (10.139) k=O 1 z 1 As a second example, consider the following: 794 The z-Transform Chap. 10 Example 1 0.36 Consider the causal LTI system described by the difference equation y[n] + 3y[n- 1] = x[n], (10.140) together with the condition of initial rest. The system function for this system is 1 J{(z) = 1 + 3z- • (10.141) 1 Suppose that the input to the system is x[n] = au[n], where a is a given constant. In this case, the unilateral (and bilateral) z-transform of the output y[n] is 'Y(z) = J{( z)X(z) = (1 + 3z-l~(l - z-1) (10.142) = (3/4)a + (1/4)a . 1+3z-1 1-z-1 Applying Example 10.32 to each term of eq. (10.142) yields y[n] ~ a[~ + (~ )<-3)"" ]u[n] (10.143) An important point to note here is that the convolution property for unilateral z- transforms applies only if the signals x 1 [n] and x2[n] in eq. (10.138) are both identically zero for n < 0. While it is generally true that the bilateral transform of x 1 [n] * x2[n] equals the product of the bilateral transforms of x1 [n] and x2 [n], the unilateral transform of x1 [n]* x2 [n] in general does not equal the product of the unilateral transforms if x 1 [n] or x2 [n] is nonzero for n < 0. This point is explored further in Problem 10 .41. Much of the importance of the unilateral z-transform lies in its application to analyz- ing causal systems and, in particular, systems characterized by linear constant- coefficient difference equations with possibly nonzero initial conditions. In Section 10.7 we saw how the bilateral transform-particularly the shifting property for bilateral z-transforms- could be used to analyze and compute solutions for LTI systems characterized by such difference equations, together with the assumption of initial rest. As we will now see, the shifting property for unilateral transforms, which differs from its bilateral counterpart, plays an analogous role for initialized systems. To develop the shifting property for the unilateral transform, consider the signal y[n] = x[n- 1]. (10.144) Then 00 'Y(z) = L x[n- 1]z-n n=O = x[ -1] + L x[n- 1]z-n n=l 00 = x[ -1] + L x[n]z-(n+l), n=O Sec. 10.9 The Unilateral z-Transform 795 or 'Y(z) = x[ -1] + z -I~ x[n]z-n, (10.145) n=O so that 'Y(z) = x[-1] + z- 1X(z). (10.146) By repeated application of eq. (10.146), the unilateral transform of w[n] = y[n - 1] = x[n - 2] (10.147) is W(z) = x[ -2] + x[ -l]z- 1 + z-2X(z). (10.148) Continuing this iterative procedure, we can also determine the unilateral transform of x[n- m] for any positive value of m. Eq. (10.146) is sometimes referred to as the time delay property, since y[n] in eq. (10.144) is a delayed version of x[n]. There is also a time advance property for unilateral transforms that relates the transform of an advanced version of x[n] to X(z). Specifically, as shown in Problem 10.60, 'UZ x[n + 1] ~ zX(z) - zx[O]. (10.149) 1 0.9.3 Solving Difference Equations Using the Unilateral z-Transform The following example illustrates the use of unilateral z-transforms and the time delay property to solve linear constant-coefficient difference equations with nonzero initial con- ditions: Example 1 0.37 Consider again the difference equation (10.140) with x[n] = au[n] and with the initial condition y[-1] = {3. (10.150) Applying the unilateral transform to both sides of eq. ( 10 .140) and using the linearity and time delay properties, we obtain (10.151) Solving for 'Y(z) yields cy z = _ 3{3 + a ( ) 1 + 3z- 1 (1 + 3z- z- (10.152) 1)(1- 1)' 796 The z-Transform Chap. 10 Referring to Example 10.36 and, in particular, eq. (10.142), we see that the second term on the right-hand side of eq. (10.152) equals the unilateral z-transform of there- sponse of the system when the initial condition in eq. (10.150) is zero (/3 = 0). That is, this term represents the response of the causal LTI system described by eq. (10.140), to- gether with the condition of initial rest. As in continuous-time, this response is frequently referred to as the zero-state response, i.e., the response when the initial condition or state is zero. The first term on the right-hand side of eq. (10.152) is interpreted as the unilateral transform of the zero-input response-i.e., the response of the system when the input is zero (a = 0). The zero-input response is a linear function of the value f3 of the ini- tial condition. Moreover, eq. (10.152) illustrates the fact that the solution of a linear constant-coefficient difference equation with nonzero initial state is the superposition of the zero-state a~d zero-input responses. The zero-state response, obtained by setting the initial condition to zero, corresponds to the response of the causal LTI system defined by the difference equation and the condition of initial rest. The zero-input response is there- sponse to the initial condition alone with the input set to zero. Problems 10.20 and 10.42 provide other examples illustrating the use of unilateral transforms to solve difference equations with nonzero initial conditions. Finally, for any values of a and {3, we can expand cy(z) in eq. (10.152) by the method of partial fractions and invert the result to obtain y[n]. For example, if a = 8 and {3 = 1, 3 2 cy(z) = 1 + 3z- (10.153) 1 + 1 - z- 1 ' and applying the unilateral transform pair in Example 10.32 to each term yields y[n] = [3(-3t + 2]u[n], for n 2 0. (10.154) 1 0. 1 0 SUMMARY In this chapter, we have developed the z-transform for discrete-time signals and systems. The discussion and development closely paralleled the corresponding treatment of the Laplace transform for continuous-time signals, but with some important differences. For example, in the complex s-plane the Laplace transform reduces to the Fourier transform on the imaginary axis, whereas in the complex z-plane the z-transform reduces to the Fourier transform on the unit circle. For the Laplace transform the ROC consists of a strip or half-plane (i.e., a strip extending to infinity in one direction), whereas for the z-transform the ROC is a ring, perhaps extending outward to infinity or inward to include the origin. As with the Laplace transform, time-domain characteristics such as the right -sided, left- sided, or two-sided nature of a sequence and the causality or stability of an LTI sy&tem can be associated with properties of the region of convergence. In particular, for rational z-transforms, these time-domain characteristics can be associated with the pole locations in relation to the region of convergence. Because of the properties of z-transforms, LTI systems, including those described by linear constant-coefficient difference equations, can be analyzed in the transform do- main by algebraic manipulations. System function algebra also is a very usefpl tool for the analysis of interconnections of LTI systems and for the construction of block diagram representations of LTI systems described qy difference equations."
10.10 Problems,"796 The z-Transform Chap. 10 Referring to Example 10.36 and, in particular, eq. (10.142), we see that the second term on the right-hand side of eq. (10.152) equals the unilateral z-transform of there- sponse of the system when the initial condition in eq. (10.150) is zero (/3 = 0). That is, this term represents the response of the causal LTI system described by eq. (10.140), to- gether with the condition of initial rest. As in continuous-time, this response is frequently referred to as the zero-state response, i.e., the response when the initial condition or state is zero. The first term on the right-hand side of eq. (10.152) is interpreted as the unilateral transform of the zero-input response-i.e., the response of the system when the input is zero (a = 0). The zero-input response is a linear function of the value f3 of the ini- tial condition. Moreover, eq. (10.152) illustrates the fact that the solution of a linear constant-coefficient difference equation with nonzero initial state is the superposition of the zero-state a~d zero-input responses. The zero-state response, obtained by setting the initial condition to zero, corresponds to the response of the causal LTI system defined by the difference equation and the condition of initial rest. The zero-input response is there- sponse to the initial condition alone with the input set to zero. Problems 10.20 and 10.42 provide other examples illustrating the use of unilateral transforms to solve difference equations with nonzero initial conditions. Finally, for any values of a and {3, we can expand cy(z) in eq. (10.152) by the method of partial fractions and invert the result to obtain y[n]. For example, if a = 8 and {3 = 1, 3 2 cy(z) = 1 + 3z- (10.153) 1 + 1 - z- 1 ' and applying the unilateral transform pair in Example 10.32 to each term yields y[n] = [3(-3t + 2]u[n], for n 2 0. (10.154) 1 0. 1 0 SUMMARY In this chapter, we have developed the z-transform for discrete-time signals and systems. The discussion and development closely paralleled the corresponding treatment of the Laplace transform for continuous-time signals, but with some important differences. For example, in the complex s-plane the Laplace transform reduces to the Fourier transform on the imaginary axis, whereas in the complex z-plane the z-transform reduces to the Fourier transform on the unit circle. For the Laplace transform the ROC consists of a strip or half-plane (i.e., a strip extending to infinity in one direction), whereas for the z-transform the ROC is a ring, perhaps extending outward to infinity or inward to include the origin. As with the Laplace transform, time-domain characteristics such as the right -sided, left- sided, or two-sided nature of a sequence and the causality or stability of an LTI sy&tem can be associated with properties of the region of convergence. In particular, for rational z-transforms, these time-domain characteristics can be associated with the pole locations in relation to the region of convergence. Because of the properties of z-transforms, LTI systems, including those described by linear constant-coefficient difference equations, can be analyzed in the transform do- main by algebraic manipulations. System function algebra also is a very usefpl tool for the analysis of interconnections of LTI systems and for the construction of block diagram representations of LTI systems described qy difference equations. Chap. 10 Problems 797 For the most part in this chapter, we have focused on bilateral z-transforms. However, as with the Laplace transform, we have also introduced a second form of the z-transform known as the unilateral z-transform. The unilateral transform, which can be viewed as the bilateral transform of a signal whose values for n < 0 have been set to zero, is particularly useful for analyzing systems described by linear constant-coefficient difference equations with nonzero initial conditions. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 10.1. Determine the constraint on r = \z\ for each of the following sums to converge: (a) ~ (~)n+1 2 -n (b) ~(~)-n+lzn n=-1 n=l (c) ~{ l+(~l)""}z-n (d) ~ (4)1n1 cos(in)z-n n=O n= -oc 10.2. Consider the signal x[n] = S1 ) II ( u[n - 3]. Use eq. (10.3) to evaluate the z-transform of this signal, and specify the corre- sponding region of convergence. 10.3. Let Determine the constraints on the complex number a and the integer n0 , given that the ROC of X(z) is 1 < \z\ < 2. 10.4. Consider the signal x[n] = { C1)n cos( in), n~O 0, n>O Determine the poles and ROC for X(z). 10.5. For each of the following algebraic expressions for the z-transform of a signal, determine the number of zeros in the finite z-plane and the number of zeros at infinity."
Problems,"Chap. 10 Problems 797 For the most part in this chapter, we have focused on bilateral z-transforms. However, as with the Laplace transform, we have also introduced a second form of the z-transform known as the unilateral z-transform. The unilateral transform, which can be viewed as the bilateral transform of a signal whose values for n < 0 have been set to zero, is particularly useful for analyzing systems described by linear constant-coefficient difference equations with nonzero initial conditions. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 10.1. Determine the constraint on r = \z\ for each of the following sums to converge: (a) ~ (~)n+1 2 -n (b) ~(~)-n+lzn n=-1 n=l (c) ~{ l+(~l)""}z-n (d) ~ (4)1n1 cos(in)z-n n=O n= -oc 10.2. Consider the signal x[n] = S1 ) II ( u[n - 3]. Use eq. (10.3) to evaluate the z-transform of this signal, and specify the corre- sponding region of convergence. 10.3. Let Determine the constraints on the complex number a and the integer n0 , given that the ROC of X(z) is 1 < \z\ < 2. 10.4. Consider the signal x[n] = { C1)n cos( in), n~O 0, n>O Determine the poles and ROC for X(z). 10.5. For each of the following algebraic expressions for the z-transform of a signal, determine the number of zeros in the finite z-plane and the number of zeros at infinity. 798 The z-Transform Chap. 10 z-2(1- z-1) (c) (1- ±z- 1)(1 + ±z- 1) 10.6. Let x[n] be an absolutely summable signal with rational z-transform X(z). If X(z) is known to have a pole at z = 112, could x[n] be (a) a finite-duration signal? (b) a left- sided signal? (c) a right-sided signal? (d) a two-sided signal? 10.7. Suppose that the algebraic expression for the z-transform of x[n] is 1 - lz-2 X(z) = 4 . (1 + ±z-2)(1 + iz-1 + ~z-2) How many different regions of convergence could correspond to X(z)? 10.8. Let x[n] be a signal whose rational z-transform X(z) contains a pole at z = 112. Given that x 1 [n] ~ (U x[n] is absolutely summable and = S1 ) fl x2[n] ( x[n] is not absolutely summable, determine whether x[n] is left sided, right sided, or two sided. 10.9. Using partial-fraction expansion and the fact that z 1 anu[n] ~ - az _1 , lzl > lal, 1 find the inverse z-transform of 10.10. Consider the following algebraic expression for the z-transform X(z) of a signal x[n]: 1 + z- 1 X (z) = --+-- =--- 1 lz-1. 3 Chap. 10 Problems 799 (a) Assuming the ROC to be lzl > 1/3, use long division to determine the values of x[O], x[1], and x[2]. (b) Assuming the ROC to be lzl < 113, use long division to determine the values of x[O], x[ -1], and x[ -2]. 10.11. Find the inverse z-transform of 10 1 [ 1' 024 - z- ] X(z) = 1,024 1 - ~z-1 , lzl > 0. 10.12. By considering the geometric interpretation of the magnitude of the Fourier trans- form from the pole-zero plot, determine, for each of the following z-transforms, whether the corresponding signal has an approximately lowpass, bandpass, or highpass characteristic: -1 (a) X(z) = z 8 _ , lzl > ~ 1 + gZ 1 1 + _§_z- 1 (b) X(z) = 9 + lzl > ~ 1 - ~z- 1 ~z-2 ' 9 81 (c) X(z) = 1 _ , I 64 zl > 98 1 + SfZ 2 10.13. Consider the rectangular signal O::sn::s5 x[n] = { ~ otherwise Let g[n] = x[n] - x[n - 1]. (a) Find the signal g[n] and directly evaluate its z-transform. (b) Noting that n x[n] = L g[k], k=-X use Table 10.1 to determine the z-transform of x[n]. 10.14. Consider the triangular signal n-1 2::sn::s7 g[n] = 13- ~. 8 :::; n :::; 12 . { 0, otherwise (a) Determine the value of no such that g[n] = x[n] * x[n - no], where x[n] is the rectangular signal considered in Problem 10.13. (b) Use the convolution and shift properties in conjunction with X(z) found in Problem 10.13 to determine G(z). Verify that your answer satisfies the initial- value theorem. 800 The z-Transform Chap. 10 10.15. Let y[n] = g1 )II ( u[n]. Determine two distinct signals such that each has a z-transform X(z) which satisfies both of the following conditions: 1. [X(z) +X(- z)]/2 = Y(z2). 2. X(z) has only one pole and only one zero in the z-plane. 10.16. Consider the following system functions for stable LTI systems. Without utilizing the inverse z-transform, determine in each case whether or not the corresponding system is causal. 1 4 -1 + I -2 (a) - 3z 2z z- 1(1- 4z- 1)(1- ~z- 1 ) z-! (b) 2 z2 + I z _ 3 2 16 z + 1 (c) z +:!- !z-2 - ~z- 3 3 2 3 10.17. Suppose we are given the following five facts about a particular LTI systemS with impulse response h[n] and z-transform H(z): 1. h[n] is real. 2. h[n] is right sided. 3. limH(z) = 1. z---->oo 4. H(z) has two zeros. 5. H (z) has one of its poles at a nonreallocation on the circle defined by lzl = 3/4. Answer the following two questions: (a) IsS causal? (b) IsS stable? 10.18. Consider a causal LTI system whose input x[n] and output y[n] are related through the block diagram representation shown in Figure Pl0.18. x[n] ~8-----•~l----~·~~ y[n] Figure P1 0.18 (a) Determine a difference equation relating y[n] and x[n]. (b) Is this system stable? 10.19. Determine the unilateral z-transform of each of the following signals, and specify the corresponding regions of convergence: Chap. 10 Problems 801 (a) XJ [n] = Ci)nu[n + 5] (b) x2[n] = S[n + 3] + S[n] + 2nu[ -n] (c) x3[n] = (4)1nl 10.20. Consider a system whose input x[n] and output y[n] are related by y[n - 1] + 2y[n] = x[n]. (a) Determine the zero-input response of this system if y[ -1] = 2. (b) Determine the zero-state response of the system to the input x[ n] = ( 1/4) n u [ n]. (c) Determine the output of the system for n 2: 0 when x[n] = (114)nu[n] and y[ -1] = 2. BASIC PROBLEMS 10.21. Determine the z-transform for each of the following sequences. Sketch the pole- zero plot and indicate the region of convergence. Indicate whether or not the Fourier transform of the sequence exists. (a) S[n + 5] (b) S[n- 5] (c) ( -l)nu[n] (d) C4Y+ 1 u[n + 3] (e) (-~)nu[-n- 2] (0 Ci)nu[3- n] (g) 2nu[ -n] + (i)nu[n- 1] (h) (~)n-2u[n- 2] 10.22. Determine the z-transform for the following sequences. Express all sums in closed form. Sketch the pole-zero plot and indicate the region of convergence. Indicate whether the Fourier transform of the sequence exists. (a) (4)n{u[n + 4]- u[n- 5]} (b) n(4)1nl (c) lnl(4)1nl (d) 4n case; n + *]u[ -n- 1] 10.23. Following are several z-transforms. For each one, determine the inverse z-transform using both the method based on the partial-fraction expansion and the Taylor's se- ries method based on the use of long division. 1- z- 1 1 X(z) = 1-.!.z-2' lzl > 2· 4 1- z- 1 1 X(z) = 1- .!.z-2' lzl < 2· 4 z-1-.!. 2 1 X(z) = 1- .!.z- 1' lzl > 2· 2 z-1-.!. X(z) = 2 1 1- .!.z- 1' lzl < 2· 2 z- 1 -.!. 1 X(z) = (1 - 4z-~)2' lzl > 2. z- 1 -.!. 1 X(z) = (1- 4z-~)2' lzl < 2. 802 The z-Transform Chap. 10 10.24. Using the method indicated, determine the sequence that goes with each of the following z-transforms: (a) Partial fractions: X(z) = + ~z-I + z_ 2, and x[n] is absolutely summable. 1 (b) Long division: 1 - !z- 1 X(z) = 2 and x[n] is right sided. 1 + !z- 1' 2 (c) Partial fractions: 3 X(z) = _ , and x[n] is absolutely summable. z--1 4 - -18 z 1 10.25. Consider a right-sided sequence x[n] with z-transform 1 X(z) = • (P10.25-1) ( 1 - 21 z-1 )(I - z-1) (a) Carry out a partial-fraction expansion of eq. (P10.25-1) expressed as a ratio of polynomials in z- 1, and from this expansion, determine x[n]. (b) Rewrite eq. (P10.25-1) as a ratio of polynomials in z, and carry out a partial- fraction expansion of X(z) expressed in terms of polynomials in z. From this expansion, determine x[n], and demonstrate that the sequence obtained is identical to that obtained in part (a). 10.26. Consider a left-sided sequence x[n] with z-transform 1 X(z) = . ( 1 - ~ z-1 )(1 - z- I) (a) Write X(z) as a ratio of polynomials in z instead of z- 1• (b) Using a partial-fraction expression, express X(z) as a sum of terms, where each term represents a pole from your answer in part (a). (c) Determine x[n]. 10.27. A right-sided sequence x[n] has z-transform 3z- 10 + z-7 - sz-2 + 4z-l + 1 X(z) = Determine x[n] for n < 0. 10.28. (a) Determine the z-transform of the sequence x[n] = B[n] - 0.95 B[n- 6]. (b) Sketch the pole-zero pattern for the sequence in part (a). (c) By considering the behavior of the pole and zero vectors as the unit circle is traversed, develop an approximate sketch of the magnitude of the Fourier transform of x[n]. 10.29. By considering the geometric determination of the frequency response as discussed in Section 10.4, sketch, for each of the pole-zero plots in Figure P10.29, the mag- nitude of the associated Fourier transform. Chap. 10 Problems 803 9m 9m Unit circle CRc (a) (b) 9m 9m Unit circle CRc CRc (c) 9m (d) eRe Circle of radius 0.9 (e) Figure P1 0.29 804 The z-Transform Chap. 10 10.30. Consider a signal y[n] which is related to two signals x 1 [n] and x2 [n] by y[n] = x,[n + 3] * x2 [-n + 1] where x, [n] = (4 )"" u[n] and x2[n] = (~ )"" u[n]. Given that z an u[n] ~ 1 - az-t' lzl > Ia I, use properties of the z-transform to determine the z-transform Y(z) of y[n]. 10.31. We are given the following five facts about a discrete-time signal x[n] with z- transform X(z): 1. x[n] is real and right-sided. 2. X(z) has exactly two poles. 3. X(z) has two zeros at the origin. 4. X(z) has a pole at z = ~ej7T13 • 5. X(l) = ~· Determine X(z) and specify its region of convergence. 10.32. Consider an LTI system with impulse response ~~· n2:0 h[n] = { n<O and input Q:::;n:::;N-1 x[n] = { ~: otherwise (a) Determine the output y[n] by explicitly evaluating the discrete convolution of x[n] and h[n]. (b) Determine the output y[n] by computing the inverse z-transform of the product of the z-transforms of the input and the unit sample response. 10.33. (a) Determine the system function for the causal LTI system with difference equa- tion 1 1 y[n]- 2y[n- 1] + 4y[n- 2] = x[n]. (b) Using z-transforms, determine y[n] if x[n] = (4 )"" u[n]. Chap. 10 Problems 805 10.34. A causal LTI system is described by the difference equation y[n] = y[n- 1] + y[n- 2] + x[n- 1]. (a) Find the system function H(z) = Y(z)/X(z) for this system. Plot the poles and zeros of H(z) and indicate the region of convergence. (b) Find the unit sample response of the system. (c) You should have found the system to be unstable. Find a stable (noncausal) unit sample response that satisfies the difference equation. 10.35. Consider an LTI system with input x[n] and output y[n] for which 5 y[n - 1] - 2y [n] + y[n + 1] = x[n]. The system may or may not be stable or causal. By considering the pole-zero pattern associated with the preceding differ- ence equation, determine three possible choices for the unit sample response of the system. Show that each choice satisfies the difference equation. 10.36. Consider the linear, discrete-time, shift-invariant system with input x[ n] and output y[n] for which 10 y[n- 1] - 3 y[n] + y[n + 1] = x[n]. The system is stable. Determine the unit sample response. 10.37. The input x[n] and output y[n] of a causal LTI system are related through the block-diagram representation shown in Figure Pl0.37. x[n] ----.- + 1-----....,.------~ y[n] Figure P1 0.37 (a) Determine a difference equation relating y[n] and x[n]. (b) Is this system stable? 10.38. Consider a causal LTI systemS with input x[n] and a system function specified as H(z) = H1 (z)H2(z), where 806 The z-Transform Chap. 10 and A block diagram corresponding to H(z) may be obtained as a cascade connection of a block diagram for H 1( z) followed by a block diagram for H2(z). The result is shown in Figure PI 0.38, in which we have also labeled the intermediate signals e1 [n], e2[n], !1 [n], and f2[n]. x[n] y[n] Figure P1 0.38 (a) How is e1 [n] related to / 1 [n]? (b) How is e2[n] related to f2[n]? (c) Using your answers to the previous two parts as a guide, construct a direct- form block diagram for S that contains only two delay elements. (d) Draw a cascade-form block diagram representation for S based on the obser- vation that 1+ 1 H(z) = ( l4 z- )(1- 2z-I) 1 + ~ z- 1 1 - ~ z- 1 • (e) Draw a parallel-form block diagram representation for S based on the obser- vation that 5/3 14/3 H(z) = 4 + ----=--- 1 + lz-I 1- lz-I. 2 4 10.39. Consider the following three system functions corresponding to causal LTI sys- tems: H1 (z) = ---------,-----=------ (1 - z- 1 + ~z- 2)(1 - ~z- 1 + ~z-2 )' 1 H2(z) = -----,------,------- (1- z-I + ~z-2)(1 - ~z-I + z-2)' 1 H3(z) = ---------,------- (1- z-I + ~z-2)(1- z-I + ~z-2). Chap. 10 Problems 807 (a) For each system function, draw a direct-form block diagram. (b) For each system function, draw a block diagram that corresponds to the cas- cade connection of two second-order block diagrams. Each second-order block diagram should be in direct form. (c) For each system function, determine whether there exists a block diagram rep- resentation which is the cascade of four first-order block diagrams with the constraint that all the coefficient multipliers must be real. 10.40. Determine the unilateral z-transform for each of the sequences in Problem 10.21. 10.41. Consider the following two signals: 1 )n+ I XJ [n] = (2 u[n + 1], x2[n] = 1 )n (4 u[n]. Let X 1( z) and X1( z) respectively be the unilateral and bilateral z-transforms of x 1 [n], and let X 2(z) and X2(z) respectively be the unilateral and bilateral z- transforms of x2 [ n]. (a) Take the inverse bilateral z-transform of X1( z)X2(z) to determine g[n] = XJ [n] * x2[n]. (b) Take the inverse unilateral z-transform ofX 1(z)X2(z) to obtain a signal q[n] for n 2: 0. Observe that q[n] and g[n] are not identical for n 2: 0. 10.42. For each of the following difference equations and associated input and initial con- ditions, determine the zero-input and zero-state responses by using the unilateral z-transform: (a) y[n] + 3y[n- 1] = x[n], x[n] = 1 )ll (2 u[n], y[-1] = 1. 1 1 (b) y[n] - 2y [n- 1] = x[n] - 2x[n- 1], x[n] = u[n], y[-1]=0. 1 1 (c) y[n]- 2y[n- 1] = x[n]- 2x[n- 1], x[n] = u[n], y[-1] = 1. ADVANCED PROBLEMS 10.43. Consider an even sequence x[n] (i.e., x[n] = x[ -n]) with rational z-transform X(z). 808 The z-Transform Chap. 10 (a) From the definition of the z-transform, show that X(z) = x(H (b) From your results in part (a), show that if a pole (zero) of X(z) occurs at z = zo, then a pole (zero) must also occur at z = 1/z0 . (c) Verify the result in part (b) for each of the following sequences: (1) o[n + 1] + o[n - 1] (2) o[n + 1] - ~o[n] + o[n- 1] 10.44. Let x[n] be a discrete-time signal with z-transform X(z). For each of the following signals, determine the z-transform in terms of X(z): (a) Llx[n], where Ll is the first-difference operator defined by Llx[n] = x[n] - x[n- 1] (b) XI [n] = { x[n/2], n even 0, n odd (c) x 1 [n] = x[2n] 10.45. Determine which of the following z-transforms could be the transfer function of a discrete-time linear system that is not necessarily stable, but for which the unit sample response is zero for n < 0. State your reasons clearly. (1- z-1)2 (b) (z- 1)2 (a) 1 - .! z- 1 z - .! 2 2 (z- ~)5 (z- ~)6 (c) (d) (z - ~ )6 (z - ~ )5 10.46. A sequence x[n] is the output of an LTI system whose input is s[n]. The system is described by the difference equation x[n] = s[n] - e8a s[n - 8], where 0 < a < 1. (a) Find the system function X(z) H1 (z) = S(z), and plot its poles and zeros in the z-plane. Indicate the region of convergence. (b) We wish to recover s[n] from x[n] with an LTI system. Find the system func- tion H ( ) = Y(z) 2 z X(z) Chap. 10 Problems 809 such that y[n] = s[n]. Find all possible regions of convergence for H2(z), and for each, tell whether or not the system is causal or stable. (c) Find all possible choices for the unit impulse response h2 [n] such that y[n] = h2[n] * x[n] = s[n]. 10.47. The following is known about a discrete-time LTI system with input x[n] and out- put y[n]: 1. If x[n] = ( -2)n for all n, then y[n] = 0 for all n. 2. If x[n] = (112)nu[n] for all n, then y[n] for all n is of the form y[n] = ll[n] +a(~)"" u[n], where a is a constant. (a) Determine the value of the constant a. (b) Determine the response y[n] if the input x[n] is x[n] = 1, for all n. 10.48. Suppose a second-order causal LTI system has been designed with a real impulse response hi [n] and a rational system function HI (z). The pole-zero plot for HI (z) is shown in Figure P10.48(a). Now consider another causal second-order system with impulse response h2 [n] and rational system function H2(z). The pole-zero plot for H2(z) is shown in Figure P10.48(b). Determine a sequence g[n] such that the following three conditions hold: (1) h2[n] = g[n]h 1 [n] (2) g[n] = 0 for n < 0 (3) _Lig[kJI = 3 k=O 1 (fl.e 1 <Re (a) (b) Figure Pl 0.48 810 The z-Transform Chap. 10 10.49. In Property 4 of Section 10.2, it was stated that if x[n] is a right-sided sequence and if the circle lzl = r0 is in the ROC, then all finite values of z for which 1z1 > r0 will also be in the ROC. In this discussion an intuitive explanation was given. A more formal argument parallels closely that used for Property 4 of Section 9 .2, relating to the Laplace transform. Specifically, consider a right-sided sequence x[n] = 0, n < NJ, and for which L, ix[n]jr0n L, jx[n]jr0n < oo. n=-oo n=N1 Then if ro :::; r1, (P10.49-1) where A is a positive constant. (a) Show that eq. (Pl0.49-l) is true, and determine the constant A in terms of r0 , r 1, andN1• (b) From your result in part (a), show that Property 4 of Section 10.2 follows. (c) Develop an argument similar to the foregoing one to demonstrate the validity of Property 5 of Section 10 .2. 10.50. A discrete-time system with the pole-zero pattern shown in Figure Pl0.50(a) is referred to as a first-order all-pass system, since the magnitude of the frequency response is constant regardless of frequency. (a) Demonstrate algebraically that jH(ejw)i is constant. To demonstrate the same property geometrically, consider the vector dia- gram in Figure P10.50(b). We wish to show that the length ofv2 is proportional to the length of v1 independently of the frequency w. Unit circle ROC: izl>a (a) Figure P1 O.SOa Chap. 10 Problems 811 (b) Figure P1 O.SOb (b) Express the length of v 1 using the law of cosines and the fact that v 1 is one leg of a triangle for which the other two legs are the unit vector and a vector of . length a. (c) In a manner similar to that in part (b), determine the length of v2 and show that it is proportional in length to v1 independently of w. 10.51. Consider a real-valued sequence x[n] with rational z-transform X(z). (a) From the definition of the z-transform, show that X(z) = X*(z*). (b) From your result in part (a), show that if a pole (zero) of X(z) occurs at z = z0 , then a pole (zero) must also occur at z = z~. (c) Verify the result in part (b) for each of the following sequences: (1) x[n] = (4)nu[n] (2) x[n] = o[n] - 4o[n- 1] + io[n- 2] (d) By combining your results in part (b) with the result of Problem 10.43(b), show that for a real, even sequence, if there is a pole (zero) of H (z) at z = peF1, then there is also a pole (zero) of H(z) at z = (11 p)ei8 and at z = (11 p)e- i 8 . 10.52. Consider a sequence xdn] with z-transform X1( z) and a sequence x2 [n] with z- transform X2(z), where Show that X2(z) = X1 (11z), and from this, show that if X1 (z) has a pole (or zero) at z = zo, then X2(z) has a pole (or zero) at z = 1/zo. 10.53. (a) Carry out the proof for each of the following properties in Table 10.1: (1) Property set forth in Section 10.5.2 (2) Property set forth in Section 10.5.3 (3) Property set forth in Section 10.5.4 812 The z-Transform Chap. 10 (b) WithX(z) denoting the z-trarisform of x[n] and Rx the ROC of X(z), determine, in terms of X(z) and Rx. the z-transform and associated ROC for each of the following sequences: (1) x*[n] (2), z0x[n], where zo is a complex number 10.54. In Section 10.5.9, we stated and proved the initial-value theorem for causal se- quences. (a) State and prove the corresponding theorem if x[n] is anticausal (i.e., if x[n] = 0, n > 0). (b) Show that if x[n] = 0, n < 0, then x[1] = lim z(X(z) - x[O]). z~oo 10.55. Let x[n] denote a causal sequence (i.e., if x[n] = 0, n < 0) for which x[O] is nonzero and finite. (a) Using the initial-value theorem, show that there are no poles or zeros of X(z) at z = oo. (b) Show that, as a consequence of your result in part (a), the number of poles of X(z) in the finite z-plane equals the number of zeros of X(z) in the finite z-plane. (The finite z-plane excludes z = oo.) 10.56. In Section 10.5.7, we stated the convolution property for the z-transform. To show that this property holds, we begin with the convolution sum expressed as 00 X3[n] = XI [n] * X2[n] = ~ XI [k]x2[n- k]. (P10.56-l) k= -00 (a) By taking the z-transform of eq. (P10.56-1) and using eq. (10.3), show that X3(Z) = ~ XI [k]X2(Z), k= -00 where X2(z) is the transform of x2 [n - k]. (b) Using your result in part (a) and property 10.5.2 in Table 10.1, show that 00 X3(z) = X2(z) ~ xi [k]z-k. k= -00 (c) From part (b), show that as stated in eq. (10.81). 10.57. Let XI(z) = xi[O] + xi[1]z-I + · · · + xi[NJ]z-N', X2(z) = x2[0] + x2[l]z-I + · · · + x2[N2]z-N2 • Define Chap. 10 Problems 813 and let M f(z) = L, y[k]z-k. k=O (a) Express Min terms of N1 and N2. (b) Use polynomial multiplication to determine y[O], y[l], and y[2]. (c) Use polynomial multiplication to show that, for 0 :::; k :::; M, y[k] = L, XI [m]x2[k- m]. m= -oo 10.58. A minimum-phase system is a system that is causal and stable and for which the inverse system is also causal and stable. Determine the necessary constraints on the location in the z-plane of the poles and zeros of the system function of a minimum- phase system. 10.59. Consider the digital filter structure shown in Figure P10.59. k k 3 4 Figure P1 0.59 (a) Find H(z) for this causal filter. Plot the pole-zero pattern and indicate there- gion of convergence. (b) For what values of the k is the system stable? (c) Determine y[n] if k = 1 and x[n] = (2/3)n for all n. 10.60. Consider a signal x[n] whose unilateral z-transform is ~(z). Show that the unilat- eral z-transform of y[n] = x[n + 1] may be specified as 'Y(z) = z~(z) - zx[O]. 10.61. If~(z) denotes the unilateral z-transform of x[n], determine, in terms of~(z), the unilateral z-transform of: (a) x[n + 3] (b) x[n - 3] (c) L ~= _ 00 x[k] EXTENSION PROBLEMS 10.62. The autocorrelation sequence of a sequence x[n] is defined as cf>xAn] = L, x[k]x[n + k]. k= -oo Determine the z-transform of cf>xx[n] in terms of the z-transform of x[n]. 814 The z-Transform Chap. 10 10.63. By using the power-series expansion oo wi log(l - w) = - L --:-, lwl < 1, i = 1 l determine the inverse of each of the following two z-transforms: (a) X(z) = log(l - 2z), lzl < 1 (b) X(z) = log(l -1z- 1 ), lzl > 1 10.64. By first differentiating X(z) and using the appropriate properties of the z-transform, determine the sequence for which the z-transform is each of the following: (a) X(z) = log(l - 2z), lzl < 1 (b) X(z) = log(l-1z- 1), lzl > 1 Compare your results for (a) and (b) with the results obtained in Problem 10.63, in which the power-series expansion was used. 10.65. The bilinear transformation is a mapping for obtaining a rational z-transform Hd(Z) from a rational Laplace transform Hc(s). This mapping has two important proper- ties: 1. If Hc(s) is the Laplace transform of a causal and stable LTI system, then Hd(Z) is the z-transform of a causal and stable LTI system. 2. Certain important characteristics of iHc(jw )I are preserved in iHd(eiw)i. In this problem, we illustrate the second of these properties for the case of all-pass filters. (a) Let a-s Hc(s) = --, s+a where a is real and positive. Show that (b) Let us now apply the bilinear transformation to Hc(s) in order to obtain Hd(Z). That is, Show that Hd(Z) has one pole (which is inside the unit circle) and one zero (which is outside the unit circle). (c) For the system function Hd(Z) derived in part (b), show that iHd(eiw)i = 1. 10.66. The bilinear transformation, introduced in the previous problem, may also be used to obtain a discrete-time filter, the magnitude of whose frequency response is sim- ilar to the magnitude of the frequency response of a given continuous-time low- pass filter. In this problem, we illustrate the similarity through the example of a continuous-time second-order Butterworth filter with system function Hc(s). Chap. 10 Problems 815 (a) Let Hd(Z) = Hc(s)Js= 1-z-1 • l+z-1 Show that (b) Given that 1 Hc(s) = (s + e/rr1 4 )(s + e- j1r 14 ) and that the corresponding filter is causal, verify that Hc(O) = 1, that IHc(Jw )J decreases monotonically with increasing positive values of w, that IHc(j)J2 = 112 (i.e., that We = 1 is the half-power frequency), and that Hc(oo) = 0. (c) Show that if the bilinear transformation is applied to Hc(s) of part (b) in order to obtain Hd(Z), then the following may be asserted about Hd(Z) and Hd(ejw): 1. Hd(Z) has only two poles, both of which are inside the unit circle. 2. Hd(ej0 ) = 1. 3. IHd(ejw)l decreases monotonically as w goes from 0 to TT. 4. The half-power frequency of Hd(ejw) is TT/2. 1 1 lJNEARFEEDBACKSYSTEMS 11.0 INTRODUCTION It has long been recognized that in many situations there are particular advantages to be gained by using feedback-that is, by using the output of a system to control or modify the input. For example, it is common in electromechanical systems, such as a motor whose shaft position is to be maintained at a constant angle, to measure the error between the desired and the true position and to use this error in the form of a signal to tum the shaft in the appropriate direction. This is illustrated in Figure 11.1, where we have depicted the use of a de motor for the accurate pointing of a telescope. In Figure 11.1 (a) we have indicated pictorially what such a system would look like, where v(t) is the input voltage to the motor and 8(t) is the angular position of the telescope platform. The block diagram for the motor-driven pointing system is shown in Figure 11.1 (b). A feedback system for controlling the position of the telescope is illustrated in Figure 11.1 (c), and a block diagram equivalent to this system is shown in Figure 11.1(d). The external, or reference, input to this feedback system is the desired shaft angle 8 D· A potentiometer is used to convert the angle into a voltage K1 8 D proportional to 8 D· Similarly, a second potentiometer produces a voltage K18 (t) proportional to the actual platform angle. These two voltages are compared, producing an error voltage K1 (8 D - 8(t)), which is amplified and then used to drive the electric motor. Figure 11.1 suggests two different methods for pointing the telescope. One of these is the feedback system of Figures 11.1(c) and (d). Here, the input that we must provide is the desired reference angle 8 D· Alternatively, if the initial angle, the desired angle, and the detailed electrical and mechanical characteristics of the motor-shaft assembly were known exactly, we could specify the precise history of the input voltage v(t) that would first accelerate and then decelerate the shaft, bringing the platform to a stop at the desired 816"
11 Linear Feedback Systems,"1 1 lJNEARFEEDBACKSYSTEMS 11.0 INTRODUCTION It has long been recognized that in many situations there are particular advantages to be gained by using feedback-that is, by using the output of a system to control or modify the input. For example, it is common in electromechanical systems, such as a motor whose shaft position is to be maintained at a constant angle, to measure the error between the desired and the true position and to use this error in the form of a signal to tum the shaft in the appropriate direction. This is illustrated in Figure 11.1, where we have depicted the use of a de motor for the accurate pointing of a telescope. In Figure 11.1 (a) we have indicated pictorially what such a system would look like, where v(t) is the input voltage to the motor and 8(t) is the angular position of the telescope platform. The block diagram for the motor-driven pointing system is shown in Figure 11.1 (b). A feedback system for controlling the position of the telescope is illustrated in Figure 11.1 (c), and a block diagram equivalent to this system is shown in Figure 11.1(d). The external, or reference, input to this feedback system is the desired shaft angle 8 D· A potentiometer is used to convert the angle into a voltage K1 8 D proportional to 8 D· Similarly, a second potentiometer produces a voltage K18 (t) proportional to the actual platform angle. These two voltages are compared, producing an error voltage K1 (8 D - 8(t)), which is amplified and then used to drive the electric motor. Figure 11.1 suggests two different methods for pointing the telescope. One of these is the feedback system of Figures 11.1(c) and (d). Here, the input that we must provide is the desired reference angle 8 D· Alternatively, if the initial angle, the desired angle, and the detailed electrical and mechanical characteristics of the motor-shaft assembly were known exactly, we could specify the precise history of the input voltage v(t) that would first accelerate and then decelerate the shaft, bringing the platform to a stop at the desired 816"
11.0 Introduction,"1 1 lJNEARFEEDBACKSYSTEMS 11.0 INTRODUCTION It has long been recognized that in many situations there are particular advantages to be gained by using feedback-that is, by using the output of a system to control or modify the input. For example, it is common in electromechanical systems, such as a motor whose shaft position is to be maintained at a constant angle, to measure the error between the desired and the true position and to use this error in the form of a signal to tum the shaft in the appropriate direction. This is illustrated in Figure 11.1, where we have depicted the use of a de motor for the accurate pointing of a telescope. In Figure 11.1 (a) we have indicated pictorially what such a system would look like, where v(t) is the input voltage to the motor and 8(t) is the angular position of the telescope platform. The block diagram for the motor-driven pointing system is shown in Figure 11.1 (b). A feedback system for controlling the position of the telescope is illustrated in Figure 11.1 (c), and a block diagram equivalent to this system is shown in Figure 11.1(d). The external, or reference, input to this feedback system is the desired shaft angle 8 D· A potentiometer is used to convert the angle into a voltage K1 8 D proportional to 8 D· Similarly, a second potentiometer produces a voltage K18 (t) proportional to the actual platform angle. These two voltages are compared, producing an error voltage K1 (8 D - 8(t)), which is amplified and then used to drive the electric motor. Figure 11.1 suggests two different methods for pointing the telescope. One of these is the feedback system of Figures 11.1(c) and (d). Here, the input that we must provide is the desired reference angle 8 D· Alternatively, if the initial angle, the desired angle, and the detailed electrical and mechanical characteristics of the motor-shaft assembly were known exactly, we could specify the precise history of the input voltage v(t) that would first accelerate and then decelerate the shaft, bringing the platform to a stop at the desired 816 v(t) ..__ ______ ___. Cl:) 8(t) I (a) v(t)----~1 Motor 1---.........;~ 8(t) Input Platform voltage angular position (b) K1[8 0 - e (t)] So - Potentiometer Comparator Amplifier (gain K2) Potentiometer (c) 9o~v K H Motor • 8(t) I (d) Figure 11. 1 Use of feedback to control the, angular position of a telescope: (a) de motor-driven telescope platform; (b) block diagram of the system in (a); (c) feedback system for pointing the telescope; (d) block diagram of the system in (c) (here, K = K1K2). 817 818 Linear Feedback Systems Chap. 11 position without the use of feedback, as in Figures 11.1 (a) and (b). A system operating in accordance with Figures 11.1 (a) and (b) is typically referred to as an open-loop system, in contrast to the closed-loop system of Figures ll.l(c) and (d). In a practical environ- ment, there are clear advantages to controlling the motor-shaft angle with the closed-loop system rather than with the open-loop system. For example, in the closed-loop system, when the shaft has been rotated to the correct position, any disturbance from this position will be sensed, and the resulting error will be used to provide a correction. In the open- loop system, there is no mechanism for providing a correction. As another advantage of the closed-loop system, consider the effect of errors in modeling the characteristics of the motor-shaft assembly. In the open -loop system, a precise characterization of the system is required to design the correct input. In the closed-loop system, the input is simply the de- sired shaft angle and does not require precise knowledge of the system. This insensitivity of the closed-loop system to disturbances and to imprecise knowledge of the system are two important advantages of feedback. The control of an electric motor is just one of a great many examples in which feed- back plays an important role. Similar uses of feedback can be found in a wide variety of applications, such as chemical process control, automotive fuel systems, household heating systems, and aerospace systems, to name just a few. In addition, feedback is also present in many biological processes and in the control of human motion. For example, when a person reaches for an object, it is usual during the reaching process to monitor visually the distance between the hand and the object so that the velocity of the hand can be smoothly decreased as the distance (i.e., the error) between the hand and the object decreases. The effectiveness of using the system output (hand position) to control the input is clearly demonstrated by alternatively reaching with and without the use of visual feedback. In addition to its use in providing an error-correcting mechanism that can reduce sen- sitivity to disturbances and to errors in the modeling of the system that is to be controlled, another important characteristic of feedback is its potential for stabilizing a system that is inherently unstable. Consider the problem of trying to balance a broomstick in the palm of the hand. If the hand is held stationary, small disturbances (such as a slight breeze or inadvertent motion of the hand) will cause the broom to fall over. Of course, if one knows exactly what disturbances will occur, and if one can control the motion of the hand per- fectly, it is possible to determine in advance how to move the hand to balance the broom. This is clearly unrealistic; however, by always moving the hand in the direction in which the broom is falling, the broom can be balanced. This, of course, requires feedback in order to sense the direction in which the broom is falling. A second example that is closely related to the balancing of a broom is the problem of controlling a so-called inverted pen- dulum, which is illustrated in Figure 11.2. As shown, an inverted pendulum consists of a thin rod with a weight at the top. The bottom of the rod is mounted on a cart that can move in either direction along a track. Again, if the cart is kept stationary, the inverted pendulum Figure 11 .2 An inverted pendulum. Sec. 11.1 Linear Feedback Systems 819 will topple over. The problem of stabilizing the pendulum is one of designing a feedback system that will move the cart to keep the pendulum vertical. This example is examined in Problem 11.56. A third example, which again bears some similarity to the balancing of a broom, is the problem of controlling the trajectory of a rocket. In this case, much as the movement of the hand is used to compensate for disturbances in the position of the broom, the direction of the thrust of the rocket is used to correct for changes in aerodynamic forces and wind disturbances that would otherwise cause the rocket to deviate from its course. Again, feedback is important, because these forces and disturbances are never precisely known in advance. The preceding examples provide some indication of why feedback may be useful. In the next two sections we introduce the basic block diagrams and equations for linear feedback systems and discuss in more detail a number of applications of feedback and control, both in continuous time and in discrete time. We also point out how feedback can have harmful as well as useful effects. These examples of the uses and effects of feedback will give us some insight into how changes in the parameters in a feedback control system lead to changes in the behavior of the system. Understanding this relationship is essential in designing feedback systems that have certain desirable characteristics. With this material as background, we will then develop, in the remaining sections of the chapter, several specific techniques that are of significant value in the analysis and design of continuous- time and discrete-time feedback systems. 11. 1 LINEAR FEEDBACK SYSTEMS The general configuration of a continuous-time LTI feedback system is shown in Fig- ure 11.3(a) and that of a discrete-time LTI feedback system in Figure 11.3(b ). Because of x(t) ~~e(t) ----+. .-~ H(s) y(t) r(t) G(s) (a) x[n]e[n] + ~ H(z) y[n] r[n] G(z) Figure 11.3 Basic feedback system configurations in (a) continuous time (b) and (b) discrete time."
11.1 Linear Feedback Systems,"Sec. 11.1 Linear Feedback Systems 819 will topple over. The problem of stabilizing the pendulum is one of designing a feedback system that will move the cart to keep the pendulum vertical. This example is examined in Problem 11.56. A third example, which again bears some similarity to the balancing of a broom, is the problem of controlling the trajectory of a rocket. In this case, much as the movement of the hand is used to compensate for disturbances in the position of the broom, the direction of the thrust of the rocket is used to correct for changes in aerodynamic forces and wind disturbances that would otherwise cause the rocket to deviate from its course. Again, feedback is important, because these forces and disturbances are never precisely known in advance. The preceding examples provide some indication of why feedback may be useful. In the next two sections we introduce the basic block diagrams and equations for linear feedback systems and discuss in more detail a number of applications of feedback and control, both in continuous time and in discrete time. We also point out how feedback can have harmful as well as useful effects. These examples of the uses and effects of feedback will give us some insight into how changes in the parameters in a feedback control system lead to changes in the behavior of the system. Understanding this relationship is essential in designing feedback systems that have certain desirable characteristics. With this material as background, we will then develop, in the remaining sections of the chapter, several specific techniques that are of significant value in the analysis and design of continuous- time and discrete-time feedback systems. 11. 1 LINEAR FEEDBACK SYSTEMS The general configuration of a continuous-time LTI feedback system is shown in Fig- ure 11.3(a) and that of a discrete-time LTI feedback system in Figure 11.3(b ). Because of x(t) ~~e(t) ----+. .-~ H(s) y(t) r(t) G(s) (a) x[n]e[n] + ~ H(z) y[n] r[n] G(z) Figure 11.3 Basic feedback system configurations in (a) continuous time (b) and (b) discrete time. 820 Linear Feedback Systems Chap. 11 the typical applications in which feedback is utilized, it is natural to restrict the systems in these figures to be causal. This will be our assumption throughout the chapter. In that case, the system functions in Figure 11.3 can be interpreted either as unilateral or as bilateral transforms, and, as a consequence of causality, the ROC's associated with them will always be to the right of the rightmost pole for Laplace transforms and outside the outermost pole for z-transforms. It should also be noted that the convention used in Figure 11.3(a) is that r(t), the signal fed back, is subtracted from the input x(t) to form e(t). The identical convention is adopted in discrete time. Historically, this convention arose in tracking-system applica- tions, where x(t) represented a desired command and e(t) represented the error between the command and the actual response r(t). This was the case, for example, in our discus- sion of the pointing of a telescope. In more general feedback systems, e(t) and e[n], the disCrete-time counterpart of e(t), may not correspond to or be directly interpretable as error signals. The system function H(s) in Figure 11.3(a) or H(z) in Figure 11.3(b) is referred to as the system function of the forward path and G(s) or G(z) as the system function of the feedback path. The system function of the overall system of Figure 11.3(a) or (b) is referred to as the closed-loop system function and will be denoted by Q(s) or Q(z). In Sections 9.8.1 and 10 .8.1, we derived expressions for the system functions of feedback interconnections of LTI systems. Applying these results to the feedback systems of Figure 11.3, we obtain Q(s) = _Y(_s) = __H _(_s_) _ (11.1) X(s) 1 + G(s)H(s)' Q(z) = Y(z) = H(z) (11.2) X(z) 1 + G(z)H(z) · Equations ( 11.1) and ( 11.2) represent the fundamental equations for the study of LTI feed- back systems. In the following sections, we use these equations as the basis for gaining insight into the properties of feedback systems and for developing several tools for their analysis. 11.2 SOME APPLICATIONS AND CONSEQUENCES OF FEEDBACK In the introduction, we provided a brief, intuitive look at some of the properties and uses of feedback systems. In this section, we examine a number of the characteristics and ap- plications of feedback in somewhat more quantitative terms, using the basic feedback equations (11.1) and (11.2) as a starting point. Our purpose is to provide an introduction to and an appreciation for the applications of feedback, rather than to develop any of these applications in detail. In the sections that follow, we focus in more depth on several specific techniques for analyzing feedback systems that are useful in a wide range of problems, including many of the applications that we are about to describe. 11.2.1 Inverse System Design In some applications, one would like to synthesize the inverse of a given continuous-time system. Suppose that this system has system function P(s), and consider the feedback system shown in Figure 11.4. Applying equation ( 11.1) with H (s) = K and G(s) = P(s ),"
11.2 Some Applications and Consequences of Feedback,"820 Linear Feedback Systems Chap. 11 the typical applications in which feedback is utilized, it is natural to restrict the systems in these figures to be causal. This will be our assumption throughout the chapter. In that case, the system functions in Figure 11.3 can be interpreted either as unilateral or as bilateral transforms, and, as a consequence of causality, the ROC's associated with them will always be to the right of the rightmost pole for Laplace transforms and outside the outermost pole for z-transforms. It should also be noted that the convention used in Figure 11.3(a) is that r(t), the signal fed back, is subtracted from the input x(t) to form e(t). The identical convention is adopted in discrete time. Historically, this convention arose in tracking-system applica- tions, where x(t) represented a desired command and e(t) represented the error between the command and the actual response r(t). This was the case, for example, in our discus- sion of the pointing of a telescope. In more general feedback systems, e(t) and e[n], the disCrete-time counterpart of e(t), may not correspond to or be directly interpretable as error signals. The system function H(s) in Figure 11.3(a) or H(z) in Figure 11.3(b) is referred to as the system function of the forward path and G(s) or G(z) as the system function of the feedback path. The system function of the overall system of Figure 11.3(a) or (b) is referred to as the closed-loop system function and will be denoted by Q(s) or Q(z). In Sections 9.8.1 and 10 .8.1, we derived expressions for the system functions of feedback interconnections of LTI systems. Applying these results to the feedback systems of Figure 11.3, we obtain Q(s) = _Y(_s) = __H _(_s_) _ (11.1) X(s) 1 + G(s)H(s)' Q(z) = Y(z) = H(z) (11.2) X(z) 1 + G(z)H(z) · Equations ( 11.1) and ( 11.2) represent the fundamental equations for the study of LTI feed- back systems. In the following sections, we use these equations as the basis for gaining insight into the properties of feedback systems and for developing several tools for their analysis. 11.2 SOME APPLICATIONS AND CONSEQUENCES OF FEEDBACK In the introduction, we provided a brief, intuitive look at some of the properties and uses of feedback systems. In this section, we examine a number of the characteristics and ap- plications of feedback in somewhat more quantitative terms, using the basic feedback equations (11.1) and (11.2) as a starting point. Our purpose is to provide an introduction to and an appreciation for the applications of feedback, rather than to develop any of these applications in detail. In the sections that follow, we focus in more depth on several specific techniques for analyzing feedback systems that are useful in a wide range of problems, including many of the applications that we are about to describe. 11.2.1 Inverse System Design In some applications, one would like to synthesize the inverse of a given continuous-time system. Suppose that this system has system function P(s), and consider the feedback system shown in Figure 11.4. Applying equation ( 11.1) with H (s) = K and G(s) = P(s ), Sec. 11.2 Some Applications and Consequences of Feedback 821 + K y(t) Figure 11 .4 Form of a feedback system used in implementing the in- P(s) verse of the system with system func- tion P(s). we find that the closed-loop system function is K Q(s) = 1 + KP(s) (11.3) If the gain K is sufficiently large so that K P(s) >> 1, then 1 Q(s) = P(s)' (11.4) in which case the feedback system approximates the inverse of the system with system function P(s). It is important to note that the result in eq. (11.4) requires that the gain K be suffi- ciently high, but is otherwise not dependent on the precise value of the gain. Operational amplifiers are one class of devices that provide this kind of gain and are widely used in feedback systems. One common application of the inversion inherent in eq. ( 11.4) is in the implementation of integrators. A capacitor has the property that its current is proportional to the derivative of the voltage. By inserting a capacitor in the feedback path around an operational amplifier, the differentiation property of the capacitor is inverted to provide integration. This specific application is explored in more detail in Problems 11.50-11.52. Although our discussion is for the most part restricted to linear systems, it is worth pointing out that this same basic approach is commonly used in inverting a nonlinearity. For example, systems for which the output is the logarithm of the input are commonly im- plemented by utilizing the exponential current-voltage characteristics of a diode as feed- back around an operational amplifier. This is explored in more detail in Problem 11.53. 11 .2.2 Compensation for Nonideal Elements Another common use of feedback is to correct for some of the nonideal properties of the open -loop system. For example, feedback is often used in the design of amplifiers to pro- vide constant-gain amplification in a given frequency band, and in fact, it is this applica- tion, pioneered by H. S. Black at Bell Telephone Laboratories in the 1920s, that is generally considered to have been the catalyst for the development of feedback control as a practical and useful system design methodology. Specifically, consider an open-loop frequency response H(jw) which provides am- plification over the specified frequency band, but which is not constant over that range. For example, operational amplifiers or the vacuum tube amplifiers of concern to Black and his colleagues typically provide considerable, but not precisely controlled, amplification. 822 Linear Feedback Systems Chap. 11 While such devices can provide raw amplification levels of several orders of magnitude, the price one pays for this includes uncertain levels of amplification that can fluctuate with frequency, time, temperature, etc., and that can also introduce unwanted phase and nonlinear distortions. What Black proposed was placing such a powerful, but uncertain and erratic, amplifier in a feedback loop as in Figure 11.3(a) with G(s) chosen to be constant, i.e., G(s) = K. In this case, assuming the closed-loop system is stable, its frequency response is H(jw) Q(jw) = 1 + KH(jw )' (11.5) If, over the specified frequency range, IKH(jw)l >> 1, (11.6) then Q(jw) = ~· (11.7) That is, the closed-loop frequency response is constant, as desired. This of course assumes that the system in the feedback path can be designed so that its frequency response G(jw) has a constant gain Kover the desired frequency band, which is precisely what we assumed we could not ensure for H (jw ). The difference between the requirement on H (jw) and that on G(jw ), however, is that H(jw) must provide amplification, whereas, from eq. (11.7), we see that for the overall closed-loop system to provide a gain greater than unity, K must be less than 1. That is, G(jw) must be an attenuator over the specified range of frequencies. In general, an attenuator with approximately flat frequency characteristics is considerably easier to realize than an amplifier with approximately flat frequency response (since an attenuator can be constructed from passive elements). The use of feedback to flatten the frequency response incurs some cost, however, and it is this fact that led to the considerable skepticism with which Black's idea was met. In particular, from eqs. (11.6) and (11.7), we see that IH(jw)l >> ~ = Q(jw), (11.8) so that the closed-loop gain l!K will be substantially less than the open-loop gain iH(jw)i. This apparently significant loss of gain, attributable to what Black referred to as degen- erative or negative feedback, was initially viewed as a serious weakness in his negative- feedback amplifier. Indeed, the effect had been known for many years and had led to the conviction that negative feedback was not a particularly useful mechanism. However, Black pointed out that what one gave up in overall gain was often more than offset by the reduced sensitivity of the overall closed-loop amplifier: The closed-loop system function is essentially equal to eq. (11.7), independently of variations in H(jw), as long as IH(jw)l is large enough. Thus, if the open-loop amplifier is initially designed with considerably more gain than is actually needed, the closed-loop amplifier will provide the desired lev- els of amplification with greatly reduced sensitivity. This concept and its application to extending the bandwidth of an amplifier are explored in Problem 11.49. Sec. 11.2 Some Applications and Consequences of Feedback 823 11.2.3 Stabilization of Unstable Systems As mentioned in the introduction, one use of feedback systems is to stabilize systems that, without feedback, are unstable. Examples of this kind of application include the control of the trajectory of a rocket, the regulation of nuclear reactions in a nuclear power plant, the stabilization of an aircraft, and the natural and regulatory control of animal populations. To illustrate how feedback can be used to stabilize an unstable system, let us consider a simple first-order continuous-time system with b H(s) = --. (11.9) s-a With a > 0, the system is unstable. Choosing the system function G(s) to be a constant gain K, we see that the closed-loop system function in eq. (11.1) becomes H(s) Q(s) = 1 + KH(s) b (11.10) s-a+Kb. The closed-loop system will be stable if the pole is moved into the left half of the s-plane. This will be the case if Kb>a. (11.11) Thus, we can stabilize the system with a constant gain in the feedback loop if that gain is chosen to satisfy eq. ( 11.11 ). This type of feedback system is referred to as a proportional feedback system, since the signal that is fed back is proportional to the output of the system. As another example, consider the second-order system b H(s) = --. (11.12) s2 +a If a > 0, the system is an oscillator (i.e., H(s) has its poles on the jw-axis), and the impulse response of the system is sinusoidal. If a < 0, H(s) has one pole in the left-half plane and one in the right-half plane. Thus, in either case, the system is unstable. In fact, as considered in Problem 11.56, the system function given in eq. (11.12) with a < 0 can be used to model the dynamics of the inverted pendulum described in the introduction. Let us first consider the use of proportional feedback for this second-order system; that is, we take G(s) = K. (11.13) Substituting into eq. ( 11.1 ), we obtain b Q(s) = s2 +(a+ Kb). (11.14) 824 Linear Feedback Systems Chap. 11 In our discussion of second-order systems in Chapter 6, we considered a transfer function of the form (11.15) For such a system to be stable, wn must be real and positive (i.e., w~ > 0), and C must be positive (corresponding to positive damping). From eqs. (11.14) and (11.15), it follows that with proportional feedback we can only influence the value of w~, and consequently, we cannot stabilize the system because we cannot introduce any damping. To suggest a type of feedback that can be used to stabilize this system, recall the mass-spring-dashpot mechanical system described in our examination of second-order systems in Section 6.5.2. We saw that damping in that system was the result of the inclusion of a dashpot, which provided a restoring force proportional to the velocity of the mass. This suggests that we consider proportional-plus-derivative feedback, that is, a G(s) of the form (11.16) which yields b (11.17) Q(s) = s2 + bK2s +(a+ Ktb)"" The closed-loop poles will be in the left-half plane, and hence, the closed-loop system will be stable as long as we choose K 1 and K2 to guarantee that (11.18) The preceding discussion illustrates how feedback can be used to stabilize con tin- uous-time systems. The stabilization of unstable systems is an important application of feedback for discrete-time systems as well. Examples of discrete-time systems that are unstable in the absence of feedback are models of population growth. To illustrate how feedback can prevent the unimpeded growth of populations, let us consider a simple model for the evolution of the population of a single species of animal. Let y[n] denote the number of animals in the nth generation, and assume that without the presence of any impeding influences, the birthrate is such that the population would double each generation. In this case, the basic equation for the population dynamics of the species is y[n] = 2y[n- 1] + e[n], (11.19) where e[n] represents any additions to or deletions from the population that are caused by external influences. This population model is obviously unstable, with an impulse response that grows exponentially. However, in any ecological system, there are a number of factors that will inhibit the growth of a population. For example, limits on the food supply for the species will manifest themselves through a reduction in population growth when the number of animals becomes large. Similarly, if the species has natural enemies, it is often reasonable to assume that the population of the predators will grow when the population of the prey increases and, consequently, that the presence of natural enemies will retard population growth. In addition to natural influences such as these, there may be effects introduced by Sec. 11.2 Some Applications and Consequences of Feedback 825 humans that are aimed at population control. For example, the food supply or the predator population may fall under human regulation. In addition, stocking lakes with fish or im- porting animals from other areas can be used to promote growth, and the control of hunting or fishing can also provide a regulative effect. Because all of these influences depend on the size of the population (either naturally or by design), they represent feedback effects. Based on the preceding discussion, we can separate e[n] into two parts by means of the equation e[n] = x[n] - r[n], (11.20) where r[n] represents the effect of the regulative influences described in the previous para- graph and x[ n] incorporates any other external effects, such as the migration of animals or natural disasters or disease. Note that we have included a minus sign in eq. (11.20). This is consistent with our convention of using negative feedback, and here it also has the physical interpretation that, since the uninhibited growth of the population is unstable, the feedback term plays the role of a retarding influence. To see how the population can be controlled by the presence of this feedback term, suppose that the regulative influences ac- count for the depletion of a fixed proportion f3 of the population in each generation. Since, according to our model, the surviving fraction of each generation will double in size, it follows that y[n] = 2(1 - f3)y[n - 1] + x[n]. (11.21) Comparing eq. (11.21) with eqs. (11.19) and (11.20), we see that r[n] = 2{3 y[n - 1]. (11.22) The factor of 2 here represents the fact that the depletion of the present population de- creases the number of births in the next generation. This example of the use of feedback is illustrated in Figure 11.5. Here, the system function of the forward path is obtained from eq. (11.19) as 1 H(z) = 1 - 2z-I, (11.23) while from eq. ( 11.22) the system function of the feedback path is G(z) = 2{3z- 1 • (11.24) + e[n] x[n] 1 + y[n] 1 -2z- 1 - 1 Figure 11.5 Block diagram of a 2~z- 2~y[n-1] simple feedback model of population dynamics. 826 Linear Feedback Systems Chap. 11 Consequently, the closed-loop system function is H(z) Q(z) = 1 + G(z)H(z) (11.25) 1- 2(1 - {3)z- 1 • If f3 < 1/2, the closed-loop system is still unstable, whereas it is stable1 if 112 < f3 < 3/2. Clearly, this example of population growth and control is extremely simplified. For instance, the feedback model of eq. (11.22) does not account for the fact that the part of r[ n] which is due to the presence of natural enemies depends upon the population of the predators, which in tum has its own growth dynamics. Such effects can be incorporated by making the feedback model more complex to reflect the presence of other dynamics in an ecological system, and the resulting models for the evolution of interacting species are extremely important in ecological studies. However, even without the incorporation of these effects, the simple model that we have described here does illustrate the basic ideas of how feedback can prevent both the unlimited proliferation of a species and its extinction. In particular, we can see at an elementary level how human-induced factors can be used. For example, if a natural disaster or an increase in the population of natural enemies causes a drastic decrease in the population of a species, a tightening of limits on hunting or fishing and accelerated efforts to increase the population can be used to decrease f3 in order to destabilize the system to allow for rapid growth, until a normal-size population is again attained. Note also that for this type of problem, it is not usually the case that one wants strict stability. If the regulating influences are such that f3 = 112, and if all other external influences are zero (i.e., if x[n] = 0), then y[n] = y[n - 1]. Therefore, as long as x[n] is small and averages to zero over several generations, a value of f3 = 1/2 will result in an essentially constant population. However, for this value of f3 the system is unstable, since eq. ( 11.21) then reduces to y[n] = y[n - 1] + x[n]. (11.26) That is, the system is equivalent to an accumulator. Thus, if x[n] is a unit step, the output grows without bound. Consequently, if a steady trend is expected in x[n], caused, for example, by a migration of animals into a region, a value of f3 > 1/2 would need to be used to stabilize the system and thus to keep the population within bounds and maintain an ecological balance. 11.2.4 Sampled-Data Feedback Systems In addition to dealing with problems such as the one just described, discrete-time feedback techniques are of great importance in a wide variety of applications involving continuous- time systems. The flexibility of digital systems has made the implementation of sampled- data feedback systems an extremely attractive option. In such a system, the output of a continuous-time system is sampled, some processing is done on the resulting sequence of samples, and a discrete sequence of feedback commands is generated. This sequence 1A lthough, in the context of our population example, f3 could never exceed unity, since f3 > 1 corre- sponds to removing more than 100% of the population. Sec. 11.2 Some Applications and Consequences of Feedback 827 is then converted to a continuous-time signal that is fed back to and subtracted from the external input to produce the actual input to the continuous-time system. Clearly, the constraint of causality on feedback systems imposes a restriction on the process of converting the discrete-time feedback signal to a continuous-time signal (e.g., ideal lowpass filtering or any noncausal approximation of it is not allowed). One of the most widely used conversion systems is the zero-order hold (introduced in Section 7 .1.2). The structure of a sampled-data feedback system involving a zero-order hold is depicted in Figure 11.6(a). In the figure, we have a continuous-time LTI system with system function H(s) that is sampled to produce a discrete-time sequence p[n] = y(nT). (11.27) The sequence p[n] is then processed by a discrete-time LTI system with system function G(z), and the resulting output is put through a zero-order hold to produce the continuous-time signal z(t) = d[n] for nT :::; t < (n + l)T. (11.28) This signal is subtracted from the external input x(t) to produce e(t). -----~+~e(t~) + x(t) H(s) y(t) z(t) Zero-order Ideal C/D hold p[n] G(z) (a) F(z) I + e[n]: Z d r[n] --~ + 1---...1. ~ er~~f~ er ----. H(s) f---+ Ideal C/D 1---:---...----t~ p[n] I I ~--------------------------------- G(z) (b) Figure 11.6 (a) A sampled-data feedback system using a zero-order hold; (b) equivalent discrete-time system. 828 Linear Feedback Systems Chap. 11 Suppose also that x(t) is constant over intervals of length T. That is, x(t) = r[n] for nT ::; t < (n + l)T, (11.29) where r[n] is a discrete-time sequence. This is an approximation that is usually valid in practice, as the sampling rate is typically fast enough so that x(t) does not change appre- ciably over intervals of length T. Furthermore, in many applications, the external input is itself actually generated by applying a zero-order hold operation to a discrete sequence. For example, in systems such as advanced aircraft, the external inputs represent human operator commands that are themselves first processed digitally and then converted back to continuous-time input signals. Because the zero-order hold is a linear operation, the feedback system of Figure 11.6(a) when x(t) is given by eq. (11.29) is equivalent to the system of Figure 11.6(b ). As shown in Problem 11.60, the discrete-time system with input e[n] and output p[n] is an LTI system with system function F(z) that is related to the continuous-time system function H(s) by means of a step-invariant transformation. That is, if s(t) is the step response of the continuous-time system, then the step response q[n] of the discrete- time system consists of equally spaced samples of s(t). Mathematically, q[n] = s(nT) for all n. (11.30) Once we have determined F(z), we have a completely discrete-time feedback system model (Figure 11.6(b)) exactly capturing the behavior of the continuous-time feedback system (Figure 11.6(a)) at the sampling instants t = nT, and we can then consider de- signing the feedback system function G(z) to achieve our desired objectives. An example of designing such a sampled-data feedback system to stabilize an unstable continuous-time system is examined in detail in Problem 11.60. 11.2.5 Tracking Systems As mentioned in Section 11.0, one of the important applications of feedback is in the design of systems in which the objective is to have the output track or follow the input. There is a broad range of problems in which tracking is an important component. For example, the telescope-pointing problem discussed in Section 11.0 is a tracking problem: The fe~dback system of Figures 11.1 (c) and (d) has as its input the desired pointing angle, and the purppse of the feedback loop is to provide a mechanism for driving the telescope to follow' the input. In airplane autopilots the input is the desired flight path of the vehicle, and the autopilot feedback system uses the aircraft control surfaces (rudder, ailerons, and elevator) and thrust control in order to keep the aircraft on the prescribed course. To illtH;trate some of the issues that arise in the design of tracking systems, con- sider the discrete-time feedback system depicted in Figure 11.7(a). The examination of discrete-time tracking systems of this form often arises in analyzing the characteristics of sampled-data tracking systems for continuous-time applications. One example of such a sy~tem is a digital autopilot. In Figure 11.7(a), H p(Z) denotes the system function of the systerp. whose output is to be controlled. This system is often referred to as the plant, a term that can be traced to applications such as the control of power plant~, heating systems, and chemical-processing plants. The system function l[c(z) represents a compensator, which is the element to be designed. Here, the input to the compensator is the tracking error- Sec. 11.2 Some Applications and Consequences of Feedback 829 (a) x[n] 1---....--~y[n] Figure 11.7 (a) Discrete-time tracking system; (b) tracking sys- d[n] tern of (a) with a disturbance d[n] in the feedback path accounting for the (b) presence of measurement errors. that is, the difference e[n] between the input x[n] and the output y[n]. The output of the compensator is the input to the plant (for example, the actual voltage applied to the motor in the feedback system of Figures 11.1 (c) and (d) or the actual physical input to the drive system of the rudder of an aircraft). To simplify notation, let H(z) = Hc(z)Hp (Z). In this case, the application of eq. ( 11.2) yields the relationship H(z) Y(z) = 1 + H(z)X(z). (11.31) Also, since Y(z) = H(z)E(z), it follows that 1 E(z) = 1 + H(z) X(z), 0 1.32) or, specializing to z = eiw, we obtain jw) 1 X( jw) E(e· = 1 + H(eJw) e . (11.33) Equation ( 11.33) provides us with some insight into the design of tracking systems. Specif- ically, for good tracking performance, we would like e[n] or, equivalently, E(eiw) to be small. That is, 1 X jw) ~ 0 1 + H(eiw) (e - · (11.34) Consequently, for that range of frequencies for which X(eiw) is nonzero, we would like IH (e Jw )I to be large. Thus, we have one of the fundamental principles of feedback system design: Good tracking performance requires a large gain. This desire for a large gain, however, must typically be tempered, for several reasons. One reason is that if the gain is too large, the closed-loop system may have undesirable characteristics (such as too little 830 Linear Feedback Systems Chap. 11 damping) or might in fact become unstable. This possibility is discussed in the next section and is also addressed by the methods developed in subsequent sections. In addition to the issue of stability, there are other reasons for wanting to limit the gain in a tracking system. For example, in implementing such a system, we must measure the output y[n] in order to compare it to the command input x[n], and any measuring device used will have inaccuracies and error sources (such as thermal noise in the electronics of the device). In Figure 11.7(b), we have included these error sources in the form of a disturbance input d[n] in the feedback loop. Some simple system function algebra yields the following relationship between Y(z) and the transforms X(z) and D(z) of x[n] and d[n]: H(z) ] [ H(z) ] Y(z) = [ 1 + H(z) X(z) - 1 + H(z) D(z) . (11.35) From this expression, we see that in order to minimize the influence of d[n] on y[n], we would like H(z) to be small so that the second term on the right-hand side of eq. (11.35) is small. From the preceding development, we see that the goals of tracking and of minimiz- ing the effect of measurement errors are conflicting, and one must take this into account in coming up with an acceptable system design. In general, the design depends on more de- tailed information concerning the characteristics of the input x[n] and the disturbance d[n]. For example, in many applications x[n] has a significant amount of its energy concentrated at low frequencies, while measurement error sources such as thermal noise have a great deal of energy at high frequencies. Consequently, one usually designs the compensator Hc(Z) so that IH(ejw)l is large at low frequencies and is small for w near ±7T. There are a variety of other issues that one must consider in designing tracking sys- tems, such as the presence of disturbances at other points in the feedback loop. (For exam- ple, the effect of wind on the motion of an aircraft must be taken into account in designing an autopilot.) The methods of feedback system analysis introduced in this chapter provide the necessary tools for examining each of these issues. In Problem 11.57, we use some of these tools to investigate several other aspects of the problem of designing tracking systems. 11.2.6 Destabilization Caused by Feedback As well as having many applications, feedback can have undesirable effects and can in fact cause instability. For example, consider the telescope-pointing system illustrated in Figure 11.1. From the discussion in the preceding section, we know that it would be de- sirable to have a large amplifier gain in order to achieve good performance in tracking the desired pointing angle. On the other hand, as we increase the gain, we are likely to obtain faster tracking response at the expense of a reduction in system damping, resulting in sig- nificant overshoot and ringing in response to changes in the desired angle. Furthermore, instability can result if the gain is increased too much. Another common example of the possible destabilizing effect of feedback is feed- back in audio systems. Consider the situation depicted in Figure 11.8(a). Here, a loud- speaker produces an audio signal that is an amplified version of the sounds picked up by a microphone. Note that in addition to other audio inputs, the sound coming from the speaker itself may be sensed by the microphone. How strong this particular signal is depends upon Sec. 11.2 Some Applications and Consequences of Feedback 831 Speaker Amplifier (a) Total audio input to the microphone audio 1-----..... ---1~ Speaker inputs K1 output + (b) Ext~rnal 5J audio + ·-----~ K 1-----..----t•~ Speaker 1 o~p~ 1 -~ Figure 11 .8 (a) Pictorial repre- inputs L.___ sentation of the phenomenon of audio 1 feedback; (b) block diagram represen- - .-I_-K_2_e_ _s_T ...,I .....•. f---..... tation of (a); (c) block diagram in (b) redrawn as a negative feedback sys- tem. (Note: e-sr is the system function (c) of a T-second time delay.) the distance between the speaker and the microphone. Specifically, because of the attenu- ating properties of air, the strength of the signal reaching the microphone from the speaker decreases as the distance between the speaker and the microphone increases. In addition, due to the finite speed of propagation of sound waves, there is time delay between the signal produced by the speaker and that sensed by the microphone. This audio feedback system is represented in block diagram form in Figure 11.8(b). Here, the constant K2 in the feedback path represents the attenuation, and Tis the prop- agation delay. The constant K 1 is the amplifier gain. Also, note that the output from the feedback path is added to the external input. This is an example of positive feedback. As discussed at the beginning of the section, the use of a negative sign in the definition of the basic feedback system of Figure 11.3 is purely conventional, and positive and nega- tive feedback systems can be analyzed using the same tools. For example, as illustrated in Figure 11.8(c), the feedback system of Figure 11.8(b) can be written as a negative feedback 832 Linear Feedback Systems Chap. 11 system by adding a minus sign to the feedback-path system function. From this figure and from eq. (11.1), we can determine the closed-loop system function: Q(s) = K1 . (11.36) 1- K1K2e-sT Later we will return to this example, and, using a technique that we will develop in Section 11.3, we will show that the system of Figure 11.8 is unstable if (11.37) Since the attenuation due to the propagation of sound through the air decreases (i.e., K2 increases) as the distance between the speaker and the microphone decreases, if the mi- crophone is placed too close to the speaker, so that eq. (11.37) is satisfied, the system will be unstable. The result of this instability is an excessive amplification and distortion of audio signals. It is interesting to note that positive, or what Black referred to as regenerative, feed- back had also been known for some time before he invented his negative feedback am- plifier and, ironically, had been viewed as a very useful mechanism (in contrast to the skeptical view of negative feedback). Indeed, positive feedback can be useful. For exam- ple, it was already known in the 1920s that the destabilizing influence of positive feedback could be used to generate oscillating signals. This use of positive feedback is illustrated in Problem 11.54. In this section, we have described a number of the applications of feedback. These and others, such as the use of feedback in the implementation of recursive discrete-time filters (see Problem 11.55), are considered in more detail in the problems at the end of the chapter. From our examination of the uses of feedback and the possible stabilizing and destabilizing effects that it can have, it is clear that some care must be taken in designing and analyzing feedback systems to ensure that the closed-loop system behaves in a desir- able fashion. Specifically, in Sections 11.2.3 and 11.2.6, we have seen several examples of feedback systems in which the characteristics of the closed-loop system can be signif- icantly altered by changing the values of one or two parameters in the feedback system. In the remaining sections of this chapter, we develop several techniques for analyzing the effect of changes in such parameters on the closed-loop system and for designing systems to meet desired objectives such as stability, adequate damping, etc. 11 .3 ROOT-LOCUS ANALYSIS OF LINEAR FEEDBACK SYSTEMS As we have seen in a number of the examples and applications we have discussed, a useful type of feedback system is that in which the system has an adjustable gain K as- sociated with it. As this gain is varied, it is of interest to examine how the poles of the closed-loop system change, since the locations of these poles tell us a great deal about the behavior of the system. For example, in stabilizing an unstable system, the adjustable gain is used to move the poles into the left-half plane for a continuous-time system or inside the unit circle for a discrete-time system. In addition, in Problem 11.49, we show that feedback can be used to broaden the bandwidth of a first-order system by moving the pole so as to decrease the time constant of the system. Furthermore, just as feedback can be used"
11.3 Root-Locus Analysis of Linear Feedback Systems,"832 Linear Feedback Systems Chap. 11 system by adding a minus sign to the feedback-path system function. From this figure and from eq. (11.1), we can determine the closed-loop system function: Q(s) = K1 . (11.36) 1- K1K2e-sT Later we will return to this example, and, using a technique that we will develop in Section 11.3, we will show that the system of Figure 11.8 is unstable if (11.37) Since the attenuation due to the propagation of sound through the air decreases (i.e., K2 increases) as the distance between the speaker and the microphone decreases, if the mi- crophone is placed too close to the speaker, so that eq. (11.37) is satisfied, the system will be unstable. The result of this instability is an excessive amplification and distortion of audio signals. It is interesting to note that positive, or what Black referred to as regenerative, feed- back had also been known for some time before he invented his negative feedback am- plifier and, ironically, had been viewed as a very useful mechanism (in contrast to the skeptical view of negative feedback). Indeed, positive feedback can be useful. For exam- ple, it was already known in the 1920s that the destabilizing influence of positive feedback could be used to generate oscillating signals. This use of positive feedback is illustrated in Problem 11.54. In this section, we have described a number of the applications of feedback. These and others, such as the use of feedback in the implementation of recursive discrete-time filters (see Problem 11.55), are considered in more detail in the problems at the end of the chapter. From our examination of the uses of feedback and the possible stabilizing and destabilizing effects that it can have, it is clear that some care must be taken in designing and analyzing feedback systems to ensure that the closed-loop system behaves in a desir- able fashion. Specifically, in Sections 11.2.3 and 11.2.6, we have seen several examples of feedback systems in which the characteristics of the closed-loop system can be signif- icantly altered by changing the values of one or two parameters in the feedback system. In the remaining sections of this chapter, we develop several techniques for analyzing the effect of changes in such parameters on the closed-loop system and for designing systems to meet desired objectives such as stability, adequate damping, etc. 11 .3 ROOT-LOCUS ANALYSIS OF LINEAR FEEDBACK SYSTEMS As we have seen in a number of the examples and applications we have discussed, a useful type of feedback system is that in which the system has an adjustable gain K as- sociated with it. As this gain is varied, it is of interest to examine how the poles of the closed-loop system change, since the locations of these poles tell us a great deal about the behavior of the system. For example, in stabilizing an unstable system, the adjustable gain is used to move the poles into the left-half plane for a continuous-time system or inside the unit circle for a discrete-time system. In addition, in Problem 11.49, we show that feedback can be used to broaden the bandwidth of a first-order system by moving the pole so as to decrease the time constant of the system. Furthermore, just as feedback can be used Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 833 to relocate the poles to improve system performance, as we saw in Section 11.2.6, there is the potential danger that with an improper choice of feedback a stable system can be destabilized, which is usually undesirable. In this section, we discuss a particular method for examining the locus (i.e., the path) in the complex plane of the poles of the closed-loop system as an adjustable gain is varied. The procedure, referred to as the root-locus method, is a graphical technique for plotting the closed-loop poles of a rational system function Q(s) or Q(z) as a function of the value of the gain. The technique works in an identical manner for both continuous-time and discrete-time systems. 11 .3. 1 An Introductory Example To illustrate the basic nature of the root-locus method for analyzing a feedback system, let us reexamine the discrete-time example considered in the preceding section and specified by the system functions z [eq. (11.23)] H(z) = 1 - 2z-' (11.38) z-2 and [e q. ( II. 24)] G(z) = 2{3z- 1 = 213 , (11.39) z where {3 now is viewed as an adjustable gain. Then, as we noted earlier, the closed-loop system function is 1 z [eq. (11.25)] Q(z) = 1- 2(1- {3)z- 1 z- (11.40) 2(1 - {3). In this example, it is straightforward to identify the closed-loop pole as being located at z = 2(1 - {3). In Figure 11.9(a), we have plotted the locus of the pole for the system as {3 varies from 0 to + oo. In part (b) of the figure, we have plotted the locus as {3 varies from 0 to -oo. In each plot, we have indicated the point z = 2, which is the open -loop pole [i.e., it is the pole of Q(z) for {3 = 0]. As {3 increases from 0, the pole moves to the left of the point z = 2 along the real axis, and we have indicated this by including an arrow on the thick line to show how the pole changes as {3 is increased. Similarly, for {3 < 0, the pole of Q(z) moves to the right of z = 2, and the direction of the arrow in Figure 11.9(b) indicates how the pole changes as the magnitude of {3 increases. For 1/2 < {3 < 3/2, the pole lies inside the unit circle, and thus, the system is stable. As a second example, consider a continuous-time feedback system with s H(s) = -- (11.41) s-2 and 2{3 G(s) = -, (11.42) s where {3 again represents the adjustable gain. Since H (s) and G(s) in this example are algebraically identical to H(z) and G(z), respectively, in the preceding example, the same 834 Linear Feedback Systems Chap. 11 Unit circle - -,/ I \ I I 2 <R.e (a) Unit circle - -,/ I \ I I I 2 (Jl.e Figure 1 1 . 9 Root locus for the closed-loop system of eq. (11.40) as the value of {3 is varied: (a) {3 > 0; (b) {3 < 0. Note that we have marked the point z = 2 that corresponds to (b) the pole location when {3 = 0. will be true for the closed-loop system function s Q(s) = s- 2(1 - {3) (11.43) vis-a-vis Q(z), and the locus of the pole as a function of f3 will be identical to the locus in that example. The relationship between these two examples stresses the fact that the locus of the poles is determined by the algebraic expressions for the system functions of the for- ward and feedback paths and is not inherently associated with whether the system is a continuous-time or discrete-time system. However, the interpretation of the result is inti- mately connected with its continuous-time or discrete-time context. In the discrete-time case it is the location of the poles in relation to the unit circle that is important, whereas in the continuous-time case it is their location in relation to the imaginary axis. Thus, as we have seen for the discrete-time example in eq. ( 11.40), the system is stable for 112 < f3 < 3/2, while the continuous-time system of eq. ( 11.43) is stable for f3 > 1. 11.3.2 Equation for the Closed-Loop Poles In the simple example considered in the previous section the root locus was easy to plot, since we could first explicitly determine the closed-loop pole as a function of the gain parameter and then plot the location of the pole as we changed the gain. For more complex Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 835 systems, one cannot expect to find such simple closed-form expressions for the closed-loop poles. However, it is still possible to sketch accurately the locus of the poles as the value of the gain parameter is varied from -oo to +oo, without actually solving for the location of the poles for any specific value of the gain. This technique for determining the root locus is extremely useful in gaining insight into the characteristics of a feedback system. Also, as we develop the method, we will see that once we have determined the root locus, there is a relatively straightforward procedure for determining the value of the gain parameter that produces a closed-loop pole at any specified location along the root locus. We will phrase our discussion in terms of the Laplace transform variables, with the understanding that it applies equally well to the discrete-time case. Consider a modification of the basic feedback system of Figure 11.3(a), where either G(s) or H(s) is cascaded with an adjustable gain K. This is illustrated in Figure 11.10. In ei- ther of these cases, the denominator of the closed-loop system function is 1 + KG(s)H(s).2 Therefore, the equation for the poles of the closed-loop system are the solutions of the equation 1 + KG(s)H(s) = 0. (11.44) x(t) + + K ~ H(s) y(t) - G(s) Q(s) = 1 + ~~~:~G(s) (a) x(t) --..:. ~+8 • H(s) y(t) - K ~ G(s) - Figure 11.1 0 Feedback systems H(s) containing an adjustable gain: (a) sys- Q(s) = 1 + KH(s)G(s) tem in which the gain is located in the forward path; (b) system with the gain (b) in the feedback path. 21n the following discussion, we assume for simplicity that there is no pole-zero cancellation in the product G(s)H(s). The presence of such pole-zero cancellations does not cause any real difficulties, and the procedure we will outline here is easily extended to that case (Problem 11.32). In fact, the simple example at the start ofthis section [e qs. (11.41) and ( 11.42)] does involve a pole-zero cancellation, at s = 0. 836 Linear Feedback Systems Chap. 11 Rewriting eq. (11.44), we obtain the basic equation determining the closed-loop poles: 1 G(s)H(s) = - K' (11.45) The technique for plotting the root locus is based on the properties of this equation and its solutions. In the remainder of this section, we will discuss some of these properties and indicate how they can be exploited in determining the root locus. 11.3.3 The End Points of the Root Locus: The Closed-Loop Poles for K = 0 and IK l = +oo Perhaps the most immediate observation that one can make about the root locus is that obtained by examining eq. (11.45) forK = 0 and IKI = oo. ForK = 0, the solution of this equation must yield th~ poles of G(s)H(s), since l!K = oo. To illustrate, recall the example given by eqs. (11.41) and (11.42). If we let {3 play the role of K, we see that eq. (11.45) becomes 2 1 (11.46) s-2 73' Therefore, for {3 = 0, the pole of the system will be located at the pole of 2/(s - 2) (i.e., at s = 2), which agrees with what we depicted in Figure 11.9. Suppose now that IKI = oo. Then l!K = 0, so that the solutions of eq. (11.45) must approach the zeros of G(s)H(s). If the order of the numerator of G(s)H(s) is smaller than that of the denominator, then some of these zeros, equal in number to the difference in order between the denominator and numerator, will be at infinity. Referring again to eq. (11.46), since the order of the denominator of 2/(s- 2) is 1, while the order of the numerator is zero, we conclude that\n this example there is one zero at infinity and no zeros in the finite s-plane. Thus, as lf31 ~ oo, the closed-loop pole approaches infinity. Again, this agrees with Figure 11.9, in which the magnitude of the pole increases without bound as lf31 ~ oo for either {3 > 0 or {3 < 0. While the foregoing observations provide us with basic information as to the closed- loop pole locations for the extreme values of K, the following result is the key to our being able to plot the root locus without actually solving for the closed-loop poles as explicit functions of the gain. 11 .3.4 The Angle Criterion Consider again eq. (11.45). Since the right-hand side of this equation is real, a point s0 can be a closed-loop pole only if the left-hand side of the equation, i.e., G(s0)H(s0), is also real. Writing G(so)H(so) = IG(so)H(so)l ei<tG(so)H(so), (11.47) we see that, for G(s0)H(s0) to be real, it must be true that ei<tG(so)H(so) = ± 1. (11.48) Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 837 That is, for s0 to be a closed-loop pole, we must have <r.G (so) H (so) = integer multiple of 7T. (11.49) Returning to eq. (11.46), we see immediately that in order for 2/(so- 2) to be real, it is necessary that s0 be real. For more complex system functions, it is not as easy to determine the values of so for which G (so) H (so) is real. However, as we will see, the use of the angle criterion given by eq. (11.49), together with the geometric method described in Chapter 9 for evaluating <r.G (so) H (so), greatly facilitates the determination of the root locus. The angle criterion given by eq. (11.49) provides us with a direct method for deter- mining whether a point s0 could be a closed-loop pole for some value of the gain K. A further examination of eq. (11.45) gives us a way in which to calculate the value of the gain corresponding to any point on the root locus. Specifically, suppose that s0 satisfies <r.G (so) H (so) = odd multiple of 7T. (11.50) Then ei-t.G(so)H(so) = -1, and from eq. (11.47) we see that G(so)H(so) = -IG(so)H(so)l. (11.51) Substituting eq. (11.51) into eq. (11.45), we find that if K = 1 (11.52) IG(so)H(so)l' then s0 is a solution of the equation and hence a closed-loop pole. Similarly, if s0 satisfies the condition <r.G (so) H (so) = even multiple of 7T, (11.53) then G (so) H (so) = IG (so) H (so) I. (11.54) Thus, if K =- 1 (11.55) IG (so) H (so)l' then s0 is a solution of eq. (11.45) and hence a closed-loop pole. For the example given in eq. (11.46), if so is on the real line and s0 < 2, then <r. (-2 )= -7T, (11.56) so- 2 and from eq. (11.52), the value of {3 for which s0 is the closed-loop pole is {3 = _1_ = 2 - so (11.57) ls0~21 2 838 Linear Feedback Systems Chap. 11 That is, so = 2(1 - {3), (11.58) which agrees with eq. (11.43). Summarizing the last two observations that we have made, we see that the root locus for the closed-loop system, that is, the set of points in the complex s-plane that are closed- loop poles for some value of K asK varies from -oc., to +oc.,, are precisely those points that satisfy the angle condition of eq. ( 11.49). Furthermore: 1. A point so for which <t.G (so) H (so) = odd multiple of 7T (11.59) is on the root locus and is a closed-loop pole for some value of K > 0. The value of the gain that makes s0 a closed-loop pole is given by eq. ( 11.52). 2. A point s0 for which <t.G (so) H (so) = even multiple of 7T (11.60) is on the root locus and is a closed-loop pole for some value of K < 0. The value of the gain that makes s0 a closed-loop pole is given by eq. ( 11.55). Therefore, we have now reduced the problem of determining the root locus to that of searching for points that satisfy the angle requirements given by eqs. (11.59) and (11.60). These equations can be refined further to a set of properties that aid in sketching the root locus. Before discussing these properties, however, let us consider a simple example. Example 1 1 . 1 Let H(s) = s + 1, G(s) = s + 2"" (11.61) Recall that in Section 9.4 we discussed the geometric evaluation of Laplace transforms. In that section, we saw that the angle of the rational Laplace transform k=l n (11.62) ll (s- ak) k=l evaluated at some point s0 in the complex plane equals the sum of the angles of the vectors from each of the zeros to s0 minus the sum of the angles from each of the poles to s0 . Applying this to the product of G(s)H(s), where G(s) and H(s) are as given in eq. ( 11.61 ), we can determine geometrically those points in the s-plane that satisfy the angle criteria, eqs. (11.59) and (11.60), and therefore can sketch the root locus. In Figure 11.11, we have plotted the poles of G(s )H (s) and have denoted by () and <P the angles from each of the poles to the point s0 . Let us first test the angle criterion Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 839 -2 -1 CRe Figure 11.11 Geometric procedure for evaluating angle criterion in Exam- ple 11.1. for points so on the real axis. To begin with, the angle contribution from both poles is zero when so is on the real axis to the right of - 1. Thus, <r..G (so) H (so) = 0 = 0 · 1T, s0 real and greater than - 1, (11.63) and by eq. (11.60), these points are on the root locus for K < 0. For points between the two poles, the pole at -1 contributes an angle of -1T, and the pole at -2 contributes 0. Thus, <r..G(so)H(so) = -1T, so real, -2 < so < - 1. (11.64) These points are on the locus for K > 0. Finally, each pole contributes an angle of -7r when s0 is real and less than -2, so that <r..G (so) H (so) = -21T, s0 real and less than -2. Therefore, these points are on the locus for K < 0. Let us now examine points in the upper half of the s-plane. (Since the impulse responses are real-valued, the complex poles occur in conjugate pairs. Therefore, we can immediately determine the poles in the lower half-plane after we have examined the upper half.) From Figure 11.11, the angle of G (so) H (so) at the point s0 is <r..G (so) H (so) = -((} + cp). (11.65) Also, it is clear that as so ranges over the upper half-plane (but not the real axis), we have 0 < (} < 1T, 0 < 4J < 1T. (11.66) Thus, -21T < <r..G(s)H(s) < 0, (11.67) Therefore, we see immediately that no point in the upper half-plane can be on the locus forK< 0 [since <r..G(s)H(s) never equals an even multiple of 1r]. In addition, if s0 is to be on the locus for K > 0, we must have <r..G (so) H (so) = -((} + cp) = -1T, (11.68) 840 Linear Feedback Systems Chap. 11 or () = 7T- ¢. (11.69) Examining the geometry of Figure 11.11, we see that this occurs only for those points located on the straight line that is parallel to the imaginary axis and that bisects the line joining the poles at - 1 and -2. We have now examined the entire s-plane and have determined all those points on the root locus. In addition, we know that for K = 0, the closed-loop poles equal the poles of G(s)H(s), and as IKI ~ oo, the closed-loop poles go to the zeros of G(s)H(s), which in this case are both at infinity. Putting these results together, we can draw the entire root locus, depicted in Figure 11.12, in which we have indicated the direction of increasing IKI, both for K > 0 and for K < 0. -2 -1 (a) -2 -1 (b) Figure 11.12 Root locus for Example 11.1: (a) K > 0; (b) K < 0. The poles of G(s)H(s), which are located at s = -1 and s = -2, are indicated. Note from the figure that forK > 0 there are two branches of the root locus and that the same is true for K < 0. The reason for the existence of two branches is that in this example the closed-loop system is a second-order system and consequently has two poles for any specified value of K. Therefore, the root locus has two branches, each of which traces the location of one of the closed-loop poles asK is varied, and for any particular value of K, there is one closed-loop pole on each branch. Again, if we wish to calculate the value of K for which a specific point s0 on the locus is a closed-loop pole, we can use eqs. (11.52) and (11.55). Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 841 11 . 3. 5 Properties of the Root Locus The procedure outlined in the preceding subsection provides us, in principle, with a method for determining the root locus for any continuous-time or discrete-time LTI feedback sys- tem. That is, we simply determine, graphically or otherwise, all those points that satisfy eq. (11.59) or eq. (11.60). Fortunately, there are a number of other geometric properties concerning root loci that make the sketching of a locus far less tedious. To begin our dis- cussion of these properties, let us assume that we have placed G(s)H(s) in the standard form nm (s- f3k) Sm + bm-ISm-l + ... + bo G(s)H(s) = k=l (11.70) sn + an-ISn-l + ... + ao nn (s- ak) k=l where the f3k's denote the zeros and the ak's denote the poles. In general, these may be complex. Also, from eq. (11.70), we see that we are assuming that the leading coeffi- cient in both the numerator anp the denominator of G(s)H(s) is + 1. This can always be achieved by dividing the numerator and denominator by the denominator coefficient of sn and absorbing the resulting numerator coefficient of sm into the gain K. For example, K 2s + 1 =K ~s + _!_ (2) s + _!_ 2 3 3 =-K 2 (11.71) 3s + 5s + 2 s2 + 2_s + ~ 3 s2 + 2_s + ~' 3 3 3 3 and the quantity (2/3 )K i~ then regarded as the overall gain that is varied in determining the root locus. We assume, in addition, that m $ n, (11.72) which is the case that is usually encountered in practice. (Problem 11.33 considers the case m > n.) The following are some properties that include earlier observations and that aid in sketching the root locus. Property 1: ForK = 0, the solutions of eq. (11.45) are the poles of G(s)H(s). Since we are assuming n poles, the root locus has n bra~ches, each one starting (for K = 0) at a pole of G(s)H(s). Property 1 includes the g~neral version of a fact which we noted in Example 11.1: that there is one branch of the root locus for each closed-loop pole. The next property is also simply a restatement of one of our earlier observations. Property 2: As IKI ~ oo, each branch of the root locus approaches a zero of G(s)H(s). Since we are assuming that m $ n, n- m of these zeros are at infinity. 842 Linear Feedback Systems Chap. 11 Property 3: Parts of the real s-axis that lie to the left of an odd number of real poles and zeros of G(s)H(s) are on the root locus for K > 0. Parts of the real s-axis that lie to the left of an even number (possibly zero) of poles and zeros of G(s)H(s) are on the root locus forK < 0. We can show that Property 3 is true as follows: From our discussion in Example 11.1 and from Figure 11.13(a), we see that if a point on the real s-axis is to the right of a real pole or zero of G(s)H(s), that pole or zero contributes zero to <t-G(s0)H(s0 ). On the other hand, if s0 is to the left of a zero, that zero contributes +7T, whereas if s0 is to the left of a pole, we get a contribution of -7r (since we subtract the pole angles). Hence, if s0 is to the left of an odd number of real poles and zeros, the total contribution of these poles and zeros is an odd multiple of 7T, whereas if s0 is to the left of an even number of real poles and zeros, the total contribution is an even multiple of 7T. From eqs. (11.59) and (11.60), we will have the result stated in Property 3 if we can show that the total contribution from all poles and zeros with nonzero imaginary parts is an even multiple of 7T. The key here is that such poles and zeros occur in complex-conjugate pairs, and we can consider the contribution from each such pair, as illustrated in Figure 11.13(b). The symmetry in the picture clearly indicates that the sum of the angles from this pair to any point s0 on the real axis is precisely 27T. Summing over all conjugate zero pairs and subtracting the sum over all conjugate pole pairs, we get the desired result. Thus, any segment of the real line Angle of zero radians Angle of 7r radians t So (a) Figure 11. 13 (a) Angle contribu- tion from real poles and zeros to a point on the real axis; (b) total angle contribution from a complex-conjugate (b) pole pair to a point on the real axis. Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 843 between real poles or zeros is on the root locus either forK > 0 or forK < 0, depending on whether it lies to the left of an odd or an even number of poles and zeros of G(s)H(s). As one consequence of Properties 1 through 3, consider a segment of the real axis between two poles of G(s )H (s ), with no zeros between these poles. From Property 1 the root locus begins at the poles, and from Property 3 the entire portion of the real axis between the two poles will lie on the root locus for a positive or negative range of values of K. Therefore, as IKI increases from zero, the two branches of the root locus that begin at these poles move toward each other along the segment of the real axis between the poles. From Property 2, as IKI increases toward infinity, each branch of the root locus must approach a zero. Since there are no zeros along that portion of the real axis, the only way that this can happen is if the branches break off into the complex plane for IKI sufficiently large. This is illustrated in Figure 11.12, in which the locus for K > 0 has a portion between two real poles. AsK is increased, the root locus eventually leaves the real axis, forming two complex-conjugate branches. Summarizing this discussion, we have the following property of the root locus: Property 4: Branches of the root locus between two real poles must break off into the complex plane for IKilarge enough. Properties 1-4 serve to illustrate how characteristics of the root locus can be de- duced from eqs. (11.45), (11.59), and (11.60). In many cases, plotting the poles and zeros of G(s)H(s) and then using these four properties suffices to provide a reasonably accurate sketch of the root locus. (See Examples 11.2 and 11.3.) In addition to these properties, however, there are numerous other characteristics of the root locus that allow one to ob- tain sketches of increasing accuracy. For example, from Property 2, we know that n - m branches of the root locus approach infinity. In fact, these branches approach infinity at specific angles that can be calculated, and therefore, the branches are asymptotically par- allel to lines at these angles. Moreover, it is possible to draw in the asymptotes and then determine the point at which they intersect. These two properties and several others are illustrated in Problems 11.34-11.36 and 11.41-11.42. A more detailed development of the root-locus method can be found in more advanced texts such as those listed in the bibliography at the end of the book. In the remainder of this section we present two examples, one in continuous time and one in discrete time, that illustrate how the four properties that we have just described allow us to sketch the root locus and to deduce the stability characteristics of a feedback system as the gain K is varied. Example 11 .2 Let s- 1 G(s)H(s) = (s + 1)(s + 2) (11.73) From Properties 1 and 2, the root locus for either K positive or K negative starts at the points s = - 1 and s = -2. One branch terminates at the zero at s = 1 and the other at infinity. Let us first consider K > 0. The root locus in this case is illustrated in Fig- ure 11.14(a). From Property 3, we can identify the regions of the real axis that are on 844 Linear Feedback Systems Chap. 11 9m -2 -1 (a) 9m (b) Figure 11.14 Root locus for Example 11.2: (a) K > 0; (b) K < 0. The poles of G(5)H(5) at 5 = -1 and 5 = -2 and the zero of G(5)H(5) at 5 = 1 are indicated in the figure. the root locus for K > 0-specifically , CRe{ s} < - 2 and - 1 < CRe{ s} < 1. Therefore, one branch of the root locus forK > 0 originates at s = - 1 and approaches s = 1 as K ~ oo. The other begins at s = -2 and extends to the left toward CRe{s} = -oo asK ~ +oo. Thus, we see that for K > 0, if K is sufficiently large, the system will become unstable, as one of the closed-loop poles moves into the right-half plane. The procedure that we have used for sketching the root locus does not, of course, indicate the value of K for which this instability develops. However, for this particular example, we see that the value of K for which the instability occurs corresponds to the root locus passing through s = 0. Consequently, from eq. (11.52), the corresponding value of K is 1 K= =2 (11.74) IG(O)H(O)I . Thus, the system is stable for 0 ::; K < 2, but is unstable for K ~ 2. ForK < 0, the portions of the real axis lying on the root locus are CR..e{s} > 1 and -2 < CR..e{s} < -1. Thus, the root locus again starts at the points s = -2 and s = -1, moving into the region -2 < CR..e{s} < -1. At some point, it breaks off into the complex plane and follows a trajectory such that it returns to the real axis for s > 1. Upon the root locus' return to the real axis, one branch moves to the left toward the zero at s = 1 and the other to the right towards = oo, as indicated in Figure 11.14(b), in which we have displayed an accurate plot of the root locus for K < 0. Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 845 Rules can also be developed to indicate the locations at which the root locus leaves and enters the real axis. Even without that precise a description, however, we can sketch the general shape of the root locus in Figure 11.14(b) and can therefore deduce that for K < 0, the system also becomes unstable for IKI sufficiently large. Example 1 1 . 3 Consider the discrete-time feedback system illustrated in Figure 11.15. In this case, (11.75) + K x[n] 1---.....- ~ y[n] Figure 11. 1 5 Discrete-time feedback system of Example 11.3. As discussed at the beginning of this section, the techniques for sketching the root locus of a discrete-time feedback system are identical to those used in the continuous-time case. Therefore, in a manner exactly analogous to that of the preceding example, we can deduce the basic form of the root locus for this example, which is illustrated in Figure 11.16. In this case the portion of the real axis between the two poles of G(z)H(z) (at z = 1/4 and z = 112) is on the root locus forK > 0, and asK increases the locus breaks off into the complex plane and returns to the axis at some point in the left-half plane. From there, one branch approaches the zero of G(z)H(z) at z = 0, and the other approaches infinity as K ~ oo. The form of the root locus for K < 0 consists of two branches on the real axis, one approaching 0 and the other infinity. As we remarked earlier, while the form of the root locus does not depend on whether the system is a continuous-time or discrete-time system, any conclusion re- garding stability based on examining the locus certainly does. For this example, we can conclude that for IKI sufficiently large the system is unstable, since one of the two poles has magnitude greater than 1. In particular, from the K > 0 root locus in Figure 11.16(a), we see that the transition from stability to instability occurs when one of the closed-loop poles is at z = -1. From eq. (11.52), the corresponding value of K is K= 1 =_!2 (11.76) IG ( -l)H (- 1) I 8 . Similarly, from Figure 11.16(b ), the stability-instability transition occurs when one of the closed-loop poles is at z = 1, and from eq. (11.55), the corresponding value of K is 1 3 K = - = -- (11.77) IG(l)H(l)l 8' Putting this together, we see that the closed-loop system in Figure 11.16 is stable if 846 Linear Feedback Systems Chap. 11 9m. - - - - - , , , ',/Unit circle // / I ' \ \ I I I \ I CR.e I I (a) 9m. - - - - - , , , , /Unit circle ' I ' \ I \ I \ 1 1 CRe 4 2 (b) Figure 11 .16 Root locus for Example 11.3: (a) K > 0; (b) K < 0. The poles of G(z)H(z) at z = 1/4 and z = 1/2 and the zero of G(z)H(z) at z = 0 are indicated in the figure. 3 15 -- < K <- (11.78) 8 8 and is unstable for K outside this range. 11 .4 THE NYQUIST STABILITY CRITERION As developed in Section 11.3, the root-locus technique provides detailed information con- ceming the location of closed-loop poles as the system gain is varied. From root-locus plots, one can determine the damping and the stability characteristics of the system as K is varied. Determination of the root locus requires the analytic description of the system"
11.4 The Nyquist Stability Criterion,"846 Linear Feedback Systems Chap. 11 9m. - - - - - , , , ',/Unit circle // / I ' \ \ I I I \ I CR.e I I (a) 9m. - - - - - , , , , /Unit circle ' I ' \ I \ I \ 1 1 CRe 4 2 (b) Figure 11 .16 Root locus for Example 11.3: (a) K > 0; (b) K < 0. The poles of G(z)H(z) at z = 1/4 and z = 1/2 and the zero of G(z)H(z) at z = 0 are indicated in the figure. 3 15 -- < K <- (11.78) 8 8 and is unstable for K outside this range. 11 .4 THE NYQUIST STABILITY CRITERION As developed in Section 11.3, the root-locus technique provides detailed information con- ceming the location of closed-loop poles as the system gain is varied. From root-locus plots, one can determine the damping and the stability characteristics of the system as K is varied. Determination of the root locus requires the analytic description of the system Sec. 11.4 The Nyquist Stability Criterion 847 functions of the forward and feedback paths and is applicable only when these transforms are rational. For example, it cannot be directly applied in situations in which our knowledge of the system functions is obtained purely from experimentation. In this section, we introduce another method for the determination of the stability of feedback systems as a function of an adjustable gain parameter. This technique, referred to as the Nyquist criterion, differs from the root-locus method in two basic ways. Unlike the root-locus method, the Nyquist criterion does not provide detailed information concerning the location of the closed-loop poles as a function of K, but rather, simply determines whether or not the system is stable for any specified value of K. On the other hand, the Nyquist criterion can be applied to nonrational system functions and in situations in which no analytic description of the forward and feedback path system functions is available. Our objective in this section is to outline the basic ideas behind the Nyquist criterion for both continuous-time and discrete-time systems. As we will see, both the discrete- time and continuous-time Nyquist tests are the result of the same fundamental concept, although, as with the root-locus method, the actual criteria for stability differ because of the differences between continuous and discrete time. More detailed developments of the ideas behind the Nyquist criterion and its use in the design of feedback systems can be found in texts on the analysis and synthesis of feedback systems and automatic control systems, including those listed in the bibliography at the end of the book. To introduce the method, let us recall that the poles of the closed-loop systems of Figure 11.10 and their discrete-time counterparts are the solutions of the equations 1 + KG(s)H(s) = 0 (continuous time) (11.79) and 1 + KG(z)H(z) = 0 (discrete time). (11.80) For discrete-time systems, we want to determine whether any of the solutions of eq. (11.80) lie outside the unit circle, and for continuous-time systems whether any of the solutions of eq. (11.79) lie in the right half of the s-plane. The Nyquist criterion fixes this by examina- tion of the values of G(s)H(s) along the jw-axis and the values of G(z)H(z) along the unit circle. The basis for this is the encirclement property, which we develop in the following subsection. 11.4.1 The Encirclement Property Consider a general rational function W(p), where pis a complex variable,3 and suppose that we plot W (p) for values of p along a closed contour in the p-plane, which we traverse in a clockwise direction. This is illustrated in Figure 11.17 for a function W (p) that has two zeros and no poles. In Figure 11.17(a) we show a closed contour C in the p-plane, and in Figure 11.17 (b) we plot the closed contour of the values of W (p) asp varies around C. 3Because we will use the property we are about to develop for both continuous-time and discrete-time feedback systems, we have chosen to describe it in terms of a general complex variable p. In the next subsection we use this property to analyze continuous-time feedback systems, where the complex variable iss. Following this, in Section 11.4.3 we use the encirclement property for discrete-time feedback systems, in which context the complex variable is z. 848 Linear Feedback Systems Chap. 11 p-plane (a) W-plane Figure 11 . 1 7 Basic encirclement property. The closed curve in (b) rep- resents a plot of the values of W(p) ffi-e for values of p along the curve C in (a). Here, the arrow on the curve C in (a) indicates the direction in which C is traversed, and the arrow in (b) indicates the corresponding direction (b) along the contour of values of W(p). In this example, there is one zero of W(p) inside the contour and one zero of W(p) outside the contour. At any point p on C, the angle of W(p) is the sum of the angles of the two vectors v1 and v2 to the point p. As we traverse the contour once, the angle 4> 1 of the vector from the zero inside the contour encounters a net change of -21T radians, whereas the angle 4>2 of the vector from the zero outside the contour encounters no net change. Thus, on the plot of W(p), there is a net change in angle of -21T. Said another way, the plot of W (p) in Figure 11.17 (b) encircles the origin once in the clockwise direction. More generally, for an arbitrary rational W(p), as we traverse a closed contour in the clock- wise direction, any poles and zeros of W (p) outside the contour will contribute no net change to the angle of W(p), whereas each zero inside the contour will contribute a net change of -21T and each pole inside will contribute a net change of +21T. Since each net change of -21T in W(p) corresponds to one clockwise encirclement of the origin in the plot of W(p), we can state the following basic encirclement property: Encirclement Property: As a closed contour C in the p-plane is traversed once in the clockwise direction, the corresponding plot of W (p) for values of p along the contour encircles the origin in the clockwise direction a net number of times equal to the number of zeros minus the number of poles contained within the contour. In applying this statement, a counterclockwise encirclement is interpreted as the neg- ative of one clockwise encirclement. For example, if there is one pole and no zeros inside the contour, there will be one counterclockwise, or equivalently, minus-one clockwise, encirclement. Sec. 11.4 The Nyquist Stability Criterion 849 Example 11 .4 Consider the function p-1 w( p) = (p + 1) (p2 + p + 1). (11.81) 9m 9m X (a) 9m 9m p-plane W-plane X (b) 9m 9m p-plane W-plane (c) Figure 11. 18 Basic encirclement property for Example 11.4: (a) the con- tour encircles no poles or zeros and consequently W(p) has no encirclements of the origin; (b) the contour encircles one pole and therefore W(p) has one encirclement of the origin; (c) the contour encircles three poles and therefore W(p) has three encirclements of the origin; 850 Linear Feedback Systems Chap. 11 !1m !1m p-plane W-plane <R.e X (d) !1m !1m p-plane W-plane CRe CRe (e) Figure 11.18 Continued (d) the contour encircles one pole and one zero and therefore W(p) has no encirclements of the origin; (e) the contour encir- cles three poles and one zero. W(p) has two encirclements of the origin. In Figure 11.18, we depict several closed contours in the complex p-plane and the corre- sponding plots of W(p) along each of these contours. In Figure 11.18(a), the contour C1 does not encircle any of the poles or zeros of W(p), and consequently, the plot of W(p) has no net encirclements of zero. In Figure 11.18(b ), only the pole at p = - 1 is contained within the contour Cz, and the plot of W(p) encircles the origin once in the counterclock- wise direction (minus-one clockwise encirclements). In Figure 11.18( c), C3 encircles all three poles, and the plot of W(p) encircles the origin three times in a counterclockwise direction. In Figure 11.18( d), C4 encircles one pole and one zero, and therefore, the plot of W (p) has no net encirclements of the origin. Finally, in Figure 11.18( e), all of the poles and the one zero of W(p) are contained within C5, and thus, the plot of W(p) along this contour has two net counterclockwise encirclements of the origin. 11.4.2 The Nyquist Criterion for Continuous-Time LTI Feedback Systems In this section, we exploit the encirclement property in examining the stability of the continuous-time feedback system of Figure 11.10. Stability of this system requires that Sec. 11.4 The Nyquist Stability Criterion 851 no zeros of 1 + KG(s)H(s), or equivalently, of the function 1 R(s) = K + G(s)H(s) (11.82) lie in the right half of the s-plane. Thus, in applying the general result developed in the preceding subsection, we can consider the contour indicated in Figure 11.19. From the plot of R(s), as s traverses the contour C we can obtain a count of the number of zeros minus the number of poles of R(s) contained within the contour by counting the number of clockwise encirclements of the origin. As M increases to infinity, this then corresponds to the number of zeros minus the number of poles of R(s) in the right half of the s-plane. 9m jM CRe Figure 11. 19 Closed contour con- taining a portion of the right-half plane; -jM as M ~ oo, the contour encloses the entire right-half plane. Let us examine the evaluation of R(s) along the contour in Figure 11.19 as M in- creases to infinity. Along the semicircular portion of the contour extending into the right- half plane, we must ensure that R(s) remains bounded as M increases. Accordingly, we will assume that R(s) has at least as many poles as zeros. In that case, = b sn + b -1 sn - 1 + . . . + bo R(s) n n (11.83) ansn + an-1sn- 1 + ... + ao and lim R(s) = bn = constant. (11.84) isl~oo an Therefore, as M increases to infinity, the value of R(s) does not change as we traverse the semicircular part of the contour, and consequently, the constant value along this part is equal to the value of R(s) at the end points [i.e., R(jw) at w = ±oo]. Therefore, the plot of R(s) along the contour of Figure 11.19 can be obtained by plotting R(s) along the part of the contour that coincides with the imaginary axis-that is, the plot of R(jw) as w varies from -oo to +oo. Since R(jw) equals 1/K + G(jw)H(jw), R(s) along the contour can be drawn from knowledge of G(jw) and H(jw ). If both the forward- and feedback-path systems are stable, G(jw) and H(jw) are simply the frequency-response functions of these systems. However, the encirclement property for the general function W(p) is simply a property of complex functions; it has nothing to 852 Linear Feedback Systems Chap. 11 do with wnether W(p) arose as the Laplace or z-transform of any signal and, conse- quently, has nothing to do with regions of convergence. Thus, even if the forward- and feedback-path systems are unstable, if we examine the plot of the function R(jw) = 1/ K + G(jw )H(jw) for -oo < w < oo, we know that the net number of clockwise en- circlements of the origin will equal the number of zeros minus the number of poles of R(s) that lie in the right-half plane. Furthermore, from eq. (11.82), we see that the poles of R(s) are simply the poles of G(s)H(s), while the zeros of R(s) are the closed-loop poles. In addition, since G(jw)H(jw) = R(jw)- 1/K, it follows that the plot of G(jw)H(jw) encircles the point -1/ K exactly as many times as R(jw) encircles the origin. The plot of G(jw )H(jw) as w varies from -oo to +oo is called the Nyquist plot. From the encirclement property, we then see that The number of right-half The number of clockwise plane closed-loop poles encirclements of the point (11.85) minus the number of right- -1/K by the Nyquist plot half plane poles of G(s)H(s). While the open-loop system G(s)H(s) may have unstable poles, for the closed-loop system to be stable we require no right-half plane closed-loop poles. This yields the continuous- time Nyquist stability criterion: Continuous-Time Nyquist Stability Criterion: For the closed-loop system to be stable, the net number of clockwise encirclements of the point - 1/ K by theNy quist plot of G(jw )H(jw) must equal minus the number of right-half-plane poles of G(s)H(s). Equivalently, the net number of counterclockwise encirclements of the point -1/ K by the Nyquist plot of G(jw )H(jw) must equal the number of right-half-plane poles of G(s)H(s). For example, if the forward- and feedback-path systems are stable, then the Nyquist plot is simply the plot of the frequency response of the cascade of these two systems. In this case, since there are no poles of G(s)H(s) in the right-half plane, the Nyquist criterion requires that, for stability, the net number of encirclements of the point -1/K must be zero. Example 11.5 Let 1 1 G(s) = -- H(s) = --. (11.86) s + 1' !s + 1 2 The Bode plot for G(jw )H(jw) is shown in Figure 11.20. The Nyquist plot depicted in Figure 11.21 is constructed directly from the plots of the log magnitude and phase of G(jw)H(jw). That is, each point on the Nyquist plot has polar coordinates con- sisting of the magnitude IG (j w )H (j w) I and angle <r G(jw )H (j w) for some value of w. The coordinates of G(jw )H(jw) for w < 0 are obtained from the values for w > 0 through the use of the conjugate symmetry property of G(jw )H(jw ). This property manifests itself geometrically in a very simple way, which facilitates the sketching of the Nyquist plot for any feedback system composed of systems with Sec. 11.4 The Nyquist Stability Criterion 853 20 0 dB 3 -20 I 3 -40 (9 -60 0 Cl -80 .Q 0 C\J -100 -120 0.01 0.1 10 100 w 0 3- I -TI/2 3 (9 -3TI/4 \1 -TI w Figure 11.20 Bode plot for G(jw )H(jw) in Example 11.5. gm{G(jw)H(jw)} w=O ffi.e{G (jw) H (jw)} Figure 11 .21 Nyquist plot of G(jw )H(jw) for Example 11.5. The arrow on the curve indicates the direction of increasing w. 854 Linear Feedback Systems Chap. 11 real impulse responses. Specifically, since IG(-jw)H(-jw)l = IG(jw)H(jw)l and <t.G(- jw )H(- jw) = - <t.G(jw )H(jw ), the Nyquist plot of G(jw )H(jw) for w :s 0 is a reflection about the real axis of the plot for w ~ 0. Note also that we have included an arrow on the Nyquist plot in Figure 11.21. This arrow indicates the direction of in- creasing w. That is, it indicates the direction in which the Nyquist plot is traversed (as w varies from -x to +x) for the counting of encirclements in the application of the Nyquist criterion. In this example there are no right-half-plane open-loop poles, and consequently, the Nyquist criterion requires that, for stability, there be no net encirclements of the point -11K. Thus, by inspection of Figure 11.21, we see that the closed-loop system will be stable if the point - 11 K falls outside the Nyquist contour-that is, if 1 1 - :s 0 or -- K K > 1, (11.87) which is equivalent to or O>K>-1. (11.88) Combining these two conditions, we obtain the result that the closed-loop system will be stable for any choice of K greater than - 1. Example 11 .6 Consider now s + 1 G(s)H(s) = (11.89) (s - 1) ( ~ s + 1) · The Nyquist plot for this system is indicated in Figure 11.22. For this example, G(s)H(s) has one right-half-plane pole. Thus, for stability we require one counterclockwise encir- clement of the point - 11 K, which in turn requires that the point - 11 K fall inside the contour. Hence, we will have stability if and only if - 1 < - 1/ K < 0, that is, if K > 1. gm{G(jw)H(jw)} -1 / ffi~{G(jw)H(jw)} w=O Figure 11.22 Nyquist plot for Example 11.6. The arrow on the curve indi- cates the direction of increasing w. In the foregoing discussion, we have introduced and illustrated a form for the Nyquist stability criterion that applies to an extremely large class of feedback systems. In addition, Sec. 11.4 The Nyquist Stability Criterion 855 there are a number of refinements and extensions of the criterion that allow it to be used for many other feedback systems as well. For example, as we have developed it, the Nyquist plot can be drawn without any difficulties for stable or unstable G(s )H (s ), as long as there are no poles of G(s)H(s) exactly on the jw-axis. When such poles do occur, the value of G(jw )H(jw) is infinite at those points. However, as considered in Problem 11.44, the Nyquist criterion can be modified to allow for poles of G(s)H(s) on the jw-axis. In ad- dition, as mentioned at the beginning of this section, the Nyquist criterion can also be extended to the case in which G(s) and H(s) are not rational. For example, it can be shown that if the forward- and feedback-path systems are both stable, the Nyquist criterion is the same when the system functions are nonrational as it is when they are rational. That is, the closed-loop system is stable if there are no net encirclements of the point -1/K. To illustrate the application of the Nyquist criterion for nonrational system functions, we present the following example: Example 11.7 Consider the acoustic feedback example discussed in Section 11.2.6. Referring to Fig- ure 11.8(b), let K = K1K2 and G(s)H(s) = -e-sT = e-(sT+j1T)' (11.90) where we have used the fact that e-J1T = -1. In this case, G(jw)H(jw) = e-j(wT+1T)' (11.91) and as w varies from -oo to oo, G(jw )H(jw) traces out a circle of radius 1 in the clock- wise direction, with one full revolution for every change of 21TIT in w. This is illus- trated in Figure 11.23. Since the forward- and feedback-path systems are stable [the cascade G(s)H(s) is simply a time delay], the Nyquist stability criterion indicates that the closed-loop system will be stable if and only if -11 K does not fall inside the unit circle. Equivalently, we require for stability that IKI < 1. (11.92) 9m{G(jw)H(jw)} Figure 11.23 Nyquist plot for Example 11.7. Since K 1 and K 2 represent an acoustic gain and attenuation, respectively, they are both positive, which yields the stability criterion (11.93) 856 Linear Feedback Systems Chap. 11 11.4.3 The Nyquist Criterion for Discrete-Time LTI Feedback Systems As in the continuous-time case, the Nyquist stability criterion for discrete-time systems is based on the fact that the difference in the number of poles and zeros inside a contour, for a rational function, can be determined by examining a plot of the value of the function along the contour. The difference between the continuous-time and discrete-time cases is the choice of the contour. For the discrete-time case, stability of the closed-loop feedback system requires that no zeros of 1 R(z) = K + G(z)H(z) (11.94) lie outside the unit circle. Recall that the encirclement property relates to poles and zeros inside any specified contour. On the other hand, in examining the stability of a discrete-time system, we are concerned with the zeros of R(z) outside the unit circle. Therefore, in order to make use of the encirclement property, we first make a simple modification. Let us consider the rational function (11.95) obtained by replacing z by its reciprocal. As seen in Problem 10.43, if zo is a zero (pole) of R(z), then 1/z0 is a zero (pole) of R(z). Since 1/ \zo\ is less than 1 if \zo\ > 1, any zero or pole of R(z) outside the unit circle corresponds to a zero or pole of R(z) inside the unit circle. From the basic encirclement property, we know that as z traverses the unit circle in a clockwise direction, the net number of clockwise encirclements of the origin by R(z) equals the difference between the number of its zeros and poles inside the unit circle. However, from the previous paragraph, this equals the difference between the number of zeros and poles of R(z) outside the unit circle. Furthermore, on the unit circle, z = ejw and 1I z = e- jw . Therefore, (11.96) From this, we see that evaluating R(z) as z traverses the unit circle in the clockwise di- rection is identical to evaluating R(z) as z traverses the unit circle in the counterclockwise direction. In sum, then, The number of clockwise encirclements of the origin by the plot of as the The number of zeros of R(z) R( e.iw) unit circle is traversed in the outside the unit circle minus (11.97) counterclockwise direction the number of poles of R(z) (e.g., as w increases from 0 outside the unit circle. to 27T) Much as in the continuous-time case, counting the encirclements of the origin by R(e.iw) is equivalent to counting the number of encirclements of the point -1/K by the Sec. 11.4 The Nyquist Stability Criterion 857 plot of G(efw)H(efw), again referred to as the Nyquist plot, which is graphed as w varies from 0 to 27T. Also, the poles of R(z) are precisely the poles of G(z)H(z), and the zeros of R(z) are the closed-loop poles. Therefore, the encirclement property stated in the preceding paragraph implies that the net number of clockwise encirclements by the Nyquist plot of the point -1/K equals the numtier of closed-loop poles outside the unit circle minus the number of poles of G( z)H ( z) outside the unit circle. In order that the closed-loop system be stable, we require no closed-loop poles outside the unit circle. This yields the discrete-time Nyquist stability criterion: Discrete-Time Nyquist Stability Criterion: For the closed-loop system to be sta- ble, the net number of clockwise encirclements of the point -1/K by the Nyquist plot of G(efw)H(efw) as w varies from 0 to 27T must equal minus the number of poles of G(z)H(z) that lie outside the unit circle. Equivalently, the net number of counterclock- wise encirclements of the point -1/K by the Nyquist plot of G(efw)H(efw) as w varies from 0 to 27T must equal the number of poles of G(z)H(z) outside the unit circle. Example 1 1 . 8 Let z-2 G(z)H (z) = -~- (11.98) 1 + lz-1 2 The Nyquist plot of this curve is shown in Figure 11.24. Since G(z)H(z) has no poles outside the unit circle, for the stability of the closed-loop system there must be no encir- clements of the point -1/K. From the figure, we see that this will be the case either if -1/K < -1 or if -1/K > 2. Thus, the system is stable for -112 < K < 1. 9m {G(eiw)H(eiw)} -1 Figure 11 .24 Nyquist plot for Example 11.8. The arrow on the curve in- dicates the direction in which the curve is traversed as w increases from 0 to 27T. Just as in continuous time, if the forward and feedback pdths are stable, then the Nyquist plot can be obtained from the frequency responses H ( efw) and G( efw) of these 858 Linear Feedback Systems Chap. 11 systems. If the forward and feedback paths are unstable, then the frequency responses are not defined. Nevertheless, the function G(z)H(z) can still be evaluated on the unit circle, and the Nyquist stability criterion can be applied. As we have seen in this section, the Nyquist stability criterion provides a useful method for determining the range of values of the gain K for which a continuous-time or discrete-time feedback is stable (or unstable). This criterion and the root-locus method are extremely important tools in the design and implementation of feedback systems, and each has its own uses and limitations. For example, the Nyquist criterion can be applied to nonrational system functions, whereas the root-locus method cannot. On the other hand, root-locus plots allow us to examine not only stability, but also other characteristics of the closed-loop system response, such as damping, oscillation frequency, and so on, which are readily identifiable from the location of the poles of the closed-loop system. In the next section, we introduce an additional tool for the analysis of feedback systems that highlights another important characteristic of closed-loop system behavior. 11.5 GAIN AND PHASE MARGINS In this section, we introduce and examine the concept of the margin of stability in a feed- back system. It is often of interest not only to know whether a feedback system is stable, but also to determine how much the gain in the system can be perturbed and how much additional phase shift can be added to the system before it becomes unstable. Information such as this is important because in many applications the forward and feedback system functions are known only approximately or may change slightly during operation because of wear, the effect of high temperatures on components, or similar influences. As an example, consider the telescope-pointing system described in Section 11.0 and illustrated in Figures 11.1 (c) and (d). This system consists of a motor, a potentiometer converting the shaft angle to a voltage, and an amplifier that is used to amplify the voltage representing the difference between the desired and the actual shaft angles. Assuming that we have obtained approximate descriptions of each of these components, we can set the amplifier gain so that the system will be stable if these descriptions are accurate. However, the amplifier gain and the constant of proportionality that describes the angle-voltage char- acteristic of the potentiometer are never known exactly, and therefore, the actual gain in the feedback system may differ from the nominal value assumed in designing the system. Furthermore, the damping characteristics of the motor cannot be determined with absolute precision, and thus, the actual time constant of the motor response may differ from the ap- proximate value in the specification of the system. For example, if the actual motor time constant is larger than the nominal value used in the design, the motor will respond more sluggishly than anticipated, thereby producing an effective time delay in the feedback system. As we have discussed in earlier chapters, and as we will again in Example 11.11, time delays have the effect of increasing the negative phase in the frequency response of a system, and this phase shift can have a destabilizing influence on the system. Because of the possible presence of gain and phase errors such as those we have just described, it is clearly desirable to set the amplifier gain so that there is some margin for error-that is, so that the actual system will remain stable even if it differs somewhat from the approximate model used in the design process."
11.5 Gain and Phase Margins,"858 Linear Feedback Systems Chap. 11 systems. If the forward and feedback paths are unstable, then the frequency responses are not defined. Nevertheless, the function G(z)H(z) can still be evaluated on the unit circle, and the Nyquist stability criterion can be applied. As we have seen in this section, the Nyquist stability criterion provides a useful method for determining the range of values of the gain K for which a continuous-time or discrete-time feedback is stable (or unstable). This criterion and the root-locus method are extremely important tools in the design and implementation of feedback systems, and each has its own uses and limitations. For example, the Nyquist criterion can be applied to nonrational system functions, whereas the root-locus method cannot. On the other hand, root-locus plots allow us to examine not only stability, but also other characteristics of the closed-loop system response, such as damping, oscillation frequency, and so on, which are readily identifiable from the location of the poles of the closed-loop system. In the next section, we introduce an additional tool for the analysis of feedback systems that highlights another important characteristic of closed-loop system behavior. 11.5 GAIN AND PHASE MARGINS In this section, we introduce and examine the concept of the margin of stability in a feed- back system. It is often of interest not only to know whether a feedback system is stable, but also to determine how much the gain in the system can be perturbed and how much additional phase shift can be added to the system before it becomes unstable. Information such as this is important because in many applications the forward and feedback system functions are known only approximately or may change slightly during operation because of wear, the effect of high temperatures on components, or similar influences. As an example, consider the telescope-pointing system described in Section 11.0 and illustrated in Figures 11.1 (c) and (d). This system consists of a motor, a potentiometer converting the shaft angle to a voltage, and an amplifier that is used to amplify the voltage representing the difference between the desired and the actual shaft angles. Assuming that we have obtained approximate descriptions of each of these components, we can set the amplifier gain so that the system will be stable if these descriptions are accurate. However, the amplifier gain and the constant of proportionality that describes the angle-voltage char- acteristic of the potentiometer are never known exactly, and therefore, the actual gain in the feedback system may differ from the nominal value assumed in designing the system. Furthermore, the damping characteristics of the motor cannot be determined with absolute precision, and thus, the actual time constant of the motor response may differ from the ap- proximate value in the specification of the system. For example, if the actual motor time constant is larger than the nominal value used in the design, the motor will respond more sluggishly than anticipated, thereby producing an effective time delay in the feedback system. As we have discussed in earlier chapters, and as we will again in Example 11.11, time delays have the effect of increasing the negative phase in the frequency response of a system, and this phase shift can have a destabilizing influence on the system. Because of the possible presence of gain and phase errors such as those we have just described, it is clearly desirable to set the amplifier gain so that there is some margin for error-that is, so that the actual system will remain stable even if it differs somewhat from the approximate model used in the design process. Sec. 11.5 Gain and Phase Margins 859 In this section, we introduce one method for quantifying the margin of stability in a feedback system. To do this, we consider a closed-loop system, depicted in Figure 11.25, that has been designed to be stable based on nominal values for the forward- and feedback- path system functions. For our discussion here, we let H(s) and G(s) denote these nominal values. Also, since the basic concepts are identical for both continuous-time and discrete- time systems, we will again focus our development on the continuous-time case, and at the end of the section we illustrate the application of these ideas to a discrete-time example. x(t) -~--u--.+- ...­.-~ H(s) y(t) Figure 11.25 Typical feedback system designed to be stable, as- G(s) suming nominal descriptions for H(s) and G(s). To assess the margin of stability in our feedback system, suppose that the actual system is as depicted in Figure 11.26, where we have allowed for the possibility of a gain K and phase shift cp in the feedback path. In the nominal system K is unity and cp is zero, but in the actual system either or both may have a different value. Therefore, it is of interest to know how much variation can be tolerated in these quantities without losing closed-loop system stability. In particular, the gain margin of the feedback system is defined as the minimum amount of additional gain K, with cp = 0, that is required so that the closed- loop system becomes unstable. Similarly, the phase margin is the additional amount of phase shift, with K = 1, that is required for the system to be unstable. By convention, the phase margin is expressed as a positive quantity; that is, it equals the magnitude of the additional negative phase shift at which the feedback system becomes unstable. x(t+) --ta • H(s) ""' y(t) G(s)~- K ~ Figure 11 .26 Feedback system containing possible gain and phase devia- tions from the nominal description depicted in Figure 11.25. Since the closed-loop system of Figure 11.25 is stable, the system of Figure 11.26 can become unstable if, asK and cp are varied, at least one pole of the closed-loop system crosses the jw-axis. If a pole of the closed-loop system is on the jw-axis at, say, w = w 0 , then at this frequency (11.99) 860 Linear Feedback Systems Chap. 11 or (11.100) Note that with K = 1 and <P = 0, by our assumption of stability for the nominal feedback system of Figure 11.25, there is no value of w 0 for which eq. (11.100) is satisfied. The gain margin of this system is the minimum value of K > 1 for which eq. (11.100) has a solution for some w 0 with <P = 0. That is, the gain margin is the smallest value of K for which the equation KG (jwo)H (jwo) = -1 (11.101) has a solution w 0 . Similarly, the phase margin is the minimum value of <P for which eq. (11.1 00) has a solution for some w 0 when K = 1. In other words, the phase margin is the smallest value of <P > 0 for which the equation (11.102) has a solution. To illustrate the calculation and graphical interpretation of gain and phase margins, we consider the following example. Example 11.9 Let 4(1 + ls) G(s)H(s) = 2 . (11.103) s(l + 2s)(l + 0.05s + (0.125s)2 ) The Bode plot for this example is shown in Figure 11.27. Note that, as discussed in Problem 6.3·1, the factor of lljw in G(jw)H(jw) contributes -90° (-'TT/2 radians) of phase shift and a 20-dB-per-decade increase in IG(jw )H(jw )1. To determine the gain margin, we observe that, with 4> = 0, the only frequency at which eq. (11.101) can be satisfied is that for which -9:G(jw0 )H(jw0 ) = -'TT. At this frequency, the gain mar- gin in decibels can be identified by inspection of Figure 11.27. We first examine Fig- ure 11.27(b) to determine the frequency w1 at which the angle curve crosses the line -7r radians. Locating the point at this same frequency in Figure 11.27(a) provides us with the value of IG(JwJ)H(jwl)l. For eq. (11.101) to be satisfied for wo = w1, K must equall/IG(Jw 1) H(jw 1) I. This value is the gain margin. As illustrated in Figure 11.27(a), the gain margin expressed in decibels can be identified as the amount the log-magnitude curve woulq have to be shifted up so that the curve intersects the 0-dB line at the fre- quency w 1• In a similar fashion, we can determine the phase margin. Note first that the only frequency at which eq. (11.102) can be satisfied is that for which IG(jw0)H(jw0 )1 = 1, or equivalently, 20 log10 IG(jw0)H(jw0 )1 = 0. To determine the phase margin, we first find the frequency w 2 in Figure 11.27(a) at which the log-magnitude curve crosses the 0-dB line. Locating the point at this same frequency in Figure 11.27(b) then provides us with the value of -9:G (iw 2 )H (iw2). For eq. (11.102) to be satisfied for w0 = w 2 , the angle of the left-hand side of this equation must be -'TT. The value of 4> for which this is true is the phase margin. As illustrated in Figure 11.27(b), the phase margin can be identified as the amount the angle curve would have to be lowered so that the curve intersects the line -7r at the frequency w2. Sec. 11.5 Gain and Phase Margins 861 40 3 20 I 3 OdB (9 0 Oi -20 ..Q 0 N -40 -60 0.1 1.0 (J) 10.0 7T 2 0 3 I 3 7T 2 (9 \:f -'IT -----------~--------- (1)2 (J)\! -237T 0.1 1.0 (J) 10.0 Figure 11 .27 Use of Bode plots to calculate gain and phase margins for the system of Example 11.9. In determining gain and phase margins, it is not always of interest to identify ex- plicitly the frequency at which the poles will cross the jw-axis. As an alternative, we can identify the gain and phase margins from a log magnitude-phase diagram. For example, the log magnitude-phase diagram for the system of Figure 11.27 is shown in Figure 11.28. In this figure, we plot 20 log 10 /G(jw)H(jw)/ versus <r.G(jw)H(jw) as w varies from 0 to +oo. Therefore, because of the conjugate symmetry of G(jw )H(jw ), the plot contains the same information as the Nyquist plot, in which (Jlc{G(jw )H(jw )} is plotted versus dm{G(jw )H(jw )} for -oo < w < oo. As we have indicated, the phase margin can be read off by locating the intersection of the log magnitude-phase plot with the 0-dB line. That is, the phase margin is the amount of additional negative phase shift required to shift the log magnitude-phase curve so that it intersects the 0-dB line with exactly 180° ( 7r ra- dians) of phase shift. Similarly, the gain margin is directly obtained from the intersec- tion of the log magnitude-phase curve with the line -7r radians, and this represents the amount of additional gain needed so that the curve crosses the line -7r with a magnitude ofOdB. The following examples provide several other elementary illustrations of log mag- nitude-phase diagrams: 862 Linear Feedback Systems Chap. 11 40 20 3 I OdB 3 (9 0 o; -20 .2 0 N -40 -60 -21T 31T -1T 1T 0 2 2 Figure 11 .28 Log magnitude- phase plot for the system of Exam- 1:G(jw)H(jw) ple 11.9. Example 11. 1 0 Let G(s)H(s) = TS + , T > 0. (11.104) 1 20 Phase margin 3 I 0 dB 3 (9 0 o; -20 _Q 0 N -40 1:G(jw)H(jw) Figure 11 .29 Log magnitude-phase plot for the first-order system of Ex- ample 11.10. Sec. 11.5 Gain and Phase Margins 863 In this case, we obtain the log magnitude-phase plot depicted in Figure 11.29. This has a phase margin of 71', and since the curve does not intersect the line -71', the system has infinite gain margin (i.e., we can increase the gain as much as we like and maintain stability). This is consistent with the conclusion that we can draw by examining the system illustrated in Figure 11.30( a). In Figure 11.30(b ), we have depicted the root locus for this system with 4> = 0 and K > 0. From the figure, it is evident that the system is stable for any positive value of K. In addition, if K = 1 and 4> = 71', so that ei¢ = -1, the closed-loop system function for the system of Figure 11.30(a) is l!TS, which has a pole at s = 0, so that the system is unstable. + 1 x(t)~ + , -- __, TS+1 y(t) - e-i<P (a) T Cfl.e (b) Figure 11 .30 (a) First-order feedback system with possible gain and phase variations in the feedback path; (b) root locus for this system with 1> = 0, K > 0. Example 1 1 . 1 1 Suppose we now consider the second-order system 1 H(s) = 2 1, G(s) = 1. (11.105) s + s + The system H (s) has an undamped natural frequency of 1 and a damping ratio of 0.5. The log magnitude-phase plot for this system is illustrated in Figure 11.31. Again we have in- finite gain margin, but a phase margin of only 7T/2, since it can be shown by a straightfor- ward calculation that jH(jw )j = 1 for w = 1, and at this frequency <r.H(jw) = -71'/2. We can now illustrate the type of problem that can be solved using the concepts of gain and phase margins. Suppose that the feedback system specified by eq. (11.105) cannot be realized. Rather, some unavoidable time delay is introduced into the feedback 864 Linear Feedback Systems Chap. 11 path. That is, (11.106) where T is the time delay. What we would like to know is how small this delay must be to ensure the stability of the closed-loop system. 20 I OdB ------------------,I..-__- --_--_-~ ~-----1 3 1 0 o; -20 .Q 0 N -40 -60 L-------L-------~------~---~ -21T 31T 0 2 <tG(jw)H(jw) Figure 11.31 Log magnitude-phase plot for the second-order system of Example 11.11. The first point to note is that (11.107) so the delay does not change the magnitude of H(jw )G(jw ). On the other hand, <r.e-iWT = -wrradians. (11.108) Thus, every point on the curve in Figure 11.31 is shifted to the left. The amount of the shift is proportional to the value of w for each point on the log magnitude-phase curve. From this discussion, we see that instability will occur once the phase margin is reduced to zero, and this will happen when the phase shift introduced by the delay is equal to - 'TT'/2 at w = 1. That is, the critical value r* of the time delay satisfies (11.109) or (assuming that the units of w are radians/second) r* = 1.57 seconds. (11.110) Thus, for any time delay T < r*, the system remains stable. Example 1 1 . 1 2 Consider again the acoustic feedback system discussed in Section 11.2.6 and Exam- ple 11.7. Here, we assume that the system of Figure 11.8 has been designed with K 1 K2 < 1, so that the closed-loop system is stable. In this case, the log magnitude-phase plot for Sec. 11.5 Gain and Phase Margins 865 40.------,~-------,~-------~~------~.-----~ 20 r- - Gain margin 1 ~ 0 dB r---------------!-------------- :------ -- .s2_ 20 log10 (K 1 K 2)-r-------....,.~------.... 0 Oi :I ""' w=O .Q -201- - 0 C\J I -40~----~1------~1 -------~~------l~----~ -4TI -3TI -2TI -'IT 0 <tG(jw)H(jw) Figure 11 .32 Log magnitude-phase plot for Example 11.12. G(s)H(s) = K 1K2e-(sT+ j7T) is illustrated in Figure 11.32. From the figure, we see that the system has infinite phase margin and a gain margin in decibels of -20 log 10 (K1K2) (i.e., this is precisely the gain factor that, when multiplied by K 1K2, equals 1). As indicated at the start of the section, the definitions of the gain and phase margin are the same for discrete-time feedback systems as for continuous-time systems. Specifi- cally, if we have a stable discrete-time feedback system, the gain margin is the minimum amount of additional gain required in the feedback system such that the closed-loop sys- tem becomes unstable. Similarly, the phase margin is the minimum amount of additional negative phase shift required for the feedback system to be unstable. The following ex- ample illustrates the graphical calculation of phase and gain margins f~r a discrete-time feedback system; the procedure is essentially the same as for continuous-time systems. Example 1 1 . 1 3 In this example, we illustrate the concept of gain and phase margin for the discrete-time feedback system shown in Figure 11.33. Here, 7 J2 -1 2 G(z)H(z) = 4 , (11.111) 1 - -7 J-2z - I + 49 z -2 8 64 and by direct calculation we can check that the feedback system is stable for K = 1 and cf> = 0. In Figure 11.34, we have displayed the log magnitude-phase diagram for + 1 x[n] ~ + 1_ 7V2 + 49 -2 y[n] -1 8 2 64 2 7BV2z - 1 -EJ- K ~ Figure 11.33 Discrete-time feedback system of Example 11.13. 866 Linear Feedback Systems Chap. 11 40~----------~----------~------------~----------~ 3 -~ 20 I 3 -~ (!) 0 Oi Phase margin = 0.0685 rad ..Q _f ____________________ _ o OdB ----------------------- C\1 Gain margin = 1.68dB -20~----------~----------~------------~----------~ -2'TT 0 Figure 11 .34 Log magnitude-phase diagram for the discrete-time feedback system of Example 11.13. the system; that is, we have plotted 20 log 10 IG(ejw)H(ejw)l versus <t.G(ejw)H(ejw) as w varies from 0 to 27T. The system has a gain margin of 1.68 dB and a phase margin of 0.0685 radians (3.93°). In concluding this section, it should be stressed that the gain margin is the minimum value of gain that moves one or more of the closed-loop poles onto the jw-axis in continu- ous time or the unit circle in discrete time and, consequently, causes the system to become unstable. It is important to note, however, that this does not imply that the system is un- stable for all values of gain above the value specified by the gain margin. For example, as illustrated in Problem 11.47, asK increases, the root locus may move from the left-half plane into the right-half plane and then cross back into the left-half plane. The gain margin provides us with the information about how much the gain can be increased until the poles first reach the jw-axis, but it tells us nothing about the possibility that the system may again be stable for even larger values of the gain. To obtain such information, we must either refer to the root locus or use the Nyquist stability criterion. (See Problem 11.47.).4 11 .6 SUMMARY In this chapter, we have examined a number of the applications and several techniques for the analysis of feedback systems. We have seen how the use of Laplace and z-transforms allows us to analyze these systems algebraically and graphically. In Section 11.2 we in- dicated several of the applications of feedback, including the design of inverse systems, 4For detailed discussions of this point and also of gain and phase margins and log magnitude-phase diagrams in general, see the texts on feedback listed in the bibliography at the end of the book."
11.6 Summary,"866 Linear Feedback Systems Chap. 11 40~----------~----------~------------~----------~ 3 -~ 20 I 3 -~ (!) 0 Oi Phase margin = 0.0685 rad ..Q _f ____________________ _ o OdB ----------------------- C\1 Gain margin = 1.68dB -20~----------~----------~------------~----------~ -2'TT 0 Figure 11 .34 Log magnitude-phase diagram for the discrete-time feedback system of Example 11.13. the system; that is, we have plotted 20 log 10 IG(ejw)H(ejw)l versus <t.G(ejw)H(ejw) as w varies from 0 to 27T. The system has a gain margin of 1.68 dB and a phase margin of 0.0685 radians (3.93°). In concluding this section, it should be stressed that the gain margin is the minimum value of gain that moves one or more of the closed-loop poles onto the jw-axis in continu- ous time or the unit circle in discrete time and, consequently, causes the system to become unstable. It is important to note, however, that this does not imply that the system is un- stable for all values of gain above the value specified by the gain margin. For example, as illustrated in Problem 11.47, asK increases, the root locus may move from the left-half plane into the right-half plane and then cross back into the left-half plane. The gain margin provides us with the information about how much the gain can be increased until the poles first reach the jw-axis, but it tells us nothing about the possibility that the system may again be stable for even larger values of the gain. To obtain such information, we must either refer to the root locus or use the Nyquist stability criterion. (See Problem 11.47.).4 11 .6 SUMMARY In this chapter, we have examined a number of the applications and several techniques for the analysis of feedback systems. We have seen how the use of Laplace and z-transforms allows us to analyze these systems algebraically and graphically. In Section 11.2 we in- dicated several of the applications of feedback, including the design of inverse systems, 4For detailed discussions of this point and also of gain and phase margins and log magnitude-phase diagrams in general, see the texts on feedback listed in the bibliography at the end of the book. Chap. 11 Problems 867 the stabilization of unstable systems, and the design of tracking systems. We also saw that feedback can destabilize, as well as stabilize, a system. In Section 11.3, we described the root-locus method for plotting the poles of the closed-loop system as a function of a gain parameter. Here, we found that the geometric evaluation of the phase of a rational Laplace transform or z-transform allowed us to gain a significant amount of insight into the properties of the root locus. These properties of- ten permit us to obtain a reasonably accurate sketch of the root locus without performing complex calculations. In contrast to the root-locus method, the Nyquist criterion of Section 11.4 is a tech- nique for determining the stability of a feedback system, again as a function of a variable gain, without obtaining a detailed description of the location of the closed-loop poles. The Nyquist criterion is applicable to nonrational system functions and thus can be used when all that is available are experimentally determined frequency responses. The same is true of the gain and phase margins described in Section 11.5. These quantities provide a measure of the margin of stability in a feedback system and therefore are of importance to design- ers in that they allow them to determine how robust a feedback system is to discrepancies between estimates of the forward- and feedback-path system functions and their actual values. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 11.1. Consider the interconnection of discrete-time LTI systems shown in Figure P11.1. Express the overall system function for this interconnection in terms of H0(z), H1 (z), and G(z). H0(z) + + 1 x[n] + H1(z) + y[n] - I G(z) Figure P11.1"
Problems,"Chap. 11 Problems 867 the stabilization of unstable systems, and the design of tracking systems. We also saw that feedback can destabilize, as well as stabilize, a system. In Section 11.3, we described the root-locus method for plotting the poles of the closed-loop system as a function of a gain parameter. Here, we found that the geometric evaluation of the phase of a rational Laplace transform or z-transform allowed us to gain a significant amount of insight into the properties of the root locus. These properties of- ten permit us to obtain a reasonably accurate sketch of the root locus without performing complex calculations. In contrast to the root-locus method, the Nyquist criterion of Section 11.4 is a tech- nique for determining the stability of a feedback system, again as a function of a variable gain, without obtaining a detailed description of the location of the closed-loop poles. The Nyquist criterion is applicable to nonrational system functions and thus can be used when all that is available are experimentally determined frequency responses. The same is true of the gain and phase margins described in Section 11.5. These quantities provide a measure of the margin of stability in a feedback system and therefore are of importance to design- ers in that they allow them to determine how robust a feedback system is to discrepancies between estimates of the forward- and feedback-path system functions and their actual values. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 11.1. Consider the interconnection of discrete-time LTI systems shown in Figure P11.1. Express the overall system function for this interconnection in terms of H0(z), H1 (z), and G(z). H0(z) + + 1 x[n] + H1(z) + y[n] - I G(z) Figure P11.1 868 Linear Feedback Systems Chap. 11 11.2. Consider the interconnection of discrete-time LTI systems shown in Figure P11.2. Express the overall system function for this interconnection in terms of H 1( s), H2(s), G1 (s), and G2(s). + + x(t) + H2(s) + H1(s) y(t) - - G1(s) G2(s) Figure P11.2 11.3. Consider the continuous-time feedback system depicted in Figure 11.3(a) with 1 H(s) = s- 1 and G(s) = s- b. For what real values of b is the feedback system stable? 11.4. A causal LTI system S with input x(t) and output y(t) is represented by the differ- ential equation S is to be implemented using the feedback configuration of Figure 11.3(a) with H(s) = 1/(s + 1). Determine G(s). 11.5. Consider the discrete-time feedback system depicted in Figure 11.3(b) with 1 H(z) = 1 - .!.z-1 and G(z) = 1- bz- 1 • 2 For what real values of b is the feedback system stable? 11.6. Consider the discrete-time feedback system depicted in Figure 11.3(b) with . -1 H(z) 1- z-N z = and G(z) = 1- z-N· Is this system IIR or FIR? 11.7. Suppose the closed-loop poles of a feedback system satisfy 1 1 (s + 2)(s + 3) K' Use the root-locus method to determine the values of K for which the feedback system is guaranteed to be stable. Chap. 11 Problems 869 11.8. Suppose the closed-loop poles of a feedback system satisfy s- 1 1 (s + l)(s + 2) K"" Use the root-locus method to determine the negative values of K for which the feedback system is guaranteed to be stable. 11.9. Suppose the closed-loop poles of a feedback system satisfy (s + l)(s + 3) 1 (s + 2)(s + 4) K"" Use the root-locus method to determine whether there are any values of the ad- justable gain K for which the system's impulse response has an oscillatory compo- nent of the form e-at cos( w 0t + cf> ), where w 0 ~ 0. 11.10. The root locus corresponding to G(s)H(s) = -1/K is illustrated in Figure Pl1.10. In this figure, the start (K = 0) and end of each branch of the root locus are marked by a 'e' symbol. Specify the poles and zeros of G(s)H(s). -1 <Re -j K<O -1 CRe Figure P11.1 0 870 Linear Feedback Systems Chap. 11 11.11. Suppose the closed-loop poles of a discrete-time feedback system satisfy Using the root-locus method, determine the positive values of K for which this system is stable. 11.12. Each of the four locations z = 1/2, z = 1/4, z = 0, and z = - 112 is a single-order pole or zero of G(z)H(z). Furthermore, G(z)H(z) is known to have only two poles. What information can you deduce about the poles and zeros of G(z)H(z) from the fact that for all K, the root locus corresponding to G(z)H(z) = K is on the real axis. 11.13. Consider the block diagram of Figure P 11.13 for a discrete-time system. Use the root-locus method to determine the values of K for which the system is guaranteed to be stable. x[n+] ~ + y[n] D 1 + D 4 K Figure P11.13 11.14. Let C be a closed path that lies on the unit circle in the p-plane and that is traversed in the clockwise direction in order to evaluate W(p). For each of the following expressions for W(p), determine the net number of times the plot of W(p) encircles the origin in a clockwise direction: (1-kp- 1 ) (a) W(p) = o-±p-1> b (l-2p-l) ( ) W(p) = (t-~p-IJ0-2p-1+4p-2J Chap. 11 Problems 871 11.15. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 1 G(s)H(s) = (s + l) = - K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. Hint: In sketching the Nyquist plot, you may find it useful to sketch the corresponding Bode plot first. It also is helpful to determine the values of w for which G(jw )H(jw) is real. 11.16. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 G(s)H(s) = (s + 1)(s/10 + 1) K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.17. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 G(s)H(s) = (s + 1)4 K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.18. Consider a discrete-time feedback system whose closed-loop poles satisfy -3 1 G(z)H(z) = z = - K. Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.19. Consider a feedback system, either in continuous-time or discrete-time, and sup- pose that the Nyquist plot for the system passes through the point - 1/ K. Is the feedback system stable or unstable for this value of the gain? Explain your answer. 11.20. Consider the basic continuous-time feedback system of Figure 11.3(a). Determine the phase and gain margin for the following specification of H(s) and G(s): s + 1 H(s) = , G(s) = 1. s 2 + s + 1 BASIC PROBLEMS 11.21. Consider the feedback system of Figure P11.21. Find the closed-loop poles and zeros of this system for the following values of K: (i) K = 0.1 (ii) K = 1 (iii) K = 10 (iv) K = 100 872 Linear Feedback Systems Chap. 11 + x(t) --~~~ 1----.....- -~ y(t) s+1 s+100 Figure P11.21 11.22. Consider the basic feedback system of Figure 11.3(a). Determine the closed-loop system impulse response for each of the following specifications of the system functions in the forward and feedback paths: (a) H(s) = (s+ l)l(s+ 3), G(s) = 1 (b) H(s) = s~ 3 ,G(s) = s~l (c) H(s) = 4, G(s) = e-s/3 11.23. Consider the basic feedback systems of Figure 11.3(b). Determine the closed-loop system impulse response for each of the following specifications of the system functions in the forward and feedback paths: 1 ( a ) H( ) _ z- G( ) _ 2 I -1 z - l-~z- 1 , z - 3 - 6z (b) H(z) = ~ - ~z- 1 , G(z) = 1 _z~~-~ 2 11.24. Sketch the root loci for K > 0 and K < 0 for each of the following: (a) G(s)H(s) = s~ 1 (b) G(s)H(s) = (s-l)l(s+ 3) 1 (c) G(s)H(s) = s2 +s+ 1 (d) G(s)H(s) = s.~ 1 2 (e) G(s)H(s) = (s:}> (f) G(s)H(s) = s2+2s+2 s2(s-l) (g) G(s)H(s) = (s+ l)(s-l) s(s2+2s+2) (h) G(s)H(s) = (s;~~;·~ 3 ) 11.25. Sketch the root loci for K > 0 and K < 0 for each of the following: (a) G(z)H(z) = ~- 1 z 1 -4 (b) G(z)H(z) = 2 zL~ (c) G(z)H(z) = z-.~~~tz~~~) 4 (d) G(z)H(z) = z- 1 - z-2 (e) G(z)H(z) is the system function of the causal LTI system described by the difference equation y[n] - 2y[n - 1] = x[n - 1] - x[n - 2]. Chap. 11 Problems 873 11.26. Consider a feedback system with G(s)H(s) = (s- a)(s- b) s(s + 3)(s + 6) Sketch the root locus for K > 0 and K < 0 for the following values of a and b: (a) a = 1, b = 2 (b) a = -2, b = 2 (c) a = -4, b = 2 (d) a = -7, b = 2 (e) a = -1, b = -2 (f) a = -4, b = -2 (g) a = -7, b = -2 (h) a = -5, b = -4 (i) a = -7, b = -4 (j) a= -7, b = -8 11.27. Consider a feedback system with s+2 H(s) = , G(s) = K. s 2 + 2 s + 4 (a) Sketch the root locus forK> 0. (b) Sketch the root locus forK < 0. (c) Find the smallest positive value of K for which the closed-loop impulse re- sponse does not exhibit any oscillatory behavior. 11.28. Sketch the Nyquist plot for each of the following specifications of G(s)H(s), and use the continuous-time Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Note: In sketch- ing the Nyquist plots, you may find it useful to sketch the corresponding Bode plots first. It also is helpful to determine the values of w for which G(jw )H(jw) is real. (a) G(s)H(s) = s~ I (b) G(s)H(s) = s2 ~ I (c) G(s)H(s) = (s] 1)2 (d) G(s)H(s) = (s] 03 (e) G(s)H(s) = (}+-t)2 (f) G(s)H(s) = c:~/)2 (g) G(s)H(s) = ;;~~ (h) G(s)H(s) = s2+~s+2 (i) G(s)H(s) = ~ (j) G(s)H(s) = s+ I s2-2s+2 (s+IOO)(s-I)2 s2 (k) G(s)H(s) = (s+ I)3 11.29. Consider the basic continuous-time feedback system of Figure 11.3(a). Sketch the log magnitude-phase diagram, and roughly determine the phase and gain margin, for each of the following choices of G(s) and H(s). You may find it useful to use the straight- line anproximations to the Bode plots developed in Chapter 6 to aid you in sketching the log magnitude-phase diagrams. Be careful, however, to take into account how the actual frequency response deviates from its approximation near break frequencies when there are underdamped second-order terms present. (See Section 6.5.2.) (a) H(s) = 2lOs+ I G(s) = 1 s +s+ I? (b) H(s) = s2ilO+ I G(s) = 1 s +s+ I' (c) H(s) = (s+I/(s+IO)' G(s) = 100 (d) H(s) = (s-!1)3, G(s) = s~ I (e) H(s) = (s+ I\;~~ IO), G(s) = 1 874 Linear Feedback Systems Chap. 11 (f) H(s) = 1-s/100 G(s) = lOs+ 1 (s+J)2' s/10+1 (g) H(s) = - 1 - G(s) = - 1 s(s+ I)' s+l Note: Your sketch for part (g) should reflect the fact that for this feedback system IG(jw )H(jw )I ~ oo as w ~ 0; what is the phase of G(jw )H(jw) for w = o+' i.e., for w an infinitesimal amount larger than 0? 11.30. Sketch the Nyquist plot for each of the following specifications of G(z)H(z), and use the discrete-time Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. [Note: In sketch- ing the Nyquist plots, you may find it useful to first sketch the magnitude and phase plots as a function of frequency or at least calculate IG(efw)H(efw)l and <r.G(efw)H(efw) at several points. Also, it is helpful to determine the values of w for which G( efw )H ( efw) is real.] (a) G(z)H(z) = z~l (b) G(z)H(z) = 2 ~ 2 2 (c) G(z)H(z) = z- 1 (d) G(z)H(z) = z- 2 1 (e) G(z)H(z) = (z+ ~ ) ~ (f) G( )H( ) - z-/3 (z- > Z Z - z(z+ 1/,/3) (g) G(z)H(z) = 22_~+~ (h) G(z)H(z) = z- ~ z(z-2) 2 (i) G(z)H(z) = c=:} > 11.31. Consider the basic discrete-time system in Figure 11.3(b ). Sketch the log magnitude- phase diagram, and roughly determine the phase and gain margin, for each of the following choices of G(z) and H(z). You may find it useful to determine the values of w for which either IG(efw)H(efw)l = 1 or <r.G(efw) = -7T. (a) H(z) = z- 1, G(z) = ~ 7-1 I (b) H(z) = 1 _~lz 1 , G(z) = 2 2 (c) H(z) = (l-~:::- 1 )1( 1 +h-l)' 2 G(z) = z- (d) H(z) = :::_: 2: G(z) ~ 1 (e) H(z) = -1-~ , G(z) = z+ 2 ~7 ~ l 2 (f) H(z) = ~ 1 ,.+', G(z) = 1 - ~z- -I (g) H(z) = _2_:+l'G(z) = 1 ,, ~ 3 (h) H(z) = 2~ 1 ,G(z) = iz- 1 (Note: Your sketch for part (h) should reflect the fact that, for this feedback system, G(z)H(z) has a pole at z = 1; what are the values of <r.G(efw)H(efw) for eiw just on either side of the point z = 1? ) ADVANCED PROBLEMS 11.32. (a) Consider the feedback system of Figure 11.1 O(b) with H() = N1(s) (P11.32-1) s D1 (s)' Chap. 11 Problems 875 Assume that there is no pole-zero cancellation in the product G(s)H(s). Show that the zeros of the closed-loop system function consist of the zeros of H(s) and the poles of G(s). (b) Use the result of part (a) together with the appropriate property of the root locus to confirm that, with K = 0, the closed-loop system zeros are the zeros of H(s) and the closed-loop poles are the poles of H(s). (c) While it is usual for H(s) and G(s) in eq. (P11.32-1) to be in reduced form [i.e., the polynomials N 1( s) and D 1( s) have no common factors, and the same is true of N2(s) and D2(s)], it may happen that N 1( s) and D2(s) have common factors or N2(s) and D 1( s) have common factors. To see what occurs when such common factors are present, let p(s) denote the greatest common factor of N2(s) and D 1( s). That is, NJ(S) and p(s) are both polynomials and have no common factors. Similarly, DJ(S) and q(s) are polynomials and have no common factors. Show that the closed-loop sys- tem function can be written as Q(s) = p(s) [ H__<s) "" ]· (P11.32-2) q(s) 1 + KG(s)H(s) where H(s) = N1 (s)/ p(s) D1 (s)/q(s) and G(s) = N2(s)/q(s). D2(s)/p(s) Therefore, from eq. (P11.32-2) and part (a), we see that the zeros of Q(s) are the zeros of p(s), the zeros of H(s), and the poles of G(s), while the poles of Q(s) are the zeros of q(s) and the solutions of 1 + KG(s)H(s) = 0. (P11.32-3) By construction, there is no pole-zero cancellation in the product G(s)H(s), and thus, we can apply the root-locus method described in Section 11.3 to sketch the locations of the solutions of eq. (P11.32-3) asK is varied. (d) Use the procedure outlined in part (c) to determine the closed-loop zeros, any closed-loop poles whose locations are independent of K, and the locus of the remaining closed-loop poles for K > 0 when s + 1 s+2 H(s) = (s + 4)(s + 2)' G(s) = s +I' 876 Linear Feedback Systems Chap. 11 (e) Repeat part (d) for 1 + z- 1 z-I H(z) = 1 - lz-I, G(z) = 1 + z-I. 2 (f) Let z2 1 H(z) = (z- 2)(z + 2)' G(z) = 2 . z (i) Sketch the root locus for K > 0 and for K < 0. (ii) Find all the values of K for which the overall system is stable. (iii) Find the impulse response of the closed-loop system when K = 4. 11.33. Consider the feedback system of Figure 11.10(a), and suppose that m ncs -/h) G(s)H(s) = _k:-1 -- n(s- ak) k=I where m > n.5 In this case G(s)H(s) has m- n poles at infinity (see Chapter 9), and we can adapt the root-locus rules given in the text by noting that (1) there are m branches of the root locus and (2) forK = 0, all branches of the root locus begin at poles of G(s)H(s), m- n of which are at infinity. Furthermore, as IKI ~ 00, these branches converge to them zeros of G(s)H(s), namely, {3 1, {32, .•• , f3m· Use these facts to assist you in sketching the root locus (for K > 0 and for K < 0) for each of the following: (a) G(s)H(s) = s - 1 (b) G(s)H(s) = (s + l)(s + 2) (c) G(s)H(s) = 2 (s+ ~~~+ ) 11.34. In Section 11.3, we derived a number of properties that can be of value in deter- mining the root locus for a feedback system. In this problem, we develop several additional properties. We derive these properties in terms of continuous-time sys- tems, but, as with ali root-locus properties, they hold as well for discrete-time root loci. For our discussion of these properties, we refer to the basic equation satisfied by the closed-loop poles, namely, 1 G(s)H(s) = (Pl1.34-1) K' 5Note that for a continuous-time system, the condition m > n implies that the system with system func- tion G(s)H(s) involves differentiation of the input. [In fact, the inverse transform of G(s)H(s) includes singu- larity functions up to the order m - n.] In discrete time, if G(z)H (z), written as a ratio of polynomials in z, has m > n, it is necessarily the system function of a noncausal system. [In fact, the inverse transform of G(z)H(z) has a nonzero value at time n- m < 0.] Thus, the case considered in this problem is actually of interest only for continuous-time systems. Chap. 11 Problems 877 where m n<s- {3d G(s)H(s) = _k:-1 -- (P11.34-2) n(s- ak) k=l s Figure P11.34 Throughout this problem, we assume that m ~ n. (a) From Property 2, we know that n- m branches of the root locus go to ze- ros of G(s)H(s) located at infinity. In this first part, we demonstrate that it is straightforward to determine the angles at which these branches approach in- finity. Specifically, consider searching the remote part of the s-plane [i.e., the region where lsi is extremely large and far from any of the poles and zeros of G(s)H(s)]. This region is illustrated in Figure Pll.34. Use the geometry of the picture, together with the angle criterion for K > 0 and for K < 0, to deduce that: • For K > 0, the n - m branches of the root locus that approach infinity do so at the angles (2k + 1)7T k = 0, 1, ... , n - m - 1. n-m 878 Linear Feedback Systems Chap. 11 • For K < 0, the n - m branches of the root locus that approach infinity do so at the angles 2k1T' k = 0, 1, ... , n - m - 1. n- m' Thus, the branches of the root locus that approach infinity do so at specified angles that are arranged symmetrically. For example, for n - m = 3 and K > 0, we see that the asymptotic angles are 1T'I3, 1T', and 57T'/3. The result of part (a), together with one additional fact, allows us to draw in the asymptotes for the branches of the root locus that approach infinity. Specifically, all of the n - m asymptotes intersect at a single point on the real axis. This is derived in the next part of the problem. (b) (i) As a first step, consider a general polynomial equation sr+fr-tSr-l +···+fo = (s-gt)(s-6)···(s-gr) = 0. Show that r J,--1 - Lgi· i= I (ii) Perform long division on 1/G(s)H(s) to write 1 n-m n-m-1 G(s)H(s) = S + 'Yn-m-1 S + . . . . (Pll.34-3) Show that Ill II 'Yn-m-1 =an-I- bm-1 = Lf3k- LCik. k= I k= I [See eq. (Pll.34-2).] (iii) Argue that the solution of eq. (Pll.34-l) for large sis an approximate solution of the equation sn-m + 'Yn-m-ISn-m-1 + 'Yn-m-2Sn-m-2 + ... +'Yo+ K = 0. (iv) Use the results of (i)-(iii) to deduce that the sum of then- m closed-loop poles that approach infinity is asymptotically equal to Thus, the center of gravity of these n - m poles is n- m which does not depend on K. Consequently, we haven- m closed-loop poles that approach lsi = oo at evenly spaced angles and that have a center of gravity that is independent of K. From this, we can deduce that: Chap. 11 Problems 879 The asymptotes of the n- m branches of the root locus that approach infinity intersect at the point n m Lak- Lf3k bm-1 -an-I k=l k=l n-m n-m This point of intersection of the asymptotes is the same for K > 0 and K<O. (c) Suppose that 1 G(s)H(s) = (s + 1)(s + 3)(s + 5) (i) What are the asymptotic angles for the closed-loop poles that approach infinity for K > 0 and for K < 0? (ii) What is the point of intersection of the asymptotes? (iii) Draw in the asymptotes, and use them to help you sketch the root locus forK> 0 and forK< 0. (d) Repeat part (c) for each of the following: (i) G(s)H(s) = s(s~~2~(~+4) (ii) G(s)H(s) = I 7 (iii) G(s)H(s) = s(s+ I )(s+5)(s+6) (iv) G(s)H(s) = 1 (s+2)2(s-1)2 (v) G(s)H(s) = s+3 (s+ I )(s2 + 2s+ 2) (vi) G(s)H(s) = s+l (s+ 2)2(s2 + 2s+ 2) (vii) G(s)H(s) = (s+ 100{c;~ l)(s- 2) (e) Use the result of part (a) to explain why the following statement is true: For any continuous-time feedback system, with G(s)H(s) given by eq. (P11.34- 2), if n- m ~ 3, we can make the closed-loop system unstable by choosing IKilarge enough. (f) Repeat part (c) for the discrete-time feedback system specified by z-3 G(z)H(z) = . (1- z- 1)(1 + ~z- 1 ) (g) Explain why the following statement is true: For any discrete-time feedback system with G(z)H(z) = zm + bm-IZm-1 + ... + bo' zn + an-IZn-l + ' .. + ao if n > m, we can make the closed-loop system unstable by choosing IKilarge enough. 11.35. (a) Consider again the feedback system of Example 11.2: s- 1 G(s)H(s) = (s + 1)(s + 2) 880 Linear Feedback Systems Chap. 11 The root locus forK< 0 is plotted in Figure 11.14(b). For some value of K, the closed-loop poles are on the jw-axis. Determine this value of K and the corresponding locations of the closed-loop poles by examining the real and imaginary parts of the equation 1 G(jw)H(jw) = K' which must be satisfied if the point s = jw is on the root locus for any given values of K. Use this result plus the analysis in Example 11.2 to find the full range of values of K (positive and negative) for which the closed-loop system is stable. (b) Note that the feedback system is unstable for IKI sufficiently large. Explain why this is true in general for continuous-time feedback systems for which G(s)H(s) has a zero in the right-half plane and for discrete-time feedback sys- tems for which G(z)H(z) has a zero outside the unit circle. 11.36. Consider a continuous-time feedback system with 1 G(s)H(s) = s(s + 1)(s + 2) (P11.36-1) (a) Sketch the root locus for K > 0 and for K < 0. (Hint: The results of Problem 11.34 are useful here.) (b) If you have sketched the locus correctly, you will see that for K > 0, two branches of the root locus cross the jw-axis, passing from the left-half plane into the right-half plane. Consequently, we can conclude that the closed-loop system is stable for 0 < K < K0 , where K0 is the value of the gain for which the two branches of the root locus intersect the jw-axis. Note that the sketch of the root locus does not by itself tell us what the value of K0 is or the exact point on the jw-axis where the branches cross. As in Problem 11.35, deter- mine Ko by solving the pair of equations obtained as the real and imaginary parts of G(jw )H(jw) = - ; . (P11.36-2) 0 Determine the corresponding two values of w (which are the negatives of each other, since poles occur in complex-conjugate pairs). From your root-locus sketches in part (a), note that there is a segment of the real axis between two poles which is on the root locus for K > 0, and a different segment is on the locus for K < 0. In both cases, the root locus breaks off from the real axis at some point. In the next part of this problem, we illustrate how one can calculate these breakaway points. (c) Consider the equation denoting the closed-loop poles: 1 G(s)H(s) = (P11.36-3) K"" Chap. 11 Problems 881 s '~I I I I : ~ I p(s) (a) ~p(s) IC2S==t', s I ' I (b) ' Figure P11.36 Using eq. (P11.36-1), show that an equivalent equation for the closed loop poles is p(s) = s3 + 3s2 + 2s = - K. (P11.36-4) Consider the segment of the real axis between 0 and - 1. This segment is on the root locus forK ~ 0. ForK = 0, two branches of the locus begin at 0 and - 1 and approach each other as K is increased. (i) Use the facts stated, together with eq. (P11.36-4), to explain why the function p(s) has the form shown in Figure P11.36(a) for -1 ~ s ~ 0 and why the point s + where the minimum occurs is the breakaway point (i.e., it is the point where the two branches of the K > 0 locus break from the segment of the real axis between -1 and 0). Similarly, consider the root locus for K < 0 and, more specifically, the segment of the real axis between -1 and -2 that is part of this locus. ForK = 0, two branches of the root locus begin at -1 and -2, and asK is decreased, these poles approach each other. (ii) In an analogous fashion to that used in part (i), explain why the function p(s) has the form shown in Figure P11.36(b) and why the points- where the maximum occurs is the breakaway point for K < 0. Thus, the breakaway points correspond to the the maxima and min- ima of p(s) as s ranges over the negative real line. (iii) The points at which p(s) has a maximum or minimum are the solutions of the equation dp(s) = 0 ds · Use this fact to find the breakaway points s+ and s-, and then use eq. (P11.36-4) to find the gains at which these points are closed-loop poles. In addition to the method illustrated in part (c), there are other, partially analytical, partially graphical methods for determining breakaway points. It is also possible to use a procedure similar to the one just illustrated in part (c) to find the 882 Linear Feedback Systems Chap. 11 ""break-in"" points, where two branches of the root locus merge onto the real axis. These methods plus the one illustrated are described in advanced texts such as those listed in the bibliography at the end of the book. 11.37. One issue that must always be taken into account by the system designer is the possible effect of unmodeled aspects of the system one is attempting to stabilize or modify through feedback. In this problem, we provide an illustration of why this is the case. Consider a continuous-time feedback system, and suppose that 1 H(s) = (P11.37-1) (s + 10)(s- 2) and G(s) = K. (P11.37-2) (a) Use root-locus techniques to show that the closed-loop system will be stable if K is chosen large enough. (b) Suppose that the system we are trying to stabilize by feedback actually has a system function 1 H(s) = + + . (P11.37-3) (s 10)(s- 2)(10-3s 1) The added factor can be thought of as representing a first-order system in cas- cade with the system of eq. (P11.37-1). Note that the time constant of the added first order system is extremely small and thus will appear to have a step response that is almost instantaneous. For this reason, one often neglects such factors in order to obtain simpler and more tractable models that capture all of the important characteristics of the system. However, one must still keep these neglected dynamics in mind in obtaining a useful feedback design. To see why this is the case, show that if G(s) is given by eq. (P11.37-2) and H(s) is as in eq. (P11.37-3), then the closed-loop system will be unstable if K is chosen too large. Hint: See Problem 11.34. (c) Use root -locus techniques to show that if G(s) = K(s + 100), then the feedback system will be stable for all values of K sufficiently large if H(s) is given by eq. (P11.37-1) or eq. (P11.37-3). 11.38. Consider the feedback system of Figure 11.3(b) with H(z) = Kz-1 1- z- 1 and G(z) = 1- az- 1• (a) Sketch the root locus for K > 0 and K < 0 when a = 112. (b) Repeat part (a) when a = -112. Chap. 11 Problems 883 (c) With a = -112, find a value of K for which the closed-loop impulse response is of the form (A+ Bn)an for some values of the constants A, B, and a, with Ia I < 1. (Hint: What must the denominator of the closed-loop system function look like in this case?) 11.39. Consider the feedback system of Figure P11.39 with 1 H(z) = _ l.z-I, G(z) = K. (P11.39-1) 1 2 e[n] + H(z) y[n] - G(z) Figure P11.39 (a) Plot the root locus forK > 0. (b) Plot the root locus forK < 0. (Note: Be careful with this root locus. By ap- plying the angle criterion on the real axis, you will find that asK is decreased from zero, the closed loop approaches z = +oo along the positive real axis and then returns along the negative real axis from z = -oo. Check that this is in fact the case by explicitly solving for the closed-loop pole as a function of K. At what value of K is the pole at lzl = oo?) (c) Find the full range of values of K for which the closed-loop system is stable. (d) The phenomenon observed in part (b) is a direct consequence of the fact that in this example the numerator and denominator of G(z)H(z) have the same degree. When this occurs in a discrete-time feedback system, it means that there is a delay-free loop in the system. That is, the output at a given point in time is being fed back into the system and in tum affects its own value at the same point in time. To see that this is the case in the system we are considering here, write the difference equation relating y[n] and e[n]. Then write e[n] in terms of the input and output for the feedback system. Contrast this result with that of the feedback system with 1 H(z) = G(z) = Kz- 1 _ , • (P11.39-2) 1- 1 z 1 2 The primary consequence of having delay-free loops is that such feed- back systems cannot be implemented in the form depicted. For example, for the system of eq. (P11.39-1), we cannot first calculate e[n] and then y[n], 884 Linear Feedback Systems Chap. 11 because e[n] depends on y[n]. Note that we can perform this type of calcula- tion for the system of eq. (P11.39-2), since e[n] depends on y[n- 1]. (e) Show that the feedback system of eq. (P11.39-1) represents a causal system, except for the value of K for which the closed-loop pole is at lzl = oo. 11.40. Consider the discrete-time feedback system depicted in Figure P11.40. The system in the forward path is not very well damped, and we would like to choose the feedback system function so as to improve the overall damping. By using the root- locus method, show that this can be done with G(z) = 1- 1 -1 2z . 1 + K(z-4) x[n] -,. + 7V2 + 49 ""' y[n] z2- z - 8 64 G(z) Figure P11.40 Specifically, sketch the root locus for K > 0, and specify the value of the gain K for which a significant improvement in damping is obtained. 11.41. (a) Consider a feedback system with H(z) = z + 1 , K z2 + z +.!. G(z) = z- 1"" 4 (i) Write the closed-loop system function explicitly as a ratio of two polyno- mials. (The denominator polynomial will have coefficients that depend onK.) (ii) Show that the sum of the closed-loop poles is independent of K. (b) More generally, consider a feedback system with system function Show that if m :s n- 2, the sum of the closed-loop poles is independent of K. 11.42. Consider again the discrete-time feedback system of Example 11.3: z G(z)H(z) = . (z- 21 )(z- 41 ) The root loci for K > 0 and K < 0 are depicted in Figure 11.16. Chap. 11 Problems 885 (a) Consider the root locus for K > 0. In this case, the system becomes unstable when one of the closed-loop poles is less than or equal to -1. Find the value of K for which z = -1 is a closed-loop pole. (b) Consider the root locus for K < 0. In this case, the system becomes unstable when one of the closed-loop poles is greater than or equal to 1. Find the value of K for which z = 1 is a closed-loop pole. (c) What is the full range of values of K for which the closed-loop system is stable? 11.43. Consider a discrete-time feedback system with 1 G(z)H(z) = z(z _ l) (a) Sketch the root locus forK> 0 and forK< 0. (b) If you have sketched the root locus correctly for K > 0, you will see that the two branches of the root locus cross and exit from the unit circle. Consequently, we can conclude that the closed-loop system is stable for 0 < K < K0 , where K 0 is the value of the gain for which the two branches intersect the unit circle. At what points on the unit circle do the branches exit from it? What is the value of Ko? 11.44. As mentioned in Section 11.4, the continuous-time Nyquist criterion can be ex- tended to allow for poles of G(s)H(s) on the jw-axis. In this problem, we will illustrate the general technique for doing this by means of several examples. Con- sider a continuous-time feedback system with 1 G(s)H(s) = s(s + l) (Pl1.44-l) When G(s )H( s) has a pole at s = 0, we modify the contour of Figure 11.19 by avoiding the origin. To do this, we indent the contour by adding a semicircle of infinitesimal radius E into the right-half plane. [See Figure Pll.44(a).] Thus, only a small part of the right-half plane is not enclosed by the modified contour, and its area goes to zero as we let E ~ 0. Consequently, as M ~ oo, the contour will enclose the entire right-half plane. As in the text, G(s )H (s) is a constant (in this case zero) along the circle of infinite radius. Thus, to plot G(s)H(s) along the contour, we need only plot it for the portion of the contour consisting of the jw-axis and the infinitesimal circle. (a) Show that 7T 2 and where s = jO- is the point where the infinitesimal semicircle meets the jw- axis just below the origin and s = jO+ is the corresponding point just above the origin. (b) Use the result of part (a) together with eq. (Pl1.44-l) to verify that Figure P11.44(b) is an accurate sketch of G(s)H(s) along the portions of the contour CRe (a) t w=o- w=±oo (b) Figure P11.44 886 Chap. 11 Problems 887 from - joo to jO- and jo+ to joo. In particular, check that <f._G(jw )H(jw) and IG(jw )H(jw )I behave in the manner depicted in the figure. (c) All that remains to be done is to determine the plot of G(s)H(s) along the small semicircle about s = 0. Note that as E ~ 0, the magnitude of G(s)H(s) along this contour goes to infinity. Show that as E ~ 0, the contribution of the pole at s = -1 to <f._G(s)H(s) along the semicircle is zero. Then show that as E ~ 0, <r._G(s)H(s) = -8, where 8 is as defined in Figure Pl1.44(a). Thus, since 8 varies from -Tr/2 at s = jO- to +Tr/2 at s = jO- in the counterclockwise direction, <f._G(s)H(s) must go from + Trl2 at s = jO+ to -Tr/2 at s = jO+ in the clockwise direction. The result is the complete Nyquist plot depicted in Figure P11.44(c). CRe (c) Figure P11.44 Continued 888 Linear Feedback Systems Chap. 11 (d) Using the Nyquist plot of Figure Pl1.44(c), find the range of values of K for which the closed-loop feedback system is stable. (Note: As presented in the text, the continuous-time Nyquist criterion states that, for closed-loop sys- tem stability, the net number of clockwise encirclements of the point -1/K must equal minus the net number of right-half plane poles of G(s)H(s). In the present example, note that the pole of G(s)H(s) at s = 0 is outside the modi- fied contour. Consequently, it is not included in counting the poles of G(s)H(s) in the right-half plane [i.e., only poles of G(s)H(s) strictly inside the right-half plane are counted in applying the Nyquist criterion]. Thus, in this case, since G(s)H(s) has no poles strictly inside the right-half plane, we must have no encirclements of the points = -11K for closed-loop system stability.) (e) Follow the steps outlined in parts (a)-(c) to sketch the Nyquist plots for each of the following: (i) G(s)H(s) = (.I/IO)+ 1 s(s+ I) (ii) G(s)H(s) = s(.l~ ) 1 2 (iii) G(s)H(s) = ~ [be careful in calculating 1:-G(s)H(s) along the infinites- imal semicircle] (iv) G(s)H(s) = .,f't-1n [be careful in calculating 1:-G(jw )H(jw) as w is var- ied; make sure to take the minus sign in the denominator into account] (v) G(s)H(s) = s~ 1 [same remark as for (iii)] s- In each case, use the Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Also, use another method (root locus or direct calculation of the closed-loop poles as a function of K) to provide a partial check of the correctness of your Nyquist plot. [Note: In sketching the Nyquist plots, you may find it useful to sketch the Bode plots of G(s)H(s) first. It may also be helpful to determine the values of w for which G(jw )H(jw) is real.] (f) Repeat part (e) for: (i) G(s)H(s) = .\ 2 ~ 1 (ii) G(s)H(s) = ;~:\ Note: In these cases there are trvo poles on the imaginary axis; accordingly, you will need to modify the contour of Figure 11.19 to avoid each of them. Use infinitesimal semicircles, as in Figure P11.44(a). 11.45. Consider a system with system function 1 H(s) = (P11.45-1) (s+1)(s-2) Because this system is unstable, we would like to devise some method for its sta- bilization. (a) Consider first a series compensation scheme as illustrated in Figure P11.45(a). Show that the overall system of this figure is stable if the system function s-2 C(s) = s + 3. In practice, this is not considered to be a particularly useful way to attempt to stabilize a system. Explain why. Chap. 11 Problems 889 x(t) ---t•~' C(s) H(s) 1---....,·~ y(t) (a) x(t) --~+ ~--~8--~._H_(_s)_:----+-~ y(t) (b) Fi~ure P 11 .45 (b) Suppose that instead we use a feedback system, as depicted in Figure P11.45(b ). Is it possible to stabilize this system using a constant gain, that is, C(s) = K, for the stabilizing element? Justify your answer using Nyquist techniques. (c) Show that the system of Figure P11.45(b) can be stabilized if C(s) is a propor- tional plus derivative system-that is, if C(s) = K(s + a). Consider both the case 0 < a < 1 and the case a > 1. (d) Suppose that C(s) = K(s + 2). Choose the value of K such that the closed-loop system has a pair of complex poles with a damping ratio { = 112. (Hint: In this case, the denominator of the closed-loop system must have the form s2 + w s + w 2 n n for some value of Wn > 0.) (e) Pure derivative compensation is both impossible to obtain and undesirable in practice. This is because the required amplification of arbitrarily high frequen- cies neither can be obtained nor is advisable, as all real systems are subject to some level of high-frequency disturbances. Thus, suppose that we consider a compensator of the form = ss ++ a) C(s) K ( b , a,b > 0. (P11.45-2) 890 Linear Feedback Systems Chap. 11 If b < a, this is a lag network: <:f_C(jw) < 0 for all w > 0, so that the phase of the output of the system lags the phase of the input. If b > a, <:f_C(jw) > 0 for all w > 0, and the system is then called a lead network. (i) Show that it is possible to stabilize the system with the lead compensator s+_!_ C(s) = K--2 (Pll.45-3) s+2 if K is chosen large enough. (ii) Show that it is not possible to stabilize the feedback system of Figure P11.45(b) using the lag network C(s) = Ks + 3 . s+2 Hint: Use the results of Problem 11.34 in sketching the root locus. Then determine the points on the jw-axis that are on the root locus and the values of K for which each of these points is a closed-loop pole. Use this information to prove that for no value of K are all of the closed-loop poles in the left-half plane. 11.46. Consider the continuous-time feedback system depicted in Figure P11.46(a). s+10 + y(t) (s+1)2 - 10 (s/1 00+1)2 (a) (s+ 1 O)e-sT x(t) -~~++ ~ y(t) (s+1)2 - 10 (s/100+1)2 (b) Figure P 11 .46 Chap. 11 Problems 891 (a) Use the straight-line approximations to Bode plots developed in Chapter 6 to obtain a sketch of the log magnitude-phase plot of this system. Estimate the phase and gain margins from your plot. (b) Suppose that there is an unknown delay within the feedback system, so that the actual feedback system is as shown in Figure P11.46(b ). Approximately what is the largest delay T that can be tolerated before the feedback system becomes unstable? Use your results from part (a) for this calculation. (c) Calculate more precise values of the phase and gain margins, and compare these to your results in part (a). This should give you some idea of the size of the errors that are incurred in using the approximate Bode plots. 11.47. As mentioned at the end of Section 11.5, the phase and gain margins may provide sufficient conditions to ensure that a stable feedback system remains stable. For example, we showed that a stable feedback system will remain stable as the gain is increased, until we reach a limit specified by the gain margin. This does not imply (a) that the feedback system cannot be made unstable by decreasing the gain or (b) that the system will be unstable for all values of gain greater than the gain margin limit. In this problem, we illustrate these two points. (a) Consider a continuous-time feedback system with 1 G(s)H(s) = (s- 1)(s + 2)(s + 3) Sketch the root locus for this system for K > 0. Use the properties of the root locus described in the text and in Problem 11.34 to help you draw the locus accurately. Once you do so, you should see that for small values of the gain K the system is unstable, for larger values of K the system is stable, while for still larger values of K the system again becomes unstable. Find the range of values of K for which the system is stable. Hint: Use the same method as is employed in Example 11.2 and Problem 11.35 to determine the values of K at which branches of the root locus pass through the origin and cross the jw-axis. If we set our gain somewhere within the stable range that you have just found, we can increase the gain somewhat and maintain stability, but a large enough increase in gain causes the system to become unstable. This maximum amount of increase in gain at which the closed-loop system just becomes un- stable is the gain margin. Note that if we decrease the gain too much, we can also cause instability. (b) Consider the feedback system of part (a) with the gain K set at a value of 7. Show that the closed-loop system is stable. Sketch the log magnitude-phase plot of this system, and show that there are two nonnegative values of w for which <r-G (jw)H(jw) = -7T. Further, show that, for one of these values 7IG(jw)H(jw)l < 1, and for the other 7IG(jw)H(jw)l > 1. The first value provides us with the usual gain margin-that is, the factor 11I7G(jw )H(jw )I by which we can increase the gain and cause instability. The second provides us with the factor 11I7G(jw )H(jw )I by which we can decrease the gain and just cause instability. 892 Linear Feedback Systems Chap. 11 (c) Consider a feedback system with (s/100 + 1)2 G(s)H(s) = (s + l)3 Sketch the root locus forK > 0. Show that two branches of the root locus begin in the left-half plane and, asK is increased, move into the right-half plane and then back into the left-half plane. Do this by examining the equation G(jw)H(jw) = - ~- Specifically, by equating the real and imaginary parts of this equation, show that there are two values of K 2: 0 for which the closed-loop poles lie o~ the jw-axis. Thus, if we set the gain at a small enough value so that the system is sta- ble, then we can increase the gain up until the point at which the two branches of the root locus intersect the jw-axis. For a range of values of gain beyond this point, the closed-loop system is unstable. However, if we continue to increase the gain, the system will again become stable for K large enough. (d) Sketch the Nyquist plot for the system of part (c), and confirm the conclusions reached in part (c) by applying the Nyquist criterion. (Make sure to count the net number of encirclements of -1/K .) Systems such as that considered in parts (c) and (d) of this problem are often referred to as being conditionally stable systems, because their stability properties may change several times as the gain is varied. 11.48. In this problem, we illustrate the discrete-time counterpart of the technique de- scribed in Problem 11.44. Specifically, the discrete-time Nyquist criterion can be extended to allow for poles of G(z)H(z) on the unit circle. Consider a discrete-time feedback system with z_ '""-) G(z)H(z) = _ z-I (Pll.48-l) 1 z(z- 1) · In this case, we modify the contour on which we evaluate G(z)H(z), as illustrated in Figure Pll.48(a). (a) Show that 1T 2 and ''"") - ""'"") - 1T 1:-G(e1-1T )H(e1-1T ) = , 2 where z = ei2 1T is the point below the real axis at which the small semicircle intersects the unit circle and z = ei0 + is the corresponding point above the real axis. (b) Use the results of part (a) together with eq. (Pll.48-l) to verify that Fig- ure P11.48(b) is an accurate sketch of G(z)H(z) along the portion of the Chap. 11 Problems 893 (a) fw=2'TT !w =O+ (b) Figure P11.48 894 Linear Feedback Systems Chap. 11 contour z = ejw as w varies from o+ to 27T- in a counterclockwise direc- tion. In particular, verify that the angular variation of G( ejw )H ( ejw) is as indicated. (c) Find the value of w for which <t.G(ejw)H(ejw) = -1r, and verify that at this point. [Hint: Use the geometrical method for evaluating <t G( ejw )H ( ejw) together with some elementary geometry to determine the value of w.] (d) Consider next the plot of G(z)H(z) along the small semicircle about z = 1. Note that as E ~ 0, the magnitude of G(z)H(z) along this contour goes to in- finity. Show that as E ~ 0, the contribution of the pole at z = 0 to <t.G(z)H(z) along the semicircle is zero. Then show that as E ~ 0, <t.G(z)H(z) = -(}, where(} is as defined in Figure P11.48(a). Thus, since (} varies from -7T/2 to + 7T/2 in the counterclockwise direc- tion, <t.G(z)H(z) varies from +1rl2 to -1r12 in the clockwise direction. The result is the complete Nyquist plot of Figure P11.48( c). (e) Using the Nyquist plot, find the range of values of K for which the closed- loop feedback system is stable. [Note: Since the pole of G(z)H(z) at z = 1 is !1m CRe (c) Figure P11.48 Continued Chap. 11 Problems 895 inside the modified contour, it is not included in counting the poles of G(z)H (z) outside the unit circle. That is, only poles strictly outside the unit circle are counted in applying the Nyquist criterion. Thus, in this case, since G(z)H(z) has no poles strictly outside the unit circle, we must have no encirclements of the point z = -11 K for closed-loop stability.] (f) Follow the steps outlined in parts (a), (b), and (d) to sketch the Nyquist plots for each of the following: (i) z+ f_+/3 (ii) I (z-1 )(z+ ~ + }3) (iii) z+ 1 z(z-1) (iv) ~~211~ [be careful in calculating <f-G(z)H(z) along the infinitesimal semi- circle] For each of the preceding, use the Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Also, use another method (root locus or direct calculation of the closed- loop poles as a function of K) to provide a partial check of the correctness of your Nyquist plot. Note: In sketching the Nyquist plots, you may find it useful to first sketch the magnitude and phase plots as a function of frequency or at least calculate IG(ejw)H(ejw)l and <f-G(ejw)H(ejw) at several points. Also, it is helpful to determine the values of w for which G( ejw )H ( ejw) is real. (g) Repeat part (f) for G(z)H(z) = z2 _ . 1 In this case there are two poles on the unit circle, and thus, you must modify the contour around each of these by including an infinitesimal semicircle that extends outside the unit circle, thereby placing the pole inside the contour. EXTENSION PROBLEMS 11.49. In this problem, we provide an illustration of how feedback can be used to increase the bandwidth of an amplifier. Consider an amplifier whose gain falls off at high frequencies. That is, suppose the system function of this amplifier is Ga H(s) = --. s+a (a) What is the de gain of the amplifier (i.e., the magnitude of its frequency re- sponse at 0 frequency)? (b) What is the system time constant? (c) Suppose we define the bandwidth of the system as the frequency at which the magnitude of the amplifier frequency response is 1/ J2 times its magnitude at de. What is the bandwidth of the amplifier? 896 Linear Feedback Systems Chap. 11 (d) Suppose we place the amplifier in a feedback loop as depicted in Figure P11.49. What is the de gain of the closed-loop system? What are the time constant and the bandwidth of the closed-loop system? (e) Find the value of K that leads to a closed-loop bandwidth that is exactly double the bandwidth of the open -loop amplifier. What are the corresponding closed- loop system time constant and de gain? x(t)--.....,.+~1 1----......- ---l~ y(t) Figure P 11 .49 11.50. As mentioned in the text, an important class of devices used in the implementation of feedback systems is the class of operational amplifiers. A model for such an amplifier is depicted in Figure P11.50(a). The amplifier's input is the difference >---10+ v0(t) + r Figure P11.50a between two voltages v2(t) and v1( t), and the output voltage is an amplified version of the input; that is, Va(t) = K[v2(t)- Vt (t)]. (Pll.S0-1) Consider an operational amplifier connection shown in Figure P11.50(b ). In this figure, Z1 (s) and Z2(s) are impedances. (That is, each is the system function of an LTI system whose input is the current flowing through the impedance element and whose output is the voltage across the element.) Making the approximation that the input impedance of the operational amplifier is infinite and that its output impedance is zero, we obtain the following relationship between V1( s), Vi(s), and V0 (s), the Laplace transforms of v1( t), vi(t), and v 0 (t), respectively: v, = [z,(s~si2(s)] V;(s) + [z,(s~~si2(s)] Vn(s). (Pll.50-2) Chap. 11 Problems 897 Z2 (s) Z1(s) + + v0 (t) vi(t) v1 (t) r- - Figure P11.50b Also, from eq. (Pl1.50-1) and Figure P11.50(b), we see that Va(s) = -KVt(s). (P11.50-3) (a) Show that the system function H(s) = Va(s) V;(s) for the interconnection of Figure P 11.50(b) is identical to the overall closed- loop system function for the system of Figure P11.50(c). Z2 (s) + -K Z1 (s) + + Z2 (s) + Z1(s) Z1 (s) + Z2 (s) Figure P11.50c (b) Show that if K >> 1, then H(s) = _ Z2(s). Zt(S) 11.51. (a) Suppose that in Figure P11.50(b) Z1( s) and Z2(s) are both pure resistances, say, R1 and R2 , respectively. A typical value for R2/R 1 is in the range 1 to 103, while a typical value forK is 106 . Using the results of Problem 11.50(a), calculate the actual system function for this value of K and for R2! R1 equal to 1 and then to 103, and compare each resulting value to -R2/R1• This should give you some idea of how good the approximation of Problem 11.50(b) typically is. 898 Linear Feedback Systems Chap. 11 (b) One of the important uses of feedback is in the reduction of system sensitivity to variations in parameters. This is particularly important for circuits involv- ing operational amplifiers, which have high gains that may be known only approximately. (i) Consider the circuit discussed in part (a), with R2/R 1 = 102. What is the percentage change in the closed-loop gain of the system if K changes from 106 to 5 X 105? (ii) How large must K be so that a 50% reduction in its value results in only a 1% reduction in the closed-loop gain? Again, take R 1R = 102 2 1 . 11.52. Consider the circuit of Figure P11.52. This circuit is obtained by using 1 Zis) = CS in Figure Pll.50(b) . Using the results from Problem 11.50, show that the system behaves approximately like an integrator. In what frequency range (expressed in terms of K, R, and C) does this approximation break down? c >-~. ..........- u+ v0 (t) r Figure P11.52 11.53. Consider the circuit depicted in Figure P11.53(a), which is obtained from the circuit of Figure P11.50(b) by using Z1 (s) = Rand by replacing Z2(s) with a diode that has an exponential current-voltage relationship. Assume that this relationship is of the form (P11.53-l) where M is a constant that depends upon the construction of the diode, q is the charge of an electron, k is Boltzmann's constant, and Tis absolute temperature. Note that the idealized relationship of eq. (P11.53-1) assumes that there is no pos- sibility of a negative diode current. Usually, there is some small maximum negative value of diode current, but we will neglect this possibility in our analysis. (a) Assuming that the input impedance of the operational amplifier is infinite and that its output impedance is zero, show that the following relations hold: (Pll.53-2) (Pll.53-3) Chap. 11 Problems 899 >--.......- 0+ v (t) r 0 Figure P11.53a (b) Show that forK large, the relationship between v0 (t) and vi(t) is essentially the same as in the feedback system of Figure P11.53(b), in which the system in the feedback path is a nonlinear memoryless system with input v0 (t) and output w(t) = RMe qv0 (t)lkT_ (c) Show that for K large, v (t ) ~~ -kT l n ( -V-i(-t)) (Pl1.53-4) o q RM. -K + w(t) Figure P11.53b Note that eq. (Pll.53-4) makes sense only for a negative vi(t), which is con- sistent with the requirement that the diode current cannot be negative. If a positive Vi(t) is applied, the current id(t) cannot balance the current through the resistor. Thus, a nonnegligible current is fed into the amplifier, causing it to saturate. 11.54. In this problem, we explore the use of positive feedback for generating oscillating signals. (a) Consider the system illustrated in Figure Pll.54(a). Show that x 1(t) = xi(t) if G(s)H(s) = -1. (Pll.54-1) Suppose that we connect terminals 1 and 2 in Figure P11.54(a) and make Xi(t) = 0. Then the output of the system should remain unchanged if we 900 Linear Feedback Systems Chap. 11 Xj(t) H(s) y(t) 3 1 -1 ~ G(s) (a) + xi(t+ )=O~ H(s) y(t) + ---- -1 G(s) I f+- (b) Figure P11.54 satisfy eq. (Pll.54-1). The system now produces an output without any input. Therefore, the system shown in Figure Pll.54(b) is an oscillator, provided that eq. (Pll.54-l) is satisfied. (b) A commonly used oscillator in practice is the sinusoidal oscillator. For such an oscillator, we may rewrite the condition of ~q. (Pll.54-l) as G(jwo)H(jwo) = -1. (Pll.54-2) What is the value of the closed-loop gain for the system shown in Figure Pll.54(b) at w0 when eq. (Pll.54-2) is satisfied? (c) A sinusoidal oscillator may be constructed on the basis of the principle out- lined above by using the circuit shown in Figure Pll.54(c). The input to the Figure P11.54c Chap. 11 Problems 901 amplifier is the difference between the voltages v1( t) and v2(t). In this circuit, the amplifier has a gain of A and an output resistance of R0 . Z1( s), Z2(s), and Z3(s) are impedances. (That is, each is the system function of an LTI system whose input is the current flowing through the impedance element and whose output is the voltage across the element.) It can be shown that, for this circuit, H(s) = -AZL(s) ZL(s) + Ro' where Also, we can show that (i) Show that G(s)H(s) = AZ1 (s)Z2(s) Ro(ZI (s) + Z2(s) + Z3(s)) + Z2(s)(Z1 (s) + Z3(s)) (ii) If Z1( s), Z2(s), and Z3(s) are pure reactances (i.e., inductances or ca- pacitances), we can write Z1 (jw) = jX1( jw ), Z2(jw) = jX2(jw ), and Z3(jw) = jX3(jw), where X;(jw), i = 1, 2, 3, are all real. Using there- sults of parts (b) and (i), show that a necessary condition for the circuit to produce oscillations is X1(jw) + X2(jw) + X3(jw) = 0. (iii) Show also that, in addition to the constraint of part (ii), the constraint AX1( jw) = X2(jw) has to be satisfied for the circuit to produce os- cillations. [Since X;(jw) is positive for inductances and negative for capacitances, the latter constraint requires that Z1( s) and Z2(s) be reac- tances of the same type (i.e., both should be inductances or both should be capacitances).] (iv) Let us assume that Z1( s) and Z2(s) are both inductances such that X1(jw) = X2(jw) = wL. Let us also assume that X3(jw) = -ll(wC) is a capacitance. Use the condition derived in (ii) to determine the fre- quency (in terms of L and C) at which the circuit oscillates. 11.55. (a) Consider the nonrecursive discrete-time LTI filter depicted in Figure P11.55(a). Through the use of feedback around this nonrecursive system, a recursive filter can be implemented. To do so, consider the configuration shown in Figure Pll.55(b), in which H(z) is the system function of the nonrecursive LTI system of Figure Pll.55(a). Detenninetheovernllsystemftmctionofthisfeedback 902 Linear Feedback Systems Chap. 11 system, and find the difference equation relating the input to the output of the overall system. x[n] D D D ... ~ D ~ L...-----+-l + 1----....,.~ + ~-- ... --~ y[n] (a) + K .. y[n] - H(z) (b) Figure P11.55 (b) Now suppose that H(z) in Figure P11.55(b) is the system function of a recur- sive LTI system. Specifically, suppose that H(z) = Show how one can find values of the coefficients K, c 1, ••• , cN , and do, ... , d N, such that the closed-loop system function is where the ai and hi are specified coefficients. Chap. 11 Problems 903 In this problem, we have seen that the use of feedback provides us with alterna- tive implementations of LTI systems specified by linear constant-coefficient dif- ference equations. The implementation in part (a), consisting of feedback around a nonrecursive system, is particularly interesting, as some technologies are ide- ally suited to implementing tapped delay-line structures (i.e., systems consisting of chains of delays with taps at each delay whose outputs are weighted and then summed). 11.56. Consider an inverted pendulum mounted on a movable cart, as depicted in Figure P11.56. Here, we have modeled the pendulum as consisting of a massless rod of length L with a mass m attached at the end. The variable O(t) denotes the pendu- lum's angular deflection from the vertical, g is gravitational acceleration, s(t) is the position of the cart with respect to some reference point, a(t) is the acceler- ation of the cart, and x(t) represents the angular acceleration resulting from any disturbances, such as gusts of wind. a(t) I J I s(t) Figure P11.56 Our goal in this problem is to analyze the dynamics of the inverted pendulum and, more specifically, to investigate the problem of balancing the pendulum by a judicious choice of the acceleration a(t) of the cart. The differential equation relating O(t), a(t), and x(t) is d 20(t) . LdT = g sm[O(t)] - a(t) cos[O(t)] + Lx(t). (P11.56-l) This relation merely equates the actual acceleration of the mass along a direction perpendicular to the rod to the applied accelerations [gravity, the disturbance ac- celeration due to x(t), and the cart's acceleration] along this direction. Note that eq. (P11.56-1) is a nonlinear differential equation. The detailed, exact analysis of the behavior of the pendulum requires that we examine this equa- tion; however, we can obtain a great deal of insight into the dynamics of the pendu- lum by performing a linearized analysis. Specifically, let us examine the dynamics of the pendulum when it is nearly vertical [i.e., when O(t) is small]. In this case, we 904 Linear Feedback Systems Chap. 11 can make the approximations sin[O(t)] = O(t), cos[O(t)] = 1. (P11.56-2) (a) Suppose that the cart is stationary [i.e., a(t) = 0], and consider the causal LTI system with input x(t) and output O(t) described by eq. (P11.56-1), together with the approximations given in eq. (P11.56-2). Find the system function for this system, and show that it has a pole in the right-half of the plane, implying that the system is unstable. (b) The result of part (a) indicates that if the cart is stationary, any minor angular disturbance caused by x(t) will lead to growing angular deviations from the vertical. Clearly, at some point, these deviations will become sufficiently large so that the approximations of eq. (Pl1.56-2) will no longer be valid. At this point the linearized analysis is no longer accurate, but the fact that it is ac- curate for small angular displacements allows us to conclude that the vertical equilibrium position is unstable, since small angular displacements will grow rather than diminish. We now wish to consider the problem of stabilizing the vertical position of the pendulum by moving the cart in an appropriate fashion. Suppose we try proportional feedback-that is, a(t) = KO(t). Assume that O(t) is small, so that the approximations in eq. (P11.56-2) are valid. Draw a block diagram of the linearized system with O(t) as the output, x(t) as the external input, and a(t) as the signal that is fed back. Show that the resulting closed-loop system is unstable. Find a value of K such that if x(t) = S(t), the pendulum will sway back and forth in an undamped oscillatory fashion. (c) Consider using the proportional-plus-derivative (PD) feedback, Show that one can find values of K1 and K2 that stabilize the pendulum. In fact, using g = 9.8 m/sec2 and (P11.56-3) L = 0.5 m, choose values of Kt and K2 so that the damping ratio of the closed loop system is 1 and the natural frequency is 3 rad/sec. 11.57. In this problem, we consider several examples of the design of tracking systems. Consider the system depicted in Figure P11.57. Here, Hp(s) is the system whose output is to be controlled, and Hc(s) is the compensator to be designed. Our objec- tive in choosing Hc(s) is that we would like the output y(t) to follow the input x(t). In particular, in addition to stabilizing the system, we would also like to design the system so that the error e(t) decays to zero for certain specified inputs. Chap. 11 Problems 905 --~+11'7\ e(t) d(t) x(t) V+ ·I H,(s) I • Hp(s) y(t) ..,. Figure P11.57 (a) Suppose that Hp(s) = -a- , a =1:- 0. (P11.57-1) s+a Show that if Hc(s) = K (which is known as proportional or P control), we can choose K so as to stabilize the system and so that e(t) ~ 0 if x(t) = 8(t). Show that we cannot get e(t) ~ 0 if x(t) = u(t). (b) Again let Hp(s) be as in eq. (P11.57-l), and suppose that we use proportional- plus-integral (Pl) ~ontrol-that is, Show that we can choose K1 and K2 so as tp stabilize the system, and we can also get e(t) ~ 0 if x(t) = u(t). Thus, the system can track a step. In fact, this illustrates a basic and important principle in feedback system design: To track a step [X(s) = lis], we need an integrator (1/s) in the feedback system. An extension of this principle is considered in the next problem. (c) Suppose that 1 Hp(s) = (s - 1)2. Show that we cannot stabilize this system with a PI controller, but that we can stabilize it and have it track a step if we use proportional-plus-integral-plus- differential (PID) control, i.e., 11.58. In Problem 11.57, we discussed how the presence of an integrator in a feedback system can make it possible for the system to track a step input with zero error in the steady state. In this problem, we extend the idea. Consider the feedback system depicted in Figure Pll.58, and suppose that the overall closed-loop system is stable. SupP.ose ~lso that m Kfl(s -{h) H(s) = _k_=_l- -- n-1 s1fl(s- ak) k=l 906 Linear Feedback Systems Chap. 11 e(t) + H(s) y(t) ~ Figure P11.58 where the ak and f3k are given nonzero numbers and lis a positive integer. The feedback system of Figure P11.58 is often referred to as a Type l feedback system. (a) Use the final-value theorem (Section 9.5.10) to show that a Type 1 feedback system can track a step-that is, that e(t) ~ 0 if x(t) = u(t). (b) Similarly, show that a Type 1 system cannot track a ramp, but rather, that e(t) ~ a finite constant if x(t) = U-2(t). (c) Show that, for a Type 1 system, unbounded results ensue if x(t) = U-k(t) with k > 2. (d) More generally, show that, for a Type l system: (i) e(t) ~ 0 if x(t) = U-k(t) with k :::::.; l (ii) e(t) ~a finite constant if x(t) = U(-l+l)(t) (iii) e(t) ~ oo if x(t) = U-k(t) with k > l + 1 11.59. (a) Consider the discrete-time feedback system of Figure P11.59. Suppose that 1 H(~ = . (z- 1)(z + ~) x[n] ____;_~ H(z) 1--__. ..,..____...,.... y[n] Figure P11.59 Show that this system can track a unit step in the sense that if x[n] = u[n], then lim e[n] = 0. (P11.59-l) n~oc (b) More generally, consider the feedback system of Figure P11.59, and assume that the closed-loop system is stable. Suppose that H(z) has a pole at z = 1. Chap. 11 Problems 907 Show that the system can track a unit step. [Hint: Express the transform E(z) of e[n] in terms of H(z) and the transform of u[n]; explain why all the poles of E(z) are inside the unit circle.] (c) The results of parts (a) and (b) are discrete-time counterparts of the results for continuous-time systems discussed in Problems 11.57 and 11.58. In discrete time, we can also consider the design of the systems that track specified inputs peifectly after a finite number of steps. Such systems are known as deadbeat feedback systems. Consider the discrete-time system of Figure P 11.59 with z-I H(z)= 1-z-I' Show that the overall closed-loop system is a deadbeat feedback system with the property that it tracks a step input exactly after one step: that is, if x[n] = u[n], then e[n] = 0, n ~ 1. (d) Show that the feedback system of Figure P11.59 with is a deadbeat system with the property that the output tracks a unit step per- fectly after a finite number of steps. At what time step does the error e[n] first settle to zero? (e) More generally, for the feedback system of Figure P11.59, find H(z) so that y[n] perfectly tracks a unit step for n ~ Nand, in fact, so that N-l e[n] = .L, ako[n- k], (P11.59-2) k=O where the ai are specified constants. Hint: Use the relationship between H(z) and E(z) when the input is a unit step and e[n] is given by eq. (P11.59-2). (f) Consider the system of Figure P11.59 with Show that this system tracks a ramp x[n] = (n + 1)u[n] exactly after two time steps. 11.60. In this problem, we investigate some of the properties of sampled-data feedback systems and illustrate the use of such systems. Recall from Section 11.2.4 that in a sampled-data feedback system the output of a continuous-time system is sam- pled. The resulting sequence of samples is processed by a discrete-time system, the output of which is converted to a continuous-time signal that in tum is fed back and subtracted from the external input to produce the actual input to the continuous- time system. 908 Linear Feedback Systems Chap. 11 (a) Consider the system within dashed lines in Figure 11.6(b ). This is a discrete- time system with input e[n] and output p[n]. Show that it is an LTI system. As we have indicated in the figure, we will let F(z) denote the system function of this system. (b) Show that in Figure 11.6(b) the discrete-time system with system function F(z) is related to the continuous-time system with system function H(s) by means of a step-invariant transformation. That is, if s(t) is the step response of the continuous-time system and q[n] is the step response of the discrete-time system, then q[n] = s(nT) for all n. (c) Suppose that 1 H(s) = --, ffi..e{s} > 1. s- 1 Show that (d) Suppose that H(s) is as in part (c) and that G(z) = K. Find the range of values of K for which the closed-loop discrete-time system of Figure 11.6(b) is stable. (e) Suppose that K G(z) = ----,------- 1 + !z- 1 • 2 Under what conditions on T can we find a value of K that stabilizes the overall system? Find a particular pair of values for K and T that yield a stable closed- loop system. Hint: Examine the root locus, and find the values for which the poles enter or leave the unit circle. APPENDIX pARTIAL-FRACTION EXPANSION A. 1 INTRODUCTION The purpose of this appendix is to describe the technique of partial-fraction expansion. This tool is of great value in the study of signals and systems; in particular, it is very useful in inverting Fourier, Laplace, or z-transforms and in analyzing LTI systems de- scribed by linear constant-coefficient differential or difference equations. The method of partial-fraction expansion consists of taking a function that is the ratio of polynomials and expanding it as a linear combination of simpler terms of the same type. The determination of the coefficients in the linear combination is the basic problem to be solved in obtaining the expansion. As we will see, this is a relatively straightforward problem in algebra that can be solved very efficiently with a bit of ""bookkeeping."" To illustrate the basic idea behind and role of partial-fraction expansion, consider the analysis developed in Section 6.5.2 for a second-order continuous-time LTI system specified by the differential equation d 2 y(t) d y(t) 2 2 ----;[i2 + 2{wn----;[{ + wny(t) = wnx(t). (A.l) The frequency response of this system is w~ H(jw) = (jw)2 + 2{wn(jw) + w~' (A.2) or, if we factor the denominator, (A.3) where CJ = -{wn + WnJ(2=I, c2 = -{wn - WnJ(2=I. (A.4) Having H(jw ), we are in a position to answer a variety of questions related to the system. For example, to determine the impulse response of the system, recall that for any number a with CRe{s} < 0, the Fourier transform of XJ (t) = eat u(t) (A.5) is 1 X 1(jw) = (A.6) jw -a' 909"
Appendix: Partial-Fraction Expansion,"APPENDIX pARTIAL-FRACTION EXPANSION A. 1 INTRODUCTION The purpose of this appendix is to describe the technique of partial-fraction expansion. This tool is of great value in the study of signals and systems; in particular, it is very useful in inverting Fourier, Laplace, or z-transforms and in analyzing LTI systems de- scribed by linear constant-coefficient differential or difference equations. The method of partial-fraction expansion consists of taking a function that is the ratio of polynomials and expanding it as a linear combination of simpler terms of the same type. The determination of the coefficients in the linear combination is the basic problem to be solved in obtaining the expansion. As we will see, this is a relatively straightforward problem in algebra that can be solved very efficiently with a bit of ""bookkeeping."" To illustrate the basic idea behind and role of partial-fraction expansion, consider the analysis developed in Section 6.5.2 for a second-order continuous-time LTI system specified by the differential equation d 2 y(t) d y(t) 2 2 ----;[i2 + 2{wn----;[{ + wny(t) = wnx(t). (A.l) The frequency response of this system is w~ H(jw) = (jw)2 + 2{wn(jw) + w~' (A.2) or, if we factor the denominator, (A.3) where CJ = -{wn + WnJ(2=I, c2 = -{wn - WnJ(2=I. (A.4) Having H(jw ), we are in a position to answer a variety of questions related to the system. For example, to determine the impulse response of the system, recall that for any number a with CRe{s} < 0, the Fourier transform of XJ (t) = eat u(t) (A.5) is 1 X 1(jw) = (A.6) jw -a' 909 910 Appendix while if (A.7) then 1 X2(jw) = (jw - a)2. (A.8) Therefore, if we can expand H(jw) as a sum of terms of the form of eq. (A.6) or (A.8), we can determine the inverse transform of H (jw) by inspection. For example, in Section 6.5.2 we noted that when c1 -# c2, H(jw) in eq. (A.3) could be rewritten in the form H( ).W ) _- ( w~ ) . 1 + ( w~ ) . 1 . (A.9) CJ - C2 )W - CJ C2 - Ct )W - C2 In this case, the Fourier transform pair of eqs. (A.5) and (A.6) allows us to write down immediately the inverse transform of H(jw) as h(t) = (A.10) While we have phrased the preceding discussion in terms of continuous-time Fourier transforms, similar concepts also arise in discrete-time Fourier analysis and in the use of Laplace and z-transforms. In all of these cases, we encounter the important class of rational transforms-that is, transforms that are ratios of polynomials in some variable. Also, in each of these contexts, we find reasons for expanding these transforms as sums of simpler terms such as in eq. (A.9). In this section, in order to develop a general procedure for calculating the expansions, we consider rational functions of a general variable v; that is, we examine functions of the form H(v) = f3mvm + f3m-IVm-I + ... + f3tv + f3o. (A.ll) a V 11 11 + a 11 -JVn-I + ... + a1v + ao For continuous-time Fourier analysis (jw) plays the role of v, while for Laplace transforms that role is played by the complex variables. In discrete-time Fourier analysis, vis usually taken to bee- jw, while for z-transforms, we can use either z- 1 or z. After we have developed the basic techniques of partial-fraction expansion, we will illustrate their application to the analysis of both continuous-time and discrete-time LTI systems. A.2 PARTIAL-FRACTION EXPANSION AND CONTINOUS-TIME SIGNALS AND SYSTEMS For our purposes, it is convenient to consider rational functions in one of two standard forms. The second of these, which is often useful in the analysis of discrete-time signals and systems, will be discussed shortly. The first of the standard forms is (A.12) V 11 + an-Ivn-I + ... + a1v + ao Appendix 911 In this form the coefficient of the highest order term in the denominator is 1, and the order of the numerator is at least one less than the order of the denominator. (The order of the numerator will be less than n- 1 if bn- 1 = 0.) If we are given H(v) in the form of eq. (A.11), we can obtain a rational function of the form of eq. (A.12) by performing two straightforward calculations. First, we divide both the numerator and the denominator of H ( v) by an. This yields (A.13) where f3m f3m-1 Ym = Ym-1 = an an an-1 an-2 an-1 = an-2 = an an If m < n, H ( v) is called a strictly proper rational function, and in this case, letting bo = Yo, b1 = Y1, ... , bm = Ym, and setting any remaining b's equal to zero, we see that H(v) in eq. (A.13) is already of the form of eq. (A.12). In most of the discussions in this book in which rational functions are considered, we are concerned primarily with strictly proper rational functions. However, if H(v) is not proper (i.e., if m ~ n), we can perform a preliminary calculation that allows us to write H ( v) as the sum of a polynomial in v and a strictly proper rational function. That is, H(v) = Cm-nVm-n + Cm-n-1Vm-n- 1 + ... + C1V +Co bn-1Vn- 1 + bn-2Vn-2 + ... + b1v + bo (A.14) + ------------~--------------- vn + an-1vn-1 + ... + a1V + ao The coefficients co, c1, ... , Cm-n and bo, b1, ... , bn-1 can be obtained by equating eqs. (A.13) and (A.14) and then multiplying through by the denominator. This yields (A.15) + (Cm-nVm-n + ... + Co)(vn + an-1 Vn-I + ... + ao). By equating the coefficients of equal powers of v on both sides of eq. (A.15), we can determine the c's and b's in terms of the a's andy's. For example, if m = 2 and n = 1, so that Y2v2 + Y1v +Yo bo H(v) = = c1v +co+--, (A.16) v + a1 v + a1 then eq. (A.15) becomes Y2v2 + YIV +Yo = bo + (civ + co)(v +at) = b + c1v2 0 +(co+ a1 c1)v + a1 co. Equating the coefficients of equal powers of v, we obtain the equations Y2 = C], YI = co+ a1c1, Yo= bo + a1co. 912 Appendix The first equation yields the value of c 1, which can then be used in the second to solve for c0 , which in tum can be used in the third to solve for b0 . The result is CJ = ')'2, Co = /'I - GJ')'2, bo = 'Yo-ai('YI-ai/'2). The general case of eq. (A.15) can be solved in an analogous fashion. Our goal now is to focus on the proper rational function G(v) in eq. (A.l2) and to expand it into a sum of simpler proper rational functions. To see how this can be done, consider the case of n = 3, so that eq. (A.l2) reduces to 2 G(v) = b2v + b1v + bo . (A.17) v 3 +a2v2 +a1v+ao As a first step, we factor the denominator of G( v) in order to write it in the form 2 G(v) = b2v + b1v + bo (A.l8) (v - PI )(v - P2)(v - P3) Assuming for the moment that the roots p 1, p2, and p3 of the denominator are all distinct, we would like to expand G(v ) into a sum of the form G(v) = -A1- + -A2- + -A-3 . (A.l9) v - p 1 v - P2 v - P3 The problem, then, is to determine the constants A 1, A2, and A3 . One approach is to equate eqs. (A.l8) and (A.19) and to multiply through the denominator. In this case, we obtain the equation b2v2 + b1v + bo = A1(v- P2)(v- P3) + A2(v - PI )(v - P3) (A.20) + A3(v - PI )(v - P2). By expanding the right-hand side of eq. (A.20) and then equating coefficients of equal powers of v, we obtain a set of linear equations that can be solved for A 1, A2, and A 3. Although this approach always works, there is a much easier method. Consider eq. (A.l9), and suppose that we would like to calculate A1• Then, multiplying through by v - p 1, we obtain (v- PI)G(v) = AI + A2(v- pJ) + A3(v- pi). (A.21) v- P2 v- P3 Since p1, p2, and p3 are distinct, the last two terms on the right-hand side of eq. (A.21) are zero for v = p 1• Therefore, A1 = [(v- PI)G(v)]Jv=p"" (A.22) or, using eq. (A.l8), b2pf + b1P1 + bo (A.23) (PI - P2)(PI - P3). Appendix 913 Similarly, b2p~ + htP2 + bo A2 = [(v- P2)G(v)Jiv=p2 = ( )( )' (A.24) P2- Pt P2- P3 b2p~ + htP3 + bo A3 = [(v- P3)G(v)Jiv=p = ( )( ) (A.25) 3 P3- Pt P3- P2 Suppose now that Pt = P3 ¥= p2; that is, 2 G(v) = b2v + btv + bo . (A.26) (v- pt)2(v- P2) In this case, we look for an expansion of the form G(v) = ~ + A 12 + ~. (A.27) v-pl (v-pt)2 v-p2 Here, we need the ll(v- p1)2 term in order to obtain the correct denominator in eq. (A.26) when we collect terms over a least common denominator. We also need to include the ll(v- p1) term in general. To see why this is so, consider equating eqs. (A.26) and (A.27) and multiplying them througH by the denominator of eq. (A.26): b2v2 + btv + bo = A11(v- pt)(v- P2) (A.28) + A12(v- P2) + A21(v- Pt)2. Again, if we equate coefficients of equal powers of v, we obtain three equations (for the coefficients of the v 0 , v 1, and v 2 terms). If we omit the A11 term in eq. (A.27), we will then have three equations in two unknowns, which in general will not have a solution. By including this term, we can always find a solution. In this case also, however, there is a much simpler method. Consider eq. (A.27) and multiply through by (v- p 1)2: 2 2 (v - Pt) G(v) = All (v - Pt) + A12 + A21 (v- Pt) (A.29) v- P2 From the preceding example, we see immediately how to determine A12 : A 12 = [( _ Pl )2G( )JI = b2PT + htPt + bo V V v=p1 • (A.30) Pt- P2 As for All, suppose that we differentiate eq. (A.29) with respect to v: d 2 [2(v- Pt) (v- Pt)2] -d [(v - pt) G(v)] = All + A21 - ( ) . (A.31) v v- P2 v- P2 2 It is then apparent that the final term in eq. (A.31) is zero for v = p1, and therefore, Au [:)v- Pt)2= G(v)Jiv~p, (A.32) 2b2Pt + bt b2PT + bt Pt + bo Pt - P2 (Pt - P2)2 914 Appendix Finally, by multiplying eq. (A.27) by v - p2, we find that A = [( _ )G( )JI = b2p~ + b1 P2 + bo 21 v P2 v v = P2 ( )2 (A.33) P2- PI This example illustrates all of the basic ideas behind partial-fraction expansion in the general case. Specifically, suppose that the denominator of G(v) in eq. (A.12) has distinct roots p 1, ••• , p7 with multiplicities a 1, ••• , a 7 ; that is, G(v) = bn-JVn-l + ... + blv + bo (A.34) (v- pJ)<T 1 (v- P2)cT2 ••• (v- Pr)<Tr In this case, G(v) has a partial-fraction expansion of the form v _- -A-11 G( ) - + A12 2 + ... + __A1<_ T__1_ ;___ v- PI (v- pi) (v- PI)cTi + ~ + ... + A2<T2 v - P2 (v - P2)<T2 (A.35) + ... +-Ar-! + ... + ... +( Am, V - p r V - Pr )CT r r <T; A;k = L,L, <v _ p·)k' i= I k= I I where the A; k are computed from the equation 1 A;k = (<T;I- k)! [ ddv""""; ;--""k [(v- p;)< T ;G(v)] ll v=p; (A.36) This result can be checked much as in the example: Multiply both sides of eq. (A.35) by (v- p;)<T; and differentiate repeatedly, until A;k is no longer multiplied by a power of v- p;. Then set v = p;. Example A.1 In Example 4.25, we examine an LTI system described by the differential equation d2y(t) + 4 dy(t) + 3y(t) = dx(t) 2 () (A.37) dt2 dt ----;[{ + X t . The frequency response of this system is . jw + 2 H(;w) = ( . )2 + 4 . (A.38) )W )W + 3. To determine the impulse response for this system, we expand H(jw) into a sum of simpler terms whose inverse transforms can be obtained by inspection. Making the subsitution of v for jw, we obtain the function 1H ere, we use the factorial notation r! for the product r(r- I )(r- 2) ... 2 · I. The quantity 0! is defined to be equal to I. Appendix 915 v+2 v+2 G(v) = v 2 + 4 (A.39) v + 3 (v + 1)(v + 3)"" The partial-fraction expansion for G(v) is then _ Au A21 G(v ) - --+-- (A.40) v+1 v+3' where -1 + 2 1 Au = [(v + 1)G(v)] lv=-1 = _ + = 2' (A.41) 1 3 A21 = [(v + 3)G(v)] lv= -3 + 2 1 -3 = _ + = 2· (A.42) 3 1 Thus, I I H(jw) = jw 2+ 1 + jw 2+ 3' (A.43) and the impulse response of the system, obtained by inverting eq. (A.43), is (A.44) The system described by eq. (A.37) can also be analyzed using the techniques of Laplace transform analysis, as developed in Chapter 9. The system function for this system is s+2 H(s) = s (A.45) 2 + 4s + 3' and if we substitute v for s, we obtain the same G(v) given in eq. (A.39). Thus, the partial-fraction expansion proceeds exactly as in eqs. (A.40)-(A.42), with the result that I I H(s) = _L_ + _L_. (A.46) s+1 s+3 Inverting this transform, we again obtain the impulse response, as given in eq. (A.44). ExampleA.2 We now illustrate the method of partial-fraction expansion when there are repeated fac- tors in the denominator. In Example 4.26, we considered the response of the system described in eq. (A.37) when the input was x(t) = e-1 u(t). (A.47) From eq. 4.81, the Fourier transform of the output of the system is . jw +2 Y(jw) = (jw + 1)2(jw + 3)"" (A.48) Substituting v for jw, we obtain the rational function v+2 G(v) = (v + 1)2(v + 3) · (A.49) 916 Appendix The partial-fraction expansion for this function is _ A,, A12 A21 G(v ) --- (A. 50) v + + 1 (v + + 1)2 v + 3' where, from eq. (A.36), 1 d + 7 I 1 A,, = (Z _ 1)! dv[(v 1tG(v)] v=-l = 4, (A.51) A 12 = [(v + 1)7- G(v)] I 1 v= -I = 2' (A.52) 1 A12 = [(v + 3)G(v)] lv= -3 = - 4· (A.53) Therefore, I I I 4 Y(jw) = jw ~+ 1 + (jw ~ 1)2 (A.54) jw + 3' and taking inverse transforms, we get (A.55) Again, this analysis could also have been performed using Laplace transforms, and the algebra would be identical to that given in eqs. (A.49)-(A.55). A.3 PARTIAL-FRACTION EXPANSION AND DISCRETE-TIME SIGNALS AND SYSTEMS As mentioned previously, ih performing partial-fraction expansions for discrete-time Fourier transforms or for z-transforms, it is often more convenient to deal with a slightly different form for rational functions. Suppose, then, that we have a rational function in the form (A.56) This form for G(v) can be obtained from G(v) in eq. (A.l2) by dividing the numerator and denominator by a0 . With G(v) as in eq. (A.56), the corresponding factorization of the denominator is of the form G(v) = dn-tVn-l + ... + dtV +do (A.57) (1- P]lv)a1(1 _ P2lv)a2 ••• (1 _ p;lv)ar' and the form of the partial-fraction expansion that results is (A.58) Appendix 917 The B i k can be calculated in a manner similar to that used earlier: (A.59) As before, the validity of eq. (A.59) can be determined by multiplying both sides of eq. (A.58) by (1- pj 1vYJ"";, then differentiating repeatedly with respect to v, until Bik is no longer multiplied by a power of 1 - pj 1v, and finally, setting v = Pi· Example A.3 Consider the causal LTI system in Example 5.19 characterized by the difference equation 3 1 y[n] - 4 y[n- 1] + Sy [n - 2] = 2x[n]. (A.60) The frequency response of the system is (A.61) For discrete-time transforms such as this, it is most convenient to substitute v fore- jw. Making this substitution, we obtain the rational function 2 2 G(v) = -----;;----:-- (A.62) 1- lv + !v2 (1 - !v)(l - ~v) · 4 8 Using the partial-fraction expansion specified by eqs. (A.57)-(A.59), we obtain G(v) = ~ + ____!!]J__' (A.63) 1- !v 1- !v 2 4 [( 1- ~v )a(v)t 2 -- =4 (A.64) 1- _!_ ' 2 2 [(I- HG(v)t, 2 1-2 = - 2. (A.65) Thus, 2 (A.66) and taking the inverse transform of eq. (A.66), we obtain the unit impulse response: h[n] = 4 (~ )n u[n] - 2 (~ Ju[ n]. (A.67) In Section 10.7, we developed the tools of z-transform analysis for the exami- nation of discrete-time LTI systems specified by linear constant-coefficient difference 918 Appendix equations. Applying those techniques to this example, we find that the system function can be determined by inspection from eq. (A.60) and is (A.68) Then, substituting v for z- 1, we obtain G(v) as in eq. (A.62). Thus, using the partial- fraction expansion calculations in eqs. (A.63)-(A.65), we find that 4 2 H(z) = 1- !z-1 (A.69) 2 which, when inverted, again yields the unit impulse response of eq. (A.67). ExampleA.4 Suppose that the input to the system considered in Example A.3 is x[n] ~ (~ )"" u[n]. (A.70) Then from Example 5.20, the Fourier transform of the output is (A.71) Substituting v fore- Jw yields 2 G(v) = -------=--------=-- (A.72) (1 - ~v)(l - ±v)2 • Thus, using eqs. (A.58) and (A.59), we obtain the partial-fraction expansion B11 B12 B21 G(v ) = 1 - ! v + ( 1 - ! v)2 + 1 - ! v (A.73) 4 4 2 and find B 11 ~ (-4) [:v (1- ~v) G(v)L -4, (A.74) 4 B12 ~ [(1- ~v )' G(v)t -2, (A.75) 4 821 [(1- ~v )c(v)L ~ 8. (A.76) 2 Therefore, (A.77) Appendix 919 which can be inverted by inspection as follows, using the Fourier transform pairs in Table 4.2: 11 11 11 y[n] = {- 4 (41 ) - 2(n + 1) (14) + 8 (1.2 ) } u[n]. (A.78) Example A.S Improper rational functions are often encountered in the analysis of discrete-time sys- tems. To illustrate this, and also to show how they can be analyzed using the techniques developed here, consider the causal LTI system characterized by the difference equation 5 1 11 1 y[n] + 6y[n- 1] + 6y[n- 2] = x[n] + 3x[n- 1] + 6 x[n- 2] + 3x[n- 3]. The frequency response of this system is . 1 + 3e-Jw + l..!.e- j2w + !e-J3w H(elw) = 6 3 (A.79) 1 + 1e-Jw + !e- j2w 6 6 Substituting v fore- Jw, we obtain (A. SO) This rational function can be written as the sum of a polynomial and a proper rational function: (A.81) Equating eqs. (A. SO) and (A.81 ), and multiplying by 1 + ~v + iv2 , we obtain 11 1 + 3v + v2 6 + ~v3 =(co+ bo) +(~co+ c1 + b1)v (A.82) Equating coefficients, we see that 1 5 11 -Co+ -C] 6 ~co= 1, 6 6 (A.83) 5 1 6co + C] + b1 = 3 ~ b1 6, co + bo = 1 ~ bo = 0. Thus, (A.84) 920 Appendix Also, we can use the method developed here to expand the proper rational function in eq. (A.81): --=B-1-1 ----+ B21 . (A.85) (1 + }v)(l + 4v) (1 + }v) (1 + .!.v) The coefficients are B11 = (__l;-)1 = 1, 1 + 2v v= -3 _f._!.v1 _ )I = -1 ( 1 + }v v= -2 . Therefore, we find that (A.86) and by inspection, we can determine the impulse response of this system: h [ n] = 8 [ n] + 28 [ n - 1] + [ (- ~ J- (- ~ J]u [ n]. (A.87) BIBLIOGRAPHY The purpose of this bibliography is to provide the reader with sources for additional and more advanced treatments of topics in signal and system analysis. This is by no means meant to be an exhaustive list but rather it is intended to indicate directions for further study and several references for each. We have divided the bibliography into sixteen different subject areas. The first few deal with the mathematical techniques of signal and system analysis including texts on background mathematics (calculus, differential and difference equations, and complex variables), the theory of Fourier series and of Fourier, Laplace, and z-transforms, and addi- tional topics in mathematics that are commonly encountered and used in signal and system analysis. Several of the sections that follow deal with more thorough and specialized treat- ments of topics in signals and systems introduced in this text, including filtering, sampling and discrete-time signal processing, communications, and feedback and control. We have also provided a list of other basic books on signals and systems as well as several texts on circuit theory. In addition, we have provided lists of references on several topics that rep- resent important subjects for more advanced study for those interested either in expanding their know ledge of the methods of signals and systems or in exploring applications that make use of these advanced techniques. In particular we include sections on state space models and methods, multidimensional signal and image processing, speech processing, multirate and multiresolution signal analysis, random signals and statistical signal pro- cessing, and nonlinear systems. Finally we have included a list of references dealing with a sampling of other applications and advanced topics. Together, the references collected in this bibliography should provide the reader with an appreciation for the breadth of topics and applications that comprise the field of signals and systems. B. 1 BACKGROUND AND BASIC MATHEMATICS B. 1 . 1 Calculus, Analysis, and Advanced Mathematics ARFKEN, G., and WEBER, H. J., Mathematical Methods for Physicists. 4th ed. Boston, MA: Academic Press, 1995. HILDEBRAND, F. B., Advanced Calculus for Applications. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1976. THoMAS, G. B., Jr., and FINNEY, R. L., Calculus and Analytic Geometry. 9th ed. Reading, MA: Addison-Wesley, 1996. B.1.2 Differential and Difference Equations BIRKHOFF, G., and RoTA, G.-C., Ordinary Differential Equations. 3rd ed. New York, NY: John Wiley, 1978. BoYcE, W. E., and DIPRIMA, R. C., Elementary Differential Equations. 3rd ed. New York, NY: John Wiley, 1977. 921"
Bibliography,"BIBLIOGRAPHY The purpose of this bibliography is to provide the reader with sources for additional and more advanced treatments of topics in signal and system analysis. This is by no means meant to be an exhaustive list but rather it is intended to indicate directions for further study and several references for each. We have divided the bibliography into sixteen different subject areas. The first few deal with the mathematical techniques of signal and system analysis including texts on background mathematics (calculus, differential and difference equations, and complex variables), the theory of Fourier series and of Fourier, Laplace, and z-transforms, and addi- tional topics in mathematics that are commonly encountered and used in signal and system analysis. Several of the sections that follow deal with more thorough and specialized treat- ments of topics in signals and systems introduced in this text, including filtering, sampling and discrete-time signal processing, communications, and feedback and control. We have also provided a list of other basic books on signals and systems as well as several texts on circuit theory. In addition, we have provided lists of references on several topics that rep- resent important subjects for more advanced study for those interested either in expanding their know ledge of the methods of signals and systems or in exploring applications that make use of these advanced techniques. In particular we include sections on state space models and methods, multidimensional signal and image processing, speech processing, multirate and multiresolution signal analysis, random signals and statistical signal pro- cessing, and nonlinear systems. Finally we have included a list of references dealing with a sampling of other applications and advanced topics. Together, the references collected in this bibliography should provide the reader with an appreciation for the breadth of topics and applications that comprise the field of signals and systems. B. 1 BACKGROUND AND BASIC MATHEMATICS B. 1 . 1 Calculus, Analysis, and Advanced Mathematics ARFKEN, G., and WEBER, H. J., Mathematical Methods for Physicists. 4th ed. Boston, MA: Academic Press, 1995. HILDEBRAND, F. B., Advanced Calculus for Applications. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1976. THoMAS, G. B., Jr., and FINNEY, R. L., Calculus and Analytic Geometry. 9th ed. Reading, MA: Addison-Wesley, 1996. B.1.2 Differential and Difference Equations BIRKHOFF, G., and RoTA, G.-C., Ordinary Differential Equations. 3rd ed. New York, NY: John Wiley, 1978. BoYcE, W. E., and DIPRIMA, R. C., Elementary Differential Equations. 3rd ed. New York, NY: John Wiley, 1977. 921 922 Bibliography HILDEBRAND, F. B., Finite Difference Equations and Simulations. Englewood Cliffs, NJ: Prentice Hall, 1968. LEvY, H., and LESSMAN, F., Finite Difference Equations. New York, NY: Macmillan, 1961. SIMMONS, G. F., Differential Equations: With Applications and Historical Notes. New York, NY: McGraw-Hill, 1972. B. 1 .3 Complex Variables CARRIER, G. F., KROOK, M., and PEARSON, C. E., Functions of a Complex Variable: Theory and Technique. Ithaca, NY: Hod Books, 1983. CHURCHILL, R. V., BROWN, J. W., and VERHEY, R. F., Complex Variables and Applications. 5th ed. New York, NY: McGraw-Hill, 1990. 8.2 SERIES EXPANSIONS AND TRANSFORMS 8.2.1 Fourier Series, Transforms, and Applications BRACEWELL, R.N., The Fourier Transform and Its Applications. 2nd ed. New York, NY: McGraw- Hill, 1986. CHuRCHILL, R. V., and BRowN, J. W., Fourier Series and Boundary Value Problems. 3rd ed. New York, NY: McGraw-Hill, 1978. DYM, H., and McKEAN, H. P., Fourier Series and Integrals. New York, NY: Academic Press, 1972. EDWARDS, R. E., Fourier Series: A Modern Introduction. 2nd ed. New York, NY: Springer-Verlag, 1979. GRAY, R. M., and GooDMAN, J. W., Fourier Transforms: An Introduction for Engineers. Boston, MA: Kluwer Academic Publishers, 1995. LIGHTHILL, M. J., Introduction to Fourier Analysis and Generalized Functions. New York, NY: Cam- bridge University Press, 1962. PAPOULIS, A., The Fourier Integral and Its Applications. New York, NY: McGraw-Hill, 1987. WALKER, P. L., The Theory of Fourier Series and Integrals. New York, NY: John Wiley, 1986. 8.2.2 Laplace Transforms DoETSCH, G., Introduction to the Theory and Applications of the Laplace Transformation with a Table of Laplace Transformations. New York, NY: Springer Verlag, 1974. LEPAGE, W. R., Complex Variables and the Laplace Transform for Engineers. New York, NY: McGraw-Hill, 1961. RAINVILLE, E. D., The Laplace Transform: An Introduction. New York, NY: Macmillan, 1963. 8.2.3 z-Transforms JuRY, E. 1., Theory and Application of the Z-Transform Method. Malabar, FL: R. E. Krieger, 1982. VIcH, R., Z Transform Theory and Applications. Boston, MA: D. Reidel, 1987. Bibliography 923 8.3 ADDITIONAL TOPICS IN MATHEMATICS 8.3.1 Generalized Functions ARsAc, J., Fourier Transforms and the Theory of Distributions. Translated by A. Nussbaum and G. C. Heim. Englewood Cliffs, NJ: Prentice Hall, 1966. GELFAND, I. M. et al., Generalized Functions. 5 vols. Translated by E. Saletan et al. New York, NY: Academic Press, 1964-68. HosKINS, R. F., Generalised Functions. New York, NY: Halsted Press, 1979. ZEMANIAN, A. H., Distribution Theory and Transform Analysis. New York, NY: McGraw-Hill, 1965. 8.3.2 Linear Algebra GoLUB, G. H., and VAN LoAN, C. F., Matrix Computations. 2nd ed. Baltimore: The Johns Hopkins University Press, 1989. HoRN, R. A., and JoHNSON, C. R., Matrix Analysis. New York, NY: Cambridge University Press, 1985. STRANG, G., Introduction to Linear Algebra. Wellesley, MA: Wellesley-Cambridge Press, 1993. 8.4 CIRCUIT THEORY BoBROW, L. S., Elementary Linear Circuit Analysis. New York, NY: Holt, Rinehart, and Winston, 1981. CHuA, L. 0., DESOER, C. A., and KuH, E. S., Basic Circuit Theory. New York: McGraw-Hill, 1987. IRviNE, R. G., Operational Amplifier Characteristics and Applications. Englewood Cliffs, NJ: Pren- tice Hall, 1994. RoBERGE, J. K., Operational Amplifiers: Theory and Practice. New York, NY: John Wiley, 1975. VANVALKENBURG, M. E., Network Analysis. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1974. 8.5 BASIC SIGNALS AND SYSTEMS CADzow, J. A., and VAN LANDINGHAM, H. F., Signals and Systems. Englewood Cliffs, NJ: Prentice Hall, 1985. CRuz, J. B., and VANVALKENBURG, M. E., Signals in Linear Circuits. Boston, MA: Houghton Mifflin, 1974. GABEL, R. A., and RoBERTS, R. A., Signals and Linear Systems. 3rd ed. New York, NY: John Wiley, 1987. GussoN, T. H., Introduction to System Analysis. New York, NY: McGraw-Hill, 1985. HouTs, R. C., Signal Analysis in Linear Systems. New York, NY: Saunders College, 1991. JACKSON, L. B., Signals, Systems, and Transforms. Reading, MA: Addison-Wesley, 1991. KAMEN, E., Introduction to Signals and Systems. New York, NY: Macmillan, 1987. LATHI, B. P., Linear Systems and Signals. Carmichael, CA: Berkeley-Cambridge Press, 1992. LIU, C. L., and LIU, J. W., Linear Systems Analysis. New York: McGraw-Hill, 1975. 924 Bibliography MAYHAN, R. J ., Discrete-time and Continuous-time Linear Systems. Reading, MA: Addison-Wesley, 1984. McGILLEM, C. D., and CooPER, G. R., Continuous and Discrete Signal and System Analysis. 3rd ed. New York, NY: Holt, Rinehart and Winston, 1991. NEFF, H. P., Continuous and Discrete Linear Systems. New York, NY: Harper and Row, 1984. PAPOULIS, A., Signal Analysis. New York, NY: McGraw-Hill, 1977. SIEBERT, W. M., Circuits, Signals, and Systems. Cambridge, MA: The MIT Press, 1986. SOLIMAN, S., and SRINATH, M., Continuous and Discrete Signals and Systems. New York, NY: Pren- tice Hall, 1990. TAYLOR, F. J., Principles of Signals and Systems. McGraw-Hill Series in Electrical and Computer Engineering. New York, NY: McGraw-Hill, 1994. ZIEMER, R. E., TRANTER, W. H., and FANNIN, D. R. Signals and Systems: Continuous and Discrete. 2nd ed. New York, NY: Macmillan, 1989. 8.6 DISCRETE-TIME SIGNAL PROCESSING BRIGHAM, 0. E., The Fast Fourier Transform and its Applications. Englewood Cliffs, NJ: Prentice Hall, 1988. BuRRus, C. S., McCLELLAN, J. H., OPPENHEIM, A. V., PARKS, T. W., ScHAFER, R. W., and ScHUESSLER, H. W. Computer-Based Exercises for Signal Processing Using MATIAB. Englewood Cliffs, NJ: Prentice Hall, Inc., 1994. GoLo, B., and RADER, C. M., Digital Processing of Signals. Lincoln Laboratory Publications. New York, NY: McGraw-Hill, 1969. OPPENHEIM, A. V., and ScHAFER, R. W., Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1975. . ' OPPENHEIM, A. V., and ScHAFER, R. W., Discrete-Time Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1989. · PELED, A., and Lm, B., Digital Signal Processing: Theory Design and Implementation. New York, NY: John Wiley, 1976. PROAKIS, J. G., and MANOLAKIS, D. G., Digital Signal Processing Principles, Algorithms, and Appli- cations. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1996. RABINER, L. R., and GoLD, B., Theory and Application of Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1975. RoBERTS, R. A., and MuLLIS, C. T., Digital Signal Processing. Reading, MA: Addison-Wesley, 1987. STRUM, R. D., and KIRK, D. E., First Principles of Discrete Systems and Digital Signal Processing. Addison-Wesley Series in Electrical Engineering. Reading, MA: Addison-Wesley, 1988. TRETTER, S. A., Introduction to Discrete-Time Signal Processing. New York, NY: John Wiley, 1976. B. 7 FILTER DESIGN ANTONIOU, A., Digital Filters, Analysis, Design, and Applications. 2nd ed. New York, NY: McGraw- Hill, 1993. CHRISTIAN, E., and EISENMANN, E., Filter Design Tables and Graphs. Knightdale, NC: Transmission Networks International, 1977. Bibliography 925 HAMMING, R. W., Digital Filters. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1989. HUELSMAN, L. P., and ALLEN, P. E., Introduction to the Theory and Design of Active Filters. New York, NY: McGraw-Hill, 1980. PARKS, T. W., and BuRRUS, C. S., Digital Filter Design. New York, NY: John Wiley, 1987. VAN V ALKENBURG, M. E., Analog Filter Design. New York, NY: Holt, Rinehart and Winston, 1982. WEINBERG, L., Network Analysis and Synthesis. New York, NY: McGraw-Hill, 1962. ZvEREV, A. 1., Handbook of Filter Synthesis. New York, NY: John Wiley, 1967. 8.8 STATE-SPACE MODELS AND METHODS BROCKETT, R., Finite Dimensional Linear Systems. New York, NY: John Wiley, 1970. CHEN, C. T., Linear System Theory and Design. New York, NY: Holt, Rinehart, and Winston, 1984. CLosE, C. M., and FREDERICK, D.K. Modeling and Analysis of Dynamic Systems. Boston, MA: Houghton Mifflin, 1978 GuPTA, S.C., Transform and State Variable Methods in Linear Systems. New York, NY: John Wiley, 1966. KAILATH, T., Linear Systems. Englewood Cliffs, NJ: Prentice Hall, 1980. LJUNG, L., System Identification: Theory for the User. Englewood Cliffs, NJ: Prentice Hall, 1987. LuENBERGER, D. G., Introduction to Dynamic Systems: Theory, Models, and Applications. New York, NY: John Wiley, 1979. ZADEH, L.A., and DESOER, C. A., Linear System Theory: The State Space Approach. New York, NY: McGraw-Hill, 1963. 8.9 FEEDBACK AND CONTROL ANDERSON, B. D. 0., and MooRE, J. B., Optimal Control: Linear Quadratic Methods. Englewood Cliffs, NJ: Prentice Hall, 1990. D' AZZO, J. J., and HouPIS, C. H., Linear Control System Analysis and Design: Conventional and Modern. 4th ed. NY: McGraw-Hill, 1995. DoRF, R. C., and BisHoP, R. H., Modern Control Systems. 7th ed. Reading, MA: Addison-Wesley Publishing Company, 1995. DoYLE, J. C., FRANCIS, B. A., and TANNENBAUM, A. R., Feedback Control Theory. New York, NY: Macmillan Publishing Company, 1992. HosTETTER, G. H., SAVANT, Jr., C. J., and STEFANI, R. T., Design of Feedback Control Systems. 2nd ed. Saunders College Publishing, a Division of Holt, Reinhart and Winston, Inc., 1989. Kuo, B. C., Automatic Control Systems. 7th ed. Englewood Cliffs, NJ: Prentice Hall, 1995. OGATA, K., Modern Control Engineering. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1990. OGATA, K., Discrete-Time Control Systems. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1994. RAGAZZINI, J. R., and FRANKLIN, G. F., Sampled-Data Control Systems. New York, NY: McGraw- Hill, 1958. RoHRS, C. E., MELSA, J. L., and ScHULTZ, D. G., Linear Control Systems. New York, NY: McGraw- Hill, 1993. VACCARO, R. J., Digital Control: A State-Space Approach. New York, NY: McGraw Hill, 1995. 926 Bibliography B. 1 0 COMMUNICATIONS BENNETT, W. R., Introduction to Signal Transmission. New York, NY: McGraw-Hill, 1970. BLAHUT, R. E., Digital Transmission of Information. Reading, MA: Addison-Wesley Publishing Company, 1990. BLAHUT, R. E., Algebraic Methods for Signal Processing and Communications Coding. New York, NY: Springer-Verlag, 1992. CARLSON, A. B., Communication Systems: An Introduction to Signals and Noise in Electrical Com- munication. 3rd ed. New York, NY: McGraw-Hill, 1986. CoucH, II, L. W., Modern Communication Systems Principles and Applications. Upper Saddle River, NJ: Prentice Hall, Inc., 1995. CovER, T. M., and THOMAS, J. B., Elements of Information Theory. New York, NY: John Wiley and Sons, Inc., 1991. GALLAGER, R. M., Information Theory and Reliable Communication. New York, NY: John Wiley and Sons, Inc., 1968. HAYKIN, S., Digital Communications. New York, NY: John Wiley & Sons, 1988. JAYANT, N. S., and NoLL, P., Digital Coding of Waveforms: Principles and Applications to Speech and Video. Englewood Cliffs, NJ: Prentice Hall, Inc., 1984. LATHI, B. P., Modern Digital and Analog Communication Systems. 2nd ed. New York, NY: Holt, Rinehart and Winston, Inc., 1989. LEE, E. A., and MESSERSCHMITT, D. G., Digital Communication. 2nd ed. Boston, MA: Kluwer Aca- demic Publishers, 1994. PEEBLES, JR., P. Z., Communication System Principles. Reading, MA: Addison-Wesley Publishing Company, 1976. PROAKIS, J. G., Digital Communications. 3rd ed. New York, NY: McGraw-Hill, 1995. PROAKIS, J. G. and SALEHI, M., Communication Systems Engineering. Englewood Cliffs, NJ: Prentice Hall, 1994. RoDEN, M.S., Analog and Digital Communication Systems. 4th ed. Upper Saddle River, NJ: Pren- tice Hall, Inc., 1996. ScHWARTZ, M., Information Transmission, Modulation, and Noise. 4th ed. New York, NY: McGraw- Hill, 1990. SIMON, M. K., et al., eds., Spread Spectrum Communication Handbook. Rev. ed., New York, NY: McGraw-Hill, 1994. STREMLER, F. G., Introduction to Communication Systems. 3rd ed. Addison-Wesley Series in Elec- trical Engineering, Reading, MA: Addison-Wesley, 1990. TAUB, H., and ScHILLING, D. L., Principles of Communication Systems. 2nd ed. New York, NY: McGraw-Hill, 1986. VITERBI, A. J., and OMURA, J. K., Principles of Digital Communication and Coding. New York, NY: McGraw-Hill, 1979. ZIEMER, R. E. and TRANTER, W. H., Principles of Communications Systems, Modulation, and Noise. 4th ed. Boston, MA: Houghton Mifflin Co., 1995. B. 11 MULTI-DIMENSIONAL SIGNAL, IMAGE, AND VIDEO PROCESSING BRACEWELL, R.N., Two-Dimensional Imaging. Englewood Cliffs, NJ: Prentice Hall, Inc., 1995. CASTLEMAN, K. R., Digital Image Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1996. Bibliography 927 DuDGEON, D. E., MERSEREAU, R. M., Multidimensional Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1984. GoNZALEZ, R. C., and WooDs, R. E., Digital Image Processing. Reading, MA: Addison-Wesley, 1993. JAIN, A. K., Fundamentals of Digital Image Processing. Englewood Cliffs, NJ: Prentice Hall, 1989. LIM, J. S., Two-Dimensional Signal and Image Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1990. NETRAVALI, A. N., and HASKELL, B. G., Digital Pictures: Representation, Compression, and Stan- dards. 2nd ed. New York, NY: Plenum Press, 1995. PRATT, W. K., Digital Image Processing. 2nd ed. New York, NY: John Wiley and Sons, 1991. TEKALP, A.M., Digital Video Processing. Upper Saddle River, NJ: Prentice Hall, Inc., 1995. B. 1 2 SPEECH PROCESSING DELLER, J. R., PROAKIS, J. G., and HANSEN, J. H. L., Discrete-Time Processing of Speech Signals. Upper Saddle River, NJ: Prentice Hall, 1987. KLEIJN, W. B., and P., K. K., Speech Coding and Synthesis. Amsterdam: Elsevier, 1995. LIM, J. S., ed., Speech Enhancement. Englewood Cliffs, NJ: Prentice Hall, 1983. MARKEL, J.D., and GRAY, A. H., Linear Prediction of Speech. New York, NY: Springer-Verlag, 1976. RABINER, L. R., and JuANG, B.-H., Fundamentals of Speech Recognition. Englewood Cliffs, NJ: Prentice Hall, 1993. RABINER, L. R., and ScHAFER, R. W., Digital Processing of Speech Signals. Englewood Cliffs, NJ: Prentice Hall, 1978. B. 1 3 MULTI RATE AND MULTI RESOLUTION SIGNAL ANALYSIS AKANSU, A. N., and HADDAD, R. A., Multiresolution Signal Decomposition: Transforms, Subbands and Wavelets. San Diego, CA: Academic Press, Inc., 1992. CHui, C. K., An Introduction to Wavelets. San Diego, CA: Academic Press Inc., 1992. CROCHIERE, R. E., and RABINER, L. R., Multirate Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1983. DAUBECHIES, I., Ten Lectures on Wavelets. CBMS-NSF Series on Applied Mathematics, Philadel- phia: SIAM, 1992. MALVAR, H. S., Signal Processing with Lapped Transforms. Norwood, MA: Artech House, 1992. VAIDYANATHAN, P. P., Multirate Systems and Filter Banks. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. VETTERLI, M., and KovACEVIC, J., Wavelets and Subband Coding. Englewood Cliffs, NJ: Prentice Hall, Inc., 1995. WoRNELL, G. W., Signal Processing with Fractals: A Wavelet-Based Approach. Upper Saddle River, NJ: Prentice Hall, Inc., 1996. B. 14 RANDOM SIGNALS AND STATISTICAL SIGNAL PROCESSING B.14. 1 Basic Probability DRAKE, A. W., Fundamentals ofA pplied Probability Theory. New York, NY: McGraw Hill, 1967. Ross, S., Introduction to Probability Models. 5th ed. Boston, MA: Academic Press, 1993. 928 Bibliography B.14.2 Stochastic Processes, Detection and Estimation KAY, S.M., Fundamentals of Statistical Signal Processing: Estimation Theory. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. LEoN-GARCIA, A., Probability and Random Processes for Electrical Engineering. 2nd ed. Reading, MA: Addison-Wesley Publishing Co., 1994. PAPouus, A., Probability, Random Variables, and Stochastic Processes. 3rd ed. New York, NY: McGraw-Hill, 1991. PEEBLES, JR., P. Z., Probability, Random Variables, and Random Signal Principles. 3rd ed. New York, NY: McGraw-Hill, 1993. PoRAT, B., Digital Processing of Random Signals: Theory and Methods. Englewood Cliffs, NJ: Prentice Hall, Inc., 1994. THERRIEN, C. W., Discrete Random Signals and Statistical Signal Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1992. VAN TREES, H. L., Detection, Estimation, and Modulation Theory: Part I. New York, NY: John Wiley and Sons, Inc., 1968. B. 1 5 NONLINEAR AND TIME-VARYING SYSTEMS CHuA, L. 0., Introduction to Nonlinear Network Theory. New York, NY: McGraw-Hill, 1969. D'ANGELO, H., Linear Time- Varying Systems: Analysis and Synthesis. Boston, MA: Allyn and Ba- con, 1970. GRAHAM, D., and McRuER, D., Analysis ofN onlinear Control Systems. New York, NY: Dover, 1971. HILLBORN, R. C., Chaos and Nonlinear Dynamics: An Introduction for Scientists and Engineers. New York, NY: Oxford University Press, 1994. KHALIL, H. K., Nonlinear Systems. New York, NY: Macmillan Publishing Company, 1992. LEFSCHETZ, S., Stability of Nonlinear Control Systems. Mathematics in Science and Engineering, no. 13. New York, NY: Academic Press, 1965. RICHARDS, J. A., Analysis of Periodically Time-Varying Systems. New York, NY: Springer-Verlag, 1983. STROGATZ, S. S., Nonlinear Dynamics and Chaos. Reading, MA: Addison-Wesley Publishing Com- pany, 1994. VIDYASAGER, M., Nonlinear Systems Analysis. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1993. B. 16 OTHER APPLICATIONS AND ADVANCED TOPICS Box, G. E. P., and JENKINS, G. M., Time Series Analysis: Forecasting and Control. Rev. ed. San Francisco, CA: Holden-Day, 1976. HAMILTON, J.D., Time Series Analysis. Princeton, NJ: Princeton University Press, 1994. HAYKIN, S., Adaptive Filter Theory. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1991. HERMAN, G. T., Image Reconstruction from Projections. New York, NY: Academic Press, 1980. JoHNSON, D. H. and DuDGEON, D. E., Array Signal Processing: Concepts and Techniques. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. KAK, A. C., and SLANEY, M., Principles of Computerized Tomography. Englewood Cliffs, NJ: Pren- tice Hall, 1989. Bibliography 929 KAY, S.M., Modern Spectral Estimation: Theory and Application. Englewood Cliffs, NJ: Prentice Hall, 1988. MAcovsKI, A., Medical Imaging Systems. Englewood Cliffs, NJ: Prentice Hall, 1983. MARPLE, JR., S. L., Digital Spectra/Analysis with Applications. Englewood ~liffs, NJ: Prentice Hall, 1987. OPPENHEIM, A. V., ed., Applications of Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1978. RoBINSON, E. A., et al., Geophysical Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1986. VAN TREES, H. L., Detection, Estimation, and Modulation Theory, Part II[: Radar-Sonar Signal Processing and Gaussian Signals in Noise. New York, NY: John Wiley, 1971. WmRow, B., and STEARNS, S.D., Adaptive Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1985. ANswERS 931"
Answers,"ANswERS 931 932 Answers Chapter 1 Answers 1.1. -0.5, -0.5, j, - j, j, 1 + j, 1 + j, 1 - j, 1 - j 1.2. 5eiO, 2eirr, 3e- jTTI2, e- jTTI3, J2eiTTI4, 2e- jTT12, J2eiTT14, eiTT12, e- jTT/12. 1.3. (a) Poo = 0, Eoo = ~ (b) Poo = 1, Eoo = 00 (c) Poo = ~, Eoo = oo (d) Poo = 0, Eoo = 1 (e) Poo = 1, Eoo = 00 (f) Poo = ~' Eoo = 00 1.4. (a) n < 1 and n > 7 (b) n < -6 and n > 0 (c) n < -4 and n > 2 (d) n < -2 and n > 4 (e) n < -6 and n > 0 1.5. (a) t > -2 (b) t > -1 (c) t > -2 (d) t < 1 (e) t < 9 1.6. (a) No (b) No (c) Yes 1.7. (a) lnl > 3 (b) all t (c) lnl < 3, lnl ~ 00 (d) ltl ~ 00 1.8. (a) A = 2, a = 0, w = 0, <f> = 7T (b) A = 1, a = 0, w = 3, <f> = 0 (c) A = 1, a = 1, w = 3, <f> = ¥ (d) A = 1, a = 2, w = 100, </> = ¥ 1.9. (a) T = ~ (b) Not periodic (c) N = 2 (d) N = 10 (e) Not periodic 1.10. 7T 1.11. 35 1.12. M = -1, no = -3 1.13. 4 1.14. At = 3, t1 = 0, A2 = -3, t2 = 1 1.15. (a) y[n] = 2x[n- 2] + 5x[n- 3] + 2x[n- 4] (b) No 1.16. (a) No (b) 0 (c) No 1.17. (a) No; e.g.,y( -7T) = x(O) (b) Yes 1.18. (a) Yes (b) Yes (c) C :5 (2n0 + 1)B 1.19. (a) Linear, not time invariant (b) Not linear, time invariant (c) Linear, tim~ invariant (d) Linear, not time invariant 1.20. (a) cos(3t) (b) cos(3t- 1) Chapter 2 Answers 2.1. (a) Yt [n] = 25[n + 1] + 45[n] + 25[n- 1] + 25[n- 2] - 25[n- 4] (b) Y2[n] = Yt[n + 2] (c) y3[n] = y2[n] 2.2. A = n - 9, B = n + 3 tn+l 2.3. 2[1 - 2 ]u[n] n - 6, 7 :5 n :5 11 2 4 [ ] l6, 12 :5 n :5 18 • • y n = 24 - n, 19 :5 n :5 23 0, otherwise Answers 933 2.5. N = 4 0 2.6. y[n] = { --~;, n < 2 , n 2: 0 2.7. (a) u[n- 2] - u[n- 6] (b) u[n- 4] - u[n- 8] (c) No (d) y[n] = 2u[n] - o[n] - o[n - 1] t + 3, -2 < t ::; -1 2.8. y(t) = l~ ~ ~t, -1<t:sO O<t:s1 0, elsewhere 2.9. A = t - 5, Bl t~ t - 4 O:st:sa 2.10. (a) y(t) = a:st:s1 a ' (b) a = 1 1 +a-t, I:st:s1+a 0, otherwise -00 < t ::; 3 2.11. (a) y(t) = { ~'-e;''--''' (l-e-6)e-3(r-5) , 5<t:Soo 3 (b) g(t) = e-3U-3)u(t- 3) - e-3(t-S)u(t- 5) (c) g(t) = dy(t) dt 2.12. A = 1 _~- 3 2.13. (a) A = ~ (b) g[n] = o[n] - ~o[n - 1] 2.14. hi (t), h2(t) 2.15. h2[n] 2.16. (a) True (b) False (c) True (d) True 2.17. (a) y(t) = 1 ~i[eC-I+ 3J)t- e-4t]u(t) (b) y(t) = i[e-t(cos3t+sin3t)-e-4t]u(t) 2.18. (114)n-l u[n- 1] 2.19. (a) a = ~. f3 = 1 (b) [2(~)n- (~)n]u[n] 2.20. (a) 1 (b) 0 (c) 0 Chapter 3 Answers 3.1. x(t) = 4 cos( it)+ 8 cose; t + 1) 3.2. x[n] = 1 + 2sin( 4 3 ; n + ;) + 4sin( 8 ; n + 5 ;) 3.3. wo = ~, ao = 2, a2 = a-2 = ~'as = a:__5 = -2} 0, k = 0 3.4. ak = { e-Jk1r/23siz~~)' k -::rf 0 934 Answers 3.5. w2 = w1, bk = e-Jkw 1 [a-k +ad 3.6. (a) X2(t), x3(t) (b) X2(t) j, k = 0 3.7. ak = { ~ k =.1= 0 Jo/-k' 3.8. x 1( t) = J2 sin( 7Tt), x2(t) = - J2 sin( 7Tt) 3.9. ao = 3, a1 = 1 - 2j, a2 = -1, a3 = 1 + 2j 3.10. ao = 0, a-1 = - j, a-2 = -2j, a-3 = -3j 3.11. A = 10, B = ~' C = 0 3.12. ck = 6 for all k 3.13. y(t) = 0 3.14. H(eirr/2) = H*(ei3rr12 ) = 2eirr14, H(ei0 ) = H(eirr) = 0 3.15. lkl > 8 3.16. (a) 0 (b) sine; n + *) (c) 0 3.17. S1 and S3 are not LTI. 3.18. S1 and S2 are not LTI. 3.19. (a) d~;t) + y(t) = x(t) (b) H(jw) = ( 1+1 Jw) (c) y(t) = h cos(t- *) 3.20. (a) dlr;t) + d~;t) + y(t) = x(t) (b) H(jw) = ( 1+ J~-w2 ) (c) -cost Chapter 4 Answers e- jw (b) 4e- jw 4.1. (a) 2+ jw 4+w2 4.2. (a) 2 cos w (b) -2j sin 2w 4.3. (a) ~ [eirr148(w - 27T) - e-Jrr148(w + 27T)] J (b) 27T8(w) + 1T[eirr!So(w - 61r) + e-Jrr188(w + 61r)] 4.4. (a) 1 + cos 41Tt (b) TTl 4 •5 • x(t) = - 2sin(3 3 rr(t-(3t/-2)/ 2)) ' t = k3rr + ~2 for nonzero integers k 4.6. (a) X1 (jw) = 2X(- jw) cos w (b) X2(jw) = ~e- i 2w X(/~) (c) X3(jw) = -w2e-Jw X(jw) 4.7. (a) neither, neither (b) imaginary, odd (c) imaginary, neither (d) real, even 4.8. (a) 2 si~(~/2) +1r8(w) (b) 2 sin(w/2) JW ~ 4.9. (a) si_n~ _ e~jw (b) sinw (c) s~nw _ co_sw }W JW W )W2 )W j 121T, -2 ::; w < 0 4.10. (a) X(jw) = - j/21T, 0 :s w < 2 (b) A = { 2~3 0, otherwise Answers 935 4.11. A = j, B = 3 4•1 2• (a ) _ 4jw (b)-'2 -lwl (l+w2)2 J 1TWe 4.13. (a) No (b) Yes (c) Yes 4.14. x(t) = Ji2[e-r - e- 2t]u(t) 4.15. x(t) = 2te-ltlu(t) 4.16. = £ 4o: lwl :=:; 1 (a) g(t) 1T o(t - k:) (b) X(jw) = { 1 < lwl :=:; 4 k = -oc 4.17. (a) False (b) True i· ltl < 1 4.18. h(t) = l- ~ + ~' 1 :=:; ltl :=:; 5 -w + ~· s < ltl < 7 0, otherwise 4.19. x(t) = e-4r u(t) 4.20. h(t) = )3e-t12 sin( Jf t)u(t) Chapter 5 Answers 5.1. (a) (b) 0.75e-;w 1.25-cosw 5.2. (a) 2 cos w (b) 2j sin(2w) 5.3. (a) ]{e.i7T14o(w - ~)- e- .i7T14o(w + ~)} (b) 47TB(w) + 1r{e.i7T18o(w- ~) + e-.i7T18o(w + ~)} 2 5.4. (a) XJ[n] = 1 +cos(-~n) (b) -4sin (~n) 2 7Tn sin[ !!.(n- ""- )] 5.5. x[n] = · ; 2 1 , and x[n] = 0 for n = :±:oo 7T n- 2) 5.6. (a) X1 (e.iw) = (2 cos w )X(e- jw) (b) X2(ejw) = CRe{X(e.iw)} (c) X3(e.iw) = - c;~2 X(e.iw)- 2j d:X(e.iw) + X(e.iw) 5.7. (a) imaginary, neither (b) real, odd (c) real, neither 1, n :=:; -2 5.8. x[n] = n + 3, -1 :=:; n :=:; 1 { 4, n 2:: 2 5.9. x[n] = -o[n + 2] + o[n + 1] + o[n] 5.10. A = 2 5.11. a = 1T 5.12. ~ :=:; lwei :=:; 1T 5.13. h2[n] = -2(~)''u[n] 5.14. h[n] = ~o[n] - f?o[n- 2] 936 Answers 5.15. We = 31T/4 5.16. (a) a = ± (b) N = 4 (c) No 5.17. bk = 4( -1)k 5.18. ak = ~(4)1kl 5.19. (a) H(efw) = (1-~rjw)l(l+~e-jw) (b) h[n] = ~(4 )nu[n] + ~(- ~ )nu[n] 4 -jw 5.20. (a) H(efw) = ~ 1- SC JW (b) y[n]- ~y[n- 1] = ~x[n- 1] Chapter 6 Answers 6.1. (a) A = IH(jwo)l (b) to = - <rH~~wo) 6.2. -tH(eiwo) = -n0(w0 ) + 21Tk for some integer k. 6.3. (a) A= 1 (b) T(w) > 0 forw > 0 6.4. (a) 2 cos(~n- 1r) (b) 2 sinC; n- 3 ;) 6.5. (a) g(t) = 2cos(2wct) (b) more concentrated 6.6. (a) g[n] = ( -l)n (b) more concentrated 6.7. (a) 1,000 Hz and 3,000 Hz (b) 800Hz and 3,200 Hz 6.8. 1T - w p ~ w ~ 1T 6.9. Final value = 2/5, to = 2/5 sec 20 w << 0.1 6.10. (a) 20 log 10 IH(jw )I = ~0 1dg10(w ), 0.1 << w << 40 { 32, w >> 40 20, w << 0.2 (b) 20log10 IH(jw)l = -20log10(w) + 6, 0.2 << w <<50 { -28, w >>50 20, w << 0.5 6.11. (a) 20 log 10 IH(jw )I = -20 log10(w) + 14, 0.5 << w << 50 { -40log10(w) + 48, w >>50 0, w << 1 (b) 20 log10 IH(jw )I = -40 log 10 w, 1 << w << 50 { -20 log10 w - 34, w >> 50 6 12 . ) _ O.Ol(Jw+40) • • H 2 ( JW - (jw+ l)(jw+8) 6.13. (a) not unique (b) unique 6.14. H1(jw) = 0.2 x 1o-4 Ciwc:~ol~~); 10) Answers 937 6.15. (a) critically damped (b) underdamped (c) overdamped (d) underdamped 6.16. y[n] + 4y[n- 1] = ~x[n] 6.17. (a) oscillatory (b) nonoscillatory 6.18. No 6.19. R~ 2ft 6.20. T(w) = 2 Chapter 7 Answers 7 .1. lw I > 5,0007T 7.2. (a) and (c) 7.3. (a) 8,0007T (b) 8,0007T (c) 16,0007T 7.4. (a) wo (b) wo (c) 2wo (d) 3wo 7 •5 • IH( J.W )I -_ { T, lwhl ::; W. e , w h ere w0 < We < T21 r _ w0 , xH( J·W ) 2 2 'j... -_ 0 0 , ot erw1se 7 .6. T max = _w__!!_w_+ 1 2 7.7. H(jw) = 2sin~~T/2) X ej(wT/2) 7.8. (a) Yes { O, k=O 4 (b) g(t) = .L akeik1rt, where ak = - j(4)k+l, k= -4 '(!)-k+ 1 J -4 ::; k ::; -1 2 ' 7.9. wo = 507T 7.10. (a) False (b) True (c) True 7.11. (a) Xe(jw) is real (b) Max{Xe(jw )} = 0.5 X 10-3 (c) Xe(jw) = 0 for lwl ~ 1,5007T (d) Xe(jw) = Xe(j(4J - 2,0007T)) for 0 ::; w ::; 2,0007T 7.12. lwl ~ 7507T 7.13. h[n] = o[n- 2] 714 h[] = _sin[TT(n-~)] · • n T1r(n- ~ )2 7.15. N = 2 7.16. x[n] = 4Cin(Trn/2) )2 1Tn 7 .17. ldeallowpass filter with cutoff frequency 7T/2 and passband gain of unity 7 .18. Ideallowpass filter with cutoff frequency 7T/4 and passband gain of 2. 7.19. (a) y[n] = sin(~:~n/3) (b) y[n] = ~o[n] 7.20. (a) Yes (b) No 938 Answers Chapter 8 Answers 8.1. m(t) = ~e- Jw,t 8.2. (a) No constraint necessary (b) lwei > 1,0007T 8.3. y(t) = 0 8.4. y(t) = sin 2007Tt 8.5. m = 2~ 8.6. A = 4 8.7. wo = 2wc. A = 2 8.8. (a) Yes (b) Yes, x(t) = {y(t)sinwcf}* 2 si;~,t 8.9. (a) lwl > 2wc (b) wo = We, A = 2 8.10. (a) X(jw) = 0 for lwl ~ 1,0007T (b) We = 1,0007T, A = 4 8.11. (a) u;' lwl 3 :S :S ~', Gain = 1 (b) A = 2latl. 4> = 1::at 8.12. 8 = 0.5 X 10-4 8.13. (a) p(O) = * (b) p(kT1 ) = 0 8.14. Y(jw) = 7TO(w -We) - ~1T o(w -We - Wm) - l2n~ o(w - We + Wm) -J J 8.15. wo = 0 and wo = 7T 8.16. 0 w 3 5 :S :S ; and ; :S w :S 7T 8.17. 0 :S lwl :S I 8.18. H(eiw) = { ~ . ~ ~ ~ :S ~ O ], 4 - w 8.19. N = 20 X 8.20. p[n] = L o[n - 2k] k=-X Chapter 9 Answers 9.1. (a) u > -5 (b) u < -5 (c) -oo :S u :S oo (d) no value of u (e) lui < 5 (0 u < 5 9.2. (a) e:~~s), CR~{s} > -5 (b) A = -1, to = -1, CR~{s} < -5 9.3. CR~{/3} = 3, dm{f3} arbitrary 9.4. 1 + 2j, 1 - 2j, CR~{s} < 1 9.5. (a) 1,1 (b) 0,1 (c) 1,0 9.6. (a) no (b) yes (c) no (d) yes 9.7. 4 9.8. two sided 9.9. x(t) = 4e-41 u(t) - 2e- 31 u(t) Answers 939 9.10. (a) lowpass (b) bandpass (c) highpass 9.11. IX(jw )I = 1 9.12. (a) not consistent (b) consistent (c) consistent 9.13. a = -1, {3 = 4 9.14. X(s) = 11[4(s2 - )2 + ~)(s2 + )2 + ~)], - 11 < CR£{s} < 11 9.15. X(s) = s2·:4 ; CR£{s} > 0, Y(s) = s2: 4 ; CR£{s} > 0 9.16. (a) 2 (b) a > 0 917 2 d y(t) + 10dy(t) + 16y(t) = 12x(t) + 3dx(t) • • dr2 dt dt 9.18. (a) H(s) = s2 +~\·+ 1 ,CR£{s} > -4 (b) Lowpass (c) 1 H(s) = s2 + 10_3s+ 1, CR£{s} > -0.0005 (d) Bandpass 9.19. (a) s~ 2 , CR£{s} > -2 (b) 1 + .:~~' CR£{s} > -2 (c) s~4 + s~2' CR£{s} > -2 9.20. (a) e-t u(t) - e-2t u(t) (b) e-tu(t) (c) 2e-t u(t) - e-2' u(t) Chapter 10 Answers 10.1. (a) lzl > 4 (b) lzl < 4 (c) lzl > 1 (d) 4 < lzl < 2 X(z) 1 7- 3 1 10.2. = m 1 _~!~-J; lzl > 5 5 {, 10.3. lal = 2, no arbitrary 10.4. poles at z = ~e±j7TI4 , ROC: lzl < ~ 10.5. (a) 1,1 (b) 2,0 (c) 1,2 10.6. (a) No (b) No (c) Yes (d) Yes 10.7. 3 10.8. two sided 10.9. x[n] = ~u[n] + ~( -2)nu[n] 10.10. (a) x[O] = 1, x[1] = ~' x[2] = - ~ (b) x[O] = 3, x[ -1] = -6, x[ -2] = 18 11 10.11. x[n] = { <4) 0 , ::; n.::; 9 0, otherwise 10.12. (a) highpass (b) lowpass (c) bandpass 10.13. (a) G(z) = 1 - z-6 ; lzl > 0 (b) X(z) = :=~=~; lzl > 0 7 10.14. (a) n0 = 2 (b) G(z) = ( z;~~~~ )2 940 Answers 10.15. ( ~ )nu[n] and (- ~ )nu[n] 10.16. (a) Not causal (b) Causal (c) Not causal 10.17. (a) Yes (b) Yes 10.18. (a) y[n] - ~y[n- 1] + ~y[n- 2] = x[n] - 6x[n- 1] + 8x[n- 2] (b) Yes 10.19. (a) XI (z) = _ L-~, lzl > ~ 1 4 (b) X2(Z) = 2, All z (c) X3(z) = I-L-~, lzl > ~ 2 10.20. (a) -(- ~)nu[n] (b) ~(-~)nu[n] + ~(~)nu[n] (c) - ~(- ~)nu[n] + ~(~)nu[n] Chapter 11 Answers 11.1. Ho(z) + 1 +~~~~~ (z) 112 H1(s)H2(s) • • 1+ Ht (s)Gt (s)+Ht (s)H2(s)G2(s) 11.3. b < -1 11.4. G(s) = ! s 11.5. - ~ < b < ~ 11.6. FIR 11.7. K > -6 11.8. -3 < k < 0 11.9. No, root locus stays on real axis 11.10. Double pole at s = -1, double zero at s = 1 11.11. 0 < k < ~ 11.12. Pole and zero positions alternate on the real axis 11.13. Unstable for all K 11.14. (a) 0 (b) 1 11.15. K > -1 11.16. K > -1 11.17. -1 < K < 4 11.18. -1 < K < 1 11.19. Unstable 11.20. Gain margin is infinite, phase margin is 2 tan- 1 j2 INDEX Absolutely summable impulse Analysis equation response, 113 continuous-time Fourier series, 191 Absolutely integrable impulse continuous-time Fourier response, 114 transform, 288 Accumulation property discrete-time Fourier series, 213 discrete-time Fourier series, 221 discrete-time Fourier transform, discrete-time Fourier transform, 361, 390 375-76 Angle criterion, 836-40 unilateral z transform, 793 Angle modulation, 611-13 Accumulator, 44 Angle (phase) of complex number, 71 Acoustic feedback, 830-32, 855 Anticausality, 695 Adders in block diagrams, 125, 126 Aperiodic convolution, 222 Additivity property, 53 Aperiodic signal, 12, 180 Aliasing, 527-34 continuous-time Fourier transform for, All-pass systems, 430, 498, 681-82 285-89 AM. See Amplitude modulation (AM) discrete-time Fourier transform for, Amplifier 359-62 chopper, 652 Associative property of LTI systems, operational, 821, 896-97 107-8 Amplitude modulation (AM), 236-37, Audio systems 322,324,583 feedback in, 830-32,855 pulse-train carrier, 601-4, 605 frequency-shaping filters in, 232 sinusoidal, 583-87 Autocorrelation functions, 65, 168, complex exponential carrier, 583-85 170-72, 738 demodulation for, 587-94 Automobile suspension system, analysis discrete-time, 619-23 of, 473-76 frequency-division multiplexing Average, weighted, 245 (FDM) using, 594-97 Averaging system, noncausal, 47 single-sideband, 597-601 sinusoidal carrier, 585-87 Band-limited input signals, 541 Amplitude-scaling factor, 483 Band-limited interpolation, 523-24 Analog-to-digital (A-to-D) Bandpass filters, 237-38, 326 converter, 535 Bandpass-sampling techniques, 564-65 941"
Index,
9.pdf,"Sec. 1.2 Transformations of the Independent Variable 9 x[n) n (a) x[-n) n Figure 1.9 Continuous-time signals related by a time shift. In this figure t0 < 0, so that (b) x(t - to) is an advanced version of x(t) (i.e., each point in x(t) occurs at an earlier time in Figure 1 .1 O (a) A discrete-time signal x[n]; (b) its reflec- x(t - to)). tion x[-n] about n = 0. x(t) x(t) d\ x(2t) (a) x(-t) & x(t/2) ~ (b) Figure 1.11 (a) A continuous-time signal x(t); (b) its Figure 1. 12 Continuous-time signals reflection x( - t) about t = 0. related by time scaling. 10 Signals and Systems Chap. 1 Example 1.1 Given the signal x(t) shown in Figure l.13(a), the signal x(t + 1) corresponds to an advance (shift to the left) by one unit along the taxis as illustrated in Figure l.13(b). Specifically, we note that the value of x(t) at t = to occurs in x(t + 1) at t = to - 1. For 11 'l'I 0 1 2 (a) 1~ -1 0 1 2 (b) -1 0 1 (c) 1I ' 1i11 -~ 0 2/3 4/3 (d) -2/3 0 2/3 (e) Figure 1. 13 (a) The continuous-time signal x(t) used in Examples 1.1-1.3 to illustrate transformations of the independent variable; (b) the time-shifted signal x(t + 1) ; (c) the signal x(-t + 1) obtained by a time shift and a time reversal; (d) the time-scaled signal xa t); and (e) the signal xa t + 1) obtained by time-shifting and scaling. Sec. 1.2 Transformations of the Independent Variable 11 example, the value of x(t) at t = 1 is found in x(t + 1) at t = 1 - 1 = 0. Also, since x(t) is zero fort < 0, we have x(t + 1) zero fort < -1. Similarly, since x(t) is zero for t > 2, x(t + 1) is zero for t > 1. Let us also consider the signal x( - t + 1) , which may be obtained by replacing t with -t in x(t + 1). That is, x(-t + 1) is the time reversed version of x(t + 1) . Thus, x( - t + 1) may be obtained graphically by reflecting x( t + 1) about the t axis as shown in Figure 1.13(c). Example 1.2 Given the signal x(t), shown in Figure l.13(a), the signal x(~t) corresponds to a linear compression of x(t) by a factor of~ as illustrated in Figure l.13(d). Specifically we note that the value of x(t) at t = to occurs in x(~t) at t = ~t0 . For example, the value of x(t) at t = 1 is found in x(~t) at t = ~ (1) = ~-Also, since x(t) is zero fort< 0, we have x(~t) zero fort< 0. Similarly, since x(t) is zero fort> 2, x(~t) is zero fort> ~- Example 1.3 Suppose that we would like to determine the effect of transforming the independent vari- able of a given signal, x(t), to obtain a signal of the form x(at + /3), where a and f3 are given numbers. A systematic approach to doing this is to first delay or advance x(t) in accordance with the value of f3, and then to perform time scaling and/or time reversal on the resulting signal in accordance with the value of a. The delayed or advanced signal is linearly stretched if fa[ < 1, linearly compressed if fa[ > 1, and reversed in time if a < 0. To illustrate this approach, let us show how x( ~ t + 1) may be determined for the signal x(t) shown in Figure 1.13(a). Since f3 = 1, we first advance (shift to the left) x(t) by 1 as shown· in Figure 1.l 3(b ). Since fa [ = ~, we may linearly compress the shifted signal of Figure 1.13(b) by a factor of~ to obtain the signal shown in Figure 1.13(e). In addition to their use in representing physical phenomena such as the time shift in a sonar signal and the speeding up or reversal of an audiotape, transformations of the independent variable are extremely useful in signal and system analysis. In Section 1.6 and in Chapter 2, we will use transformations of the independent variable to introduce and analyze the properties of systems. These transformations are also important in defining and examining some important properties of signals. 1.2.2 Periodic Signals An important class of signals that we will encounter frequently throughout this book is the class of periodic signals. A periodic continuous-time signal x(t) has the property that there is a positive value of T for which x(t) = x(t + T) (l.11) for all values oft. In other words, a periodic signal has the property that it is unchanged by a time shift of T. In this case, we say that x(t) is periodic with period T. Periodic continuous- time signals arise in a variety of contexts. For example, as illustrated in Problem 2.61, the natural response of systems in which energy is conserved, such as ideal LC circuits without resistive energy dissipation and ideal mechanical systems without frictional losses, are periodic and, in fact, are composed of some of the basic periodic signals that we will introduce in Section 1.3. 12 Signals and Systems Chap. 1 x(t) ···!\ [\ & [\ !\··· Figure 1. 14 A continuous-time -2T -T 0 T 2T periodic signal. An example of a periodic continuous-time signal is given in Figure 1.14. From the figure or from eq. ( 1.11 ), we can readily deduce that if x(t) is periodic with period T, then x(t) = x(t + mT) for all t and for any integer m. Thus, x(t) is also periodic with period 2T, 3T, 4T, .... The fundamental period To of x(t) is the smallest positive value ofT for which eq. ( 1.11) holds. This definition of the fundamental period works, except if x(t) is a constant. In this case the fundamental period is undefined, since x(t) is periodic for any choice ofT (so there is no smallest positive value). A signal x(t) that is not periodic will be referred to as an aperiodic signal. Periodic signals are defined analogously in discrete time. Specifically, a discrete- time signal x[n] is periodic with period N, where N is a positive integer, if it is unchanged by a time shift of N, i.e., if x[n] = x[n + N] (1.12) for all values of n. If eq. (1.12) holds, then x[n] is also periodic with period 2N, 3N, .... The fundamental period N0 is the smallest positive value of N for which eq. ( 1.12) holds. An example of a discrete-time periodic signal with fundamental period No = 3 is shown in Figure 1.15. x[n] Figure 1 . 1 5 A discrete-time pe- n riodic signal with fundamental period No= 3. Example 1.4 Let us illustrate the type of problem solving that may be required in determining whether or not a given signal is periodic. The signal whose periodicity we wish to check is given by X (t ) = { c.o s(t) i. f t < 0 . (1.13) sm(t) If t ~ 0 From trigonometry, we know that cos(t + 27T) = cos(t) and sin(t + 27T) = sin(t). Thus, considering t > 0 and t < 0 separately, we see that x(t) does repeat itself over every interval oflength 27T. However, as illustrated in Figure 1.16, x(t) also has a discontinuity at the time origin that does not recur at any other time. Since every feature in the shape of a periodic signal must recur periodically, we conclude that the signal x(t) is not periodic. Sec. 1.2 Transformations of the Independent Variable 13 x(t) Figure 1. 16 The signal x{t) considered in Example 1.4. 1 .2.3 Even and Odd Signals Another set of useful properties of signals relates to their symmetry under time reversal. A signal x(t) or x[n] is referred to as an even signal if it is identical to its time-reversed counterpart, i.e., with its reflection about the origin. In continuous time a signal is even if x(- t) = x(t), (1.14) while a discrete-time signal is even if x[- n] = x[n]. ( 1.15) A signal is referred to as odd if x( -t) = - x(t), ( 1.16) x[-n] = -x[n]. (1.17) An odd signal must necessarily be 0 at t = 0 or n = 0, since eqs. ( 1.16) and ( 1.17) require that x(O) = - x(O) and x[O] = - x[O]. Examples of even and odd continuous-time signals are shown in Figure 1.17. x(t) 0 (a) x(t) Figure 1. 1 7 (a) An even con- tinuous-time signal; (b) an odd continuous-time signal. 14 Signals and Systems Chap. 1 x[n] = { 1, n;::::: 0 0, n < 0 -3-2-1 0 1 2 3 n Sv{x[nl} = { ~: ~: ~ 2, n > 0 1 t t 1I~~ t t ... -3-2-1 0 1 2 3 n - ~· n < 0 ea{x[nl}= ?·n=O { 2, n > 0 1 2 -3-2-1 r r r ···1110123 n Figure 1. 18 Example of the even- 1 odd decomposition of a discrete-time -2 signal. An important fact is that any signal can be broken into a sum of two signals, one of which is even and one of which is odd. To see this, consider the signal 1 8v { x(t)} = 2 [x (t) + x(- t)], ( 1.18) which is referred to as the even part of x(t). Similarly, the odd part of x(t) is given by 1 0d{x(t)} = 2[x(t)- x( -t)]. (1.19) It is a simple exercise to check that the even part is in fact even, that the odd part is odd, and that x(t) is the sum of the two. Exactly analogous definitions hold in the discrete- time case. An example of the even -odd decomposition of a discrete-time signal is given in Figure 1.18. 1 .3 EXPONENTIAL AND SINUSOIDAL SIGNALS In this section and the next, we introduce several basic continuous-time and discrete-time signals. Not only do these signals occur frequently, but they also serve as basic building blocks from which we can construct many other signals. Sec. 1.3 Exponential and Sinusoidal Signals 15 1 .3. 1 Continuous-Time Complex Exponential and Sinusoidal Signals The continuous-time complex exponential signal is of the form x(t) = Ce01 , (1.20) where C and a are, in general, complex numbers. Depending upon the values of these parameters, the complex exponential can exhibit several different characteristics. Real Exponential Signals As illustrated in Figure 1.19, if C and a are real [in which case x(t) is called a real exponential], there are basically two types of behavior. If a is positive, then as t in- creases x(t) is a growing exponential, a form that is used in describing many different physical processes, including chain reactions in atomic explosions and complex chemical reactions. If a is negative, then x(t) is a decaying exponential, a signal that is also used to describe a wide variety of phenomena, including the process of radioactive decay and the responses of RC circuits and damped mechanical systems. In particular, as shown in Problems 2.61 and 2.62, the natural responses of the circuit in Figure 1.1 and the automobile in Figure 1.2 are decaying exponentials. Also, we note that for a = 0, x(t) is constant. x(t) (a) x(t) Figure 1.19 Continuous-time real exponential x(t) = Ce31 : (a) a > 0; (b) (b) a< 0. 16 Signals and Systems Chap. 1 Periodic Complex Exponential and Sinusoidal Signals A second important class of complex exponentials is obtained by constraining a to be purely imaginary. Specifically, consider (1.21) An important property of this signal is that it is periodic. To verify this, we recall from eq. (1.11) that x(t) will be periodic with period T if (1.22) Or, since it follows that for periodicity, we must have (1.23) If w 0 = 0, then x(t) = 1, which is periodic for any value ofT. If w 0 =/:- 0, then the fun- damental period To of x(t)-that is, the smallest positive value ofT for which eq. (1.23) holds-is 21T = lwol' (1.24) To Thus, the signals eiwot and e- Jwot have the same fundamental period. A signal closely related to the periodic complex exponential is the sinusoidal signal x(t) = A cos(wot + cf>), (1.25) as illustrated in Figure 1.20. With seconds as the units oft, the units of cf> and w 0 are radians and radians per second, respectively. It is also common to write w 0 = 21T fo, where fo has the units of cycles per second, or hertz (Hz). Like the complex exponential signal, the si- nusoidal signal is periodic with fundamental period T0 given by eq. (1.24). Sinusoidal and x(t) = A cos (w0t + <!>) Figure 1.20 Continuous-time sinu- soidal signal. Sec. 1.3 Exponential and Sinusoidal Signals 17 complex exponential signals are also used to describe the characteristics of many physical processes-in particular, physical systems in which energy is conserved. For example, as shown in Problem 2.61, the natural response of an LC circuit is sinusoidal, as is the simple harmonic motion of a mechanical system consisting of a mass connected by a spring to a stationary support. The acoustic pressure variations corresponding to a single musical tone are also sinusoidal. By using Euler's relation? the complex exponential in eq. (1.21) can be written in terms of sinusoidal signals with the same fundamental period: e.iwot = cos Wot + j sin wot. (1.26) Similarly, the sinusoidal signal of eq. (1.25) can be written in terms of periodic complex exponentials, again with the same fundamental period: (1.27) Note that the two exponentials in eq. (1.27) have complex amplitudes. Alternatively, we can express a sinusoid in terms of a complex exponential signal as (1.28) where, if cis a complex number, CRe{ c} denotes its real part. We will also use the notation 9m{c} for the imaginary part of c, so that, for example, A sin(wot + ¢) = A9m{ej(wut+¢l}. (1.29) From eq. (1.24), we see that the fundamental period T0 of a continuous-time sinu- soidal signal or a periodic complex exponential is inversely proportional to lw0 j, which we will refer to as the fundamental frequency. From Figure 1.21, we see graphically what this means. If we decrease the magnitude of w 0 , we slow down the rate of oscillation and therefore increase the period. Exactly the opposite effects occur if we increase the mag- nitude of w 0 . Consider now the case w0 = 0. In this case, as we mentioned earlier, x(t) is constant and therefore is periodic with period T for any positive value of T. Thus, the fundamental period of a constant signal is undefined. On the other hand, there is no am- biguity in defining the fundamental frequency of a constant signal to be zero. That is, a constant signal has a zero rate of oscillation. Periodic signals-and in particular, the complex periodic exponential signal in eq. (1.21) and the sinusoidal signal in eq. (1.25)-provide important examples of signals with infinite total energy but finite average power. For example, consider the periodic ex- ponential signal of eq. (1.21), and suppose that we calculate the total energy and average power in this signal over one period: E period = f.oTo je.iwoti2 dt ( 1.30) = foT"" I · d t = To. 2Euler's relation and other basic ideas related to the manipulation of complex numbers and exponentials are considered in the mathematical review section of the problems at the end of the chapter. 18 Signals and Systems Chap. 1 (a) (b) Figure 1 .21 Relationship between the fundamental frequency and period for continuous-time sinusoidal signals; here, w1 > £1>2 > w 3, which implies (c) that T1 < T2 < r3. 1 P period = T Eperiod = 1. (1.31) 0 Since there are an infinite number of periods as t ranges from -'X! to +oo, the total energy integrated over all time is infinite. However, each period of the signal looks exactly the same. Since the average power of the signal equals 1 over each period, averaging over multiple periods always yields an average power of 1. That is, the complex periodic ex- Sec. 1.3 Exponential and Sinusoidal Signals 19 ponential signal has finite average power equal to Px = _lim _1 f T leiwotl2 dt = 1. (1.32) r~x 2T -T Problem 1.3 provides additional examples of energy and power calculations for periodic and aperiodic signals. Periodic complex exponentials will play a central role in much of our treatment of signals and systems, in part because they serve as extremely useful building blocks for many other signals. We will often find it useful to consider sets of harmonically related complex exponentials- that is, sets of periodic exponentials, all of which are periodic with a common period T0 . Specifically, a necessary condition for a complex exponential ejwr to be periodic with period T0 is that (1.33) which implies that wT0 is a multiple of 27T, i.e., wTo = 21Tk, k = 0,::!::: 1, ±:2, 0. 0 0 ( 1.34) Thus, if we define wo 27T = ( 1.35) To' we see that, to satisfy eq. ( 1.34), w must be an integer multiple of w0 . That is, a harmoni- cally related set of complex exponentials is a set of periodic exponentials with fundamental frequencies that are all multiples of a single positive frequency w0 : k = 0, ::!::: 1, ±:2,. 0. 0 ( 1.36) Fork = 0, ¢k(t) is a constant, while for any other value of k, ¢k(t) is periodic with fun- damental frequency Ik lwo and fundamental period 27T _ To lklwo - m· ( 1.37) The kth harmonic ¢k(t) is still periodic with period T0 as well, as it goes through exactly lkl of its fundamental periods during any time interval of length T0 . Our use of the term ""harmonic"" is consistent with its use in music, where it refers to tones resulting from variations in acoustic pressure at frequencies that are integer mul- tiples of a fundamental frequency. For example, the pattern of vibrations of a string on an instrument such as a violin can be described as a superposition-i.e., a weighted sum-of harmonically related periodic exponentials. In Chapter 3, we will see that we can build a very rich class of periodic signals using the harmonically related signals of eq. ( 1.36) as the building blocks. Example 1.5 It is sometimes desirable to express the sum of two complex exponentials as the product of a single complex exponential and a single sinusoid. For example, suppose we wish to 20 Signals and Systems Chap. 1 plot the magnitude of the signal (1.38) To do this, we first factor out a complex exponential from the right side of eq. (1.38), where the frequency of this exponential factor is taken as the average of the frequencies of the two exponentials in the sum. Doing this, we obtain (1.39) which, because of Euler's relation, can be rewritten as x(t) = 2ej2.51 cos(0.5t). (1.40) From this, we can directly obtain an expression for the magnitude of x(t): lx(t)l = 21 cos(0.5t)l. (1.41) Here, we have used the fact that the magnitude of the complex exponential ej2·51 is always unity. Thus, lx(t)l is what is commonly referred to as a full-wave rectified sinusoid, as shown in Figure 1.22. lx(t)l 2 Figure 1 .22 The full-wave rectified sinusoid of Example 1.5. General Complex Exponential Signals The most general case of a complex exponential can be expressed and interpreted in terms of the two cases we have examined so far: the real exponential and the periodic complex exponential. Specifically, consider a complex exponential C eat, where C is expressed in polar form and a in rectangular form. That is, and a= r + Jwo. Then (1.42) Using Euler's relation, we can expand this further as C eat = ICiert cos(wot + 0) + JICiert sin(wot + 0). (1.43) Sec. 1.3 Exponential and Sinusoidal Signals 21 Thus, for r = 0, the real and imaginary parts of a complex exponential are sinusoidal. For r > 0 they correspond to sinusoidal signals multiplied by a growing exponential, and for r < 0 they correspond to sinusoidal signals multiplied by a decaying exponential. These two cases are shown in Figure 1.23. The dashed lines in the figure correspond to the func- tions ± ICiert. From eq. ( 1.42), we see that ICiert is the magnitude of the complex expo- nential. Thus, the dashed curves act as an envelope for the oscillatory curve in the figure in that the peaks of the oscillations just reach these curves, and in this way the envelope provides us with a convenient way to visualize the general trend in the amplitude of the oscillations. x(t) (a) x(t) Figure 1 .23 (a) Growing sinusoidal signal x(t) = Cert cos (w0 t + 8), r > 0; (b) decaying sinusoid x{t) = (b) Cert cos (w0t + 8), r < 0. Sinusoidal signals multiplied by decaying exponentials are commonly referred to as damped sinusoids. Examples of damped sinusoids arise in the response of RLC circuits and in mechanical systems containing both damping and restoring forces, such as automo- tive suspension systems. These kinds of systems have mechanisms that dissipate energy (resistors, damping forces such as friction) with oscillations that decay in time. Examples illustrating such systems and their damped sinusoidal natural responses can be found in Problems 2.61 and 2.62. 1.3.2 Discrete-Time Complex Exponential and Sinusoidal Signals As in continuous time, an important signal in discrete time is the complex exponential signal or sequence, defined by ( 1.44) 22 Signals and Systems Chap. 1 where C and a are, in general, complex numbers. This could alternatively be expressed in the form x[n] = Cef3 11 , (1.45) where Although the form of the discrete-time complex exponential sequence given in eq. (1.45) is more analogous to the form of the continuous-time exponential, it is often more convenient to express the discrete-time complex exponential sequence in the form of eq. (1.44). Real Exponential Signals If C and a are real, we can have one of several types of behavior, as illustrated in Fig- ure 1.24. If Ia I > 1 the magnitude of the signal grows exponentially with n, while if Ia I < 1 we have a decaying exponential. Furthermore, if a is positive, all the values of Ca 11 are of the same sign, but if a is negative then the sign of x[n] alternates. Note also that if a = 1 then x[n] is a constant, whereas if a = -1, x[n] alternates in value between +C and -C. Real-valued discrete-time exponentials are often used to describe population growth as a function of generation and total return on investment as a function of day, month, or quarter. Sinusoidal Signals Another important complex exponential is obtained by using the form given in eq. (1.45) and by constraining {3 to be purely imaginary (so that Ia I 1). Specifically, consider (1.46) As in the continuous-time case, this signal is closely related to the sinusoidal signal x[n] = A cos(w0n + ¢). (1.47) If we taken to be dimensionless, then both wo and cp have units of radians. Three examples of sinusoidal sequences are shown in Figure 1.25. As before, Euler's relation allows us to relate complex exponentials and sinusoids: ejwon = cos won + j sin w0n (1.48) and (1.49) The signals in eqs. (1.46) and ( 1.47) are examples of discrete-time signals with infinite total energy but finite average power. For example, since lejwonl 2 = 1, every sample of the signal in eq. (1.46) contributes 1 to the signal's energy. Thus, the total energy for -'X! < n < 'X! is infinite, while the average power per time point is obviously equal to 1. Other examples of energy and power calculations for discrete-time signals are given in Problem 1.3. Sec. 1.3 Exponential and Sinusoidal Signals 23 n (a) n (b) n (c) n Figure 1 .24 The real exponential signal x[n] = can: (a) a > 1; (b) 0 < a < 1; (d) (c) -1 <a< O; (d) a< -1. 24 Signals and Systems Chap. 1 x[n] = cos (2Tin/12 ) ' I n (a) x[n] = cos (8Tin/31) n (b) x[n] = cos (n/6) n (c) Figure 1 .25 Discrete-time sinusoidal signals. General Complex Exponential Signals The general discrete-time complex exponential can be written and interpreted in terms of real exponentials and sinusoidal signals. Specifically, if we write C and a in polar form, Sec. 1.3 Exponential and Sinusoidal Signals 25 viz., C = ICieiH and then ( 1.50) Thus, for Ia I = 1, the real and imaginary parts of a complex exponential sequence are sinusoidal. For Ia I < 1 they conespond to sinusoidal sequences multiplied by a decaying exponential, while for lal > 1 they conespond to sinusoidal sequences multiplied by a growing exponential. Examples of these signals are depicted in Figure 1.26. n (a) ' \ n (b) Figure 1.26 (a) Growing discrete-time sinusoidal signals; (b) decaying discrete-time sinusoid. 1.3.3 Periodicity Properties of Discrete-Time Complex Exponentials While there are many similarities between continuous-time and discrete-time signals, there are also a number of important differences. One of these concerns the discrete-time exponential signal e.iluon. In Section 1.3.1, we identified the following two properties of its 26 Signals and Systems Chap. 1 continuous-time counterpart eiwor: ( 1) the larger the magnitude of w0, the higher is the rate of oscillation in the signal; and (2) eiwor is periodic for any value of w0• In this section we describe the discrete-time versions of both of these properties, and as we will see, there are definite differences between each of these and its continuous-time counterpart. The fact that the first of these properties is different in discrete time is a direct conse- quence of another extremely important distinction between discrete-time and continuous- time complex exponentials. Specifically, consider the discrete-time complex exponential with frequency w 0 + 2-rr: (1.51) From eq. ( 1.51 ), we see that the exponential at frequency w0 + 2-rr is the same as that at frequency w 0 . Thus, we have a very different situation from the continuous-time case, in which the signals eiwor are all distinct for distinct values of w 0 . In discrete time, these signals are not distinct, as the signal with frequency w 0 is identical to the signals with frequencies w0 ± 2-rr, w 0 ± 4-rr, and so on. Therefore, in considering discrete-time com- plex exponentials, we need only consider a frequency interval of length 2-rr in which to choose w 0 . Although, according to eq. (1.51), any interval of length 2-rr will do, on most occasions we will use the interval 0 ::::; w0 < 2-rr or the interval --rr ::::; w 0 < 1r. Because of the periodicity implied by eq. ( 1.51), the signal eiwon does not have a continually increasing rate of oscillation as w 0 is increased in magnitude. Rather, as il- lustrated in Figure 1.27, as we increase w 0 from 0, we obtain signals that oscillate more and more rapidly until we reach w 0 = 1r. As we continue to increase W(), we decrease the rate of oscillation until we reach w 0 = 2-rr, which produces the same constant sequence as w 0 = 0. Therefore, the low-frequency (that is, slowly varying) discrete-time exponentials have values of w 0 near 0, 2-rr, and any other even multiple of 1r, while the high frequen- cies (corresponding to rapid variations) are located near w 0 = ± 1r and other odd multiples of 1r. Note in particular that for w 0 = 1r or any other odd multiple of 1r, (1.52) so that this signal oscillates rapidly, changing sign at each point in time [as illustrated in Figure 1.27(e)]. The second property we wish to consider concerns the periodicity of the discrete- time complex exponential. In order for the signal eiwon to be periodic with period N > 0, we must have (1.53) or equivalently, (1.54) For eq. ( 1.54) to hold, w 0N must be a multiple of 2-rr. That is, there must be an integer m such that woN = 2-rrm, (1.55) or equivalently, m (1.56) N (b) (c) x[n] = cos (Tin/2) x[n] = cos Tin x[n] = cos (37Tn/2) - - - - - - - - - n n ------- .... n .. . .. . (d) (e) (f) x[n] = cos (7Tin/4) x[n] = cos (15Tin/8) x[n] = cos 2Tin ~~ ~~··· ···llliiiiiiiiiiiiliiiiiiiiiiiiiii··· n n (i) (g) (h) Figure 1 .27 Discrete-time sinusoidal sequences for several different frequencies. 28 Signals and Systems Chap. 1 According to eq. (1.56), the signal ejwon is periodic if w 0!27r is a rational number and is not periodic otherwise. These same observations also hold for discrete-time sinusoids. For example, the signals depicted in Figure 1.25( a) and (b) are periodic, while the signal in Figure 1.25( c) is not. Using the calculations that we have just made, we can also determine the funda- mental period and frequency of discrete-time complex exponentials, where we define the fundamental frequency of a discrete-time periodic signal as we did in continuous time. That is, if x[ n] is periodic with fundamental period N, its fundamental frequency is 27r/ N. Consider, then, a periodic complex exponential x[n] = ejwon with w 0 =I= 0. As we have just seen, w 0 must satisfy eq. (1.56) for some pair of integers m and N, with N > 0. In Problem 1.35, it is shown that if w 0 =I= 0 and if N and m have no factors in common, then the fundamental period of x[n] is N. Using this fact together with eq. (1.56), we find that the fundamental frequency of the periodic signal ejwon is 27r wo (1.57) N m Note that the fundamental period can also be written as (1.58) These last two expressions again differ from their continuous-time counterparts. In Table 1.1, we have summarized some of the differences between the continuous-time sig- nal ejwot and the discrete-time signal ejwon. Note that, as in the continuous-time case, the constant discrete-time signal resulting from setting w 0 = 0 has a fundamental frequency of zero, and its fundamental period is undefined. TABLE 1.1 Comparison of the signals ejwot and ejwon. Distinct signals for distinct values of w0 Identical signals for values of w0 separated by multiples of 27T Periodic for any choice of w 0 Periodic only if w 0 = 27Tm/N for some integers N > 0 and m. Fundamental frequency w0 Fundamental frequency* w0/m Fundamental period Fundamental period* w0 = 0: undefined w0 = 0: undefined wo ¥-0: ~ wo Wo ¥- 0: m(~) wo *Assumes that m and N do not have any factors in common. To gain some additional insight into these properties, let us examine again the signals depicted in Figure 1.25. First, consider the sequence x[n] = cos(27rn/12), depicted in Figure 1.25(a), which we can think of as the set of samples ofthe continuous-time sinusoid x(t) = cos(27rt/12) at integer time points. In this case, x(t) is periodic with fundamental period 12 and x[n] is also periodic with fundamental period 12. That is, the values of x[n] repeat every 12 points, exactly in step with the fundamental period of x(t). Sec. 1.3 Exponential and Sinusoidal Signals 29 In contrast, consider the signal x[ n] = cos(8 7Tn/31 ), depicted in Figure 1.25 (b), which we can view as the set of samples of x(t) = cos (87Tt/31) at integer points in time. In this case, x(t) is periodic with fundamental period 31/4. On the other hand, x[n] is pe- riodic with fundamental period 31. The reason for this difference is that the discrete-time signal is defined only for integer values of the independent variable. Thus, there is no sample at timet = 3114, when x(t) completes one period (starting from t = 0). Similarly, there is no sample at t = 2 · 31/4 or t = 3 · 31/4, when x(t) has completed two or three periods, but there is a sample at t = 4 · 3114 = 31, when x(t) has completed four periods. This can be seen in Figure 1.25(b), where the pattern of x[n] values does not repeat with each single cycle of positive and negative values. Rather, the pattern repeats after four such cycles, namely, every 31 points. Similarly, the signal x[n] = cos(n/6) can be viewed as the set of samples of the signal x(t) = cos(t/6) at integer time points. In this case, the values of x(t) at integer sample points never repeat, as these sample points never span an interval that is an exact multiple of the period, 127T, of x(t). Thus, x[n] is not periodic, although the eye visually interpolates between the sample points, suggesting the envelope x(t), which is periodic. The use of the concept of sampling to gain insight into the periodicity of discrete-time sinusoidal sequences is explored further in Problem 1.36. Example 1.6 Suppose that we wish to determine the fundamental period of the discrete-time signal x[n] = ei<27ri3Jn + ei<hl4ln. (1.59) The first exponential on the right-hand side of eq. (1.59) has a fundamental period of 3. While this can be verified from eq. ( 1.58), there is a simpler way to obtain that answer. In particular, note that the angle (27T/3)n of the first term must be incremented by a multiple of 27T for the values of this exponential to begin repeating. We then immediately see that if n is incremented by 3, the angle will be incremented by a single multiple of 27T. With regard to the second term, we see that incrementing the angle (37T/4)n by 27T would require n to be incremented by 8/3, which is impossible, since n is restricted to being an integer. Similarly, incrementing the angle by 47T would require a noninteger increment of 16/3 to n. However, incrementing the angle by 61r requires an increment of 8 to n, and thus the fundamental period of the second term is 8. Now, for the entire signal x[n] to repeat, each of the terms in eq. (1.59) must go through an integer number of its own fundamental period. The smallest increment of n that accomplishes this is 24. That is, over an interval of 24 points, the first term on the right-hand side of eq. ( 1.59) will have gone through eight of its fundamental periods, the second term through three of its fundamental periods, and the overall signal x[n] through exactly one of its fundamental periods. As in continuous time, it is also of considerable value in discrete-time signal and system analysis to consider sets of harmonically related periodic exponentials-that is, periodic exponentials with a common period N. From eq. (1.56), we know that these are precisely the signals which are at frequencies which are multiples of 27T/N. That is, k = 0, ±1, .... (1.60) 30 Signals and Systems Chap. 1 In the continuous-time case, all of the harmonically related complex exponentials ei k(27TIT)t, k = 0, ± 1, ± 2, ... , are distinct. However, because of eq. ( 1.51 ), this is not the case in discrete time. Specifically, <f>k+N[n] = ei<k+N)(2TT!N)n (1.61) = eik(2TT!N)n e)27Tn = <f>k[n]. This implies that there are only N distinct periodic exponentials in the set given in eq. ( 1.60). For example, ( 1.62) are all distinct, and any other cf>k[n] is identical to one of these (e.g., cf>N[n] = <f>0[n] and <f>-I[n] = cf>N-I[n]). 1 .4 THE UNIT IMPULSE AND UNIT STEP FUNCTIONS In this section, we introduce several other basic signals-specifically, the unit impulse and step functions in continuous and discrete time-that are also of considerable importance in signal and system analysis. In Chapter 2, we will see how we can use unit impulse signals as basic building blocks for the construction and representation of other signals. We begin with the discrete-time case. 1 .4. 1 The Discrete-Time Unit Impulse and Unit Step Sequences One of the simplest discrete-time signals is the unit impulse (or unit sample), which is defined as n=rfO ll[nj = { ~: (1.63) n = 0 and which is shown in Figure 1.28. Throughout the book, we will refer too [n ] interchange- ably as the unit impulse or unit sample. 8[n] • • • • • • • • • • • • • • • Figure 1.28 Discrete-time unit im- n pulse (sample). A second basic discrete-time signal is the discrete-time unit step, denoted by u[n] and defined by u[n] = { ~: n<O o· ( 1.64) n ~ The unit step sequence is shown in Figure 1.29. Sec. 1.4 The Unit Impulse and Unit Step Functions 31 u[n] • • • • • • • • • • • JIIIIII Figure 1.29 Discrete-time unit step 0 n sequence. Interval of summation o[m] • • • • • • •I • • • . I . . . . . . . . n 0 m (a) Interval of summation o[m] • • • • • • • • • .. I . . . . . . . . . 0 n m Figure 1.30 Running sum of (b) eq. (1.66): (a) n < 0; (b) n > 0. There is a close relationship between the discrete-time unit impulse and unit step. In particular, the discrete-time unit impulse is the first difference of the discrete-time step B [n] = u[n] - u[n - 1]. (1.65) Conversely, the discrete-time unit step is the running sum of the unit sample. That is, n u[n] = ~ B[m]. (1.66) m= -oo Equation (1.66) is illustrated graphically in Figure 1.30. Since the only nonzero value of the unit sample is at the point at which its argument is zero, we see from the figure that the running sum in eq. (1.66) is 0 for n < 0 and 1 for n 2: 0. Furthermore, by changing the variable of summation from m to k = n - min eq. (1.66), we find that the discrete-time unit step can also be written in terms of the unit sample as 0 u[n] = ~ B[n- k], k='X or equivalently, u[n] = ~ B[n - k]. (1.67) k=O 32 Signals and Systems Chap. 1 Interval of summation ~----------- o[n-k] I • • • .I. n • • • 0• • • • • • • • • k (a) Interval of summation '----- ""3[n~k]---- . . . . . . . . . . . . . . I . . . . 0 n k Figure 1 .31 Relationship given in (b) eq. (1.67): (a) n < 0; (b) n > 0. Equation (1.67) is illustrated in Figure 1.31. In this case the nonzero value of 8[n- k] is at the value of k equal ton, so that again we see that the summation in eq. (1.67) is 0 for n < 0 and 1 for n 2:: 0. An interpretation of eq. ( 1.67) is as a superposition of delayed impulses; i.e., we can view the equation as the sum of a unit impulse 8[n] at n = 0, a unit impulse 8[n- 1] at n = 1, another, 8[n- 2], at n = 2, etc. We will make explicit use of this interpretation in Chapter 2. The unit impulse sequence can be used to sample the value of a signal at n = 0. In particular, since 8[n] is nonzero (and equal to 1) only for n = 0, it follows that x[n]o[n] = x[O]o[n]. (1.68) More generally, if we consider a unit impulse 8[n - n0 ] at n = n0 , then x[n]8[n - no] = x[no]8[n- no]. (1.69) This sampling property of the unit impulse will play an important role in Chapters 2 and 7. 1 .4.2 The Continuous-Time Unit Step and Unit Impulse Functions The continuous-time unit step function u(t) is defined in a manner similar to its discrete- time counterpart. Specifically, u(t) ~ { ~: t<O (1.70) t > 0' as is shown in Figure 1.32. Note that the unit step is discontinuous at t 0. The continuous-time unit impulse function 8(t) is related to the unit step in a manner analogous Sec. 1.4 The Unit Impulse and Unit Step Functions 33 u(t) Figure 1 .32 Continuous-time unit 0 step function. to the relationship between the discrete-time unit impulse and step functions. In particular, the continuous-time unit step is the running integral of the unit impulse (1.71) This also suggests a relationship between 8(t) and u(t) analogous to the expression for 8[n] in eq. (1.65). In particular, it follows from eq. (1.71) that the continuous-time unit impulse can be thought of as the first derivative of the continuous-time unit step: 8(t) = d~;t). (1.72) In contrast to the discrete-time case, there is some formal difficulty with this equa- tion as a representation of the unit impulse function, since u(t) is discontinuous at t = 0 and consequently is formally not differentiable. We can, however, interpret eq. (1.72) by considering an approximation to the unit step u11 (t), as illustrated in Figure 1.33, which rises from the value 0 to the value 1 in a short time interval of length Ll. The unit step, of course, changes values instantaneously and thus can be thought of as an idealization of u11 (t) for Ll so short that its duration doesn't matter for any practical purpose. Formally, u(t) is the limit of Uf1 (t) as Ll ~ 0. Let us now consider the derivative ~ ( ) _ du11(t) u 11 t - -----;[(' (1.73) as shown in Figure 1.34. u~(t) 0 11 Figure 1 .33 Continuous approximation to Figure 1.34 Derivative of the unit step, u~(t). u~(t). 34 Signals and Systems Chap. 1 0 0 Figure 1.35 Continuous- Figure 1.36 Scaled im- time unit impulse. pulse. Note that o~(t) is a short pulse, of duration~ and with unit area for any value of~. As~ ~ 0, o~(t) becomes narrower and higher, maintaining its unit area. Its limiting form, o(t) = lim o~(t), (1.74) ~---->0 can then be thought of as an idealization of the short pulse o~(t) as the duration~ becomes insignificant. Since o(t) has, in effect, no duration but unit area, we adopt the graphical notation for it shown in Figure 1.35, where the arrow at t = 0 indicates that the area of the pulse is concentrated at t = 0 and the height of the arrow and the ""1"" next to the arrow are used to represent the area of the impulse. More generally, a scaled impulse ko(t) will have an area k, and thus, L% kD(r)dr = ku(t). A scaled impulse with area k is shown in Figure 1.36, where the height of the arrow used to depict the scaled impulse is chosen to be proportional to the area of the impulse. As with discrete time, we can provide a simple graphical interpretation of the running integral of eq. (1.71); this is shown in Figure 1.37. Since the area of the continuous-time unit impulse o(T ) is concentrated at T = 0, we see that the running integral is 0 fort < 0 and 1 for t > 0. Also, we note that the relationship in eq. ( 1. 71) between the continuous- time unit step and impulse can be rewritten in a different form, analogous to the discrete- time form in eq. (1.67), by changing the variable of integration from T to u = t- T: u(t) = L% (j( r) dr = rD (t - <r)( -da""), or equivalently, u(t) = rD (t- a) d<r. (1.75) The graphical interpretation of this form of the relationship between u(t) and o(t) is given in Figure 1.38. Since in this case the area of o(t - u) is concentrated at the point u = t, we again see that the integral in eq. (1.75) is 0 fort < 0 and 1 fort> 0. This type of graphical interpretation of the behavior of the unit impulse under integration will be extremely useful in Chapter 2. Sec. 1.4 The Unit Impulse and Unit Step Functions 35 Interval of integration Interval of integration 8(t-u) 0 'T 0 (}' (a) (a) Interval of integration Interval of integration --------------~-----, 8(-r) : 8(t-u) I I I I 0 'T 0 (}' (b) (b) Figure 1.37 Running integral given in eq. (1.71 ): Figure 1.38 Relationship given in eq. (1.75): (a) t < 0; (b) t > 0. (a) t < 0; (b) t > 0. As with the discrete-time impulse, the continuous-time impulse has a very important sampling property. In particular, for a number of reasons it will be important to consider the product of an impulse and more well-behaved continuous-time functions x(t). The in- terpretation of this quantity is most readily developed using the definition of o(t) according to eq. (1.74). Specifically, consider Xt (t) = x(t)Dtl.(t). In Figure 1.39(a) we have depicted the two time functions x(t) and Ofl.(t), and in Fig- ure 1.39(b) we see an enlarged view of the nonzero portion of their product. By construc- tion, x1( t) is zero outside the interval 0 ::::; t ::::; ~. For~ sufficiently small so that x(t) is approximately constant over this interval, Since o(t) is the limit as~ ~ 0 of Dfl.(t), it follows that x(t)B(t) = x(O)o(t). ( 1. 76) By the same argument, we have an analogous expression for an impulse concentrated at an arbitrary point, say, t0 . That is, x(t)o (t - to) = x(to)D(t - to). 36 Signals and Systems Chap. 1 o~(t) 0 ~ (a) x(O)~ Figure 1.39 The product x(t)o~(t): (a) graphs of both functions; (b) en- 0 larged view of the nonzero portion of (b) their product. Although our discussion of the unit impulse in this section has been somewhat in- formal, it does provide us with some important intuition about this signal that will be useful throughout the book. As we have stated, the unit impulse should be viewed as an idealization. As we illustrate and discuss in more detail in Section 2.5, any real physi- cal system has some inertia associated with it and thus does not respond instantaneously to inputs. Consequently, if a pulse of sufficiently short duration is applied to such a sys- tem, the system response will not be noticeably influenced by the pulse's duration or by the details of the shape of the pulse, for that matter. Instead, the primary characteristic of the pulse that will matter is the net, integrated effect of the pulse-i.e., its area. For systems that respond much more quickly than others, the pulse will have to be of much shorter duration before the details of the pulse shape or its duration no longer matter. Nev- ertheless, for any physical system, we can always find a pulse that is ""short enough."" The unit impulse then is an idealization of this concept-the pulse that is short enough for any system. As we will see in Chapter 2, the response of a system to this idealized pulse plays a crucial role in signal and system analysis, and in the process of devel- oping and understanding this role, we will develop additional insight into the idealized signal.3 3The unit impulse and other related functions (which are often collectively referred to as singularity functions) have been thoroughly studied in the field of mathematics under the alternative names of general- ized functions and the theory of distributions. For more detailed discussions of this subject, see Distribution Theory and Transfonn Analysis, by A. H. Zemanian (New York: McGraw-Hill Book Company, 1965), Gen- eralised Functions, by R.F. Hoskins (New York: Halsted Press, 1979), or the more advanced text, Fourier Analysis and Generalized Functions, by M. J. Lighthill (New York: Cambridge University Press, 1958). Our discussion of singularity functions in Section 2.5 is closely related in spirit to the mathematical theory described in these texts and thus provides an informal introduction to concepts that underlie this topic in mathematics. Sec. 1.4 The Unit Impulse and Unit Step Functions 37 Example 1.7 Consider the discontinuous signal x(t) depicted in Figure 1.40(a). Because of the rela- tionship between the continuous-time unit impulse and unit step, we can readily calculate and graph the derivative of this signal. Specifically, the derivative of x(t) is clearly 0, except at the discontinuities. In the case of the unit step, we have seen [eq. (1.72)] that differentiation gives rise to a unit impulse located at the point of discontinuity. Further- more, by multiplying both sides of eq. (1.72) by any number k, we see that the derivative of a unit step with a discontinuity of size k gives rise to an impulse of area k at the point of discontinuity. This rule also holds for any other signal with a jump discontinuity, such as x(t) in Figure 1.40(a). Consequently, we can sketch its derivative x(t), as in Fig- ure 1.40(b), where an impulse is placed at each discontinuity of x(t), with area equal to the size of the discontinuity. Note, for example, that the discontinuity in x(t) at t = 2 has a value of- 3, so that an impulse scaled by -3 is located at t = 2 in the signal x(t). x(t) 2- - 1 - 2 3 (a) 4 -1 ~ x(t) 2f- 1 r- 2 3 (b) 4 -1 r- -2 t- -3 f- - - ~Interval of integration 1 -: 2 3 (c) 4 -1 - -2- -3- Figure 1.40 (a) The discontinuous signal x(t) analyzed in Example 1.7; (b) its derivative x(t); (c) depiction of the recovery of x(t) as the running inte- gral of x(t), illustrated for a value of t between 0 and 1. 38 Signals and Systems Chap. 1 As a check of our result, we can verify that we can recover x(t) from x(t). Specif- ically, since x(t) and x(t) are both zero fort :::::: 0, we need only check that fort > 0, 1 X(t) = { X( 7) dT. (1.77) Jo As illustrated in Figure 1.40(c), fort< 1, the integral on the right-hand side of eq. (1.77) is zero, since none of the impulses that constitute x(t) are within the interval of integra- tion. For 1 < t < 2, the first impulse (located at t = 1) is the only one within the inte- gration interval, and thus the integral in eq. (1.77) equals 2, the area of this impulse. For 2 < t < 4, the first two impulses are within the interval of integration, and the integral accumulates the sum of both of their areas, namely, 2 - 3 = - 1. Finally, for t > 4, all three impulses are within the integration interval, so that the integral equals the sum of all three areas-that is, 2 - 3 + 2 = + 1. The result is exactly the signal x(t) depicted in Figure 1.40(a). 1 .5 CONTINUOUS-TIME AND DISCRETE-TIME SYSTEMS Physical systems in the broadest sense are an interconnection of components, devices, or subsystems. In contexts ranging from signal processing and communications to elec- tromechanical motors, automotive vehicles, and chemical-processing plants, a system can be viewed as a process in which input signals are transformed by the system or cause the system to respond in some way, resulting in other signals as outputs. For example, a high- fidelity system takes a recorded audio signal and generates a reproduction of that signal. If the hi-fi system has tone controls, we can change the tonal quality of the reproduced sig- nal. Similarly, the circuit in Figure 1.1 can be viewed as a system with input voltage Vs(t) and output voltage vc(t), while the automobile in Figure 1.2 can be thought of as a system with input equal to the force f(t) and output equal to the velocity v(t) of the vehicle. An image-enhancement system transforms an input image into an output image that has some desired properties, such as improved contrast. A continuous-time system is a system in which continuous-time input signals are applied and result in continuous-time output signals. Such a system will be represented pictorially as in Figure 1.41(a), where x(t) is the input and y(t) is the output. Alterna- tively, we will often represent the input-output relation of a continuous-time system by the notation x(t) ~ y(t). (1.78) x(t) --~• Continuous-time ....... _...,. y(t) system (a) Discrete-time x[n]--~ ......- ...... y[n] system Figure 1 .41 (a) Continuous-time (b) system; (b) discrete-time system. Sec. 1.5 Continuous-Time and Discrete-Time Systems 39 Similarly, a discrete-time system-that is, a system that transforms discrete-time inputs into discrete-time outputs-will be depicted as in Figure 1.41 (b) and will sometimes be represented symbolically as x[n] ~ y[n]. (1.79) In most of this book, we will treat discrete-time systems and continuous-time systems separately but in parallel. In Chapter 7, we will bring continuous-time and discrete-time systems together through the concept of sampling, and we will develop some insights into the use of discrete-time systems to process continuous-time signals that have been sampled. 1.5.1 Simple Examples of Systems One of the most important motivations for the development of general tools for analyzing and designing systems is that systems from many different applications have very similar mathematical descriptions. To illustrate this, we begin with a few simple examples. Example 1.8 Consider the RC circuit depicted in Figure 1.1. If we regard vs(t) as the input signal and vc(t) as the output signal, then we can use simple circuit analysis to derive an equation describing the relationship between the input and output. Specifically, from Ohm's law, the current i(t) through the resistor is proportional (with proportionality constant 11 R) to the voltage drop across the resistor; i.e., '( ) Vs(t) - Vc(t) 1 t = R . (1.80) Similarly, using the defining constitutive relation for a capacitor, we can relate i(t) to the rate of change with time of the voltage across the capacitor: '( ) - cddvc( lt- [t)' (1.81) Equating the right-hand sides of eqs. (1.80) and (1.81), we obtain a differential equation describing the relationship between the input vs(t) and the output vc(t): dvc(t) 1 ( ) _ 1 ( ) dt + RC Vc t - RC Vs t . (1.82) Example 1.9 Consider Figure 1.2, in which we regard the force f(t) as the input and the velocity v(t) as the output. If we let m denote the mass of the automobile and mpv the resistance due to friction, then equating acceleration-i.e., the time derivative of velocity-with net force divided by mass, we obtain dv(t) 1 - [f(t)- pv(t) J, (1.83) dt m 40 Signals and Systems Chap. 1 i.e., dv(t) + p_v(t) = _!_ j(t). (1.84) dt m m Examining and comparing eqs. (1.82) and (1.84) in the above examples, we see that the input-output relationships captured in these two equations for these two very different physical systems are basically the same. In particular, they are both examples of first-order linear differential equations of the form dy(t) -----;[( + ay(t) = bx(t), (1.85) where x(t) is the input, y(t) is the output, and a and bare constants. This is one very simple example of the fact that, by developing methods for analyzing general classes of systems such as that represented by eq. (1.85), we will be able to use them in a wide variety of applications. Example 1. 1 0 As a simple example of a discrete-time system, consider a simple model for the balance in a bank account from month to month. Specifically, let y[n] denote the balance at the end of the nth month, and suppose that y[n] evolves from month to month according to the equation y[n] = l.Oly[n- 1] + x[n], (1.86) or equivalently, y[n] - l.Oly[n- 1] = x[n], (1.87) where x[n] represents the net deposit (i.e., deposits minus withdrawals) during the nth month and the term 1.01y[n- 1] models the fact that we accrue 1% interest each month. Example 1. 11 As a second example, consider a simple digital simulation of the differential equation in eq. (1.84) in which we resolve time into discrete intervals oflength Ll and approximate dv(t)ldt at t = nLl by the first backward difference, i.e., v(nLl) - v((n- 1)Ll) Ll In this case, if we let v[n] = v(nLl) and f[n] = j(nLl), we obtain the following discrete- time model relating the sampled signals f[n] and v[n]: m Ll v[n] - (m + pLl) v[n - 1] = (m + pLl) f[n]. (1.88) Comparing eqs. (1.87) and (1.88), we see that they are both examples of the same general first-order linear difference equation, namely, y[n] + ay[n - 1] = bx [n]. (1.89) Sec. 1.5 Continuous-Time and Discrete-Time Systems 41 As the preceding examples suggest, the mathematical descriptions of systems from a wide variety of applications frequently have a great deal in common, and it is this fact that provides considerable motivation for the development of broadly applicable tools for signal and system analysis. The key to doing this successfully is identifying classes of systems that have two important characteristics: ( 1) The systems in this class have prop- erties and structures that we can exploit to gain insight into their behavior and to develop effective tools for their analysis~ and (2) many systems of practical importance can be accurately modeled using systems in this class. It is on the first of these characteristics that most of this book focuses, as we develop tools for a particular class of systems re- ferred to as linear, time-invariant systems. In the next section, we will introduce the prop- erties that characterize this class, as well as a number of other very important basic system properties. The second characteristic mentioned in the preceding paragraph is of obvious impor- tance for any system analysis technique to be of value in practice. It is a well-established fact that a wide range of physical systems (including those in Examples 1.8-1.10) can be well modeled within the class of systems on which we focus in this book. However, a critical point is that any model used in describing or analyzing a physical system rep- resents an idealization of that system, and thus, any resulting analysis is only as good as the model itself. For example, the simple linear model of a resistor in eq. (1.80) and that of a capacitor in eq. ( 1.81) are idealizations. However, these idealizations are quite accurate for real resistors and capacitors in many applications, and thus, analy- ses employing such idealizations provide useful results and conclusions, as long as the voltages and currents remain within the operating conditions under which these simple linear models are valid. Similarly, the use of a linear retarding force to represent fric- tional effects in eq. (1.83) is an approximation with a range of validity. Consequently, although we will not address this issue in the book, it is important to remember that an essential component of engineering practice in using the methods we develop here consists of identifying the range of validity of the assumptions that have gone into a model and ensuring that any analysis or design based on that model does not violate those assumptions. 1.5.2 Interconnections of Systems An important idea that we will use throughout this book is the concept of the interconnec- tion of systems. Many real systems are built as interconnections of several subsystems. One example is an audio system, which involves the interconnection of a radio receiver, compact disc player, or tape deck with an amplifier and one or more speakers. Another is a digitally controlled aircraft, which is an interconnection of the aircraft, described by its equations of motion and the aerodynamic forces affecting it~ the sensors, which measure various aircraft variables such as accelerations, rotation rates, and heading~ a digital au- topilot, which responds to the measured variables and to command inputs from the pilot (e.g., the desired course, altitude, and speed)~ and the aircraft's actuators, which respond to inputs provided by the autopilot in order to use the aircraft control surfaces (rudder, tail, ailerons) to change the aerodynamic forces on the aircraft. By viewing such a system as an interconnection of its components, we can use our understanding of the component 42 Signals and Systems Chap. 1 Input System 1 System 2 Output (a) Output (b) System 1 Input~ System 4 Output System 3 (c) Figure 1.42 Interconnection of two systems: (a) series (cascade) intercon- nection; (b) parallel interconnection; (c) series-parallel interconnection. systems and of how they are interconnected in order to analyze the operation and behavior of the overall system. In addition, by describing a system in terms of an interconnection of simpler subsystems, we may in fact be able to define useful ways in which to synthesize complex systems out of simpler, basic building blocks. While one can construct a variety of system interconnections, there are several basic ones that are frequently encountered. A series or cascade interconnection of two systems is illustrated in Figure 1.42(a). Diagrams such as this are referred to as block diagrams. Here, the output of System 1 is the input to System 2, and the overall system transforms an input by processing it first by System 1 and then by System 2. An example of a series interconnection is a radio receiver followed by an amplifier. Similarly, one can define a series interconnection of three or more systems. A parallel interconnection of two systems is illustrated in Figure 1.42(b ). Here, the same input signal is applied to Systems 1 and 2. The symbol ""EB"" in the figure denotes addition, so that the output of the parallel interconnection is the sum of the outputs of Systems 1 and 2. An example of a parallel interconnection is a simple audio system with several microphones feeding into a single amplifier and speaker system. In addition to the simple parallel interconnection in Figure 1.42(b ), we can define parallel interconnections of more than two systems, and we can combine both cascade and parallel interconnections Sec. 1.5 Continuous-Time and Discrete-Time Systems 43 Figure 1 .43 Feedback interconnec- tion. to obtain more complicated interconnections. An example of such an interconnection is given in Figure 1.42(c).4 Another important type of system interconnection is a feedback interconnection, an example of which is illustrated in Figure 1.43. Here, the output of System 1 is the input to System 2, while the output of System 2 is fed back and added to the external input to pro- duce the actual input to System 1. Feedback systems arise in a wide variety of applications. For example, a cruise control system on an automobile senses the vehicle's velocity and adjusts the fuel flow in order to keep the speed at the desired level. Similarly, a digitally controlled aircraft is most naturally thought of as a feedback system in which differences between actual and desired speed, heading, or altitude are fed back through the autopilot in order to correct these discrepancies. Also, electrical circuits are often usefully viewed as containing feedback interconnections. As an example, consider the circuit depicted in Figure 1.44(a). As indicated in Figure 1.44(b), this system can be viewed as the feedback interconnection of the two circuit elements. + i1 (t) t i2 (t) t ti (t) C R v(t) + Capacitor i(t) i1 (t) ~ + v(t) v(t) = ~ ~-~i 1 (T)dT - Resistor Figure 1 .44 (a) Simple electrical i2 (t) . (t) _ v(t) circuit; (b) block diagram in which the 12 -R circuit is depicted as the feedback inter- connection of two circuit elements. 40n occasion, we will also use the symbol 0 in our pictorial representation of systems to denote the operation of multiplying two signals (see, for example, Figure 4.26). 44 Signals and Systems Chap. 1 1 .6 BASIC SYSTEM PROPERTIES In this section we introduce and discuss a number of basic properties of continuous-time and discrete-time systems. These properties have important physical interpretations and relatively simple mathematical descriptions using the signals and systems language that we have begun to develop. 1 . 6. 1 Systems with and without Memory A system is said to be memoryl ess if its output for each value of the independent variable at a given time is dependent only on the input at that same time. For example, the system specified by the relationship (1.90) is memory less, as the value of y[n] at any particular time n0 depends only on the value of x[n] at that time. Similarly, a resistor is a memory less system; with the input x(t) taken as the current and with the voltage taken as the output y(t), the input-output relationship of a resistor is y(t) = Rx(t), (1.91) where R is the resistance. One particularly simple memoryless system is the identity sys- tem, whose output is identical to its input. That is, the input-output relationship for the continuous-time identity system is y(t) = x(t), and the corresponding relationship in discrete time is y[n] = x[n]. An example of a discrete-time system with memory is an accumulator or summer II y[n] = L x[k], (1.92) k=-CXJ and a second example is a delay y[n] = x[n - 1]. (1.93) A capacitor is an example of a continuous-time system with memory, since if the input is taken to be the current and the output is the voltage, then y(t) = C1 f'- x x(r)dr, (1.94) where C is the capacitance. Roughly speaking, the concept of memory in a system corresponds to the presence of a mechanism in the system that retains or stores information about input values at times Sec. 1.6 Basic System Properties 45 other than the current time. For example, the delay in eq. (1.93) must retain or store the preceding value of the input. Similarly, the accumulator in eq. (1.92) must ""remember"" or store information about past inputs. In particular, the accumulator computes the running sum of all inputs up to the current time, and thus, at each instant of time, the accumulator must add the current input value to the preceding value of the running sum. In other words, the relationship between the input and output of an accumulator can be described as n-1 y[n] = L x[k] + x[n], (1.95) k=-X or equivalently, y[n] = y[n - 1] + x[n]. (1.96) Represented in the latter way, to obtain the output at the current time n, the accumulator must remember the running sum of previous input values, which is exactly the preceding value of the accumulator output. In many physical systems, memory is directly associated with the storage of energy. For example, the capacitor in eq. (1.94) stores energy by accumulating electrical charge, represented as the integral of the current. Thus, the simple RC circuit in Example 1.8 and Figure 1.1 has memory physically stored in the capacitor. Similarly, the automobile in Figure 1.2 has memory stored in its kinetic energy. In discrete-time systems implemented with computers or digital microprocessors, memory is typically directly associated with storage registers that retain values between clock pulses. While the concept of memory in a system would typically suggest storing past input and output values, our formal definition also leads to our referring to a system as having memory if the current output is dependent on future values of the input and output. While systems having this dependence on future values might at first seem unnatural, they in fact form an important class of systems, as we discuss further in Section 1.6.3. 1.6.2 lnvertibility and Inverse Systems A system is said to be invertible if distinct inputs lead to distinct outputs. As illustrated in Figure 1.45(a ) for the discrete-time case, if a system is invertible, then an inverse system exists that, when cascaded with the original system, yields an output w[n] equal to the input x[n] to the first system. Thus, the series interconnection in Figure 1.45(a) has an overall input-output relationship which is the same as that for the identity system. An example of an invertible continuous-time system is y(t) = 2x(t), (1.97) for which the inverse system is 1 w(t) = 2: y(t). (1.98) This example is illustrated in Figure 1.45(b) . Another example of an invertible system is the accumulator of eq. (1.92). For this system, the difference between two successive 46 Signals and Systems Chap. 1 y[n] x[n] System ......- -...~~ w[n] = x[n] (a) x(t) (b) y(n] •I w(n] ~ y[n] - y(n -1] ~ w(n] x[n] x[n[ --I......__Y_[n_J_=_k _}_ _x_ _x[-kJ_ _. (c) Figure 1 .45 Concept of an inverse system for: (a) a general invertible sys- tem; (b) the invertible system described by eq. (1.97); (c) the invertible system defined in eq. (1.92). values of the output is precisely the last input value. Therefore, in this case, the inverse system is w[n] = y[n] - y[n - 1], (1.99) as illustrated in Figure 1.45( c). Examples of noninvertible systems are y[n] = 0, (1.100) that is, the system that produces the zero output sequence for any input sequence, and (1.101) in which case we cannot determine the sign of the input from knowledge of the output. The concept of invertibility is important in many contexts. One example arises in systems for encoding used in a wide variety of communications applications. In such a system, a signal that we wish to transmit is first applied as the input to a system known as an encoder. There are many reasons for doing this, ranging from the desire to encrypt the original message for secure or private communication to the objective of providing some redundancy in the signal (for example, by adding what are known as parity bits) so that any errors that occur in transmission can be detected and, possibly, corrected. For lossless coding, the input to the encoder must be exactly recoverable from the output; i.e., the encoder must be invertible. 1 .6.3 Causality A system is causal if the output at any time depends only on values of the input at the present time and in the past. Such a system is often referred to as being nonanticipative, as Sec. 1.6 Basic System Properties 47 the system output does not anticipate future values of the input. Consequently, if two inputs to a causal system are identical up to some point in time to or n0 , the corresponding outputs must also be equal up to this same time. The RC circuit of Figure 1.1 is causal, since the capacitor voltage responds only to the present and past values of the source voltage. Similarly, the motion of an automobile is causal, since it does not anticipate future actions of the driver. The systems described in eqs. (1.92)- (1.94) are also causal, but the systems defined by y[n] = x[n] - x[n + 1] (1.102) and y(t) = x(t + 1) (1.103) are not. All memory less systems are causal, since the output responds only to the current value of the input. Although causal systems are of great importance, they do not by any means constitute the only systems that are of practical significance. For example, causality is not often an essential constraint in applications in which the independent variable is not time, such as in image processing. Furthermore, in processing data that have been recorded previously, as often happens with speech, geophysical, or meteorological signals, to name a few, we are by no means constrained to causal processing. As another example, in many applications, including historical stock market analysis and demographic studies, we may be interested in determining a slowly varying trend in data that also contain high-frequency fluctuations about that trend. In this case, a commonly used approach is to average data over an interval in order to smooth out the fluctuations and keep only the trend. An example of a noncausal averaging system is 1 +M y[n] 2M + 1 k~ x[ n - k]. (1.104) M Example 1. 1 2 When checking the causality of a system, it is important to look carefully at the input- output relation. To illustrate some of the issues involved in doing this, we will check the causality of two particular systems. The first system is defined by y[n] = x[ -n]. (1.105) Note that the output y[n0 ] at a positive time no depends only on the value of the input signal x[- no] at time (- n0 ), which is negative and therefore in the past of no. We may be tempted to conclude at this point that the given system is causal. However, we should always be careful to check the input-output relation for all times. In particular, for n < 0, e.g. n = -4, we see that y[ -4] = x[4 ], so that the output at this time depends on a future value of the input. Hence, the system is not causal. It is also important to distinguish carefully the effects of the input from those of any other functions used in the definition of the system. For example, consider the system y(t) = x(t) cos(t + 1). (1.106) 48 Signals and Systems Chap. 1 In this system, the output at any time t equals the input at that same time multiplied by a number that varies with time. Specifically, we can rewrite eq. ( 1.1 06) as y(t) = x(t)g(t), where g(t) is a time-varying function, namely g(t) = cos(t + 1). Thus, only the current value of the input x(t) influences the current value of the output y(t), and we conclude that this system is causal (and, in fact, memoryless). 1 .6.4 Stability Stability is another important system property. Informally, a stable system is one in which small inputs lead to responses that do not diverge. For example, consider the pendulum in Figure 1.46(a), in which the input is the applied force x(t) and the output is the angular deviation y(t) from the vertical. In this case, gravity applies a restoring force that tends to return the pendulum to the vertical position, and frictional losses due to drag tend to slow it down. Consequently, if a small force x(t) is applied, the resulting deflection from vertical will also be small. In contrast, for the inverted pendulum in Figure 1.46(b ), the effect of gravity is to apply a force that tends to increase the deviation from vertical. Thus, a small applied force leads to a large vertical deflection causing the pendulum to topple over, despite any retarding forces due to friction. The system in Figure 1.46(a) is an example of a stable system, while that in Fig- ure 1.46(b) is unstable. Models for chain reactions or for population growth with unlim- ited food supplies and no predators are examples of unstable systems, since the system response grows without bound in response to small inputs. Another example of an unsta- ble system is the model for a bank account balance in eq. (1.86), since if an initial deposit is made (i.e., x[O] = a positive amount) and there are no subsequent withdrawals, then that deposit will grow each month without bound, because of the compounding effect of interest payments. x(t) (a) x(t) Figure 1 .46 (a) A stable pendulum; (b) (b) an unstable inverted pendulum. Sec. 1.6 Basic System Properties 49 There are also numerous examples of stable systems. Stability of physical systems generally results from the presence of mechanisms that dissipate energy. For example, assuming positive component values in the simple RC circuit of Example 1.8, the resistor dissipates energy and this circuit is a stable system. The system in Example 1.9 is also stable because of the dissipation of energy through friction. The preceding examples provide us with an intuitive understanding of the concept of stability. More formally, if the input to a stable system is bounded (i.e., if its magnitude does not grow without bound), then the output must also be bounded and therefore cannot diverge. This is the definition of stability that we will use throughout this book. For exam- ple, consider applying a constant force j(t) = F to the automobile in Figure 1.2, with the vehicle initially at rest. In this case the velocity of the car will increase, but not without bound, since the retarding frictional force also increases with velocity. In fact, the velocity will continue to increase until the frictional force exactly balances the applied force; so, from eq. (1.84), we see that this terminal velocity value V must satisfy _eV=_!__F (1.107) m m' i.e., v = !'__, (1.108) p As another example, consider the discrete-time system defined by eq. (1.1 04) , and suppose that the input x[n] is bounded in magnit~de by some number, say, B, for all values of n. Then the largest possible magnitude for y[n] is also B, because y[n] is the average of a finite set of values of the input. Therefore, y[n] is bounded and the system is stable. On the other hand, consider the accumulator described by eq. (1.92). Unlike the system in eq. ( 1.104 ), this system sums all of the past values of the input rather than just a finite set of values, and the system is unstable, since the sum can grow continually even if x[n] is bounded. For example, if the input to the accumulator is a unit step u[n], the output will be 11 y[n] :Z u[k] = (n + 1)u[n]. k =-'X That is, y[O] = 1, y[l] = 2, y[2] = 3, and so on, and y[n] grows without bound. Example 1 . 1 3 If we suspect that a system is unstable, then a useful strategy to verify this is to look for a specific bounded input that leads to an unbounded output. Finding one such example enables us to conclude that the given system is unstable. If such an example does not exist or is difficult to find, we must check for stability by using a method that does not utilize specific examples of input signals. To illustrate this approach, let us check the stability of two systems, sl: y(t) = tx(t) (1.109) 50 Signals and Systems Chap. 1 and (1.110) In seeking a specific counterexample in order to disprove stability, we might try simple bounded inputs such as a constant or a unit step. For system S1 in eq. (1.109), a constant input x(t) = 1 yields y(t) = t, which is unbounded, since no matter what finite con- stant we pick, iy(t)l will exceed that constant for some t. We conclude that system 5 1 is unstable. For system S2 , which happens to be stable, we would be unable to find a bounded input that results in an unbounded output. So we proceed to verify that all bounded inputs result in bounded outputs. Specifically, let B be an arbitrary positive number, and let x(t) be an arbitrary signal bounded by B; that is, we are making no assumption about x(t), except that ix(t)i < B, (1.111) or -B < x(t) < B, (1.112) for all t. Using the definition of S2 in eq. (1.110), we then see that if x(t) satisfies eq. (1.111), then y(t) must satisfy (1.113) We conclude that if any input to S2 is bounded by an arbitrary positive number B, the corresponding output is guaranteed to be bounded by e8 . Thus, S2 is stable. The system properties and concepts that we have introduced so far in this section are of great importance, and we will examine some of these in more detail later in the book. There remain, however, two additional properties-time invariance and linearity- that play a particularly central role in the subsequent chapters of the book, and in the remainder of this section we introduce and provide initial discussions of these two very important concepts. 1.6.5 Time lnvariance Conceptually, a system is time invariant if the behavior and characteristics of the system are fixed over time. For example, the RC circuit of Figure 1.1 is time invariant if the resistance and capacitance values R and C are constant over time: We would expect to get the same results from an experiment with this circuit today as we would if we ran the identical experiment tomorrow. On the other hand, if the values of R and C are changed or fluctuate over time, then we would expect the results of our experiment to depend on the time at which we run it. Similarly, if the frictional coefficient b and mass m of the automobile in Figure 1.2 are constant, we would expect the vehicle to respond identically independently of when we drive it. On the other hand, if we load the auto's trunk with heavy suitcases one day, thus increasing m, we would expect the car to behave differently than at other times when it is not so heavily loaded. The property of time in variance can be described very simply in terms of the signals and systems language that we have introduced. Specifically, a system is time invariant if Sec. 1.6 Basic System Properties 51 a time shift in the input signal results in an identical time shift in the output signal. That is, if y[n] is the output of a discrete-time, time-invariant system when x[n] is the input, then y [n- n0 ] is the output when x [n- n0 ] is applied. In continuous time with y(t) the output corresponding to the input x(t), a time-invariant system will have y (t- to) as the output when x (t - to) is the input. To see how to determine whether a system is time invariant or not, and to gain some insight into this property, consider the following examples: Example 1 . 1 4 Consider the continuous-time system defined by y(t) = sin [ x(t)]. (1.114) To check that this system is time invariant, we must determine whether the time- invariance property holds for any input and any time shift t0 • Thus, let x 1( t) be an arbitrary input to this system, and let (1.115) be the corresponding output. Then consider a second input obtained by shifting x 1 (t) in time: x2(t) = x, (t - to). (1.116) The output corresponding to this input is Y2(t) = sin [ x2(t)] = sin [ x, (t - to)]. (1.117) Similarly, from eq. (1.115), y,(t- to) = sin[ Xt (t- to)]. (1.118) Comparing eqs. (1.117) and (1.118), we see that y2(t) = y 1 (t- to), and therefore, this system is time invariant. Example 1 . 1 5 As a second example, consider the discrete-time system y[n] = nx[n]. (1.119) This is a time-varying system, a fact that can be verified using the same formal procedure as that used in the preceding example (see Problem 1.28). However, when a system is suspected of being time varying, an approach to showing this that is often very useful is to seek a counterexample-i.e., to use our intuition to find an input signal for which the condition of time invariance is violated. In particular, the system in this example represents a system with a time-varying gain. For example, if we know that the current input value is 1, we cannot determine the current output value without knowing the current time. Consequently, consider the input signal x 1 [n] = 8[n], which yields an output y 1 [n] that is identically 0 (since n8[n] = 0). However, the input x 2[n] = 8[n -1] yields the output y2[n] = n8[n- 1] = 8[n- 1]. Thus, while x 2[n] is a shifted version of x 1 [n], Y2[n] is not a shifted version of y 1 [n]. 52 Signals and Systems Chap. 1 While the system in the preceding example has a time-varying gain and as a result is a time-varying system, the system in eq. (1.97) has a constant gain and, in fact, is time invariant. Other examples of time-invariant systems are given by eqs. (1.91)-(1.104). The following example illustrates a time-varying system. Example 1 . 1 6 Consider the system y(t) = x(2t). (1.120) This system represents a time scaling. That is, y(t) is a time-compressed (by a factor of 2) version of x(t). Intuitively, then, any time shift in the input will also be compressed by a factor of 2, and it is for this reason that the system is not time invariant. To demon- strate this by counterexample, consider the input x 1( t) shown in Figure 1.47(a) and the resulting output y 1 (t) depicted in Figure 1.47(b). If we then shift the input by 2-i.e., consider x 2(t) = x 1 (t- 2), as shown in Figure 1.47(c)-we obtain the resulting output x1(t) ~(t) -2 2 -1 (a) (b) x2(t) = x1(t-2) Y2(t) 0 4 0 2 (c) (d) (e) Figure 1.47 (a) The input x1(t) to the system in Example 1.16; (b) the output Y1 (t) corresponding to x1 (t); (c) the shifted input x2(t) = x1 (t- 2); (d) the output Y2(t) corresponding to x2(t); (e) the shifted signal Y1 (t - 2). Note that Y2 ( t) =1= Y1 ( t - 2), showing that the system is not time invariant. Sec. 1.6 Basic System Properties 53 y2(t) = x 2(2t) shown in Figure 1.47(d). Comparing Figures 1.47(d) and (e), we see that yz(t) =I= y 1 (t- 2), so that the system is not time invariant. (In fact, y2(t) = y 1 (t- 1) , so that the output time shift is only half as big as it should be for time in variance, due to the time compression imparted by the system.) 1 .6.6 Linearity A linear system, in continuous time or discrete time, is a system that possesses the impor- tant property of superposition: If an input consists of the weighted sum of several signals, then the output is the superposition-that is, the weighted sum-of the responses of the system to each of those signals. More precisely, let y 1 (t) be the response of a continuous- time system to an input x 1 (t), and let y2(t) be the output corresponding to the input x 2(t). Then the system is linear if: 1. The response to x 1 (t) + x2(t) is Yl (t) + Y2(t). 2. The response to ax1( t) is ay1( t), where a is any complex constant. The first of these two properties is known as the additivity property; the second is known as the scaling or homogeneity property. Although we have written this description using continuous-time signals, the same definition holds in discrete time. The systems specified by eqs. (1.91)-(1.100), (1.102)-(1.104), and (1.119) are linear, while those defined by eqs. (1.101) and (1.114) are nonlinear. Note that a system can be linear without being time invariant, as in eq. (1.119), and it can be time invariant without being linear, as in eqs. (1.1 0 1) and (1.114). The two properties defining a linear system can be combined into a single statement: continuous time: ax1( t) + bx2(t) ~ ay1( t) + by2(t), (1.121) discrete time: ax1[ n] + bx2[n] ~ ay1 [n] + by2[n]. ( 1.122) Here, a and b are any complex constants. Furthermore, it is straightforward to show from the definition of linearity that if xdn], k = 1, 2, 3, ... , are a set of inputs to a discrete- time linear system with corresponding outputs ydn], k = 1, 2, 3, ... , then the response to a linear combination of these inputs given by x[n] = ~ akxdn] = a1 XJ [n] + a2x2[n] + a3x3[n] + . . . (1.123) k is y[n] = ~ akyk[n] = G(VJ [n] + a2y2[n] + a3y3[n] + .... ( 1.124) k This very important fact is known as the superposition property, which holds for linear systems in both continuous and discrete time. A direct consequence of the superposition property is that, for linear systems, an input which is zero for all time results in an output which is zero for all time. For example, if x[n] ~ y[n], then the homogeneity property tells us that 0 = 0 · x[n] ~ 0 · y[n] = 0. (1.125) 54 Signals and Systems Chap. 1 In the following examples we illustrate how the linearity of a given system can be checked by directly applying the definition of linearity. Example 1 . 1 7 Consider a systemS whose input x(t) and output y(t) are related by y(t) = tx(t) To determine whether or not Sis linear, we consider two arbitrary inputs x 1( t) and x2(t). Xt (t) ~ Yt (t) = tXt (t) x2(t) ~ Yz(t) = tx2(t) Let x3(t) be a linear combination of x 1( t) and x2(t). That is, X3(t) = ax1 (t) + bx2(t) where a and b are arbitrary scalars. If x3(t) is the input to S, then the corresponding output may be expressed as y3(t) = tx:,(t) = t(ax1 (t) + bx2(t)) = atx1 (t) + btx2(t) = ay1 (t) + byz(t) We conclude that the system S is linear. Example 1 . 1 8 Let us apply the linearity-checking procedure of the previous example to another system S whose input x(t) and output y(t) are related by y(t) = x 2(t) Defining x 1( t), xz(t), and x3(t) as in the previous example, we have Xt (t) ~ Yt (t) = xi(t) Xz(t) ~ Yz(t) = x~(t) and X3(t) ~ y3(t) = x~(t) = (ax1 (t) + bx 2 2(t)) = a2 xi(t) + b2 x~(t) + 2abxt (t)xz(t) = a2y 2 1( t) + b yz(t) + 2abxt (t)xz(t) Clearly, we can specify x 1( t), xz(t), a, and b such that y3(t) is not the same as ay1( t) + byz(t).Forexample,ifx1(t) = l,x2(t) = O,a = 2,andb = O,theny3(t) = (2x 1(t))2 = 4, but 2y 1( t) = 2(x1 (t))2 = 2. We conclude that the systemS is not linear. Example 1 . 1 9 In checking the linearity of a system, it is important to remember that the system must satisfy both the additivity and homogeneity properties and that the signals, as well as any scaling constants, are allowed to be complex. To emphasize the importance of these Sec. 1.6 Basic System Properties 55 points, consider the system specified by y[n] = (Re{x[n]}. (1.126) As shown in Problem 1.29, this system is additive; however, it does not satisfy the ho- mogeneity property, as we now demonstrate. Let x 1 [n] = r[n] + js[n] (1.127) be an arbitrary complex input with real and imaginary parts r[n] and s[n], respectively, so that the corresponding output is y, [n] = r[n]. (1.128) Now, consider scaling x 1 [n] by a complex number, for example, a = j; i.e., consider the input x2[n] = jx 1 [n] = j(r[n] + js[n]) (1.129) = -s[n] + jr[n]. The output corresponding to x2 [n] is Y2[n] = CRe{x2[n]} = -s[n], (1.130) which is not equal to the scaled version of y 1 [n], ay 1 [n] = jr[n]. (1.131) We conclude that the system violates the homogeneity property and hence is not linear. Example 1 .20 Consider the system y[n] = 2x[n] + 3. (1.132) This system is not linear, as can be verified in several ways. For example, the system violates the additivity property: If x 1 [n] = 2 and x2 [n] = 3, then x, [n] ~ y, [n] = 2x, [n] + 3 = 7, (1.133) x2[n] ~ Y2[n] = 2x2[n] + 3 = 9. (1.134) However, the response to x3[n] = x 1[n] + x2[n] is y3[n] = 2[x,[n] + x2[n]] + 3 = 13, (1.135) which does not equal y 1 [n] + y2[n] = 16. Alternatively, since y[n] = 3 if x[n] = 0, we see that the system violates the ""zero-in/zero-out"" property of linear systems given in eq. (1.125). It may seem surprising that the system in the above example is nonlinear, since eq. (1.132) is a linear equation. On the other hand, as depicted in Figure 1.48, the output of this system can be represented as the sum of the output of a linear system and another signal equal to the zero-input response of the system. For the system in eq. (1.132), the linear system is x[n] ~ 2x[n], and the zero-input response is Yo[n] = 3. 56 Signals and Systems Chap. 1 Yo (t) Linear x(t)--~1 system 1---......~ + ~-----~~ y(t) Figure 1.48 Structure of an incrementally linear system. Here, y0[n] is the zero-input response of the system. There are, in fact, large classes of systems in both continuous and discrete time that can be represented as in Figure 1.48-i.e., for which the overall system output consists of the superposition of the response of a linear system with a zero-input response. As shown in Problem 1.47, such systems correspond to the class of incrementally linear systems-i.e., systems in continuous or discrete time that respond linearly to changes in the input. In other words, the difference between the responses to any two inputs to an incrementally linear system is a linear (i.e., additive and homogeneous) function of the difference between the two inputs. For example, if x 1 [n] and x2[n] are two inputs to the system specified by eq. (1.132), and if y 1 [n] and y2 [n] are the corresponding outputs, then y, [n] - Y2[n] = 2x, [n] + 3 - {2x2[n] + 3} = 2 { x, [n] - x2[n]} . (1.136) 1.7 SUMMARY In this chapter, we have developed a number of basic concepts related to continuous-time and discrete-time signals and systems. We have presented both an intuitive picture of what signals and systems are through several examples and a mathematical representation for signals and systems that we will use throughout the book. Specifically, we introduced graphical and mathematical representations of signals and used these representations in performing transformations of the independent variable. We also defined and examined several basic signals, both in continuous time and in discrete time. These included com- plex exponential signals, sinusoidal signals, and unit impulse and step functions. In ad- dition, we investigated the concept of periodicity for continuous-time and discrete-time signals. In developing some of the elementary ideas related to systems, we introduced block diagrams to facilitate our discussions concerning the interconnection of systems, and we defined a number of important properties of systems, including causality, stability, time invariance, and linearity. The primary focus in most of this book will be on the class of linear, time-invariant (LTI) systems, both in continuous time and in discrete time. These systems play a par- ticularly important role in system analysis and design, in part due to the fact that many systems encountered in nature can be successfully modeled as linear and time invariant. Furthermore, as we shall see in the following chapters, the properties of linearity and time in variance allow us to analyze in detail the behavior of LTI systems. Chap. 1 Problems 57 Basic problems emphasize the mechanics of using concepts and methods in a man- ner similar to that illustrated in the examples that are solved in the text. Advanced problems explore and elaborate upon the foundations and practical im- plications of the textual material. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The next two sections contain problems belonging to the basic and advanced categories, respectively. A final section, Mathematical Review, pro- vides practice problems on the fundamental ideas of complex arithmetic and algebra. BASIC PROBLEMS WITH ANSWERS 1.1. Express each of the following complex numbers in Cartesian form (x + jy): ~ei1T, l2e - }1r , e.i1T12 , e- }1r!2 , e.i51T!2 ,y- fL2e .i1T14 ,-JfL2e .i91TI4 ,yfL2 e- j91TI4 ,yfL2 e- )1TI4 . ~ 1.2. Express each of the following complex numbers in polar form (re.i8, with - 'TT < () ~ 'TT): 5, -2, -3j, ~ - j J!' 1 + j, (1- j)2, j(l- j), (1 + j)/(1- j), ( h + Jfi)l (l+j}3). - - 1.3. Determine the values of P x and Ex for each of the following signals: (a) x (t) = e- 21 1 u(t) (b) x2(t) = e.i(:21+1T/4J (c) x3(t) = cos(t) (d) x1[n] = (~)''u[n] (e) x2[n] = e.i(1TI2n+1TI'd) (f) x3[n] = cos(*n) 1.4. Let x[n] be a signal with x[n] = 0 for n < -2 and n > 4. For each signal given below, determine the values of n for which it is guaranteed to be zero. (a) x[n - 3] (b) x[n + 4] (c) x[- n] (d) x[ -n + 2] (e) x[ -n- 2] 1.5. Let x(t) be a signal with x(t) = 0 fort < 3. For each signal given below, determine the values oft for which it is guaranteed to be zero. (a) x(l - t) (b) x(l - t) + x(2- t) (c) x(l - t)x(2 - t) (d) x(3t) (e) x(t/3) 1.6. Determine whether or not each of the following signals is periodic: (a) x 1(t) = 2e.i(1+1TI4lu(t) (b) x2[n] = u[n] + u[ -n] (c) x3[n] = ~7=-Y~{8[n- 4k]- 8[n- 1- 4k]} 1.7. For each signal given below, determine all the values of the independent variable at which the even part of the signal is guaranteed to be zero. (a) x 1 [n] = u[n] - u[n - 4] (b) x2(t) = sin( ~t) (c) x3[n] = (~)''u[n- 3] (d) x4(t) = e- 51~(t + 2) 1.8. Express the real part of each of the following signals in the form Ae-ar cos(wt + cp), where A, a, w, and cp are real numbers with A> 0 and -7r < cp ~ 'TT: (a) x 1( t) = -2 (b) x2(t) = heJ1TI4 cos(3t + 27T) (c) x3(t) = e- 1 sin(3t + 'TT) (d) x4 (t) = je(-2-r JIOO)t 1.9. Determine whether or not each of the following signals is periodic. If a signal is periodic, specify its fundamental period. (a) x 1(t) = jej\Ot (b) x2(t) = e(-l+iJr (c) x3(n) = ej71rn (d) x4[n] = 3ej31T!n+l/2)/5 (e) x5[n] = 3ei3/5(n+l/2) 58 Signals and Systems Chap. 1 1.10. Determine the fundamental period of the signal x(t) = 2cos(10t + 1)- sin(4t -1). 1.11. Determine the fundamental period of the signal x[n] = 1 + ej4mzn - ej2mz!S. 1.12. Consider the discrete-time signal x[n] = 1 - .L o[n - 1 - k]. k=3 Determine the values of the integers M and n0 so that x[n] may be expressed as x[n] = u[Mn- no]. 1.13. Consider the continuous-time signal x(t) = o(t + 2) - o(t - 2). Calculate the value of Eoo for the signal y(t) = tx X(T)dT. 1.14. Consider a periodic signal 0 ~ t ~ 1 l<t<2 with period T = 2. The derivative of this signal is related to the ""impulse train"" g(t) = .L o(t- 2k) k=-x with period T = 2. It can be shown that dx(t) ----;[( = A I g(t - tJ) + A2g(t - t2)· Determine the values of A 1, t1, A 2, and t2. 1.15. Consider a systemS with input x[ n] and output y[ n]. This system is obtained through a series interconnection of a system S1 followed by a system S2. The input-output relationships for S 1 and S2 are YI [n] = 2xi [n] + 4xi [n - 1], 1 Y2[n] = x2[n- 2] + 2x2[n- 3], where x 1 [n] and x2 [n] denote input signals. (a) Determine the input-output relationship for systemS. (b) Does the input-output relationship of systemS change if the order in which S1 and S2 are connected in series is reversed (i.e., if S2 follows SJ)? 1.16. Consider a discrete-time system with input x[n] and output y[n]. The input-output relationship for this system is y[n] = x[n]x[n - 2]. Chap. 1 Problems 59 (a) Is the system memoryless? (b) Determine the output of the system when the input is A8[n], where A is any real or complex number. (c) Is the system invertible? 1.17. Consider a continuous-time system with input x(t) and output y(t) related by y(t) = x(sin(t)). (a) Is this system causal? (b) Is this system linear? 1.18. Consider a discrete-time system with input x[n] and output y[n] related by n+n0 y[n] = L x[k], k= n-no where n0 is a finite positive integer. (a) Is this system linear? (a) Is this system time-invariant? (c) If x[n] is known to be bounded by a finite integer B (i.e., jx[n]j < B for all n), it can be shown that y[n] is bounded by a finite number C. We conclude that the given system is stable. Express C in terms of Band n0 . 1.19. For each of the following input-output relationships, determine whether the corre- sponding system is linear, time invariant or both. (a) y(t) = t2 x(t- 1) (b) y[n] = x 2 [n- 2] (c) y[n] = x[n + 1] - x[n- 1] (d) y[n] = Od{x(t)} 1.20. A continuous-time linear systemS with input x(t) and output y(t) yields the follow- ing input-output pairs: x(t) = ei2t ~ y(t) = ei3t, x(t) = e- i 2t ~ y(t) = e-131 . (a) If x 1( t) = cos(2t), determine the corresponding output y1 (t) for systemS. (b) If x2(t) = cos(2(t - ~)), determine the corresponding output y2(t) for sys- temS. BASIC PROBLEMS 1.21. A continuous-time signal x(t) is shown in Figure P1.21. Sketch and label carefully each of the following signals: (a) x(t- 1) (b) x(2- t) (c) x(2t + 1) (d) x(4- ~) (e) [x(t) + x(-t)]u(t) (f) x(t)[8(t + ~) - 8(t- ~)] 1.22. A discrete-time signal is shown in Figure P1.22. Sketch and label carefully each of the following signals: (a) x[n- 4] (b) x[3 - n] (c) x[3n] (d) x[3n + 1] (e) x[n]u[3 - n] (f) x[n - 2]8[n - 2] (g) ~x[n] + ~( -1) 11 x[n] (h) x[(n - 1)2] 60 Signals and Systems Chap. 1 2,....__ __ -2 0 2 -1 .l 1 .l 2 2 -1 0 1 2 3 4 5 n -1 Figure P1.21 Figure P1.22 1.23. Determine and sketch the even and odd parts of the signals depicted in Figure Pl.23. Label your sketches carefully. 1 2 (a) x(t) ~ -2 -1 1 (b) The line --..__The line x(t) = - 2t for t < 0 x(t) = t for t > 0 -1 (c) Figure P1.23 1.24. Determine and sketch the even and odd parts of the signals depicted in Figure P 1.24. Label your sketches carefully. Chap. 1 Problems 61 ···llllllt1 23 n (a) 3 2 n (b) 2 n Figure P 1 .24 1.25. Determine whether or not each of the following continuous-time signals is periodic. If the signal is periodic, determine its fundamental period. (a) x(t) = 3cos(4t+ }) (b) x(t) = ei(7Tf-l) (c) x(t) = [cos(2t- })f (d) x(t) = 8v{cos(47Tt)u(t)} (e) x(t) = 8v{sin(47Tt)u(t)} (f) x(t) = L e-<2t-n) u(2t - n) n= -oo 1.26. Determine whether or not each of the following discrete-time signals is periodic. If the signal is periodic, determine its fundamental period. (a) x[n] = sin( 6 ; n + 1) (b) x[n] = cos(i- 7T) (c) x[n] = cos(in2 ) (d) x[n] = cos(~n)cos(*n) (e) x[n] = 2cos(*n) +sin( in)- 2cos(~n + ~) 1.27. In this chapter, we introduced a number of general properties of systems. In partic- ular, a system may or may not be (1) Memoryless (2) Time invariant (3) Linear (4) Causal (S) Stable Determine which of these properties hold and which do not hold for each of the following continuous-time systems. Justify your answers. In each example, y(t) de- notes the system output and x(t) is the system input. 62 Signals and Systems Chap. 1 (a) y(t) = x(t - 2) + x(2 - t) (b) y(t) = [cos(3t)]x(t) t<O (c) y(t) = J! : x( T)dT (d) y(t) = { ~(t) + x(t - 2), t 2:: 0 0 x(t) < 0 (e) y(t) = { x'(t) + x(t - 2), x(t) 2:: 0 (f) y(t) = x(t/3) (g) y(t) = d~~~t) 1.28. Determine which of the properties listed in Problem 1.27 hold and which do not hold for each of the following discrete-time systems. Justify your answers. In each example, y[n] denotes the system output and x[n] is the system input. (a) y[n] = x[- n] (b) y[n] = x[n - 2] - 2x[n - 8] (c) y[n] = nx[n] (d) y[n] = Sv{x[n - 1]} x[n], n 2:: 1 { x[n], n 2:: 1 (e) y[n] = 0, n = 0 (f) y[n] = 0, n = 0 { x[n + 1], n ::::: -1 x[n], n ::::: -1 (g) y[n] = x[4n + 1] 1.29. (a) Show that the discrete-time system whose input x[n] and output y[n] are related by y[n] = ffi-e{x[n]} is additive. Does this system remain additive if its input- output relationship is changed to y[n] = ffi-e{ei7Tnl4 x[n]}? (Do not assume that x[n] is real in this problem.) (b) In the text, we discussed the fact that the property of linearity for a system is equivalent to the system possessing both the additivity property and homogene- ity property. Determine whether each of the systems defined below is additive and/or homogeneous. Justify your answers by providing a proof for each prop- erty if it holds or a counterexample if it does not. x[n]x[n-2] [ 1] .....1- 0 (i) y(t) = _l__[dx(t)]2 (ii) y[n] = x[n-1] ' X n- ' x(t) dt { 0, x[n - 1] = 0 1.30. Determine if each of the following systems is invertible. If it is, construct the inverse system. If it is not, find two input signals to the system that have the same output. (a) y(t) = x(t- 4) (b) y(t) = cos[x(t)] (C) y[n] = nx[n] (d) y(t) = cry: X( T)dT x[n - 1], n 2:: 1 (e) y[n] = 0, n = 0 (f) y[n] = x[n]x[n - 1] { x[n], n ::::: -1 (g) y[n] = x[l - n] (h) y(t) = cr£ e-(t-T) X( T)dT (i) y[n] = L~= -r£(4yz-k x[k] (j) y(t) = d~;t) (k) [n] = { x[n + 1], n 2:: 0 (I) y(t) = x(2t) y x[n], n ::::: -1 (m) y[n] = x[2n] (n) y[n] = { x[n/2], n even 0, nodd 1.31. In this problem, we illustrate one of the most important consequences of the prop- erties of linearity and time invariance. Specifically, once we know the response of a linear system or a linear time-invariant (LTI) system to a single input or the responses to several inputs, we can directly compute the responses to many other Chap. 1 Problems 63 input signals. Much of the remainder of this book deals with a thorough exploitation of this fact in order to develop results and techniques for analyzing and synthesizing LTI systems. (a) Consider an LTI system whose response to the signal x1( t) in Figure P1.31(a) is the signal y 1 ( t) illustrated in Figure P 1.31 (b). Determine and sketch carefully the response of the system to the input x2(t) depicted in Figure P1.31(c). (b) Determine and sketch the response of the system considered in part (a) to the input x 3(t) shown in Figure P1.31(d). 0 2 0 1 2 (a) (b) I I 2 3 4) t -1 0 1 2 -1 - (c) (d) Figure P 1. 31 ADVANCED PROBLEMS 1.32. Let x(t) be a continuous-time signal, and let Yt (t) = x(2t) and Y2(t) = x(t/2). The signal y 1 (t) represents a speeded up version of x(t) in the sense that the duration of the signal is cut in half. Similarly, y2(t) represents a slowed down version of x(t) in the sense that the duration of the signal is doubled. Consider the following statements: (1) If x(t) is periodic, then y1 (t) is periodic. (2) If y 1( t) is periodic, then x(t) is periodic. (3) If x(t) is periodic, then y2(t) is periodic. (4) If y2(t) is periodic, then x(t) is periodic. For each of these statements, determine whether it is true, and if so, determine the relationship between the fundamental periods of the two signals considered in the statement. If the statement is not true, produce a counterexample to it. 1.33. Let x[ n] be a discrete-time signal, and let x[n/2] n even y 1 [n] = x[2n] and Y2[n] = O, ' { n odd · 64 Signals and Systems Chap. 1 The signals y 1 [n] and y2 [n] respectively represent in some sense the speeded up and slowed down versions of x[n]. However, it should be noted that the discrete-time notions of speeded up and slowed down have subtle differences with respect to their continuous-time counterparts. Consider the following statements: (1) If x[n] is periodic, then y 1 [n] is periodic. (2) If y 1 [n] is periodic, then x[n] is periodic. (3) If x[n] is periodic, then y2[n] is periodic. (4) If y2[n] is periodic, then x[n] is periodic. For each of these statements, determine whether it is true, and if so, determine the relationship between the fundamental periods of the two signals considered in the statement. If the statement is not true, produce a counterexample to it. 1.34. In this problem, we explore several of the properties of even and odd signals. (a) Show that if x[n] is an odd signal, then +x ~ x[n] = 0. n=-x (b) Show that if x 1 [n] is an odd signal and x2[n] is an even signal, then x 1 [n]x2[n] is an odd signal. (c) Let x[n] be an arbitrary signal with even and odd parts denoted by Xe[n] = 8v{x[n]} and X0 [n] = 0d{x[n]}. Show that +x +oo +oo ~ x2[n] = ~ x;[n] + ~ x~[n]. n=-x n=-x n= -oo (d) Although parts (a)-(c) have been stated in terms of discrete-time signals, the analogous properties are also valid in continuous time. To demonstrate this, show that where Xe(t) and X0 (t) are, respectively, the even and odd parts of x(t). 1.35. Consider the periodic discrete-time exponential time signal x[n] = eim(27TIN)n. Show that the fundamental period of this signal is No = Nlgcd(m, N), where gcd(m, N) is the greatest common divisor of m and N-that is, the largest integer that divides both m and Nan integral number of times. For example, gcd(2, 3) = 1, gcd(2, 4) = 2, gcd(8, 12) = 4. Note that No = N if m and N have no factors in common. Chap. 1 Problems 65 1.36. Let x(t) be the continuous-time complex exponential signal x(t) = e.iwot with fundamental frequency w0 and fundamental period To = 27T!w0 . Consider the discrete-time signal obtained by taking equally spaced samples of x(t)-that is, x[n] = x(nT) = e.iwonT. (a) Show that x[n] is periodic if and only if TIT0 is a rational number-that is, if and only if some multiple of the sampling interval exactly equals a multiple of the period of x(t). (b) Suppose that x[n] is periodic-that is, that T p (P1.36-1) To q' where p and q are integers. What are the fundamental period and fundamental frequency of x[n]? Express the fundamental frequency as a fraction of w 0T. (c) Again assuming that T!T0 satisfies eq. (Pl.36-1), determine precisely how many periods of x(t) are needed to obtain the samples that form a single period of x[n]. 1.37. An important concept in many communications applications is the correlation be- tween two signals. In the problems at the end of Chapter 2, we will have more to say about this topic and will provide some indication of how it is used in practice. For now, we content ourselves with a brief introduction to correlation functions and some of their properties. Let x(t) and y(t) be two signals; then the correlation function is defined as ""'""""(!) = r~ x(t + T)y( T)dT. The function <f>xxCt) is usually referred to as the autocorrelationf unction of the signal x(t), while </>xy(t) is often called a cross-correlation function. (a) What is the relationship between </>xy(t) and </>yx(t)? (b) Compute the odd part of <f>xxU). (c) Suppose that y(t) = x(t + T). Express </>xy(t) and </>yy(t) in terms of <f>xx(t). 1.38. In this problem, we examine a few of the properties of the unit impulse function. (a) Show that 1 8(2t) = 28(t). Hint: Examine Dtl(t). (See Figure 1.34.) (b) In Section 1.4, we defined the continuous-time unit impulse as the limit of the signal Dtl(t). More precisely, we defined several of the properties of 8(t) by examining the corresponding properties of Dtl(t). For example, since the signal ULI(t) = t% ih(T)dT 66 Signals and Systems Chap. 1 converges to the unit step u(t) = lim Ut,.(f), (Pl.38-1) t. ---->0 we could interpret B(t) through the equation u(t) ~ Loc li(T)dT or by viewing B(t) as the formal derivative of u(t). This type of discussion is important, as we are in effect trying to define B(t) through its properties rather than by specifying its value for each t, which is not possible. In Chapter 2, we provide a very simple characterization of the behavior of the unit impulse that is extremely useful in the study of linear time- invariant systems. For the present, however, we concentrate on demonstrating that the important concept in using the unit impulse is to understand how it behaves. To do this, consider the six signals depicted in Figure P1.38. Show rl (t) r~(t) ~m 1! D t1 t1 ~ 2~ 2 2 (a) (b) r~ (t) ri (t) (c) (d) r; (t) 2 ~ -~ (e) (f) Figure P1.38 Chap. 1 Problems 67 that each ""behaves like an impulse"" as Ll ~ 0 in that, if we let u~(t) = L~ ,-it,(r)dr, then lim u~ (t) = u(t). Ll-->0 In each case, sketch and label carefully the signal u~ (t). Note that ri (0) = ri (0) = 0 for all Ll. Therefore, it is not enough to define or to think of o(t) as being zero fort =/= 0 and infinite fort = 0. Rather, it is properties such as eq. (P1.38-1) that define the impulse. In Section 2.5 we will define a whole class of signals known as singularity functions, which are related to the unit impulse and which are also defined in terms of their properties rather than their values. 1.39. The role played by u(t), o(t), and other singularity functions in the study of linear time-invariant systems is that of an idealization of a physical phenomenon, and, as we will see, the use of these idealizations allow us to obtain an exceedingly impor- tant and very simple representation of such systems. In using singularity functions, we need, however, to be careful. In particular, we must remember that they are ideal- izations, and thus, whenever we perform a calculation using them, we are implicitly assuming that this calculation represents an accurate description of the behavior of the signals that they are intended to idealize. To illustrate, consider the equation x(t)o(t) = x(O)o(t). (P1.39-l) This equation is based on the observation that x(t)o11 (t) = x(O)o11 (t). (P1.39-2) Taking the limit of this relationship then yields the idealized one given by eq. (P1.39-1). However, a more careful examination of our derivation of eq. (P1.39-2) shows that that equation really makes sense only if x(t) is continuous at t = 0. If it is not, then we will not have x(t) = x(O) for t small. To make this point clearer, consider the unit step signal u(t). Recall from eq. (1.70) that u(t) = 0 fort < 0 and u(t) = 1 fort > 0, but that its value at t = 0 is not defined. [Note, for example, that u11(0) = 0 for all Ll, while ui(O) = 4 (from Problem 1.38(b)).] The fact that u(O) is not defined is not particularly bothersome, as long as the calculations we perform using u(t) do not rely on a specific choice for u(O). For example, if f(t) is a signal that is continuous at t = 0, then the value of roooo f(u)u(u)da does not depend upon a choice for u(O). On the other hand, the fact that u(O) is undefined is significant in that it means that certain calculations involving singular- ity functions are undefined. Consider trying to define a value for the product u(t)o(t). 68 Signals and Systems Chap. 1 To see that this cannot be defined, show that lim [u 11 (t)o(t)] = 0, 11~o but In general, we can define the product of two signals without any difficulty, as long as the signals do not contain singularities (discontinuities, impulses, or the other singularities introduced in Section 2.5) whose locations coincide. When the locations do coincide, the product is undefined. As an example, show that the signal +oc g(t) = J- oc U(T)O(t- T)dT is identical to u(t); that is, it is 0 fort < 0, it equals 1 fort > 0, and it is undefined fort = 0. 1.40. (a) Show that if a system is either additive or homogeneous, it has the property that if the input is identically zero, then the output is also identically zero. (b) Determine a system (either in continuous or discrete time) that is neither ad- ditive nor homogeneous but which has a zero output if the input is identically zero. (c) From part (a), can you conclude that if the input to a linear system is zero be- tween times t1 and t2 in continuous time or between times n1 and n2 in discrete time, then its output must also be zero between these same times? Explain your answer. 1.41. Consider a systemS with input x[n] and output y[n] related by y[n] = x[n]{g[n] + g[n- 1]}. (a) If g[n] = 1 for all n, show that Sis time invariant. (b) If g[n] = n, show that Sis not time invariant. (c) If g[n] = 1 + ( -l)n, show that Sis time invariant. 1.42. (a) Is the following statement true or false? The series interconnection of two linear time-invariant systems is itself a linear, time-invariant system. Justify your answer. (b) Is the following statement true or false? The series interconnection of two nonlinear systems is itself nonlinear. Justify your answer. (c) Consider three systems with the following input-output relationships: System 1: y[n] = { x[n/2], n even 0, n odd ' Chap. 1 Problems 69 + 1 + 1 System 2: y[n] = x[n] 2x[n- 1] 4x[n- 2], System 3: y[n] = x[2n]. Suppose that these systems are connected in series as depicted in Figure P1.42. Find the input-output relationship for the overall interconnected system. Is this system linear? Is it time invariant? y[n] Figure P1.42 1.43. (a) Consider a time-invariant system with input x(t) and output y(t). Show that if x(t) is periodic with period T, then so is y(t). Show that the analogous result also holds in discrete time. (b) Give an example of a time-invariant system and a nonperiodic input signal x(t) such that the corresponding output y(t) is periodic. 1.44. (a) Show that causality for a continuous-time linear system is equivalent to the following statement: For any time to and any input x(t) such that x(t) = 0 fort < t0, the correspond- ing output y(t) must also be zero fort < t0 . The analogous statement can be made for a discrete-time linear system. (b) Find a nonlinear system that satisfies the foregoing condition but is not causal. (c) Find a nonlinear system that is causal but does not satisfy the condition. (d) Show that invertibility for a discrete-time linear system is equivalent to the following statement: The only input that produces y[n] = 0 for all n is x[n] = 0 for all n. The analogous statement is also true for a continuous-time linear system. (e) Find a nonlinear system that satisfies the condition of part (d) but is not invert- ible. 1.45. In Problem 1.37, we introduced the concept of correlation functions. It is often im- portant in practice to compute the correlation function cf>hx(t), where h(t) is a fixed given signal, but where x(t) may be any of a wide variety of signals. In this case, what is done is to design a systemS with input x(t) and output cf>hx(t). (a) IsS linear? IsS time invariant? IsS causal? Explain your answers. (b) Do any of your answers to part (a) change if we take as the output cf>xh(t) rather than cf>hx(t)? 1.46. Consider the feedback system of Figure P1.46. Assume that y[n] = 0 for n < 0. + 'i2 e [n] I x[n] ----t•~..~. ~ ----_-_.._ ...· ~-· -y-[n_]_=_e-[n---1]-~~~~~.~.. ... ---!•~ y[n] Figure P1.46 70 Signals and Systems Chap. 1 (a) Sketch the output when x[n] = 8[n]. (b) Sketch the output when x[n] = u[n]. 1.47. (a) LetS denote an incrementally linear system, and let x 1 [n] be an arbitrary input signal to S with corresponding output y 1 [n]. Consider the system illustrated in Figure Pl.47(a). Show that this system is linear and that, in fact, the overall input-output relationship between x[n] and y[n] does not depend on the partic- ular choice of x 1 [ n]. (b) Use the result of part (a) to show that Scan be represented in the form shown in Figure 1.48. (c) Which ofthe following systems are incrementally linear? Justify your answers, and if a system is incrementally linear, identify the linear system Land the zero- input response y0 [n] or y0(t) for the representation of the system as shown in Figure 1.48. (i) y[n] = n + x[n] + 2x[n + 4] n/2, n even (ii) [ ] (n-1 )/2 Y n = { (n - 1)/2 + k~-n x[k], n odd x[n] ·+~ ·I s :cp )t y[n] (a) x1[n] Y1[n] t X (t) .~ w (t) ·I y(t) ~ d~?) )t y (t) (b) cos (1rn) v [n] z [n] z [n] v2 = [n] + x [n] ~y[n] w [n] w [n] 2 = x [n] (c) Figure P1.47 Chap. 1 Problems 71 (. "") [ ] { x[n] - x[n - 1] + 3, if x[O] 2: 0 111 Y n = x[n] - x[n- 1] - 3, if x[O] < 0 (iv) The system depicted in Figure P1.47(b). (v) The system depicted in Figure P1.47(c). (d) Suppose that a particular incrementally linear system has a representation as in Figure 1.48, with L denoting the linear system and y0 [n] the zero-input re- sponse. Show that Sis time invariant if and only if Lis a time-invariant system and y0 [ n] is constant. MATHEMATICAL REVIEW The complex number z can be expressed in several ways. The Cartesian or rectangular form for z is Z =X+ jy, where j = J=1 and x andy are real numbers referred to respectively as the real part and the imaginary part of z. As we indicated earlier, we will often use the notation x = CRe{z}, y = 9m{z}. The complex number z can also be represented in polar form as z = rej8 , where r > 0 is the magnitude of z and (} is the angle or phase of z. These quantities will often be written as r = lzi, 8 = <t:z. The relationship between these two representations of complex numbers can be de- termined either from Euler s relation, eje = cos(} + j sin 8, or by plotting z in the complex plane, as shown in Figure P1.48, in which the coordinate axes are CRe{z} along the horizontal axis and 9m{z} along the vertical axis. With respect to this graphical representation, x andy are the Cartesian coordinates of z, and rand (} are its polar coordinates. !1m y Figure P1.48 72 Signals and Systems Chap. 1 1.48. Let zo be a complex number with polar coordinates (r0, 00 ) and Cartesian coordi- nates (x0, y0 ). Determine expressions for the Cartesian coordinates of the following complex numbers in terms of x0 and YO· Plot the points zo, Zt, Z2, Z3, Z4, and zs in the complex plane when r0 = 2 and 00 = TTI4 and when r0 = 2 and 00 = TTI2. Indicate on your plots the real and imaginary parts of each point. (a) Zt = roe- J&o (b) Z2 = ro (c) Z3 = roef(&o+1T) (d) Z4 = roei(-&o+1T) (e) zs = roe.i(&o+21T) 1.49. Express each of the following complex numbers in polar form, and plot them in the complex plane, indicating the magnitude and angle of each number: (a) 1 + jj3 (b) -5 (c) -5- 5j (d) 3 + 4j (e) (1 - j}3)3 (f) (1 + j)s ( ) ("" ':l3J + ·3)(1 _ "") (h) 2- j(6/jj) (i) I+ Jfi g 1 1 2+ j(6Jjj) J3+ .i G) j(l + j)e.i7TI6 (k) ( j3 + j)2 j2e- j7T/4 (I) eirrl~-1 I+Jfi 1.50. (a) Using Euler's relationship or Figure Pl.48, determine expressions for x andy in terms of r and (J. (b) Determine expressions for r and (J in terms of x and y. (c) If we are given only rand tan 0, can we uniquely determine x andy? Explain your answer. 1.51. Using Euler's relation, derive the following relationships: (a) cos (J = ~(e.i8 + e- .i8) (b) sin (J = -d-J(e.i8 - e- .i8 ) (c) cos2 (J = ~(1 + cos 20) (d) (sinO)(sin<f>) = ~ cos((J- </>)- ~ cos((J + </>) (e) sin( (J + </>) = sin (J cos </> + cos (J sin </> 1.52. Let z denote a complex variable; that is, z = x + jy = re.i8 . The complex conjugate of z is z* = x- jy = re- J&_ Derive each of the following relations, where z, z1, and z2 are arbitrary complex numbers: (a) zz* = r 2 (b) ~ = e.i2& (c) z + z* = 2CRe{z} (d) z - z* = 2jdm{z} (e) (zt + z2)* = z~ + z; (f) (l!Zt z2)* ~~ az~ z;, where a is any real number (g) (:-'- )* = ~ .... 2 ...... 2 (h) Cfl~{~} = _!_[z1z~+~:)'z2] .c2 2 ~2""-2 1.53. Derive the following relations, where z, z1, and z2 are arbitrary complex numbers: (a) (e2)* = e2* (b) ztz; + z~z2 = 2Cfl~{ztz;} = 2eR~{z~z2} Chap. 1 Problems 73 (c) lzl = lz*l (d) lz1 z2l = lz1llz2l (e) CRe{z} :::; lzl, dm{z} :::; lzl (f) lz1z; + ziz2l :::; 21z1z2l (g) (lzii - lz2l)2 :::; lz1 + z2l2 :::; (lzii + lz2l? 1.54. The relations considered in this problem are used on many occasions throughout the book. (a) Prove the validity of the following expression: NL-1a n { N a= 1 = l~aN, n=O for any complex number a =I= 1 · 1-a This is often referred to as the finite sum formula. (b) Show that if Ia I < 1, then 00 1 Lan = 1-a· n=O This is often referred to as the infinite sum formula. (c) Show also if lal < 1, then 20.0: nan = __a _-=- n=O (1 - a)2. (d) Evaluate assuming that Ia I < 1. 1.55. Using the results from Problem 1.54, evaluate each of the following sums and ex- press your answer in Cartesian (rectangular) form: (a) L~=Oej1Tnl2 (b) L~=-2ej1Tnl2 (c) ""L:=o(~)nej1Tn!2 (d) ""L:=2(~)nej1Tn/2 (e) L~=O cos(~n) (f) ""L:=o(~)n cos(~n) 1.56. Evaluate each of the following integrals, and express your answer in Cartesian (rect- angular) form: (a) fo4ejml2dt (b) f 6 12 0 ejm dt (c) f 8 2 ejm12dt (d) fooc e-(1 + j)t dt (e) fooc e-t cos(t)dt (f) f oc e-2t sin(3t)dt 0 2 LINEAR TIME-INVARIANT SYSTEMS 2.0 INTRODUCTION In Section 1.6 we introduced and discussed a number of basic system properties. Two of these, linearity and time invariance, play a fundamental role in signal and system analysis for two major reasons. First, many physical processes possess these properties and thus can be modeled as linear time-invariant (LTI) systems. In addition, LTI systems can be analyzed in considerable detail, providing both insight into their properties and a set of powerful tools that form the core of signal and system analysis. A principal objective of this book is to develop an understanding of these proper- ties and tools and to provide an introduction to several of the very important applications in which the tools are used. In this chapter, we begin the development by deriving and examining a fundamental and extremely useful representation for LTI systems and by in- traducing an important class of these systems. One of the primary reasons LTI systems are amenable to analysis is that any such system possesses the superposition property described in Section 1.6.6. As a consequence, if we can represent the input to an LTI system in terms of a linear combination of a set of basic signals, we can then use superposition to compute the output of the system in terms of its responses to these basic signals. As we will see in the following sections, one of the important characteristics of the unit impulse, both in discrete time and in continuous time, is that very general signals can be represented as linear combinations of delayed impulses. This fact, together with the properties of superposition and time invariance, will allow us to develop a complete characterization of any LTI system in terms of its response to a unit impulse. Such a representation, referred to as the convolution sum in the discrete-time case and the convo- lution integral in continuous time, provides considerable analytical convenience in dealing 74 Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 75 with LTI systems. Following our development of the convolution sum and the convolution integral we use these characterizations to examine some of the other properties of LTI sys- tems. We then consider the class of continuous-time systems described by linear constant- coefficient differential equations and its discrete-time counterpart, the class of systems described by linear constant-coefficient difference equations. We will return to examine these two very important classes of systems on a number of occasions in subsequent chap- ters. Finally, we will take another look at the continuous-time unit impulse function and a number of other signals that are closely related to it in order to provide some additional insight into these idealized signals and, in particular, to their use and interpretation in the context of analyzing LTI systems. 2.1 DISCRETE-TIME LTI SYSTEMS: THE CONVOLUTION SUM 2. 1 . 1 The Representation of Discrete-Time Signals in Terms of Impulses The key idea in visualizing how the discrete-time unit impulse can be used to construct any discrete-time signal is to think of a discrete-time signal as a sequence of individual im- pulses. To see how this intuitive picture can be turned into a mathematical representation, consider the signal x[n] depicted in Figure 2.1(a). In the remaining parts of this figure, we have depicted five time-shifted, scaled unit impulse sequences, where the scaling on each impulse equals the value of x[n] at the particular instant the unit sample occurs. For example, x[-1]8[n + 1] = { x[- 1], n = -1 0, n -::/= -1 ' x[O]o[n] = { x[O], n = 0 0, n-::/= 0' x[l]o[n- 1] = { x[ 1], n = 1 0, n¥=1"" Therefore, the sum of the five sequences in the figure equals x[n] for -2 ::; n ::; 2. More generally, by including additional shifted, scaled impulses, we can write x[n] = ... + x[- 3]8[n + 3] + x[ -2]8[n + 2] + x[ -1]8[n + 1] + x[O]o[n] (2.1) + x[1]8[n- 1] + x[2]8[n - 2] + x[3]8[n- 3] + .... For any value of n, only one of the terms on the right-hand side of eq. (2.1) is nonzero, and the scaling associated with that term is precisely x[n]. Writing this summation in a more compact form, we have +x x[n] = ~ x[k]o[n- k]. (2.2) k=-X This corresponds to the representation of an arbitrary sequence as a linear combination of shifted unit impulses o[n- k], where the weights in this linear combination are x[k]. As an example, consider x[n] = u[n], the unit step. In this case, since u[k] = 0 for k < 0 76 Linear Time-Invariant Systems Chap. 2 x[n] n (a) x[ -2] o[n + 2] -4•- 3•- I 2-1• •0 1• 2• 3• 4• n (b) x[-1] o[n + 1] -1 -4•- 3• -2• I • • • 0 1• 2 3 4• n (c) I x[O] o[n] -4•- 3•- 2•- 1• 0 •1 2• 3• •4 n (d) x[1] o[n-1] I -4•- 3•- 2•- 1• •0 •2 •3 4• n (e) x[2] o[n-2] 2 -4•- 3•- 2•- 1• •0 •1 3•I 4• n Figure 2.1 Decomposition of a discrete-time signal into a weighted (f) sum of shifted impulses. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 77 and u[k] = 1 for k 2:: 0, eq. (2.2) becomes +x u[n] = L B[n - k], k=O which is identical to the expression we derived in Section 1.4. [See eq. (1.67).] Equation (2.2) is called the sifting property of the discrete-time unit impulse. Be- cause the sequence B[n - k] is nonzero only when k = n, the summation on the right- hand side of eq. (2.2) ""sifts"" through the sequence of values x[k] and preserves only the value corresponding to k = n. In the next subsection, we will exploit this representa- tion of discrete-time signals in order to develop the convolution -sum representation for a discrete-time LTI system. 2.1.2 The Discrete-Time Unit Impulse Response and the Convolution- Sum Representation of LTI Systems The importance of the sifting property of eqs. (2.1) and (2.2) lies in the fact that it repre- sents x[n] as a superposition of scaled versions of a very simple set of elementary functions, namely, shifted unit impulses B[n- k], each of which is nonzero (with value 1) at a single point in time specified by the corresponding value of k. The response of a linear system to x[n] will be the superposition of the scaled responses of the system to each of these shifted impulses. Moreover, the property of time in variance tells us that the responses of a time-invariant system to the time-shifted unit impulses are simply time-shifted versions of one another. The convolution -sum representation for discrete-time systems that are both linear and time invariant results from putting these two basic facts together. More specifically, consider the response of a linear (but possibly time-varying) sys- tem to an arbitrary input x[n]. We can represent the input through eq. (2.2) as a linear combination of shifted unit impulses. Let hk[n] denote the response of the linear system to the shifted unit impulse B[n - k]. Then, from the superposition property for a linear system [eqs. (1.123) and (1.124)], the response y[n] of the linear system to the input x[n] in eq. (2.2) is simply the weighted linear combination of these basic responses. That is, with the input x[n] to a linear system expressed in the form of eq. (2.2), the output y[n] can be expressed as +oo y[n] = L x[k]hk[n]. (2.3) k= -00 Thus, according to eq. (2.3), if we know the response of a linear system to the set of shifted unit impulses, we can construct the response to an arbitrary input. An interpreta- tion of eq. (2.3) is illustrated in Figure 2.2. The signal x[n] is applied as the input to a linear system whose responses h_ 1 [n], h0 [n], and h1 [n] to the signals B[n + 1], B[n], and B[n- 1], respectively, are depicted in Figure 2.2(b). Since x[n] can be written as a linear combination of B[n + 1], B[n], and B[n- 1], superposition allows us to write the response to x[n] as a linear combination of the responses to the individual shifted impulses. The individual shifted and scaled impulses that constitute x[n] are illustrated on the left-hand side of Figure 2.2( c), while the responses to these component signals are pictured on the right-hand side. In Figure 2.2(d) we have depicted the actual input x[n], which is the sum of the components on the left side of Figure 2.2(c) and the actual output y[n], which, by 78 Linear Time-Invariant Systems Chap. 2 x[n] -1 n (a) h0 [n] n n n (b) Figure 2.2 Graphical interpretation of the response of a discrete-time linear system as expressed in eq. (2.3). superposition, is the sum of the components on the right side of Figure 2.2(c). Thus, the response at time n of a linear system is simply the superposition of the responses due to the input value at each point in time. In general, of course, the responses hdn] need not be related to each other for differ- ent values of k. However, if the linear system is also time invariant, then these responses to time-shifted unit impulses are all time-shifted versions of each other. Specifically, since B[n- k] is a time-shifted version of B[n], the response hk[n] is a time-shifted version of h0 [n]; i.e., (2.4) For notational convenience, we will drop the subscript on h0 [n] and define the unit impulse (sample) response h[n] = h0 [n]. (2.5) That is, h[n] is the output ofthe LTI system when B[n] is the input. Then for an LTI system. eq. (2.3) becomes +x y[n] ~ x[k]h[n - k]. (2.6) k=-X This result is referred to as the convolution sum or superposition sum, and the oper- ation on the right-hand side of eq. (2.6) is known as the convolution of the sequences x[n] and h[n]. We will represent the operation of convolution symbolically as y[n] = x[n] * h[n]. (2.7) Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 79 x[-1]o[n +1] ... n n x[O]o[n] x[O] h0[n] ... t. ... .. tt.t .. 0 n lo n x[1] o[n-1] ... ll ... 0 n n (c) x[n] y(n] T n 0 n (d) Figure 2.2 Continued Note that eq. (2.6) expresses the response of an LTI system to an arbitrary input in terms of the system's response to the unit impulse. From this, we see that an LTI system is completely characterized by its response to a single signal, namely, its response to the unit impulse. The interpretation of eq. (2.6) is similar to the one we gave for eq. (2.3), where, in the case of an LTI system, the response due to the input x[k] applied at time k is x[k]h[n- k]; i.e., it is a shifted and scaled version (an ""echo"") of h[n]. As before, the actual output is the superposition of all these responses. Linear Time-Invariant Systems Chap.2 Example 2.1 Consider an LTI system with impulse response h[n] and input x[n], as illustrated in Figure 2.3(a). For this case, since only x[O] and x[1] are nonzero, eq. (2.6) simplifies to the expression y[n] = x[O]h[n- 0] + x[1]h[n - 1] = 0.5h[n] + 2h[n- 1]. (2.8) The sequences 0.5h[n] and 2h[n- 1] are the two echoes of the impulse response needed for the superposition involved in generating y[n]. These echoes are displayed in Fig- ure 2.3(b). By summing the two echoes for each value of n, we obtain y[n], which is shown in Figure 2.3(c). h[n] 11 • • I I 0 2 • • n 21 x[n] 0.5 • • T 0 • • • n (a) 0.5h[n] 0.5 • • T T T 0 2 • • n 2h[n-1] 21 • • • 1 1 • 0 2 3 n (b) 251 y[n] o.5T • • Ir 0 2 3 • n (c) Figure 2.3 (a) The impulse response h[n] of an LTI system and an input x[n] to the system; (b) the responses or ""echoes,"" 0.5h[n] and 2h[n - 1], to the nonzero values of the input, namely, x[O] = 0.5 and x[1] = 2; (c) the overall response y[n], which is the sum of the echos in (b). Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 81 By considering the effect of the superposition sum on each individual output sample, we obtain another very useful way to visualize the calculation of y[ n] using the convolution sum. In particular, consider the evaluation of the output value at some specific time n. A particularly convenient way of displaying this calculation graphically begins with the two signals x[k] and h[n - k] viewed as functions of k. Multiplying these two functions, we obtain a sequence g[k] = x[k]h[n - k], which, at each time k, is seen to represent the contribution of x[ k] to the output at time n. We conclude that summing all the samples in the sequence of g[k] yields the output value at the selected time n. Thus, to calculate y[n] for all values of n requires repeating this procedure for each value of n. Fortunately, changing the value of n has a very simple graphical interpretation for the two signals x[k] and h[n- k], viewed as functions of k. The following examples illustrate this and the use of the aforementioned viewpoint in evaluating convolution sums. Example 2.2 Let us consider again the convolution problem encountered in Example 2.1. The se- quence x[k] is shown in Figure 2.4(a), while the sequence h[n - k], for n fixed and viewed as a function of k, is shown in Figure 2.4(b) for several different values of n. In sketching these sequences, we have used the fact that h[n- k] (viewed as a function of k with n fixed) is a time-reversed and shifted version of the impulse response h[k]. In particular, as k increases, the argument n - k decreases, explaining the need to perform a time reversal of h[k]. Knowing this, then in order to sketch the signal h[n- k], we need only determine its value for some particular value of k. For example, the argument n - k will equal 0 at the value k = n. Thus, if we sketch the signal h[- k], we can obtain the signal h[n - k] simply by shifting to the right (by n) if n is positive or to the left if n is negative. The result for our example for values of n < 0, n = 0, 1, 2, 3, and n > 3 are shown in Figure 2.4(b). Having sketched x[k] and h[n - k] for any particular value of n, we multiply these two signals and sum over all values of k. For our example, for n < 0, we see from Figure 2.4 that x[k]h[n- k] = 0 for all k, since the nonzero values of x[k] and h[n- k] do not overlap. Consequently, y[n] = 0 for n < 0. For n = 0, since the product of the sequence x[k] with the sequence h[O- k] has only one nonzero sample with the value 0.5, we conclude that X y[O] = L x[k]h[O- k] = 0.5. (2.9) k=-X The product of the sequence x[k] with the sequence h[1 - k] has two nonzero samples, which may be summed to obtain y[1] = L x[k]h[1 - k] = 0.5 + 2.0 = 2.5. (2.10) k= -X Similarly, y[2] = L x[k]h[2- k] = 0.5 + 2.0 = 2.5, (2.11) k=-YO and 82 Linear Time-Invariant Systems Chap.2 x[k] . 21 0 .5, • - 0 • • • k (a) f h[n-k], n<O J J n-2 n-1 n •0 • • • k h[O-k] -2 -1 0 • • • k h[1-k] -1 0 • • k h[2-k] • 0 2 • k h[3-k] • • J J 0 2 3 k h[n-k], n>3 • • • • J J J 0 n-2 n-1 n k (b) Figure 2.4 Interpretation of eq. (2.6) for the signals h[n] and x[n] in Fig- ure 2.3; (a) the signal x[k] and (b) the signal h[n - k] (as a function of k with n fixed) for several values of n (n < 0; n = 0, 1, 2, 3; n > 3). Each of these signals is obtained by reflection and shifting of the unit impulse re- sponse h[k]. The response y[n] for each value of n is obtained by multiplying the signals x[k] and h[n- k] in (a) and (b) and then summing the products over all values of k. The calculation for this example is carried out in detail in Example 2.2. y[3] = L x[k]h[3 - k] = 2.0. (2.12) k =-X Finally, for n > 3, the product x[k]h[n - k] is zero for all k, from which we conclude that y[n] = 0 for n > 3. The resulting output values agree with those obtained in Exam- ple 2.1. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 83 Example 2.3 Consider an input x[n] and a unit impulse response h[n] given by x[n] = anu[n], h[n] = u[n], with 0 < a < 1. These signals are illustrated in Figure 2.5. Also, to help us in visualizing and calculating the convolution of the signals, in Figure 2.6 we have depicted the signal x[k] followed by h[- k], h[ -1- k], and h[1- k] (that is, h[n- k] for n = 0, -1, and+ 1) and, finally, h[n- k] for an arbitrary positive value of nand an arbitrary negative value of n. From this figure, we note that for n < 0, there is no overlap between the nonzero points in x[k] and h[n - k]. Thus, for n < 0, x[k]h[n- k] = 0 for all values of k, and hence, from eq. (2.6), we see that y[n] = 0, n < 0. For n 2:: 0, x[k]h[n - k] = { ak, 0 s;. k. s;. n . 0, otherwise x[n] = anu[n] 1 ........... llliiiiilttttttttt 0 n (a) 0 n (b) Figure 2.5 The signals x[n] and h[n] in Example 2.3. Thus, for n 2:: 0, n y[n] = Lak, k=O and using the result of Problem 1.54 we can write this as n 1 -an+! y[n] = Lak = --- for n 2:: 0. (2.13) k=O 1 -a Thus, for all n, 1 an+!) y[n] = ( _ a u[n]. 1 The signal y[n] is sketched in Figure 2.7. 84 I Linear Time-Invariant Systems Chap. 2 x[k] ~ a'u[k] .............. llliiiiiJitttttttT ... 0 k (a) h[-k] ... JiliiJiliil I. ................... . 0 k (b) ... Illllll Hill. :~.:.k: ... .. .. .. . .. ... -1 0 k (c) h[1-k] ... Jliillliiil II. .................. . 01 k (d) ... II I I I I J J I I I I I II I J I : [:-.k~ .......n :O ••. 0 n k (e) h[n-k] ••• IIIIII ..... ju u uuuuuuen~O··· n 0 k (f) Figure 2.6 Graphical interpretation of the calculation of the convolution sum for Example 2.3. Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 85 y[n] 1 -an +1) = u[n] ( 1-a 1-a 0 n Figure 2.7 Output for Example 2.3. The operation of convolution is sometimes described in terms of ""sliding"" the se- quence h[n- k] past x[k]. For example, suppose we have evaluated y[n] for some partic- ular value of n, say, n = n0 . That is, we have sketched the signal h[n0 - k], multiplied it by the signal x[k], and summed the result over all values of k. To evaluate y[n] at the next value of n-i.e., n = n0 + 1-we need to sketch the signal h[(n0 + 1)- k]. However, we can do this simply by taking the signal h[n0 - k] and shifting it to the right by one point. For each successive value of n, we continue this process of shifting h[n- k] to the right by one point, multiplying by x[k], and summing the result over k. Example 2.4 As a further example, consider the two sequences 1, 0 :::; n :::; 4 x [ n ] = { 0, otherwise and 11 h[n] = { a , 0 :::; n .:::; 6. 0, otherwise These signals are depicted in Figure 2.8 for a positive value of a > 1. In order to calculate the convolution of the two signals, it is convenient to consider five separate intervals for n. This is illustrated in Figure 2.9. Intervall. For n < 0, there is no overlap between the nonzero portions of x[k] and h[n - k], and consequently, y[n] = 0. Interval 2. For 0 :::; n :::; 4, n-k x[k]h[n - k] = a ' 0:::; k:::; n { 0, otherwise 86 Linear Time-Invariant Systems Chap.2 x[n] -2-1 0 1 2 3 4 5 n (a) h[n] 01234567 n (b) Figure 2.8 The signals to be convolved in Example 2.4. Thus, in this interval, n y[n] = Lan-k. (2.14) k=O We can evaluate this sum using the finite sum formula, eq. (2.13). Specifically, changing the variable of summation in eq. (2.14) from k tor = n- k, we obtain n 1-an+l y[n] = L,ar = --- r=O 1 -a Interval 3. For n > 4 but n - 6 :s 0 (i.e., 4 < n :::::; 6), n-k x[k]h[n- k] 0:::::; k:::::; 4 = a ' { 0, otherwise Thus, in this interval, 4 y[n] = Lan-k. (2.15) k=O Once again, we can use the geometric sum formula in eq. (2.13) to evaluate eq. (2.15). Specifically, factoring out the constant factor of an from the summation in eq. (2.15) yields (2.16) Interval4. For n > 6 but n- 6 :s 4 (i.e., for 6 < n :s 10), x[k]h[n - k] = { an-k, (n - 6) :::::; k :::::; 4 0, otherwise Ix [k] .......... 1111 ............. . 0 4 k (a) h[n-k] n < 0 •l l llttr •• _j• ••••••••••••••••• n 0 k n-6 I (b) h[n -k] ...... Jitt_rr ............... . 0 n k n-6 (c) h[n-k] 4<n~6 •••..•••• lI l lttr ••••••••.•••• / 0 n k n-6 (d) h[n-k] j 6<n~10 ··········-·l t I Itrrrn 0 ......... . k n-6 (e) h[n-k] l n>1 0 j ··········-······· lllttr •••• 0 n-6 n k (f) Figure 2. 9 Graphical interpretation of the convolution performed in Example 2.4. 87 88 Linear Time-Invariant Systems Chap.2 so that 4 y[n] = ~ a""- k k=n-6 We can again use eq. (2.13) to evaluate this summation. Letting r = k- n + 6, we obtain 10-n 10-n 1 -an-I I an-4- a 7 y[n] = ~ a6-r = a6 ~(a-IY = a6 -1 = ---- r=O r=O 1 -a 1 -a Interval 5. For n - 6 > 4, or equivalently, n > 10, there is no overlap between the nonzero portions of x[k] and h[n- k], and hence, y[n] = 0. Summarizing, then, we obtain 0, n<O 1- an+l 1- a ' y[n] = 6<n~10 1-a 0, 10 < n which is pictured in Figure 2.10. y[n] 0 4 6 10 n Figure 2.1 o Result of performing the convolution in Example 2.4. Example 2.5 Consider an LTI system with input x[n] and unit impulse response h[n] specified as follows: x[n] = 2""u[ -n], (2.17) h[n] = u[n]. (2.18) Sec. 2.1 Discrete-Time LTI Systems: The Convolution Sum 89 1 1 2 x[k] = 2ku[ -k] 1 1 1 16 8 4 • ' t I 1 -2 -1 0 • • • • k 1 1 r h[n-k] 1 1 1 n • • k • (a) 2 y[n] 1 2 -3 -2 -1 0 2 3 n (b) Figure 2.11 (a) The sequences x[k] and h[n- k] for the convolution prob- lem considered in Example 2.5; (b) the resulting output signal y[n]. The sequences x[k] and h[n- k] are plotted as functions of kin Figure 2.11(a). Note that x[k] is zero fork> 0 and h[n- k] is zero fork> n. We also observe that, regardless of the value of n, the sequence x[k]h[n- k] always has nonzero samples along the k-axis. When n :2: 0, x[k]h[n - k] has nonzero samples in the interval k :::; 0. It follows that, for n :2: 0, 0 0 y[n] = L x[k]h[n- k] = L 2k. (2.19) k=-X k=-X To evaluate the infinite sum in eq. (2.19), we may use the infinite sum formula, LX ak 1 = --, 0 < lal < 1. (2.20) k=O 1 -a Changing the variable of summation in eq. (2.19) from k tor = - k, we obtain 1 = 2. (2.21) 1 - (112) Thus, y[n] takes on a constant value of 2 for n ;::: 0. 90 Linear Time-Invariant Systems Chap. 2 When n < 0, x[k]h[n - k] has nonzero samples for k :::::: n. It follows that, for n < 0, II II y[n] = L x[k]lz[n- kl = L 2k. (2.22) k=-X k=-CC By performing a change of variable I = - k and then m = I + n, we can again make use of the infinite sum formula, eq. (2.20), to evaluate the sum in eq. (2.22). The result is the following for n < 0: 11 111 .vrnl = .L:c : (12 ) ' = .L-""' (12 ) m--Il = (12 ) - .L:;: (12 ) = 211 . 2 = 211+ '· (2.23) I= -11 m=O m=O The complete sequence of y[n] is sketched in Figure 2.11 (b). These examples illustrate the usefulness of visualizing the calculation of the con- volution sum graphically. Moreover, in addition to providing a useful way in which to calculate the response of an LTI system, the convolution sum also provides an extremely useful representation for LTI systems that allows us to examine their properties in great detail. In particular, in Section 2.3 we will describe some of the properties of convolution and will also examine some of the system properties introduced in the previous chapter in order to see how these properties can be characterized for LTI systems. 2.2 CONTINUOUS-TIME LTI SYSTEMS: THE CONVOLUTION INTEGRAL In analogy with the results derived and discussed in the preceding section, the goal of this section is to obtain a complete characterization of a continuous-time LTI system in terms of its unit impulse response. In discrete time, the key to our developing the convolution sum was the sifting property of the discrete-time unit impulse-that is, the mathematical representation of a signal as the superposition of scaled and shifted unit impulse functions. Intuitively, then, we can think of the discrete-time system as responding to a sequence of individual impulses. In continuous time, of course, we do not have a discrete sequence of input values. Nevertheless, as we discussed in Section 1.4.2, if we think of the unit im- pulse as the idealization of a pulse which is so short that its duration is inconsequential for any real, physical system, we can develop a representation for arbitrary continuous-time signals in terms of these idealized pulses with vanishingly small duration, or equivalently, impulses. This representation is developed in the next subsection, and, following that, we will proceed very much as in Section 2.1 to develop the convolution integral representation for continuous-time LTI systems. 2.2.1 The Representation of Continuous-Time Signals in Terms of Impulses To develop the continuous-time counterpart of the discrete-time sifting property in eq. (2.2), we begin by considering a pulse or ""staircase"" approximation, x(t), to a continuous-time signal x(t), as illustrated in Figure 2.12( a). In a manner similar to that Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 91 x(t) -~ 0 ~ 2~ k~ (a) x(-2~)8/l(t + 2~)~ x( 2A)D I -2.:1 -.:1 (b) x(-~)8/l(t + ~)~ x(-A)O -~ 0 (c) x(0)8!l(t)~ nx(O) 0 ~ (d) x(~)8 ~ (t- ~)~ x(~) Figure 2.12 Staircase approxima- (e) tion to a continuous-time signal. 92 Linear Time-Invariant Systems Chap.2 employed in the discrete-time case, this approximation can be expressed as a linear com- bination of delayed pulses, as illustrated in Figure 2.12(a)-(e). If we define ±. O:=:::t<~ ot!(t) = { (2.24) 0, otherwise then, since ~o11 (t) has unit amplitude, we have the expression x(t) = ~ x(k~)ot!(t- k~)~. (2.25) k=~x From Figure 2.12, we see that, as in the discrete-time case [eq. (2.2)], for any value oft, only one term in the summation on the right-hand side of eq. (2.25) is nonzero. As we let~ approach 0, the approximation x(t) becomes better and better, and in the limit equals x(t). Therefore, +x x(t) = lim ~ x(k~)o.1(t- k~)~. (2.26) 11~0 k= ~x Also, as~ ~ 0, the summation in eq. (2.26) approaches an integral. This can be seen by considering the graphical interpretation of the equation, illustrated in Figure 2.13. Here, we have illustrated the signals x(T), o11(t- T), and their product. We have also indicated a shaded region whose area approaches the area under x(T)ot!(t- T) as~ ~ 0. Note that the shaded region has an area equal to x(m~) where t- ~ < m~ < t. Furthermore, for this value oft, only the term with k = m is nonzero in the summation in eq. (2.26), and thus, the right-hand side of this equation also equals x(m~). Consequently, it follows from eq. (2.26) and from the preceding argument that x(t) equals the limit as~ ~ 0 of the area under x(T)o11(t- T). Moreover, from eq. (1.74), we know that the limit as~ ~ 0 of o11(t) is the unit impulse function o(t). Consequently, +x X(t) = f~x X(T)o(t- T)dT. (2.27) As in discrete time, we refer to eq. (2.27) as the sifting property of the continuous-time impulse. We note that, for the specific example of x(t) = u(t), eq. (2.27) becomes +C/0 ~X u(t) = f~ oc U( T)o(t - T)dT = Jo o(t - T)dT, (2.28) since u(T) = 0 forT< 0 and u(T) = 1 forT> 0. Equation (2.28) is identicalto eq. (1.75), derived in Section 1.4.2. Once again, eq. (2.27) should be viewed as an idealization in the sense that, for ~ ""small enough,"" the approximation of x(t) in eq. (2.25) is essentially exact for any practical purpose. Equation (2.27) then simply represents an idealization of eq. (2.25) by taking~ to be vanishingly small. Note also that we could have derived eq. (2.27) directly by using several of the basic properties of the unit impulse that we derived in Section 1.4.2. Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 93 (a) oll(t- ,.) 1 X m~ t- ~ 'T (b) Figure 2.13 Graphical interpreta- (c) tion of eq. (2.26). Specifically, as illustrated in Figure 2.14(b), the signal B(t- r) (viewed as a function of T with t fixed) is a unit impulse located at T = t. Thus, as shown in Figure 2.14(c), the signal x( r)B(t - r) (once again viewed as a function of r) equals x(t)B(t - r) [i.e., it is a scaled impulse at T = t with an area equal to the value of x(t)]. Consequently, the integral of this signal from T = -oo to T = +oo equals x(t); that is, +oo I +oo I +oo I- oo x( r)B(t - r)dr = -oo x(t)B(t- r)dr = x(t) -oo B(t - r)dr = x(t). Although this derivation follows directly from Section 1.4.2, we have included the deriva- tion given in eqs. (2.24 )-(2.27) to stress the similarities with the discrete-time case and, in particular, to emphasize the interpretation of eq. (2.27) as representing the signal x(t) as a ""sum"" (more precisely, an integral) of weighted, shifted impulses. 94 Linear Time-Invariant Systems Chap.2 x(•) T (a) T (b) x(t) Figure 2.14 (a) Arbitrary signal x( T); (b) impulse B(t- T) as a function , of T with t fixed; (c) product of these (c) two signals. 2.2.2 The Continuous-Time Unit Impulse Response and the Convolution Integral Representation of LTI Systems As in the discrete-time case, the representation developed in the preceding section provides us with a way in which to view an arbitrary continuous-time signal as the superposition of scaled and shifted pulses. In particular, the approximate representation in eq. (2.25) repre- sents the signal x(t) as a sum of scaled and shifted versions of the basic pulse signal8j_(t). Consequently, the response y(t) of a linear system to this signal will be the superposition of the responses to the scaled and shifted versions of 811 (t). Specifically, let us define hk!l (t) as the response of an LTI system to the input 8:1(t- k~). Then, from eq. (2.25) and the superposition property, for continuous-time linear systems, we see that +x y(t) = ~ x(k~)hk.'1(t)~. (2.29) k= -'X The interpretation of eq. (2.29) is similar to that for eq. (2.3) in discrete time. In particular, consider Figure 2.15, which is the continuous-time counterpart of Figure 2.2. In Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 95 x(t) I I I I I I I I I I I 011 (a) 1\ x(O)h0(t)Ll x(O) OLl (b) x(Ll) (c) 1\ x(kLl)hkll (t)Ll x(kLl) rv- kLl (d) ~(t) y(t) 0 0 (e) x(t) y(t) Figure 2.15 Graphical interpreta- 0 0 tion of the response of a continuous- time linear system as expressed in (f) eqs. (2.29) and (2.30). 96 Linear Time-Invariant Systems Chap.2 Figure 2.15(a) we have depicted the input x(t) and its approximation x(t), while in Figure 2.15(b )-(d), we have shown the responses of the system to three of the weighted pulses in the expression for x(t). Then the output Y(t) corresponding to x(t) is the superposition of all of these responses, as indicated in Figure 2.15(e). What remains, then, is to consider what happens as Ll becomes vanishingly small- i.e., as Ll ~ 0. In particular, with x(t) as expressed in eq. (2.26), x(t) becomes an increas- ingly good approximation to x(t), and in fact, the two coincide as Ll ~ 0. Consequently, the response to x(t), namely, y(t) in eq. (2.29), must converge to y(t), the response to the actual input x(t), as illustrated in Figure 2.15(f). Furthermore, as we have said, for Ll ""small enough,"" the duration of the pulse Ot,(t- kil) is of no significance, in that, as far as the system is concerned, the response to this pulse is essentially the same as the response to a unit impulse at the same point in time. That is, since the pulse Ot,(t- kil) corresponds to a shifted unit impulse as Ll ~ 0, the response hkt,(t) to this input pulse becomes the response to an impulse in the limit. Therefore, if we let h7 (t) denote the response at timet tO a unit impulse O(t - T) located at timeT, then +x y(t) = lim L x(kil)hkt/t)Ll. (2.30) 6,--.() k= -Y: As Ll ~ 0, the summation on the right-hand side becomes an integral, as can be seen graphically in Figure 2.16. Specifically, in Figure 2.16 the shaded rectangle represents one term in the summation on the right-hand side of eq. (2.30) and as Ll ~ 0 the summation approaches the area under x(T)h7 (t) viewed as a function ofT. Therefore, +x y(t) = f-x X(T)h 7 (f)dT. (2.31) The interpretation of eq. (2.31) is analogous to the one for eq. (2.29). As we showed in Section 2.2.1, any input x(t) can be represented as x(t) = rxx X(T)/l(t- T)JT. Shaded area = x(k6.)hu(t)A Figure 2.16 Graphical illustration k..l (k+1)..l of eqs. (2.30) and (2.31 ). Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 97 That is, we can intuitively think of x(t) as a ""sum"" of weighted shifted impulses, where the weight on the impulse o(t- T) is x( T)dT. With this interpretation, eq. (2.31) represents the superposition of the responses to each of these inputs, and by linearity, the weight on the response h7 (t) to the shifted impulse o(t- T) is also x(T)dT. Equation (2.31) represents the general form of the response of a linear system in continuous time. If, in addition to being linear, the system is also time invariant, then h7 (t) = h0(t - T); i.e., the response of an LTI system to the unit impulse o(t - T), which is shifted by T seconds from the origin, is a similarly shifted version of the response to the unit impulse function o(t). Again, for notational convenience, we will drop the subscript and define the unit impulse response h(t) as h(t) = ho(t); (2.32) i.e., h(t) is the response to o(t). In this case, eq. (2.31) becomes +x y(t) = -x X(T)h(t- T)dT. (2.33) J Equation (2.33), referred to as the convolution integral or the superposition integral, is the continuous-time counterpart of the convolution sum of eq. (2.6) and corresponds to the representation of a continuous-time LTI system in terms of its response to a unit impulse. The convolution of two signals x(t) and h(t) will be represented symbolically as y(t) = x(t) * h(t). (2.34) While we have chosen to use the same symbol * to denote both discrete-time and continuous-time convolution, the context will generally be sufficient to distinguish the two cases. As in discrete time, we see that a continuous-time LTI system is completely char- acterized by its impulse response-i.e., by its response to a single elementary signal, the unit impulse o(t). In the next section, we explore the implications of this as we examine a number of the properties of convolution and of LTI systems in both continuous time and discrete time. The procedure for evaluating the convolution integral is quite similar to that for its discrete-time counterpart, the convolution sum. Specifically, in eq. (2.33) we see that, for any value oft, the output y(t) is a weighted integral of the input, where the weight on x(T) is h(t- T). To evaluate this integral for a specific value oft, we first obtain the signal h(t- T) (regarded as a function ofT with t fixed) from h( T) by a reflection about the origin and a shift to the right by t if t > 0 or a shift to the left by ltl for t < 0. We next multiply together the signals x( T) and h(t - T), and y(t) is obtained by integrating the resulting product from T = -'X toT = +'X. To illustrate the evaluation of the convolution integral, let us consider several examples. 98 Linear Time-Invariant Systems Chap.2 Example 2.6 Let x(t) be the input to an LTI system with unit impulse response h(t), where x(t) = e-at u(t), a > 0 and h(t) = u(t). In Figure 2.17, we have depicted the functions h(r), x(r), and h(t- r) for a negative value oft and for a positive value oft. From this figure, we see that fort < 0, the product of x(r) and h(t- r) is zero, and consequently, y(t) is zero. Fort> 0, e-aT, 0 < T < t x(r)h(t- r) = { h . . 0, ot erwtse h('r) 0 T X(T) 11'---- 0 T h(t-T) I 11 t<O 0 h(t-T) t>O I 0 T Figure 2.17 Calculation of the convolution integral for Example 2.6. Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 99 From this expression, we can compute y(t) for t > 0: y(t) = J: e-ar dT = --1e -ar II a o !o - e-ar). a Thus, for all t, y(t) is 1 y(t) = -(1 - e-at)u(t), a which is shown in Figure 2.18. y(t) = 1 (1- e-at )u(t) a 1a --------------------- 0 Figure 2.18 Response of the system in Example 2.6 with impulse re- sponse h(t) = u(t) to the input x(t) = e-at u(t). Example 2.7 Consider the convolution of the following two signals: ~: 0 < t < T x(t) = { otherwise ' 6, 0 < t < 2T h(t) = { otherwise · As in Example 2.4 for discrete-time convolution, it is convenient to consider the evalu- ation of y(t) in separate intervals. In Figure 2.19, we have sketched x(r) and have illus- tratedh(t-r)ineachoftheintervalsofinterest.Fort < Oandfort > 3T, x(r)h(t-r) = 0 for all values ofT, and consequently, y(t) = 0. For the other intervals, the product x( r)h(t- r) is as indicated in Figure 2.20. Thus, for these three intervals, the integration can be carried out graphically, with the result that 0, t < 0 lt2 0 < t < T 2 ' y(t) = !Tt- ~T2 , T < t < 2T , - l t2 + T t + '}_ T 2 2T < t < 3 T 2 2 ' 0, 3T < t which is depicted in Figure 2.21. X(T) 1h 0 T T h(t-T) ~tT t < 0 I t 0 T t- 2T h(t-T) N: 0< t < T I 0 t T t- 2T h(t-T) rK T < t < 2T I o t- 2T h(t-T) 2T!~ 2T < t < 3T 0 \ T t- 2T h(t-T) 2Tt ~ t > 3T 0 I t- 2T Figure 2.19 Signals x( T) and h( t - T) for different values of t for Example 2.7. 100 Sec. 2.2 Continuous-Time LTI Systems: The Convolution Integral 101 0 < t < T 0 t 'T (a) X('T)h{t-'T) t-T1~ T < t < 2T 0 T 'T (b) X('T)h(t-T) 2T~lD 2T < t < 3T t-T t T 'T t-2T (c) Figure 2.20 Product x( T)h(t- T) for Example 2.7 for the three ranges of values of t for which this product is not identically zero. (See Figure 2.19.) y(t) 0 T 2T 3T Figure 2.21 Signal y(t) = x(t) * h(t) for Example 2.7. Example 2.8 Let y(t) denote the convolution of the following two signals: x(t) = e21 u(- t), (2.35) h(t) = u(t - 3). (2.36) The signals x( T) and h(t - T) are plotted as functions ofT in Figure 2.22(a). We first observe that these two signals have regions of nonzero overlap, regardless of the value 102 Linear Time-Invariant Systems Chap. 2 0 t-3 0 T (a) y(t) 0 3 (b) Figure 2.22 The convolution problem considered in Example 2.8. oft. When t- 3 :::; 0, the product of x(T) and h(t- T) is nonzero for -x < T < t- 3, and the convolution integral becomes (2.37) Fort-3 2: O,theproductx(T)h(t-T)isnonzerofor-x < T < O,sothattheconvolution integral is () 1 _v(t) = I-x e'2T dT = 2. (2.38) The resulting signal y(t) is plotted in Figure 2.22(b ). As these examples and those presented in Section 2.1 illustrate, the graphical in- terpretation of continuous-time and discrete-time convolution is of considerable value in visualizing the evaluation of convolution integrals and sums. Sec. 2.3 Properties of Linear Time-Invariant Systems 103 2.3 PROPERTIES OF LINEAR TIME-INVARIANT SYSTEMS In the preceding two sections, we developed the extremely important representations of continuous-time and discrete-time LTI systems in terms of their unit impulse re- sponses. In discrete time the representation takes the form of the convolution sum, while its continuous-time counterpart is the convolution integral, both of which we repeat here for convenience: +oo y[n] = L x[k]h[n - k] = x[n] * h[n] (2.39) k= -ex +oo y(t) = I-e x x( T)h(t- T)d'T = x(t) * h(t) (2.40) As we have pointed out, one consequence of these representations is that the charac- teristics of an LTI system are completely determined by its impulse response. It is impor- tant to emphasize that this property holds in general only for LTI systems. In particular, as illustrated in the following example, the unit impulse response of a nonlinear system does not completely characterize the behavior of the system. Example 2.9 Consider a discrete-time system with unit impulse response h[ ] = { 1, n = 0, 1 (2.41) n 0, otherwise · If the system is LTI, then eq. (2.41) completely determines its input-output behavior. In particular, by substituting eq. (2.41) into the convolution sum, eq. (2.39), we find the following explicit equation describing how the input and output of this LTI system are related: y[n] = x[n] + x[n- 1]. (2.42) On the other hand, there are many nonlinear systems with the same response-i.e., that given in eq. (2.41)-to the input o[n]. For example, both of the following systems have this property: y[n] = (x[n] + x[n - 1])2 , y[n] = max(x[n], x[n- 1]). Consequently, if the system is nonlinear it is not completely characterized by the impulse response in eq. (2.41). The preceding example illustrates the fact that LTI systems have a number of prop- erties not possessed by other systems, beginning with the very special representations that they have in terms of convolution sums and integrals. In the remainder of this section, we explore some of the most basic and important of these properties. 104 Linear Time-Invariant Systems Chap. 2 2.3. 1 The Commutative Property A basic property of convolution in both continuous and discrete time is that it is a commu- tative operation. That is, in discrete time +:r: x[n] * h[n] = h[n] * x[n] L h[k]x[n - k], (2.43) k= -'l"" and in continuous time x(t) * h(t) ~ h(t) * x(t) ~ r: h(T)X(t- T)dT. (2.44) These expressions can be verified in a straightforward manner by means of a substitution of variables in eqs. (2.39) and (2.40). For example, in the discrete-time case, if we let r = n - k or, equivalently, k = n - r, eq. (2.39) becomes x[n] * h[n] = L x[k]h[n- k] = L x[n- r]h[r] = h[n] * x[n]. (2.45) k= -X r=-x With this substitution of variables, the roles of x[n] and h[n] are interchanged. According to eq. (2.45), the output of an LTI system with input x[n] and unit impulse response h[n] is identical to the output of an LTI system with input h[n] and unit impulse response x[n]. For example, we could have calculated the convolution in Example 2.4 by first reflecting and shifting x[k], then multiplying the signals x[n- k] and h[k], and finally summing the products for all values of k. Similarly, eq. (2.44) can be verified by a change of variables, and the implications of this result in continuous time are the same: The output of an LTI system with input x(t) and unit impulse response h(t) is identical to the output of an LTI system with input h(t) and unit impulse response x(t). Thus, we could have calculated the convolution in Example 2.7 by reflecting and shifting x(t), multiplying the signals x(t - T) and h( T), and integrating over -x < T < +x. In specific cases, one of the two forms for computing convolutions [i.e., eq. (2.39) or (2.43) in discrete time and eq. (2.40) or (2.44) in continuous time] may be easier to visualize, but both forms always result in the same answer. 2.3.2 The Distributive Property Another basic property of convolution is the distributive property. Specifically, convolution distributes over addition, so that in discrete time x[n] * (h1 [n] + h2[n]) = x[n] * h1 [n] + x[n] * h2[n], (2.46) and in continuous time (2.47) This property can be verified in a straightforward manner. Sec. 2.3 Properties of Linear Time-Invariant Systems 105 Y1(t) h1(t) x(t) ~y(t) h2(t) Y2(t) (a) x(t)-----1~ 1------1·~ y(t) Figure 2.23 Interpretation of the distributive property of convolution for a parallel interconnection of LTI (b) systems. The distributive property has a useful interpretation in terms of system interconnec- tions. Consider two continuous-time LTI systems in parallel, as indicated in Figure 2.23(a). The systems shown in the block diagram are LTI systems with the indicated unit impulse responses. This pictorial representation is a particularly convenient way in which to denote LTI systems in block diagrams, and it also reemphasizes the fact that the impulse response of an LTI system completely characterizes its behavior. The two systems, with impulse responses h1( t) and h2(t), have identical inputs, and their outputs are added. Since YI (t) = x(t) * ht (t) and the system of Figure 2.23(a) has output y(t) = x(t) * ht (t) + x(t) * h2(t), (2.48) corresponding to the right-hand side of eq. (2.47). The system of Figure 2.23(b) has output y(t) = x(t) * [ht (t) + h2(t)], (2.49) corresponding to the left-hand side of eq. (2.47). Applying eq. (2.47) to eq. (2.49) and comparing the result with eq. (2.48), we see that the systems in Figures 2.23(a) and (b) are identical. There is an identical interpretation in discrete time, in which each of the signals in Figure 2.23 is replaced by a discrete-time counterpart (i.e., x(t), h1( t), h2(t), y 1( t), y2(t), and y(t) are replaced by x[n], ht [n], h2[n], y 1 [n], y2[n], and y[n], respectively). In summary, then, by virtue of the distributive property of convolution, a parallel combina- tion of LTI systems can be replaced by a single LTI system whose unit impulse response is the sum of the individual unit impulse responses in the parallel combination. 106 Linear Time-Invariant Systems Chap. 2 Also, as a consequence of both the commutative and distributive properties, we have [Xt [n] + x2[n]] * h[n] = Xt [n] * h[n] + x2[n] * h[n] (2.50) and [Xt (t) + X2(t)] * h(t) = Xt (t) * h(t) + X2(t) * h(t), (2.51) which simply state that the response of an LTI system to the sum of two inputs must equal the sum of the responses to these signals individually. As illustrated in the next example, the distributive property of convolution can also be exploited to break a complicated convolution into several simpler ones. Example 2.10 Let y[n] denote the convolution of the following two sequences: x[n] = (~ )"" u[n] + 2""u[ -n], (2.52) h[n] = u[n]. (2.53) Note that the sequence x[n] is nonzero along the entire time axis. Direct evaluation of such a convolution is somewhat tedious. Instead, we may use the distributive property to express y[n] as the sum of the results of two simpler convolution problems. In particular, ifweletx1[n] = (112Yu[n]andx2 [n] = 2nu[-n],itfollowsthat y[n] = (x1 [n] + x2[n]) * h[n]. (2.54) Using the distributive property of convolution, we may rewrite eq. (2.54) as y[n] = Y1 [n] + y2[n], (2.55) where Yl [n] = Xi [n] * h[n] (2.56) and Y2[n] = x2[n] * h[n]. (2.57) The convolution in eq. (2.56) for y 1 [n] can be obtained from Example 2.3 (with a = 112), while y2[n] was evaluated in Example 2.5. Their sum is y[n], which is shown in Figure 2.24. y[n] 4 -------------- --- 3 1 1 2 .... ~ T -3-2 -1 0 1 2 3 4 5 6 7 n Figure 2.24 The signal y[n] = x[n] * h[n] for Example 2.10. Sec. 2.3 Properties of Linear Time-Invariant Systems 107 2. 3. 3 The Associative Property Another important and useful property of convolution is that it is associative. That is, in discrete time (2.58) and in continuous time x(t) * [ht (t) * h2(t)] = [x(t) * h1 (t)] * h2(t). (2.59) This property is proven by straightforward manipulations of the summations and integrals involved. Examples verifying it are given in Problem 2.43. As a consequence of the associative property, the expressions y[n] = x[n] * ht [n] * h2[n] (2.60) and y(t) = x(t) * ht (t) * h2(t) (2.61) are unambiguous. That is, according to eqs. (2.58) and (2.59), it does not matter in which order we convolve these signals. An interpretation of the associative property is illustrated for discrete-time systems in Figures 2.25(a) and (b). In Figure 2.25(a), y[n] = w[n] * h2[n] = (x[n] * ht [n]) * h2[n]. In Figure 2.25(b ), y[n] = x[n] * h[n] = x[n] * (ht [n] * h2[n]). According to the associative property, the series interconnection of the two systems in Figure 2.25(a) is equivalent to the single system in Figure 2.25(b). This can be generalized to an arbitrary number of LTI systems in cascade, and the analogous interpretation and conclusion also hold in continuous time. By using the commutative property together with the associative property, we find another very important property of LTI systems. Specifically, from Figures 2.25(a) and (b), we can conclude that the impulse response of the cascade of two LTI systems is the convolution of their individual impulse responses. Since convolution is commutative, we can compute this convolution of h1 [n] and h2 [n] in either order. Thus, Figures 2.25(b) and (c) are equivalent, and from the associative property, these are in tum equivalent to the system of Figure 2.25(d), which we note is a cascade combination of two systems as in Figure 2.25(a), but with the order of the cascade reversed. Consequently, the unit impulse response of a cascade of two LTI systems does not depend on the order in which they are cascaded. In fact, this holds for an arbitrary number of LTI systems in cascade: The order in which they are cascaded does not matter as far as the overall system impulse response is concerned. The same conclusions hold in continuous time as well. x[n]--~L....-h2-[n_J_ :--....,.•~1 h1[n] 1----....,•~ y[n] Figure 2.25 Associative property of convolution and the implication of this and the commutative property for the (d) series interconnection of LTI systems. It is important to emphasize that the behavior of LTI systems in cascade-and, in particular, the fact that the overall system response does not depend upon the order of the systems in the cascade-is very special to such systems. In contrast, the order in which nonlinear systems are cascaded cannot be changed, in general, without changing the over- all response. For instance, if we have two memory less systems, one being multiplication by 2 and the other squaring the input, then if we multiply first and square second, we obtain y[n] = 4x2 [n]. However, if we multiply by 2 after squaring, we have y[n] = 2x2 [n]. Thus, being able to interchange the order of systems in a cascade is a characteristic par- ticular to LTI systems. In fact, as shown in Problem 2.51, we need both linearity and time invariance in order for this property to be true in general. 2.3.4 LTI Systems with and without Memory As specified in Section 1.6.1, a system is memory less if its output at any time depends only on the value of the input at that same time. From eq. (2.39), we see that the only way that this can be true for a discrete-time LTI system is if h[n] = 0 for n =i' 0. In this case Sec. 2.3 Properties of Linear Time-Invariant Systems 109 the impulse response has the form h[n] = Ko[n], (2.62) where K = h[O] is a constant, and the convolution sum reduces to the relation y[n] = Kx[n]. (2.63) If a discrete-time LTI system has an impulse response h[n] that is not identically zero for n =1:- 0, then the system has memory. An example of an LTI system with memory is the system given by eq. (2.42). The impulse response for this system, given in eq. (2.41), is nonzero for n = 1. From eq. (2.40), we can deduce similar properties for continuous-time LTI systems with and without memory. In particular, a continuous-time LTI system is memoryless if h(t) = 0 for t =1:- 0, and such a memoryless LTI system has the form y(t) = Kx(t) (2.64) for some constant K and has the impulse response h(t) = Ko(t). (2.65) Note that if K = 1 in eqs. (2.62) and (2.65), then these systems become identity systems, with output equal to the input and with unit impulse response equal to the unit impulse. In this case, the convolution sum and integral formulas imply that x[n] = x[n] * o[n] and x(t) = x(t) * o(t), which reduce to the sifting properties of the discrete-time and continuous-time unit im- pulses: +::o x[n] = ~ x[k]o[n- k] k= -oc +oo x(t) = I- x x( T)o(t - T)dT. 2.3.5 lnvertibility of LTI Systems Consider a continuous-time LTI system with impulse response h(t). Based on the discus- sion in Section 1.6.2, this system is invertible only if an inverse system exists that, when connected in series with the original system, produces an output equal to the input to the first system. Furthermore, if an LTI system is invertible, then it has an LTI inverse. (See Problem 2.50.) Therefore, we have the picture shown in Figure 2.26. We are given a sys- tem with impulse response h(t). The inverse system, with impulse response h1 (t), results in w(t) = x(t)-such that the series interconnection in Figure 2.26(a) is identical to the 110 Linear Time-Invariant Systems Chap. 2 ~Q___ x(t)~~~ w(t)=x(t) (a) Figure 2.26 Concept of an inverse x(t) ---~1 1dentity system t----~ x(t) system for continuous-time LTI sys- o(t) tems. The system with impulse re- sponse h1 {t) is the inverse of the system with impulse response h(t) if (b) h(t) * h1 (t) = o(t). identity system in Figure 2.26(b). Since the overall impulse response in Figure 2.26(a) is h(t) * h 1( t), we have the condition that h 1( t) must satisfy for it to be the impulse response of the inverse system, namely, h(t) * h1 (t) = o(t). (2.66) Similarly, in discrete time, the impulse response h 1 [n] of the inverse system for an LTI system with impulse response h[n] must satisfy h[n] *hi [n] = o[n]. (2.67) The following two examples illustrate invertibility and the construction of an inverse system. Example 2. 1 1 Consider the LTI system consisting of a pure time shift y(t) = x(t - to). (2.68) Such a system is a delay if to > 0 and an advance if to < 0. For example, if t0 > 0, then the output at time t equals the value of the input at the earlier time t - t0 . If to = 0. the system in eq. (2.68) is the identity system and thus is memoryless. For any other value of t0 , this system has memory, as it responds to the value of the input at a time other than the current time. The impulse response for the system can be obtained from eq. (2.68) by taking the input equal to o(t), i.e., h(t) = o(t- to). (2.69) Therefore, x(t- to) = x(t) * o(t- to). (2.70) That is, the convolution of a signal with a shifted impulse simply shifts the signal. To recover the input from the output, i.e., to invert the system, all that is required is to shift the output back. The system with this compensating time shift is then the inverse Sec. 2.3 Properties of Linear Time-Invariant Systems 111 system. That is, if we take h1 (t) = 5(t + to), then h(t) * h1 (t) = 5(t - to)* 5(t + to) = 5(t). Similarly, a pure time shift in discrete time has the unit impulse response 5[n- n0 ], so that convolving a signal with a shifted impulse is the same as shifting the signal. Furthermore, the inverse of the LTI system with impulse response 5[n - n0 ] is the LTI system that shifts the signal in the opposite direction by the same amount-i.e., the LTI system with impulse response 5[n + n0 ]. Example 2. 1 2 Consider an LTI system with impulse response h[n] = u[n]. (2.71) Using the convolution sum, we can calculate the response of this system to an arbitrary input: +oo y[n] = 2..:, x[k]u[n- k]. (2.72) k=-oo Since u[n- k] is 0 for n- k < 0 and 1 for n- k 2: 0, eq. (2.72) becomes n y[n] = 2..:, x[k]. (2.73) k=-X That is, this system, which we first encountered in Section 1.6.1 [see eq. (1.92)], is a summer or accumulator that computes the running sum of all the values of the input up to the present time. As we saw in Section 1.6.2, such a system is invertible, and its inverse, as given by eq. (1.99), is y[n] = x[n] - x[n- 1], (2.74) which is simply a first difference operation. Choosing x[n] = 5[n], we find that the impulse response of the inverse system is h1 [n] = 5[n] - 5[n- 1]. (2.75) As a check that h[n] in eq. (2.71) and h1 [n] in eq. (2.75) are indeed the impulse re- sponses of LTI systems that are inverses of each other, we can verify eq. (2.67) by direct calculation: h[n] * h1 [n] = u[n] * {5[n] - 5[n - 1]} = u[n] * 5[n] - u[n] * 5[n- 1] (2.76) = u[n] - u[n- 1] = 5[n]. 112 Linear Time-Invariant Systems Chap. 2 2.3.6 Causality for LTI Systems In Section 1.6.3, we introduced the property of causality: The output of a causal system depends only on the present and past values of the input to the system. By using the con- volution sum and integral, we can relate this property to a corresponding property of the impulse response of an LTI system. Specifically, in order for a discrete-time LTI system to be causal, y[n] must not depend on x[k] for k > n. From eq. (2.39), we see that for this to be true, all of the coefficients h[n- k] that multiply values of x[k] fork > n must be zero. This then requires that the impulse response of a causal discrete-time LTI system satisfy the condition h [ n] = 0 for n < 0. (2.77) According to eq. (2.77), the impulse response of a causal LTI system must be zero before the impulse occurs, which is consistent with the intuitive concept of causality. More gener- ally, as shown in Problem 1.44, causality for a linear system is equivalent to the condition of initial rest; i.e., if the input to a causal system is 0 up to some point in time, then the output must also be 0 up to that time. It is important to emphasize that the equivalence of causality and the condition of initial rest applies only to linear systems. For example, as discussed in Section 1.6.6, the system y[n] = 2x[n] + 3 is not linear. However, it is causal and, in fact, memoryless. On the other hand, if x[n] = 0, y[n] = 3 # 0, so it does not satisfy the condition of initial rest. For a causal discrete-time LTI system, the condition in eq. (2.77) implies that the convolution sum representation in eq. (2.39) becomes 11 y[n] = L x[k]h[n - k], (2.78) f.:.= -X and the alternative equivalent form, eq. (2.43), becomes y[n] = L h[k]x[n - k]. (2.79) f.:.=O Similarly, a continuous-time LTI system is causal if h(t) = 0 for t < 0, (2.80) and in this case the convolution integral is given by y(t) = Jt x( T)h(t - T)dT = ( x h( T)x(t - T)dT. (2.81) -x Jo Both the accumulator (h[n] = u[n]) and its inverse (h[n] = o[n]- o[n- 1]), de- scribed in Example 2.12, satisfy eq. (2.77) and therefore are causal. The pure time shift with impulse response h(t) = o(t- t0 ) is causal for t0 2::: 0 (when the time shift is a delay), but is noncausal for to < 0 (in which case the time shift is an advance, so that the output anticipates future values of the input). Sec. 2.3 Properties of Linear Time-Invariant Systems 113 Finally, while causality is a property of systems, it is common terminology to refer to a signal as being causal if it is zero for n < 0 or t < 0. The motivation for this terminology comes from eqs. (2. 77) and (2.80): Causality of an LTI system is equivalent to its impulse response being a causal signal. 2.3.7 Stability for LTI Systems Recall from Section 1.6.4 that a system is stable if every bounded input produces a bounded output. In order to determine conditions under which LTI systems are stable, consider an input x[n] that is bounded in magnitude: lx[n]l < B for all n. (2.82) Suppose that we apply this input to an LTI system with unit impulse response h[n]. Then, using the convolution sum, we obtain an expression for the magnitude of the output: (2.83) Since the magnitude of the sum of a set of numbers is no larger than the sum of the mag- nitudes of the numbers, it follows from eq. (2.83) that +oo ly[nJI ::5 L lh[kJIIx[n- k]l. (2.84) k= -00 From eq. (2.82), lx[n - k]l < B for all values of k and n. Together with eq. (2.84), this implies that +oo ly[nJI ::5 B L lh[kJI for all n. (2.85) k= -00 From eq. (2.85), we can conclude that if the impulse response is absolutely summable, that is, if +oo L lh[kJI < 00, (2.86) k= -00 then y[n] is bounded in magnitude, and hence, the system is stable. Therefore, eq. (2.86) is a sufficient condition to guarantee the stability of a discrete-time LTI system. In fact, this condition is also a necessary condition, since, as shown in Problem 2.49, if eq. (2.86) is not satisfied, there are bounded inputs that result in unbounded outputs. Thus, the~tability of a discrete-time LTI system is completely equivalent to eq. (2.86). In continuous time, we obtain an analogous characterization of stability in terms of the impulse response of an LTI system. Specifically, if lx(t)l < B for all t, then, in analogy with eqs. (2.83)-(2.85), it follows that 114 Linear Time-Invariant Systems Chap.2 ly(t)l ICx h(T)x(t- T)dTI ,-; rxx lh(T)IIx(t- T)ldT +:x> ~ B J-X l lh(T)!dT. Therefore, the system is stable if the impulse response is absolutely integrable, i.e., if (2.87) As in discrete time, if eq. (2.87) is not satisfied, there are bounded inputs that produce unbounded outputs; therefore, the stability of a continuous-time LTI system is equivalent to eq. (2.87). The use of eqs (2.86) and (2.87) to test for stability is illustrated in the next two examples. Example 2. 1 3 Consider a system that is a pure time shift in either continuous time or discrete time. Then, in discrete time +oo +oo L lh[n]l = L l8[n- noll = 1, (2.88) n=-'Xl n=-oc while in continuous time I+x f+x -e n ih(T)idT = -x 18(7- fo)idT = 1, (2.89) and we conclude that both of these systems are stable. This should not be surprising, since if a signal is bounded in magnitude, so is any time-shifted version of that signal. Now consider the accumulator described in Example 2.12. As we discussed in Section 1.6.4, this is an unstable system, since, if we apply a constant input to an accu- mulator, the output grows without bound. That this system is unstable can also be seen from the fact that its impulse response u[n] is not absolutely summable: X X L lu[n]l = L u[n] = oo. n=-xo n=O Similarly, consider the integrator, the continuous-time counterpart of the accumu- lator: y(t) = fx X(T)dT. (2.90) This is an unstable system for precisely the same reason as that given for the accumula- tor; i.e., a constant input gives rise to an output that grows without bound. The impulse Sec. 2.3 Properties of Linear Time-Invariant Systems 115 response for the integrator can be found by letting x(t) = 8(t), in which case h(t) = r""' 8( T)dT = u(t) and I+ x f +x lu(r)ldr = dr = oo. ~x () Since the impulse response is not absolutely integrable, the system is not stable. 2.3.8 The Unit Step Response of an LTI System Up to now, we have seen that the representation of an LTI system in terms of its unit impulse response allows us to obtain very explicit characterizations of system properties. Specifically, since h[n] or h(t) completely determines the behavior of an LTI system, we have been able to relate system properties such as stability and causality to properties of the impulse response. There is another signal that is also used quite often in describing the behavior of LTI systems: the unit step response, s[n] or s(t), corresponding to the output when x[n] = u[n] or x(t) = u(t). We will find it useful on occasion to refer to the step response, and therefore, it is worthwhile relating it to the impulse response. From the convolution -sum representation, the step response of a discrete-time LTI system is the convolution of the unit step with the impulse response; that is, s[n] = u[n] * h[n]. However, by the commutative property of convolution, s[n] = h[n] * u[n], and therefore, s[n] can be viewed as the response to the input h[n] of a discrete-time LTI system with unit impulse response u[n]. As we have seen in Example 2.12, u[n] is the unit impulse response of the accumulator. Therefore, 11 s[n] = L h[k]. (2.91) k=-00 From this equation and from Example 2.12, it is clear that h[ n] can be recovered from s[ n] using the relation h[n] = s[n] - s[n - 1]. (2.92) That is, the step response of a discrete-time LTI system is the running sum of its impulse response [eq. (2.91)]. Conversely, the impulse response of a discrete-time LTI system is the first difference of its step response [eq. (2.92)]. Similarly, in continuous time, the step response of an LTI system with impulse re- sponse h(t) is given by s(t) = u(t) * h(t), which also equals the response of an integra- tor [with impulse response u(t)] to the input h(t). That is, the unit step response of a continuous-time LTI system is the running integral of its impulse response, or s(t) = too h(T)dT, (2.93) 116 Linear Time-Invariant Systems Chap.2 and from eq. (2.93), the unit impulse response is the first derivative of the unit step re- sponse, 1 or h(t ) -_ dds(tt) -_ ' s ( ) t. (2.94) Therefore, in both continuous and discrete time, the unit step response can also be used to characterize an LTI system, since we can calculate the unit impulse response from it. In Problem 2.45, expressions analogous to the convolution sum and convolution integral are derived for the representations of an LTI system in terms of its unit step response. 2.4 CAUSAL LTI SYSTEMS DESCRIBED BY DIFFERENTIAL AND DIFFERENCE EQUATIONS An extremely important class of continuous-time systems is that for which the input and output are related through a linear constant-coefficient differential equation. Equations of this type arise in the description of a wide variety of systems and physical phenomena. For example, as we illustrated in Chapter 1, the response of the RC circuit in Figure 1.1 and the motion of a vehicle subject to acceleration inputs and frictional forces, as depicted in Figure 1.2, can both be described through linear constant-coefficient differential equations. Similar differential equations arise in the description of mechanical systems containing restoring and damping forces, in the kinetics of chemical reactions, and in many other contexts as well. Correspondingly, an important class of discrete-time systems is that for which the in- put and output are related through a linear constant-coefficient difference equation. Equa- tions of this type are used to describe the sequential behavior of many different processes. For instance, in Example 1.10 we saw how difference equations arise in describing the accumulation of savings in a bank account, and in Example 1.11 we saw how they can be used to describe a digital simulation of a continuous-time system described by a dif- ferential equation. Difference equations also arise quite frequently in the specification of discrete-time systems designed to perform particular operations on the input signal. For example, the system that calculates the difference between successive input values, as in eq. (1.99), and the system described by eq. (1.104) that computes the average value of the input over an interval are described by difference equations. Throughout this book, there will be many occasions in which we will consider and examine systems described by linear constant -coefficient differential and difference equa- tions. In this section we take a first look at these systems to introduce some of the basic ideas involved in solving differential and difference equations and to uncover and explore some of the properties of systems described by such equations. In subsequent chapters, we develop additional tools for the analysis of signals and systems that will add considerably both to our ability to analyze systems described by such equations and to our understanding of their characteristics and behavior. 1Throughout this book, we will use both the notations indicated in eq. (2.94) to denote first derivatives. Analogous notation will also be used for higher derivatives. Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 117 2 .4. 1 Linear Constant-Coefficient Differential Equations To introduce some of the important ideas concerning systems specified by linear constant- coefficient differential equations, let us consider a first-order differential equation as in eq. (1.85), viz., dy(t) ----;[! + 2y(t) = x(t), (2.95) where y(t) denotes the output of the system and x(t) is the input. For example, comparing eq. (2.95) to the differential equation (1.84) for the velocity of a vehicle subject to applied and frictional forces, we see that eq. (2.95) would correspond exactly to this system if y(t) were identified with the vehicle's velocity v(t), if x(t) were taken as the applied force f(t), and if the parameters in eq. (1.84) were normalized in units such that b/m = 2 and 1/m = 1. A very important point about differential equations such as eq. (2.95) is that they provide an implicit specification of the system. That is, they describe a relationship be- tween the input and the output, rather than an explicit expression for the system output as a function of the input. In order to obtain an explicit expression, we must solve the differential equation. To find a solution, we need more information than that provided by the differential equation alone. For example, to determine the speed of an automobile at the end of a 10 -second interval when it has been subjected to a constant acceleration of 1 m/sec2 for 10 seconds, we would also need to know how fast the vehicle was moving at the start of the interval. Similarly, if we are told that a constant source voltage of 1 volt is applied to the RC circuit in Figure 1.1 for 10 seconds, we cannot determine what theca- pacitor voltage is at the end of that interval without also knowing what the initial capacitor voltage is. More generally, to solve a differential equation, we must specify one or more auxil- iary conditions, and once these are specified, we can then, in principle, obtain an explicit expression for the output in terms of the input. In other words, a differential equation such as eq. (2.95) describes a constraint between the input and the output of a system, but to characterize the system completely, we must also specify auxiliary conditions. Different choices for these auxiliary conditions then lead to different relationships between the in- put and the output. For the most part, in this book we will focus on the use of differential equations to describe causal LTI systems, and for such systems the auxiliary conditions take a particular, simple form. To illustrate this and to uncover some of the basic properties of the solutions to differential equations, let us take a look at the solution of eq. (2.95) for a specific input signal x(t).2 20ur discussion of the solution of linear constant-coefficient differential equations is brief, since we as- sume that the reader has some familiarity with this material. For review, we recommend a text on the solution of ordinary differential equations, such as Ordinary Differential Equations (3rd ed.), by G. Birkhoff and G.-C. Rota (New York: John Wiley and Sons, 1978), or Elementary Differential Equations (3rd ed.), by W.E. Boyce and R.C. DiPrima (New York: John Wiley and Sons, 1977). There are also numerous texts that discuss differential equations in the context of circuit theory. See, for example, Basic Circuit Theory, by L.O. Chua, C.A. Desoer, and E.S. Kuh (New York: McGraw-Hill Book Company, 1987). As mentioned in the text, in the following chapters we present other very useful methods for solving linear differential equations that will be sufficient for our purposes. In addition, a number of exercises involving the solution of differential equations are included in the problems at the end of the chapter. 118 Linear Time-Invariant Systems Chap.2 Example 2. 1 4 Consider the solution of eq. (2.95) when the input signal is x(t) = K e""' u(t), (2.96) where K is a real number. The complete solution to eq. (2.96) consists of the sum of a particular solution, y""(t), and a homogeneous solution, yh(t), i.e., (2.97) where the particular solution satisfies eq. (2.95) and y11 (t) is a solution of the homoge- neous differential equation dv(t) ~ + 2y(t) = 0. (2.98) A common method for finding the particular solution for an exponential input signal as in eq. (2.96) is to look for a so-called forced response-i.e., a signal of the same form as the input. With regard to eq. (2.95), since x(t) = Ke""' fort> 0, we hypothesize a solution for t > 0 of the form (2.99) where Y is a number that we must determine. Substituting eqs. (2.96) and (2.99) into eq. (2.95) fort > 0 yields (2.100) Canceling the factor e""' from both sides of eq. (2.100), we obtain 3Y + 2Y = K, (2.101) or K y =- 5' (2.102) so that K \' (t) = - 3 5 e ' ' t > 0. (2.103) . p In order to determine y11 (t), we hypothesize a solution of the form Yh(t) = Ae 11 • (2.104) Substituting this into eq. (2.98) gives Ase 11 + 2Aesr = Aes'(s + 2) = 0. (2.105) From this equation, we see that we must takes = -2 and that Ae- 2' is a solution to eq. (2.98) for any choice of A. Utilizing this fact and eq. (2.103) in eq. (2.97), we find that the solution of the differential equation for t > 0 is .v (t) = Ae-~''"") + -K5e ·' ' ' t > 0. (2.106) Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 119 As noted earlier, the differential equation (2.95) by itself does not specify uniquely the response y(t) to the input x(t) in eq. (2.96). In particular, the constant A in eq. (2.106) has not yet been determined. In order for the value of A to be determined, we need to specify an auxiliary condition in addition to the differential equation (2.95). As explored in Problem 2.34, different choices for this auxiliary condition lead to different solutions y(t) and, consequently, to different relationships between the input and the output. As we have indicated, for the most part in this book we focus on differential and difference equations used to describe systems that are LTI and causal, and in this case the auxiliary condition takes the form of the condition of initial rest. That is, as shown in Problem 1.44, for a causal LTI system, if x(t) = 0 fort < t0 , then y(t) must also equal 0 fort < t0 . From eq. (2.96), we see that for our example x(t) = 0 fort < 0, and thus, the condition of initial rest implies that y(t) = 0 fort < 0. Evaluating eq. (2.1 06) at t = 0 and setting y(O) = 0 yields K O=A+5-' or Thus, for t > 0, (2.107) while for t < 0, y(t) = 0, because of the condition of initial rest. Combining these two cases, we obtain the full solution (2.108) Example 2.14 illustrates several very important points concerning linear constant- coefficient differential equations and the systems they represent. First, the response to an input x(t) will generally consist of the sum of a particular solution to the differential equation and a homogeneous solution-i.e., a solution to the differential equation with the input set to zero. The homogeneous solution is often referred to as the natural response of the system. The natural responses of simple electrical circuits and mechanical systems are explored in Problems 2.61 and 2.62. In Example 2.14 we also saw that, in order to determine completely the relation- ship between the input and the output of a system described by a differential equation such as eq. (2.95), we must specify auxiliary conditions. An implication of this fact, which is illustrated in Problem 2.34, is that different choices of auxiliary conditions lead to different relationships between the input and the output. As we illustrated in the ex- ample, for the most part we will use the condition of initial rest for systems described by differential equations. In the example, since the input was 0 for t < 0, the condition of initial rest implied the initial condition y(O) = 0. As we have stated, and as illustrated in 120 Linear Time-Invariant Systems Chap.2 Problem 2.33, under the condition of initial rest the system described by eq. (2.95) is LTI and causal.3 For example, if we multiply the input in eq. (2.96) by 2, the resulting output would be twice the output in eq. (2.1 08). It is important to emphasize that the condition of initial rest does not specify a zero initial condition at a fixed point in time, but rather adjusts this point in time so that the response is zero until the input becomes nonzero. Thus, if x(t) = 0 for t ~ to for the causal LTI system described by eq. (2.95), then y(t) = 0 for t ~ t0, and we would use the initial condition y(t0 ) = 0 to solve for the output for t > t0 . As a physical example, consider again the circuit in Figure 1.1, also discussed in Example 1.8. Initial rest for this example corresponds to the statement that, until we connect a nonzero voltage source to the circuit, the capacitor voltage is zero. Thus, if we begin to use the circuit at noon today, the initial capacitor voltage as we connect the voltage source at noon today is zero. Similarly, if we begin to use the circuit at noon tomorrow instead, the initial capacitor voltage as we connect the voltage source at noon tomorrow is zero. This example also provides us with some intuition as to why the condition of initial rest makes a system described by a linear constant-coefficient differential equation time invariant. For example, if we perform an experiment on the circuit, starting from initial rest, then, assuming that the coefficients Rand C don't change over time, we would expect to get the same results whether we ran the experiment today or tomorrow. That is, if we perform identical experiments on the two days, where the circuit starts from initial rest at noon on each day, then we would expect to see identical responses-i.e., responses that are simply time-shifted by one day with respect to each other. While we have used the first-order differential equation (2.95) as the vehicle for the discussion of these issues, the same ideas extend directly to systems described by higher order differential equations. A general Nth-order linear constant-coefficient differential equation is given by ~ dky(t) _ ~ b dk x(t) L ak-kd - L k-dk · (2.109) k=O t k=O t The order refers to the highest derivative of the output y(t) appearing in the equation. In the case when N = 0, eq. (2.1 09) reduces to _ 1 ~ b d k x( t) y ( t ) - - L k--k-. (2.110) ao k=O dt In this case, y(t) is an explicit function of the input x(t) and its derivatives. For N 2: 1, eq. (2.1 09) specifies the output implicitly in terms of the input. In this case, the analysis of the equation proceeds just as in our discussion of the first-order differential equation in Example 2.14. The solution y(t) consists of two parts-a particular solution to eq. (2.109) 3ln fact, as is also shown in Problem 2.34, if the initial condition for eq. (2.95) is nonzero, the resulting system is incrementally linear. That is, the overall response can be viewed, much as in Figure 1.48, as the superposition of the response to the initial conditions alone (with input set to 0) and the response to the input with an initial condition of 0 (i.e., the response of the causal LTI system described by eq. (2.95) ). Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 121 plus a solution to the homogeneous differential equation (2.111) The solutions to this equation are referred to as the natural responses of the system. As in the first-order case, the differential equation (2.1 09) does not completely spec- ify the output in terms of the input, and we need to identify auxiliary conditions to deter- mine completely the input-output relationship for the system. Once again, different choices for these auxiliary conditions result in different input-output relationships, but for the most part, in this book we will use the condition of initial rest when dealing with systems de- scribed by differential equations. That is, if x(t) = 0 fort::; t0 , we assume that y(t) = 0 for t::; t0 , and therefore, the response for t > to can be calculated from the differential equation (2.1 09) with the initial conditions _ dy(to) _ _ dN-iy(to) _ Y ( to ) - dt - ... - dtN-i - 0 . (2.112) Under the condition of initial rest, the system described by eq. (2.109) is causal and LTI. Given the initial conditions in eq. (2.112), the output y(t) can, in principle, be determined by solving the differential equation in the manner used in Example 2.14 and further illus- trated in several problems at the end of the chapter. However, in Chapters 4 and 9 we will develop some tools for the analysis of continuous-time LTI systems that greatly facilitate the solution of differential equations and, in particular, provide us with powerful methods for analyzing and characterizing the properties of systems described by such equations. 2.4.2 linear Constant-Coefficient Difference Equations The discrete-time counterpart of eq. (2.1 09) is the Nth-order linear constant -coefficient difference equation N M L aky[n- k] = L bkx[n - k]. (2.113) k=O k=O An equation of this type can be solved in a manner exactly analogous to that for differential equations. (See Problem 2.32.)4 Specifically, the solution y[n] can be written as the sum of a particular solution to eq. (2.113) and a solution to the homogeneous equation N Laky[n- k] = 0. (2.114) k=O 4For a detailed treatment of the methods for solving linear constant-coefficient difference equations, see Finite Difference Equations, by H. Levy and F. Lessman (New York: Macmillan, Inc., 1961), or Finite Difference Equations and Simulations (Englewood Cliffs, NJ: Prentice-Hall, 1968) by F. B. Hildebrand. In Chapter 6, we present another method for solving difference equations that greatly facilitates the analysis of linear time-invariant systems that are so described. In addition, we refer the reader to the problems at the end of this chapter that deal with the solution of difference equations. 122 Linear Time-Invariant Systems Chap.2 The solutions to this homogeneous equation are often referred to as the natural responses of the system described by eq. (2.113). As in the continuous-time case, eq. (2.113) does not completely specify the output in terms of the input. To do this, we must also specify some auxiliary conditions. While there are many possible choices for auxiliary conditions, leading to different input-output relationships, we will focus for the most part on the condition of initial rest-i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 as well. With initial rest, the system described by eq. (2.113) is LTI and causal. Although all of these properties can be developed following an approach that di- rectly parallels our discussion for differential equations, the discrete-time case offers an alternative path. This stems from the observation that eq. (2.113) can be rearranged in the form y[n] = -1 { LMb kx[n- k]- LNa ky[n- k] } . (2.115) ao k=O k=I Equation (2.115) directly expresses the output at time n in terms of previous values of the input and output. From this, we can immediately see the need for auxiliary conditions. In order to calculate y[n], we need to know y[n- 1], ... , y[n- N]. Therefore, if we are given the input for all n and a set of auxiliary conditions such as y[-N], y[-N + 1] , ... , y[ -1 ], eq. (2.115) can be solved for successive values of y[n]. An equation of the form of eq. (2.113) or eq. (2.115) is called a recursive equation, since it specifies a recursive procedure for determining the output in terms of the input and previous outputs. In the special case when N = 0, eq. (2.115) reduces to y[n] = ±(b k )x[n - k]. (2.116) k=O ao This is the discrete-time counterpart of the continuous-time system given in eq. (2.110). Here, y[n] is an explicit function of the present and previous values of the input. For this reason, eq. (2.116) is often called a nonrecursive equation, since we do not recursively use previously computed values of the output to compute the present value of the output. Therefore, just as in the case of the system given in eq. (2.110), we do not need auxiliary conditions in order to determine y[n]. Furthermore, eq. (2.116) describes an LTI system, and by direct computation, the impulse response of this system is found to be bn h[n] = G;;• 0 ~ n ~ M (2.117) { 0, otherwise That is, eq. (2.116) is nothing more than the convolution sum. Note that the impulse re- sponse for it has finite duration; that is, it is nonzero only over a finite time interval. Because of this property, the system specified by eq. (2.116) is often called a .finite impulse response (FIR) system. Although we do not require auxiliary conditions for the case of N = 0, such condi- tions are needed for the recursive case when N ;;:::: 1. To illustrate the solution of such an equation, and to gain some insight into the behavior and properties of recursive difference equations, let us examine the following simple example: Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 123 Example 2. 1 5 Consider the difference equation 1 y[n] - 2 y[n- 1] = x[n]. (2.118) Eq. (2.118) can also be expressed in the form 1 y[n] = x[n] + 2y [n- 1], (2.119) highlighting the fact that we need the previous value of the output, y[n- 1], to calculate the current value. Thus, to begin the recursion, we need an initial condition. For example, suppose that we impose the condition of initial rest and consider the input x[n] = Ko[n]. (2.120) In this case, since x[ n] = 0 for n ::::; - 1, the condition of initial rest implies that y[ n] = 0 for n ::::; - 1, so that we have as an initial condition y[ -1] = 0. Starting from this initial condition, we can solve for successive values of y[n] for n 2 0 as follows: 1 y[O] = x[O] + 2 y[ -1] = K, (2.121) 1 1 y[1] = x[l] + 2 y[O] = 2 K, (2.122) 2 y[2] = x[2] + 1 (1 ) 2 y[1] = 2 K, (2.123) y[n] = x[n] + 12y [n- 1] = (1 )II 2 K. (2.124) Since the system specified by eq. (2.118) and the condition of initial rest is LTI, its input- output behavior is completely characterized by its impulse response. Setting K = 1, we see that the impulse response for the system considered in this example is h[n] 1 )II = (2 u[n]. (2.125) Note that the causal LTI system in Example 2.15 has an impulse response of infinite duration. In fact, if N ~ 1 in eq. (2.113), so that the difference equation is recursive, it is usually the case that the LTI system corresponding to this equation together with the condition of initial rest will have an impulse response of infinite duration. Such systems are commonly referred to as infinite impulse response ( IIR) systems. As we have indicated, for the most part we will use recursive difference equations in the context of describing and analyzing systems that are linear, time-invariant, and causal, and consequently, we will usually make the assumption of initial rest. In Chapters 5 and 10 we will develop tools for the analysis of discrete-time systems that will provide us 124 Linear Time-Invariant Systems Chap.2 with very useful and efficient methods for solving linear constant-coefficient difference equations and for analyzing the properties of the systems that they describe. 2.4.3 Block Diagram Representations of First-Order Systems Described by Differential and Difference Equations An important property of systems described by linear constant -coefficient difference and differential equations is that they can be represented in very simple and natural ways in terms of block diagram interconnections of elementary operations. This is significant for a number of reasons. One is that it provides a pictorial representation which can add to our understanding of the behavior and properties of these systems. In addition, such representations can be of considerable value for the simulation or implementation of the systems. For example, the block diagram representation to be introduced in this section for continuous-time systems is the basis for early analog computer simulations of systems described by differential equations, and it can also be directly translated into a program for the simulation of such a system on a digital computer. In addition, the corresponding representation for discrete-time difference equations suggests simple and efficient ways in which the systems that the equations describe can be implemented in digital hardware. In this section, we illustrate the basic ideas behind these block diagram representations by constructing them for the causal first-order systems introduced in Examples 1.8-1.11. In Problems 2.57-2.60 and Chapters 9 and 10, we consider block diagrams for systems described by other, more complex differential and difference equations. We begin with the discrete-time case and, in particular, the causal system described by the first-order difference equation y[n] + ay[n- 1] = bx[n]. (2.126) To develop a block diagram representation of this system, note that the evaluation of eq. (2.126) requires three basic operations: addition, multiplication by a coefficient, and delay (to capture the relationship between y[n] and y[n - 1] ). Thus, let us define three basic network elements, as indicated in Figure 2.27. To see how these basic elements can be used to represent the causal system described by eq. (2.126), we rewrite this equation in the form that directly suggests a recursive algorithm for computing successive values of the output y[n]: _v[n] = -ay[n - 1] + bx[n]. (2.127) This algorithm is represented pictorially in Figure 2.28, which is an example of a feedback system, since the output is fed back through a delay and a multiplication by a coefficient and is then added to bx[n]. The presence of feedback is a direct consequence of the recur- sive nature of eq. (2.127). The block diagram in Figure 2.28 makes clear the required memory in this system and the consequent need for initial conditions. In particular, a delay corresponds to a mem- ory element, as the element must retain the previous value of its input. Thus, the initial value of this memory element serves as a necessary initial condition for the recursive cal- culation specified pictorially in Figure 2.28 and mathematically in eq. (2.127). Of course, if the system described by eq. (2.126) is initially at rest, the initial value stored in the memory element is zero. Sec. 2.4 Causal LTI Systems Described by Differential and Difference Equations 125 x2[n] x [n] ___, ..,~~. .....- -~•~x1 [n] + x [n] 1 2 (a) a x[n] ____. ...,. _____ ax[n] (b) Figure 2.27 Basic elements for the block diagram representation x[n] ·I D ....,__----!•~ x[n-1] of the causal system described by eq. (2.126): (a) an adder; (b) multi- plication by a coefficient; (c) a unit (c) delay. b x[n] D Figure 2.28 Block diagram repre- -a sentation for the causal discrete-time ....__ __,.. ......... _.... y[n -1] system described by eq. (2.126). Consider next the causal continuous-time system described by a first-order differen- tial equation: dy(t) ~ + ay(t) = bx(t). (2.128) As a first attempt at defining a block diagram representation for this system, let us rewrite it as y(t) = -! dy(t) + ~x(t). (2.129) a dt a The right-hand side of this equation involves three basic operations: addition, multiplica- tion by a coefficient, and differentiation. Therefore, if we define the three basic network elements indicated in Figure 2.29, we can consider representing eq. (2.129) as an inter- connection of these basic elements in a manner analogous to that used for the discrete-time system described previously, resulting in the block diagram of Figure 2.30. While the latter figure is a valid representation of the causal system described by eq. (2.128), it is not the representation that is most frequently used or the representation that leads directly to practical implementations, since differentiators are both difficult to implement and extremely sensitive to errors and noise. An alternative implementation that 126 Linear Time-Invariant Systems Chap. 2 (a) a x(t) ---...-ot---- ax(t) (b) Figure 2.29 One possible set of x(t)~d~~t) basic elements for the block diagram representation of the continuous-time system described by eq. (2.128): (a) an adder; (b) multiplication by a (c) coe11icient; (c) a differentiator. b/a x(t)....,.--t~ y(t) D Figure 2.30 Block diagram representation for the system in eqs. (2.128) and (2.129), using adders, -1/a dy(t) multiplications by coefficients, and dt di fferentiators. is much more widely used can be obtained by first rewriting eq. (2.128) as dJv(tt) = bx(t) - ay(t) (2.130) and then integrating from -x tot. Specifically, if we assume that in the system described by eq. (2.130) the value of y( -oo) is zero, then the integral of dy(t)ldt from -oo tot is precisely y(t). Consequently, we obtain the equation y(l) = Lz [h x(T) - ay( T) 1 dT. (2.131) In this form, our system can be implemented using the adder and coefficient multiplier indicated in Figure 2.29, together with an integrator, as defined in Figure 2.31. Figure 2.32 is a block diagram representation for this system using these elements. Sec. 2.5 Singularity Functions 127 Figure 2.31 Pictorial representation of an integrator. b x(t) ---t--( 1-----.......o~~ y(t) Figure 2.32 Block diagram rep- resentation for the system in eqs. (2.128) and (2.131 ), using adders, -a multiplications by coefficients, and in- tegrators. Since integrators can be readily implemented using operational amplifiers, repre- sentations such as that in Figure 2.32 lead directly to analog implementations, and indeed, this is the basis for both early analog computers and modem analog computation systems. Note that in the continuous-time case it is the integrator that represents the memory stor- age element of the system. This is perhaps more readily seen if we consider integrating eq. (2.130) from a finite point in time t0 , resulting in the expression y(t) = y(to) + J.r [bx(T)- ay(T)] dT. (2.132) to Equation (2.132) makes clear the fact that the specification of y(t) requires an initial con- dition, namely, the value of y (t0). It is precisely this value that the integrator stores at time t0 . While we have illustrated block diagram constructions only for the simplest first- order differential and difference equations, such block diagrams can also be developed for higher order systems, providing both valuable intuition for and possible implementations of these systems. Examples of block diagrams for higher order systems can be found in Problems 2.58 and 2.60. 2.5 SINGULARITY FUNCTIONS In this section, we take another look at the continuous-time unit impulse function in order to gain additional intuitions about this important idealized signal and to introduce a set of related signals known collectively as singularity functions. In particular, in Section 1.4.2 we suggested that a continuous-time unit impulse could be viewed as the idealization of a pulse that is ""short enough"" so that its shape and duration is of no practical consequence- i.e., so that as far as the response of any particular LTI system is concerned, all of the area under the pulse can be thought of as having been applied instantaneously. In this section, we would first like to provide a concrete example of what this means and then use the interpretation embodied within the example to show that the key to the use of unit impulses and other singularity functions is in the specification of how LTI systems respond to these idealized signals; i.e., the signals are in essence defined in terms of how they behave under convolution with other signals. 128 Linear Time-Invariant Systems Chap.2 2.5.1 The Unit Impulse as an Idealized Short Pulse From the sifting property, eq. (2.27), the unit impulse o(t) is the impulse response of the identity system. That is, x(f) = x(t) * D(f) (2.133) for any signal x(t). Therefore, if we take x(t) = o(t), we have o(t) = o(t) * o(t). (2.134) Equation (2.134) is a basic property of the unit impulse, and it also has a significant im- plication for our interpretation of the unit impulse as an idealized pulse. For example, as in Section 1.4.2, suppose that we think of o(t) as the limiting form of a rectangular pulse. Specifically, let D.::,.(t) correspond to the rectangular pulse defined in Figure 1.34, and let (2.135) Then r.::,.(f) is as sketched in Figure 2.33. If we wish to interpret o(t) as the limit as~ ~ 0 of D.::,.(t), then, by virtue of eq. (2.134), the limit as~ ~ 0 for r.::,.(t) must also be a unit impulse. In a similar manner, we can argue that the limits as~ ~ 0 of r.::,.(t) * r.::,.(t) or r!l(t) * D.::,.(t) must be unit impulses, and so on. Thus, we see that for consistency, if we define the unit impulse as the limiting form of some signal, then in fact, there is an unlimited number of very dissimilar-looking signals, all of which behave like an impulse in the limit. The key words in the preceding paragraph are ""behave like an impulse,"" where, as we have indicated, what we mean by this is that the response of an LTI system to all of these signals is essentially identical, as long as the pulse is ""short enough,"" i.e.,~ is ""small enough."" The following example illustrates this idea: r ... (t) 1 ..1 Figure 2.33 The signal r.. . (t) 0 2..1 defined in eq. (2.135). Example 2. 1 6 Consider the LTI system described by the first-order differential equation d v(t) -dt · + 2 .v (t) = x(t), (2.136) together with the condition of initial rest. Figure 2.34 depicts the response of this system to 8::.(t), r::.(t), r::.(t) * 8::.(t), and r::.(t) * r::.(t) for several values of d. Ford large enough, the responses to these input signals differ noticeably. However, ford sufficiently small, the responses are essentially indistinguishable, so that all of the input signals ""behave"" in the same way. Furthermore, as suggested by the figure, the limiting form of all of these responses is precisely e- 21 u(t). Since the limit of each of these signals as d ~ 0 is the unit impulse, we conclude that e- 21 u(t) is the impulse response for this system.5 5 In Chapters 4 and 9, we will describe much simpler ways to determine the impulse response of causal LTI systems described by linear constant-coefficient differential equations. Sec. 2.5 Singularity Functions 129 0.5 1 2 1 2 Responses to x(t) = o:,(t) Responses to x(t) = r:,(t) (a) (b) 2 Responses to x(t) = o:,(t)•r:,(t) Responses to x(t) = r:,(t)•r:,(t) (c) (d) h(t) = e - 21u (t) 0.5 (e) Figure 2.34 Interpretation of a unit impulse as the idealization of a pulse whose duration is ""short enough"" so that, as far as the response of an LTI system to this pulse is concerned, the pulse can be thought of as having been applied instantaneously: (a) responses of the causal LTI system de- scribed by eq. (2.136) to the input all (t) for ~ = 0.25, 0.1, and 0.0025; (b) responses of the same system to rfl (t) for the same values of ~; (c) re- sponses to otl(t)*rtl(t); (d) responses to rfl(t)*rtl(t); (e) the impulse response h(t) = e-2t u(t) for the system. Note that, for ~ = 0.25, there are noticeable differences among the responses to these different signals; however, as ~ becomes smaller, the differences diminish, and all of the responses converge to the impulse response shown in (e). 130 Linear Time-Invariant Systems Chap.2 One important point to be emphasized is that what we mean by ""~ small enough"" depends on the particular LTI system to which the preceding pulses are applied. For example, in Figure 2.35, we have illustrated the responses to these pulses for different 0.5 0.1 0.2 0.1 0.2 Responses to x(t) = 3~(t) Responses to x(t) = r~(t) (a) (b) 0.1 0.2 Responses to x(t) = 3~(t)• r ~(t) Responses to x(t) = r ~ (t)• r ~ (t) (c) (d) h(t) = e - 201u (t) 0.5 (e) Figure 2.35 Finding a value of~ that is ""small enough"" depends upon the system to which we are applying inputs: (a) responses of the causal LTI system described by eq. (2.137) to the input 8D.(t) for~= 0.025, 0.01, and 0.00025; (b) responses to rfl(t); (c) responses to 8fl(t)*rfl(t); (d) responses to rfl(t) * rfl(t); (e) the impulse response h(t) = e-201 u(t) for the system. Com- paring these responses to those in Figure 2.34, we see that we need to use a smaller value of ~ in this case before the duration and shape of the pulse are of no consequence. Sec. 2.5 Singularity Functions 131 values of Ll for the causal LTI system described by the first-order differential equation dy(t) ---;[{ + 20y(t) = x(t). (2.137) As seen in the figure, we need a smaller value of Ll in this case in order for the responses to be indistinguishable from each other and from the impulse response h(t) = e- 201 u(t) for the system. Thus, while what we mean by ""Ll small enough"" is different for these two systems, we can find values of Ll small enough for both. The unit impulse is then the idealization of a short pulse whose duration is short enough for all systems. 2.5.2 Defining the Unit Impulse through Convolution As the preceding example illustrates, for Ll small enough, the signals Ba(t), ra(t), ra(t) * Ba(t), and ra(t) * ra(t) all act like impulses when applied to an LTI system. In fact, there are many other signals for which this is true as well. What it suggests is that we should think of a unit impulse in terms of how an LTI system responds to it. While usually a function or signal is defined by what it is at each value of the independent variable, the primary importance of the unit impulse is not what it is at each value oft, but rather what it does under convolution. Thus, from the point of view of linear systems analysis, we may alternatively define the unit impulse as that signal which, when applied to an LTI system, yields the impulse response. That is, we define o(t) as the signal for which x(t) = x(t) * o(t) (2.138) for any x(t). In this sense, signals, such as Ba(t), ra(t), etc., which correspond to short pulses with vanishingly small duration as Ll ~ 0, all behave like a unit impulse in the limit because, if we replace o(t) by any of these signals, then eq. (2.138) is satisfied in the limit. All the properties of the unit impulse that we need can be obtained from the opera- tional definition given by eq. (2.138). For example, if we let x(t) = 1 for all t, then +oc 1 = X(t) = x(t) * D(t) = D(t) *X( f) = J-0 £ D( T)X(t - T) dT +oo = J- oc D( T) dT, so that the unit impulse has unit area. It is sometimes useful to use another completely equivalent operational definition of o(t). To obtain this alternative form, consider taking an arbitrary signal g(t), reversing it in time to obtain g( -t), and then convolving this with o(t). Using eq. (2.138), we obtain +oo g(-t) = g(-t) * D(t) = J-o c g(T- t)D(T)dT, which, for t = 0, yields +oo g(O) = J- oc g(T)D(T)dT. (2.139) 132 Linear Time-Invariant Systems Chap.2 Therefore, the operational definition of o(t) given by eq. (2.138) implies eq. (2.139). On the other hand, eq. (2.139) implies eq. (2.138). To see this, let x(t) be a given signal, fix a time t, and define g(T) = X(t- T). Then, using eq. (2.139), we have +x f +x X(t) = g(0) = f- x g( T) 0( T) dT = -x X(t - T) 0( T) dT, which is precisely eq. (2.138). Therefore, eq. (2.139) is an equivalent operational definition of the unit impulse. That is, the unit impulse is the signal which, when multiplied by a signal g(t) and then integrated from -oo to +oo, produces the value g(O). Since we will be concerned principally with LTI systems, and thus with convolution, the characterization of o(t) given in eq. (2.138) will be the one to which we will refer most often. However, eq. (2.139) is useful in determining some of the other properties of the unit impulse. For example, consider the signal f(t) o(t), where f(t) is another signal. Then, from eq. (2.139), r~~ g(r)f(r)O(r)dT = g(O)f(O). (2.140) On the other hand, if we consider the signal f(O) o(t), we see that r~x g(r)f(O)O(r)dr = g(O)f(O). (2.141) Comparing eqs. (2.140) and (2.141), we find thatthe two signals f(t) o(t) and f(O) o(t) be- have identically when they are multiplied by any signal g(t) and then integrated from -oo to +oo. Consequently, using this form of the operational definition of signals, we conclude that J(t) o(t) = J(O) o(t), (2.142) which is a property that we derived by alternative means in Section 1.4.2. [See eq. ( 1.76).] 2.5.3 Unit Doublets and Other Singularity Functions The unit impulse is one of a class of signals known as singularity functions, each of which can be defined operationally in terms of its behavior under convolution. Consider the LTI system for which the output is the derivative of the input, i.e., dx(t) y ( t) = -- (2.143) dt The unit impulse response of this system is the derivative of the unit impulse, which is called the unit doublet u 1( t). From the convolution representation for LTI systems, we have Sec. 2.5 Singularity Functions 133 dx(t) -----;[( = x(t) * u1( t) (2.144) for any signal x(t). Just as eq. (2.138) serves as the operational definition of l>(t), we will take eq. (2.144) as the operational definition of u1( t). Similarly, we can define u2(t), the second derivative of l>(t), as the impulse response of an LTI system that takes the second derivative of the input, i.e., (2.145) From eq. (2.144 ), we see that ddt (d---x--(;t[{) ) = X(t) * UJ (t) * UJ (t), (2.146) and therefore, U2(t) = UJ (t) * UJ (t). (2.147) In general, uk(t), k > 0, is the kth derivative of l>(t) and thus is the impulse response of a system that takes the kth derivative of the input. Since this system can be obtained as the cascade of k differentiators, we have Uk(t) = UJ (t) * · · · * UJ (t). (2.148) k times As with the unit impulse, each of these singularity functions has properties that can be derived from its operational definition. For example, if we consider the constant signal x(t) = 1, we find that dx(t) I+ oo 0 = -d- = X(t) * u1(t) = UJ(T)X(t- T)dT f -oc +oo = I-o c UJ(T)dT, so that the unit doublet has zero area. Moreover, if we convolve the signal g( -t) with u1( t), we obtain +oo dg( -t) I g(T- t)UJ(T)dT = g(-t) * UJ(t) = -d- -g'(-t), -oc f which, for t = 0, yields +oo -g'(O) = I-o o g(T)UJ(T)dT. (2.149) 134 Linear Time-Invariant Systems Chap.2 In an analogous manner, we can derive related properties of u 1( f) and higher order singu- larity functions, and several of these properties are considered in Problem 2.69. As with the unit impulse, each of these singularity functions can be informally re- lated to short pulses. For example, since the unit doublet is formally the derivative of the unit impulse, we can think of the doublet as the idealization of the derivative of a short pulse with unit area. For instance, consider the short pulse o~(t) in Figure 1.34. This pulse behaves like an impulse as Ll ~ 0. Consequently, we would expect its derivative to be- have like a doublet as Ll ~ 0. As verified in Problem 2.72, do~(t)/dt is as depicted in Figure 2.36: It consists of a unit impulse at t = 0 with area + 1/Ll, followed by a unit impulse of area -1/Ll at t = Ll, i.e., do:?> = ~ [o(t) - ou - .:l)]. (2.150) Consequently, using the fact that x(t) * o(t- t0 ) = x(t- t0 ) [see eq. (2.70)], we find that do~(t) x(t) - x(t- Ll) dx(t) x(t) * ----;]{ = Ll = df' (2.151) where the approximation becomes increasingly accurate as Ll ~ 0. Comparing eq. (2.151) with eq. (2.144), we see that d81.(t)/dt does indeed behave like a unit doublet as Ll ~ 0. In addition to singularity functions that are derivatives of different orders of the unit impulse, we can also define signals that represent successive integrals of the unit im- pulse function. As we saw in Example 2.13, the unit step is the impulse response of an integrator: Therefore, u(t) = L, 8(T)dT, (2.152) d8..l(t) dt _1.. ..1 Figure 2.36 The derivative do-'(t)ldt of the short rectangular pulse o-' (t) of Figure 1.34. Sec. 2.5 Singularity Functions 135 and we also have the following operational definition of u(t): x(t) * u(t) = r x('r) dT. (2.153) 00 Similarly, we can define the system that consists of a cascade of two integrators. Its impulse response is denoted by u_2(t), which is simply the convolution of u(t), the impulse response of one integrator, with itself: U-2(1) = u(t) * u(t) = roo u(T ) dT. (2.154) Since u(t) equals 0 fort < 0 and equals 1 fort > 0, it follows that U-2(t) = tu(t). (2.155) This signal, which is referred to as the unit ramp function, is shown in Figure 2.37. Also, we can obtain an operational definition for the behavior of u_2(t) under convolution from eqs. (2.153) and (2.154): x(t) * U-2(t) = x(t) * u(t) * u(t) = a~oo x(<T)d<T )· u(t) (2.156) = LU~oo x(u)du)dT In an analogous fashion, we can define higher order integrals of 8(t) as the impulse responses of cascades of integrators: U-k(t) = U(t) * ... * U(t) = Jt U-(k-1)( T) dT. (2.157) ~-Cfe k times The convolution of x(t) with u_ 3(t), u_4 (t), ... generate correspondingly higher order integrals of x(t). Also, note that the integrals in eq. (2.157) can be evaluated directly (see Figure 2.37 Unit ramp function. 136 Linear Time-Invariant Systems Chap.2 Problem 2.73), as was done in eq. (2.155), to obtain tk-1 u_k(t) = (k _ )! u(t). (2.158) 1 Thus, unlike the derivatives of o(t), the successive integrals of the unit impulse are func- tions that can be defined for each value oft [eq. (2.158)], as well as by their behavior under convolution. At times it will be worthwhile to use an alternative notation for o(t) and u(t), namely, o(t) = uo(t), (2.159) u(t) = U-1 (t). (2.160) With this notation, uk(t) for k > 0 denotes the impulse response of a cascade of k differ- entiators, u0(t) is the impulse response of the identity system, and, for k < 0, uk(t) is the impulse response of a cascade of Ik l integrators. Furthermore, since a differentiator is the inverse system of an integrator, u(t) * U1 (t) = O(t), or, in our alternative notation, u_ 1( t) * u1 (t) = uo(t). (2.161) More generally, from eqs. (2.148), (2.157), and (2.161 ), we see that for any integers k and r, (2.162) If k and rare both positive, eq. (2.162) states that a cascade of k differentiators followed by r more differentiators yields an output that is the (k + r)th derivative of the input. Similarly, if k is negative and r is negative, we have a cascade of lkl integrators followed by another lrl integrators. Also, if k is negative and r is positive, we have a cascade of lkl integrators followed by r differentiators, and the overall system is equivalent to a cascade of lk + rl integrators if k + r < 0, a cascade of k + r differentiators if k + r > 0, or the identity system if k + r = 0. Therefore, by defining singularity functions in terms of their behavior under convolution, we obtain a characterization that allows us to manipulate them with relative ease and to interpret them directly in terms of their significance for LTI systems. Since this is our primary concern in the book, the operational definition for singularity functions that we have given in this section will suffice for our purposes.6 6 As mentioned in Chapter 1, singularity functions have been heavily studied in the field of mathematics under the alternative names of generalized functions and distribution theory. The approach we have taken in this section is actually closely allied in spirit with the rigorous approach taken in the references given in footnote 3 of Section 1.4. Chap.2 Problems 137 2.6 SUMMARY In this chapter, we have developed important representations for LTI systems, both in dis- crete time and in continuous time. In discrete time we derived a representation of signals as weighted sums of shifted unit impulses, and we then used this to derive the convolution- sum representation for the response of a discrete-time LTI system. In continuous time we derived an analogous representation of continuous-time signals as weighted integrals of shifted unit impulses, and we used this to derive the convolution integral representation for continuous-time LTI systems. These representations are extremely important, as they allow us to compute the response of an LTI system to an arbitrary input in terms of the sys- tem's response to a unit impulse. Moreover, in Section 2.3 the convolution sum and integral provided us with a means of analyzing the properties of LTI systems and, in particular, of relating LTI system properties, including causality and stability, to corresponding proper- ties of the unit impulse response. Also, in Section 2.5 we developed an interpretation of the continuous-time unit impulse and other related singularity functions in terms of their behavior under convolution. This interpretation is particularly useful in the analysis of LTI systems. An important class of continuous-time systems consists of those described by linear constant-coefficient differential equations. Similarly, in discrete time, linear constant- coefficient difference equations play an equally important role. In Section 2.4, we exam- ined simple examples of differential and difference equations and discussed some of the properties of systems described by these types of equations. In particular, systems de- scribed by linear constant-coefficient differential and difference equations together with the condition of initial rest are causal and LTI. In subsequent chapters, we will develop additional tools that greatly facilitate our ability to analyze such systems. Chapte1 2 P1Dblem1 The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. Extension problems introduce applications, concepts, or methods beyond those pre- sented in the text. BASIC PROBLEMS WITH ANSWERS 2.1. Let x[n] = 8[n] + 28[n- 1] - 8[n- 3] and h[n] = 28[n + 1] + 28[n- 1]. Compute and plot each of the following convolutions: (a) YI [n] = x[n] * h[n] (b) Y2[n] = x[n + 2] * h[n] (c) Y3 [n] = x[n] * h[n + 2] 138 Linear Time-Invariant Systems Chap. 2 2.2. Consider the signal n-1 h[n] = ~ {u[n + 3] - u[n - 10]}. ( ) Express A and B in terms of n so that the following equation holds: h[n - k] = ( _!_ )n- k- I A < k < B 2 ' - - . { 0, elsewhere 2.3. Consider an input x[n] and a unit impulse response h[n] given by x[n] = (U-2 u[n- 2], h[n] = u[n + 2]. Determine and plot the output y[n] = x[n] * h[n]. 2.4. Compute and plot y[n] = x[n] * h[n], where x[n] = { ~ 3:Sn:S8 otherwise ' ~: 4 ::; n ::; 15 h[n] = { otherwise 2.5. Let x[n ] = 1, 0 ::; n ::; 9 and h[ n ] = { 1, 0 ::; n ::; N { 0, elsewhere 0, elsewhere ' where N ::; 9 is an integer. Determine the value of N, given that y[n] = x[n] * h[n] and y[4] = 5, y[l4] = 0. 2.6. Compute and plot the convolution y[n] = x[n] * h[n], where x[n] = (~ r· u[-n- I] and h[n] = u[n- 1]. 2.7. A linear systemS has the relationship CIJ y[n] = L x[k]g[n - 2k] k=-c;IO between its input x[n] and its output y[n], where g[n] = u[n] - u[n- 4]. Chap.2 Problems 139 (a) Determine y[n] when x[n] = o[n- 1]. (b) Determine y[n] when x[n] = o[n- 2]. (c) IsS LTI? (d) Determine y[n] when x[n] = u[n]. 2.8. Determine and sketch the convolution of the following two signals: t + 1, 0 ::; t ::; 1 x(t) = 2 - t, 1 < t ::; 2 , { 0, elsewhere h(t) = o(t + 2) + 2o(t + 1). 2.9. Let h(t) = e21u(-t+4)+e-21 u(t-5). Determine A and B such that -2(t-T) e ' r<A h(t- T) = 0, A<r<B. { e2(t-T), B<T 2.10. Suppose that ~ 0:St:S1 x(t) = { elsewhere and h(t) = x(tla), where 0 <a ::; 1. (a) Determine and sketch y(t) = x(t) * h(t). (b) If dy(t)ldt contains only three discontinuities, what is the value of a? 2.11. Let x(t) = u(t- 3)- u(t- 5) and h(t) = e-3t u(t). (a) Compute y(t) = x(t) * h(t). (b) Compute g(t) = (dx(t)ldt) * h(t). (c) How is g(t) related to y(t)? 2.12. Let 00 y(t) = e-tu(t) * .:L set- 3k). k= -00 Show that y(t) = Ae-t for 0 ::; t < 3, and determine the value of A. 140 Linear Time-Invariant Systems Chap. 2 2.13. Consider a discrete-time system 5 1 with impulse response 1 )fl h[n] = (S u[n]. (a) Find the integer A such that h[n] - Ah[n- 1] = o[n]. (b) Using the result from part (a), determine the impulse response g[n] of an LTI system 52 which is the inverse system of 5 1. 2.14. Which of the following impulse responses correspond(s) to stable LTI systems? (a) h1(t) = e-(1- 2j)ru(t) (b) h2(t) = e-r cos(2t)u(t) 2.15. Which of the following impulse responses correspond(s) to stable LTI systems? (a) h 1[n] = ncos(*n)u[n] (b) h2[n] = 311 u[ -n + 10] 2.16. For each of the following statements, determine whether it is true or false: (a) If x[n] = 0 for n < N 1 and h[n] = 0 for n < N2, then x[n] * h[n] = 0 for n < N1 + N2. (b) If y[n] = x[n] * h[n], then y[n- 1] = x[n- 1] * h[n- 1]. (c) If y(t) = x(t) * h(t), then y( -t) = x( -t) * h( -t). (d) If x(t) = 0 for t > T1 and h(t) = 0 for t > T2, then x(t) * h(t) = 0 for t > T1 + T2. 2.17. Consider an LTI system whose input x(t) and output y(t) are related by the differ- ential equation d dt y(t) + 4 y(t) = x(t). (P2.17-1) The system also satisfies the condition of initial rest. (a) If x(t) = e<-l+3j)tu(t), what is y(t)? (b) Note that CRe{x(t)} will satisfy eq. (P2.17-1) with CRe{y(t)}. Determine the out- put y(t) of the LTI system if x(t) = e -r cos(3t)u(t). 2.18. Consider a causal LTI system whose input x[n] and output y[n] are related by the difference equation 1 y[n] = 4 y[n - 1] + x[n]. Determine y[n] if x[n] = o[n - 1]. 2.19. Consider the cascade of the following two systems 5 1 and 52, as depicted in Figure P2.19: x[n]-j s, w[n]•l s, 1---~•..,.._ y[n] Figure P2. 1 9 Chap.2 Problems 141 S 1 : causal LTI, 1 w[n] = 2w[n- 1] + x[n]; S2 : causal LTI, y[n] = ay[n- 1] + f3w[n]. The difference equation relating x[n] and y[n] is: 1 3 y[n] = - Sy [n - 2] + 4 y[n - 1] + x[n]. (a) Determine a and f3. (b) Show the impulse response of the cascade connection of S1 and S2• 2.20. Evaluate the following integrals: (a) f _xx uo(t) cos(t) dt (b) ffi"" sin(27Tt) B(t + 3) dt (c) f ~5 u1 (1 - T) cos(27TT) dT BASIC PROBLEMS 2.21. Compute the convolution y[n] = x[n] * h[n] of the following pairs of signals: 11 (a) x[n] = a u[n], } a ¥= f3 h[n] = {3 11 u[n], (b) x[n] = h[n] = anu[n] (c) x[n] = (- ~ )"" u[n - 4] h[n] = 4""uC2 - n] (d) x[n] and h[n] are as in Figure P2.21. x[n] h[n] 1 ... JIIII ..... • ••• I I I I I I ... I I I I I I .... -1 0 1 2 3 4 5 n 0 1 2 3 4 5 6 7 8 9 10 11 12 1314 15 16 n Figure P2.21 2.22. For each of the following pairs of waveforms, use the convolution integral to find the response y(t) of the LTI system with impulse response h(t) to the input x(t). Sketch your results. (a) x(t) = e-ar u(t)} (Do this both when a ¥= f3 and when a = f3 .) h(t) = e-f3t u(t) 142 Linear Time-Invariant Systems Chap.2 (b) x(t) = u(t) - 2u(t - 2) + u(t - 5) h(t) = e2 t u(l - t) (c) x(t) and h(t) are as in Figure P2.22(a). (d) x(t) and h(t) are as in Figure P2.22(b). (e) x(t) and h(t) are as in Figure P2.22(c). x(t) h(t) 2 One period of sin Tit 2 (a) x(t) h(t) 2 _1 3 (b) x(t) h(t) -1 (c) Figure P2.22 2.23. Let h(t) be the triangular pulse shown in Figure P2.23(a), and let x(t) be the impulse train depicted in Figure P2.23(b ). That is, +x x(t) = L, 8(t - kT). k=-X Determine and sketch y(t) = x(t) * h(t) for the following values ofT: (a) T = 4 (b) T = 2 (c) T = 3/2 (d) T = I Chap. 2 Problems 143 h(t) -1 (a) ···t t t t { t t t t t t··· - 2T - T 0 T 2T 3T (b) Figure P2.23 2.24. Consider the cascade interconnection of three causal LTI systems, illustrated in Fig- ure P2.24(a). The impulse response h2[n] is h2[n] = u[n] - u[n - 2], and the overall impulse response is as shown in Figure P2.24(b ). x[n] y[n] (a) • • -1 0 1 2 3 4 5 6 7 n (b) Figure P2.24 (a) Find the impulse response h1 [n]. (b) Find the response of the overall system to the input x[n] = o[n] - o[n- 1]. 144 Linear Time-Invariant Systems Chap.2 2.25. Let the signal y[n] = x[n] * h[n], where x[n] = 3""u[ -n - I] + (~ )"" u[n] and h[n] = (~ )"" u[n + 3]. (a) Determine y[n] without utilizing the distributive property of convolution. (b) Determine y[n] utilizing the distributive property of convolution. 2.26. Consider the evaluation of y[n] = XJ [n] * x2[n] * x3[n], where x 1 [n] = (0.5Yu[n], x2[n] = u[n + 3], and x3[n] = B[n]- B[n- 1]. (a) Evaluate the convolution x1 [n] * x2 [n]. (b) Convolve the result of part (a) with x3[n] in order to evaluate y[n]. (c) Evaluate the convolution x2 [n] * x3 [n]. (d) Convolve the result of part (c) with x 1 [n] in order to evaluate y[n]. 2.27. We define the area under a continuous-time signal v(t) as +oo A\. = I-o o v(t) dt. Show that if y(t) = x(t) * h(t), then 2.28. The following are the impulse responses of discrete-time LTI systems. Determine whether each system is causal and/or stable. Justify your answers. (a) h[n] = (~)nu[n] (b) h[n] = (0.8)nu[n + 2] (c) h[n] = (~)nu[-n] (d) h[n] = (5)nu[3 - n] (e) h[n] = (- ~)nu[n] + (1.01) 11u[n- 1] (f) h[n] = (- ~)nu[n] + (l.Ol)n u[l - n] (g) h[n] = n(~)'1 u[n- 1] 2.29. The following are the impulse responses of continuous-time LTI systems. Determine whether each system is causal and/or stable. Justify your answers. (a) h(t) = e-4r u(t- 2) (b) h(t) = e-6t u(3 - t) (c) h(t) = e-2ru (t + 50) (d) h(t) = e21 u( -1 - t) Chap.2 Problems 145 (e) h(t) = e-61tl (0 h(t) = te-t u(t) (g) h(t) = (2e-t - e(t-IOO)!IOO)u(t) 2.30. Consider the first -order difference equation y[n] + 2y[n - 1] = x[n]. Assuming the condition of initial rest (i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 ), find the impulse response of a system whose input and output are related by this difference equation. You may solve the problem by rearranging the difference equation so as to express y[n] in terms of y[n -1] and x[n] and generating the values of y[O], y[ + 1] , y[ + 2], ... in that order. 2.31. Consider the LTI system initially at rest and described by the difference equation y[n] + 2y[n - 1] = x[n] + 2x[n - 2]. Find the response of this system to the input depicted in Figure P2.31 by solving the difference equation recursively. x[n] 2 1 ••••• T(J J rr\ •••• -2-1 o 12 34 n FigureP2.31 2.32. Consider the difference equation 1 y[n] - y[n- 1] = x[n], (P2.32-1) 2 and suppose that x[n] = (~ )"" u[n]. (P2.32-2) Assume that the solution y[n] consists of the sum of a particular solution Yp[n] to eq. (P2.32-l) and a homogeneous solution Yh[n] satisfying the equation 1 Yh[n]- 2Yh[n- 1] = 0. (a) Verify that the homogeneous solution is given by Yh[n] = AG)"" (b) Let us consider obtaining a particular solution Yp[n] such that Yp[n]- 1Y p[n- 1] = (1 )n 2 3 u[n]. 146 Linear Time-Invariant Systems Chap.2 By assuming that Yp[n] is of the form B( *) '1 for n 2: 0, and substituting this in the above difference equation, determine. the value of B. (c) Suppose that the LTI system described by eq. (P2.32-1) and initially at rest has as its input the signal specified by eq. (P2.32-2). Since x[n] = 0 for n < 0, we have that y[n] = 0 for n < 0. Also, from parts (a) and (b) we have that y[n] has the form y[n] =A (""12 )11 11 + B (1""3 ) for n :::::: 0. In order to solve for the unknown constant A, we must specify a value for y[n] for some n 2: 0. Use the condition of initial rest and eqs. (P2.32-1) and (P2.32-2) to determine y[O]. From this value determine the constant A. The result of this calculation yields the solution to the difference equation (P2.32-1) under the condition of initial rest, when the input is given by eq. (P2.32-2). 2.33. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation dy(t) -----;[( + 2y(t) = x(t). (P2.33-1) The system also satisfies the condition of initial rest. (a) (i) Determine the system output y 1( t) when the input is x 1( t) = e3 t u(t). (ii) Determine the system output y2(t) when the input is x2(t) = e2t u(t). (iii) Determine the system output y3(t) when the input is x3(t) = ae3t u(t) + {3e2tu(t), where a and {3 are real numbers. Show that y3(t) = ay1( t) + {3 Y2(t). (iv) Now let x 1 (t) and x2(t) be arbitrary signals such that x 1(t) = 0, fort< t1, x2(t) = 0, fort < t2. Letting Y1 (t) be the system output for input x 1 (t), y2(t) be the system output for input x2(t), and y3(t) be the system output for x3(t) = ax1( t) + {3x2(t), show that y3(t) = ay1 (t) + {3 Y2(t). We may therefore conclude that the system under consideration is linear. (b) (i) Determine the system output y 1 (t) when the input is x 1 (t) = K e2t u(t). (ii) Determine the system output y2(t) when the input is x2(t) = K e2(t-T) u(t - T). Show that Y2(t) = Y1 (t - T). (iii) Now let x1 (t) be an arbitrary signal such that x 1( t) = 0 fort < t0 . Letting Y! (t) be the system output for input x1 (t) and y2(t) be the system output for x2(t) = x 1( t - T), show that Y2(t) = Y! (t - T). Chap. 2 Problems 147 We may therefore conclude that the system under consideration is time invariant. In conjunction with the result derived in part (a), we conclude that the given system is LTI. Since this system satisfies the condition of initial rest, it is causal as well. 2.34. The initial rest assumption corresponds to a zero-valued auxiliary condition being imposed at a time determined in accordance with the input signal. In this problem we show that if the auxiliary condition used is nonzero or if it is always applied at a fixed time (regardless of the input signal) the corresponding system cannot be LTI. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation (P2.33-1). (a) Given the auxiliary condition y(1) = 1, use a counterexample to show that the system is not linear. (b) Given the auxiliary condition y(l) = 1, use a counterexample to show that the system is not time invariant. (c) Given the auxiliary condition y(1) = 1, show that the system is incrementally linear. (d) Given the auxiliary condition y(l) = 0, show that the system is linear but not time invariant. (e) Given the auxiliary condition y(O) + y(4) = 0, show that the system is linear but not time invariant. 2.35. In the previous problem we saw that application of an auxiliary condition at a fixed time (regardless of the input signal) leads to the corresponding system being not time-invariant. In this problem, we explore the effect of fixed auxiliary conditions on the causality of a system. Consider a system whose input x(t) and output y(t) satisfy the first-order differential equation (P2.33-1 ). Assume that the auxiliary condition associated with the differential equation is y(O) = 0. Determine the output of the system for each of the following two inputs: (a) x 1( t) = 0, for all t 0 t<-1 (b) X 2 ( t) = { }: t > _ 1 Observe that if y 1( t) is the output for input x 1( t) and y2(t) is the output for input x2(t), then y1 (t) and y2(t) are not identical fort < -1, even though x 1 (t) and x2(t) are identical fort < - 1. Use this observation as the basis of an argument to conclude that the given system is not causal. 2.36. Consider a discrete-time system whose input x[n] and output y[n] are related by y(n] = (~ )y[n- I] + x[n]. (a) Show that if this system satisfies the condition of initial rest (i.e., if x[n] = 0 for n < n0 , then y[n] = 0 for n < n0 ), then it is linear and time invariant. (b) Show that if this system does not satisfy the condition of initial rest, but instead uses the auxiliary condition y[O] = 0, it is not causal. [Hint: Use an approach similar to that used in Problem 2.35.] 148 Linear Time-Invariant Systems Chap. 2 2.37. Consider a system whose input and output are related by the first-order differential equation (P2.33-l ). Assume that the system satisfies the condition of final rest [i. e., if x(t) = 0 fort > t0 , then y(t) = 0 fort > t0]. Show that this system is not causal. [Hint: Consider two inputs to the system, x 1 (t) = 0 and x2(t) = er(u(t)- u(t- 1)), which result in outputs y 1( t) and y2(t), respectively. Then show that y 1( t) =I= y2(t) fort< 0.] 2.38. Draw block diagram representations for causal LTI systems described by the fol- lowing difference equations: (a) y[n] = *y[n- 1] + ~x[n] (b) y[n] = *y[n- 1] + x[n- 1] 2.39. Draw block diagram representations for causal LTI systems described by the fol- lowing differential equations: (a) y(t) = -( ~) dy(t)ldt + 4x(t) (b) dy(t)ldt + -3y(t) = x(t) ADVANCED PROBLEMS 2.40. (a) Consider an LTI system with input and output related through the equation y(t) = f~ e (t-T) x(T- 2)dr. What is the impulse response h(t) for this system? (b) Determine the response of the system when the input x(t) is as shown in Figure P2.40. -1 2 Figure P2.40 2.41. Consider the signal x[n] = a 11 u[n]. (a) Sketch the signal g[n] = x[n] - ax[n - 1]. (b) Use the result of part (a) in conjunction with properties of convolution in order to determine a sequence h[n] such that 11 x[n] * h[n] = 1 ) (2 {u[n + 2] - u[n - 2]}. 2.42. Suppose that the signal x(t) = u(t + 0.5) - u(t - 0.5) Chap.2 Problems 149 is convolved with the signal (a) Determine a value of w 0 which ensures that y(O) = 0, where y(t) = x(t) * h(t). (b) Is your answer to the previous part unique? 2.43. One of the important properties of convolution, in both continuous and discrete time, is the associativity property. In this problem, we will check and illustrate this prop- erty. (a) Prove the equality [x(t) * h(t)] * g(t) = x(t) * [h(t) * g(t)] (P2.43-l) by showing that both sides of eq. (P2.43-l) equal I+ oo I+ oo -oo -oo x( r)h(u)g(t - T - u) dr du. (b) Consider two LTI systems with the unit sample responses hdn] and h2 [n] shown in Figure P2.43(a). These two systems are cascaded as shown in Figure P2.43(b). Let x[n] = u[n]. h1[n] = (- ~ )nu[n] 3 • 0000 ri • n h2[n] = u[n] +~ u[n-1] ~ 2 - - - - : x[n] 0 1 2 3 4 n (a) (b) Figure P2.43 150 Linear Time-Invariant Systems Chap.2 (i) Compute y[n] by first computing w[n] = x[n] * h 1 [n] and then computing y[n] = w[n] * h2[n]; that is, y[n] = [x[n] * h1 [n]] * h2[n]. (ii) Now find y[n] by first convolving h1 [n] and h2[n] to obtain g[n] h1 [n] * h2[n] and then convolving x[n] with g[n] to obtain y[n] x[n] * [hi [n] * h2[n]]. The answers to (i) and (ii) should be identical, illustrating the associativity prop- erty of discrete-time convolution. (c) Consider the cascade of two LTI systems as in Figure P2.43(b), where in this case h1 [n] = sin 8n and and where the input is x[n] = B[n] - aB[n- 1]. Determine the output y[n]. (Hint: The use of the associative and commutative properties of convolution should greatly facilitate the solution.) 2.44. (a) If x(t) = 0, Jtl > TJ, and h(t) = 0, JtJ > T2, then x(t) * h(t) = 0, ltl > T3 for some positive number T3 . Express T3 in terms of T1 and T2 . (b) A discrete-time LTI system has input x[n], impulse response h[n], and output y[n]. If h[n] is known to be zero everywhere outside the interval No :s; n :s; N1 and x[n] is known to be zero everywhere outside the interval N2 :s; n :s; N3 , then the output y[n] is constrained to be zero everywhere, except on some interval N4 :s; n :s; Ns. (i) Determine N4 and N 5 in terms of N 0 , N1, N 2 , and N 3 . (ii) If the interval N 0 :s; n :s; N1 is of length Mh, N 2 :s; n :s; N 3 is of length Mx, and N4 :s; n :s; N5 is of length My, express My in terms of M 11 and Mx. (c) Consider a discrete-time LTI system with the property that if the input x[n] = 0 for all n 2: 10, then the output y[n] = 0 for all n 2: 15. What condition must h[n], the impulse response of the system, satisfy for this to be true? (d) Consider an LTI system with impulse response in Figure P2.44. Over what in- terval must we know x(t) in order to determine y(O)? Chap.2 Problems 151 h(t) -2 -1 6 Figure P2.44 2.45. (a) Show that if the response of an LTI system to x(t) is the output y(t), then the response of the system to x'(t) = dx(t) dt is y'(t). Do this problem in three different ways: (i) Directly from the properties of linearity and time invariance and the fact that '( ) _ . x(t) - x(t - h) X t - 1Ill h . h---'>0 (ii) By differentiating the convolution integral. (iii) By examining the system in Figure P2.45. x(t) y(t) Figure P2.45 (b) Demonstrate the validity of the following relationships: (i) y'(t) = x(t) * h'(t) (ii) y(t) = cCxx(r)dr) * h'(t) = Cx[x'(r) * h(r)Jdr = x'(t) * cf~xh(r)dr) [Hint: These are easily done using block diagrams as in (iii) of part (a) and the fact that UJ (t) * U-] (t) = O(t).] (c) An LTI system has the response y(t) = sin w 0t to input x(t) = e-St u(t). Use the result of part (a) to aid in determining the impulse response of this system. (d) Let s(t) be the unit step response of a continuous-time LTI system. Use part (b) to deduce that the response y(t) to the input x(t) is (P2.45-1) Show also that (P2.45-2) 152 Linear Time-Invariant Systems Chap.2 (e) Use eq. (P2.45-1) to determine the response of an LTI system with step response s(t) = (e- 3'- 2e- 2' + l)u(t) to the input x(t) = e' u(t). (0 Let s[n] be the unit step response of a discrete-time LTI system. What are the discrete-time counterparts of eqs. (P2.45-l) and (P2.45-2)? 2.46. Consider an LTI systemS and a signal x(t) = 2e- 3t u(t - 1). If x(t) ~ y(t) and ddx(tt) ~ -3y(t) + e-2tu(t), determine the impulse response h(t) of S. 2.47. We are given a certain linear time-invariant system with impulse response h0(t). We are told that when the input is x0(t) the output is y0(t), which is sketched in Figure P2.47. We are then given the following set of inputs to linear time-invariant systems with the indicated impulse responses: Input x(t) Impulse response h(t) (a) x(t) = 2xo(t) h(t) = h0(t) (b) x(t) = x0(t)- xo(t- 2) h(t) = ho(t) (c) x(t) = xo(t - 2) h(t) = h0(t + 1) (d) x(t) = x0( -t) h(t) = ho(t) (e) x(t) = x0( -t) h(t) = h0(- t) (0 x(t) = xb(t) h(t) = hb(t) [Here xb(t) and hb(t) denote the first derivatives of x0(t) and h0(t), respectively.] 0 2 Figure P2.47 In each of these cases, determine whether or not we have enough information to determine the output y(t) when the input is x(t) and the system has impulse re- sponse h(t). If it is possible to determine y(t), provide an accurate sketch of it with numerical values clearly indicated on the graph. Chap. 2 Problems 153 2.48. Determine whether each of the following statements concerning LTI systems is true or false. Justify your answers. (a) If h(t) is the impulse response of an LTI system and h(t) is periodic and nonzero, the system is unstable. (b) The inverse of a causal LTI system is always causal. (c) If lh[n]l :::::; K for each n, where K is a given number, then the LTI system with h[n] as its impulse response is stable. (d) If a discrete-time LTI system has an impulse response h[n] of finite duration, the system is stable. (e) If an LTI system is causal, it is stable. (f) The cascade of a non causal LTI system with a causal one is necessarily non- causal. (g) A continuous-time LTI system is stable if and only if its step response s(t) is absolutely integrable-that is, if and only if (h) A discrete-time LTI system is causal if and only if its step response s[n] is zero for n < 0. 2.49. In the text, we showed that if h[n] is absolutely summable, i.e., if +oc L, lh[kJI < oo, k= -oc then the LTI system with impulse response h[n] is stable. This means that absolute summability is a sufficient condition for stability. In this problem, we shall show that it is also a necessary condition. Consider an LTI system with impulse response h[n] that is not absolutely summable; that is, +oc L, ih[kJI = oo. k= -oc (a) Suppose that the input to this system is 0, if h[ -n] = 0 x[n] = { h[-n] if h[ -n] =I= 0 · lh[-nll' Does this input signal represent a bounded input? If so, what is the smallest number B such that lx[n]l :::::; B for all n? 154 Linear Time-Invariant Systems Chap.2 (b) Calculate the output at n = 0 for this particular choice of input. Does the re- sult prove the contention that absolute summability is a necessary condition for stability? (c) In a similar fashion, show that a continuous-time LTI system is stable if and only if its impulse response is absolutely integrable. 2.50. Consider the cascade of two systems shown in Figure P2.50. The first system, A, is known to be LTI. The second system, B, is known to be the inverse of system A. Let y 1 (t) denote the response of system A to x 1 (t), and let y2(t) denote the response of system A to x2(t). x(t) --1 Sy~!m I y(t) •I Sy~em 1----1•~ x(t) Figure P2.50 (a) What is the response of system B to the input ay1( t) + by2(t), where a and bare constants? (b) What is the response of system B to the input y1( t- T)? 2.51. In the text, we saw that the overall input-output relationship of the cascade of two LTI systems does not depend on the order in which they are cascaded. This fact, known as the commutativity property, depends on both the linearity and the time in variance of both systems. In this problem, we illustrate the point. (a) Consider two discrete-time systems A and B, where system A is an LTI system with unit sample response h[n] = (l/2)""u[n]. System B, on the other hand, is linear but time varying. Specifically, if the input to system B is w[n], its output is z[n] = nw[n]. Show that the commutativity property does not hold for these two systems by computing the impulse responses of the cascade combinations in Figures P2.5l(a) and P2.51(b), respectively. x[n] ....,_--t~ System y[n] x[n] ....,_--t~ System y[n] B A (a) (b) Figure P2. 51 (b) Suppose that we replace system B in each of the interconnected systems of Figure P2.51 by the system with the following relationship between its input w[n] and output z[n]: z[n] = w[n] + 2. Repeat the calculations of part (a) in this case. Chap. 2 Problems 155 2.52. Consider a discrete-time LTI system with unit sample response h[n] = (n + l)a""u[n], where Ia I < 1. Show that the step response of this system is = 1 a n a ) n] ] s[n] [ (a _ 1)2 - (a _ 1)2 a + (a _ 1) (n + 1 a u[n . (Hint: Note that 2.53. (a) Consider the homogeneous differential equation (P2.53-1) Show that if s0 is a solution of the equation N p(s) = L aksk = 0, (P2.53-2) k=O then Aesot is a solution of eq. (P2.53-1 ), where A is an arbitrary complex con- stant. (b) The polynomial p(s) in eq. (P2.53-2) can be factored in terms of its roots SJ, ... , Sr as where the Si are the distinct solutions of eq. (P2.53-2) and the ui are their multiplicities-that is, the number of times each root appears as a solution of the equation. Note that CT I + U2 + · · . + CT r = N. In general, if ui > 1, then not only is Aesit a solution of eq. (P2.53-1), but so is Atj e·1J, as long as j is an integer greater than or equal to zero and less than or equal to ui - 1. To illustrate this, show that if Ui = 2, then Atesit is a solution of eq. (P2.53-l). [Hint: Show that if sis an arbitrary complex number, then 156 Linear Time-Invariant Systems Chap.2 Thus, the most general solution of eq. (P2.53-l) is r fT 1 -] ~ ~ A··tjes;t LL I.J ' i= I j=O where the Ai.i are arbitrary complex constants. (c) Solve the following homogeneous differential equations with the specified aux- iliary conditions: (i) d~i~·~n + 3d~~n + 2y(t) = 0, y(O) = 0, y'(O) = 2 (ii) dd~y) + 3 d;;~n + 2y(t) = o, y(O) = I, y'(O) = -1 (iii) d~i~·;n + 3 d~;~n + 2y(t) = 0, y(O) = 0, y'(O) = 0 (iv) d~i~y) + 2 d:;~n + y(t) = 0, y(O) = 1, y'(O) = 1 (v) dd~·~n + d~i~;'l - d~;~n - y(t) = 0, y(O) = 1, y'(O) = 1, y""(O) = -2 2 (vi) d y~n + 2 dyUJ + 5y(t) = 0 v(O) = 1 y'(O) = dt- dt ' . ' 2.54. (a) Consider the homogeneous difference equation N Laky[n- k] = 0, (P2.54-1) k=O Show that if zo is a solution of the equation N Lakz-k = 0, (P2.54-2) k=O then Azg is a solution of eq. (P2.54-l ), where A is an arbitrary constant. (b) As it is more convenient for the moment to work with polynomials that have only nonnegative powers of z, consider the equation obtained by multiplying both sides of eq. (P2.54-2) by zN: N p(z) = L akzN-k = 0. (P2.54-3) k=O The polynomial p(z) can be factored as p(z) = ao(z - ZJ yr 1 ••• (z - z,.)u', where the z1, ••• , z,. are the distinct roots of p(z). Show that if y[n] = nzn-l, then ±a ky[n- k] = dp(z) zn-N + (n- N)p(z)zn-N-l. k=O dz Use this fact to show that if (Ji = 2, then both Az? and Bnz?- 1 are solutions of eq. (P2.54-1), where A and Bare arbitrary complex constants. More generally, one can use this same procedure to show that if O""i > 1, then Chap.2 Problems 157 A n! zn-r r!(n- r)! is a solution of eq. (P2.54-l) for r = 0, 1, ... , ui- 1.7 (c) Solve the following homogeneous difference equations with the specified aux- iliary conditions: (i) y[n] + ~y[n- 1] + iy[n- 2] = 0; y[O] = 1, y[ -1] = -6 (ii) y[n] - 2y[n- 1] + y[n - 2] = 0; y[O] = 1, y[l] = 0 (iii) y[n] - 2y[n- 1] + y[n- 2] = 0; y[O] = 1, y[10] = 21 (iv) y[n] - f!- y[n- 1] + ~y[n- 2] = 0; y[O] = 0, y[ -1] = 1 2.55. In the text we described one method for solving linear constant-coefficient difference equations, and another method for doing this was illustrated in Problem 2.30. If the assumption of initial rest is made so that the system described by the difference equation is LTI and causal, then, in principle, we can determine the unit impulse response h[n] using either of these procedures. In Chapter 5, we describe another method that allows us to determine h[n] in a more elegant way. In this problem we describe yet another approach, which basically shows that h[n] can be determined by solving the homogeneous equation with appropriate initial conditions. (a) Consider the system initially at rest and described by the equation 1 y[n]- ly[n- 1] = x[n]. (P2.55-1) Assuming that x[n] = S[n], what is y[O]? What equation does h[n] satisfy for n 2:: 1, and with what auxiliary condition? Solve this equation to obtain a closed-form expression for h[n]. (b) Consider next the LTI system initially at rest and described by the difference equation 1 y[n]- ly[n- 1] = x[n] + 2x[n- 1]. (P 2.55-2) This system is depicted in Figure P2.55(a) as a cascade of two LTI systems that are initially at rest. Because of the properties of LTI systems, we can reverse the order of the systems in the cascade to obtain an alternative representation of the same overall system, as illustrated in Figure P2.55(b ). From this fact, use the result of part (a) to determine the impulse response for the system de- scribed by eq. (P2.55-2). (c) Consider again the system of part (a), with h[n] denoting its impulse response. Show, by verifying that eq. (P2.55-3) satisfies the difference equation (P2.55- 1), that the response y[ n] to an arbitrary input x[ n] is in fact given by the con- volution sum +oc y[n] = L h[n - m]x[m]. (P2.55-3) 7Here, weareusingfactorialnotation-thatis, k! = k(k - l)(k- 2) ... (2)(1), whereO! is defined to be 1. 158 Linear Time-Invariant Systems Chap. 2 z[n] x[n] z[n] = x[n] + 2x[n-1] 1 ....,._~~ y[n]- 2y[n-1] = z[n] y[n] (a) w[n] x[n] ...... w[n]- ~w[n-1] = x[n] y[n] = w[n] + 2w[n-1] ~ y[n] (b) Figure P2.55 (d) Consider the LTI system initially at rest and described by the difference equa- tion N ~ aky[n - k] = x[n]. (P2.55-4) k=O Assuming that a0 =I= 0, what is y[O] if x[n] = B[n]? Using this result, specify the homogeneous equation and initial conditions that the impulse response of the system must satisfy. Consider next the causal LTI system described by the difference equation N M ~ aky[n - k] = ~ bkx[n - k]. (P2.55-5) k=O k=O Express the impulse response of this system in terms of that for the LTI system described by eq. (P2.55-4). (e) There is an alternative method for determining the impulse response of the LTI system described by eq. (P2.55-5). Specifically, given the condition of initial rest, i.e., in this case, y[-N] = y[-N + 1] = ... = y[ -1] = 0, solve eq. (P2.55-5) recursively when x[n] = B[n] in order to determine y[O], ... , y[M]. What equation does h[n] satisfy for n 2:: M? What are the appropriate initial conditions for this equation? (0 Using either of the methods outlined in parts (d) and (e), find the impulse re- sponses of the causal LTI systems described by the following equations: (i) y[n] - y[n - 2] = x[n] (ii) y[n] - y[n - 2] = x[n] + 2x[n - 1] (iii) y[n] - y[n - 2] = 2x[n] - 3x[n - 4] (iv) y[n] - ( ]3!2)y[n- 1] + ~y[n- 2] = x[n] 2.56. In this problem, we consider a procedure that is the continuous-time counterpart of the technique developed in Problem 2.55. Again, we will see that the problem of determining the impulse response h(t) for t > 0 for an LTI system initially at rest and described by a linear constant-coefficient differential equation reduces to the problem of solving the homogeneous equation with appropriate initial conditions. Chap.2 Problems 159 (a) Consider the LTI system initially at rest and described by the differential equa- tion ddy(tt) + 2y(t) = x(t). (P2.56-l) Suppose that x(t) = o(t). In order to determine the value of y(t) immediately after the application of the unit impulse, consider integrating eq. (P2.56-1) from t = o- to t = o+ (i.e., from ""just before"" to ""just after"" the application of the impulse). This yields o+ o+ y(O+)- y(O-) + 2 fa_ y(T)dT = fa_ D(T)dT = 1. (P2.56-2) Since the system is initially at rest and x(t) = 0 fort < 0, y(O-) = 0. To satisfy eq. (P2.56-2) we must have y(O+) = 1. Thus, since x(t) = 0 for t > 0, the impulse response of our system is the solution of the homogeneous differential equation dy(t) + 2y(t) = 0 dt with initial condition Solve this differential equation to obtain the impulse response h(t) for the sys- tem. Check your result by showing that +CX) y(t) = J-oc h(t - T)X( T) dT satisfies eq. (P2.56-l) for any input x(t). (b) To generalize the preceding argument, consider an LTI system initially at rest and described by the differential equation N dky(t) .L,ak-k- = x(t) (P2.56-3) k=O dt with x(t) = o(t). Assume the condition of initial rest, which, since x(t) = 0 for t < 0, implies that - dy - dN-1 y - y(O ) = d t (0 ) = .. . = d tN -I (0 ) = 0. (P2.56-4) Integrate both sides of eq. (P2.56-3) once from t = o- to t = o+, and use eq. (P2.56-4) and an argument similar to that used in part (a) to show that the 160 Linear Time-Invariant Systems Chap.2 resulting equation is satisfied with + dy + y(O ) = dt (0 ) = ... (P2.56-5a) and (P2.56-5b) Consequently, the system's impulse response fort> 0 can be obtained by solv- ing the homogeneous equation ~ dky(t) - 0 Lak--- k=O dtk with initial conditions given by eqs. (P2.56-5). (c) Consider now the causal LTI system described by the differential equation ~ dky(t) _ ~ b dkx(t) Lak k -Lk k"" (P2.56-6) k=O dt k=O dt Express the impulse response of this system in terms of that for the system of part (b). (Hint: Examine Figure P2.56.) __.. ~ dkw(t) w(t) M dkw(t) x(t) L ak -k- = x(t) y(t) = l bk -k- r---+- y(t) k = 0 dt k = 0 dt Figure P2. 56 (d) Apply the procedures outlined in parts (b) and (c) to find the impulse responses for the LTI systems initially at rest and described by the following differential equations: (i) d~i~~t) + 3 d~~n + 2y(t) = x(t) (ii) d~i~~t) + 2 d;;~tl + 2y(t) = x(t) (e) Use the results of parts (b) and (c) to deduce that if M ~ N in eq. (P2.56-6), then the impulse response h(t) will contain singularity terms concentrated at t = 0. In particular, h(t) will contain a term of the form M-N L lXrUr(t), r=O where the a,. are constants and the ur(t) are the singularity functions defined in Section 2.5. (f) Find the impulse responses of the causal LTI systems described by the following differential equations: (i) d;;~n + 2y(t) = 3 d~~;n + x(t) (ii) l('-y~t) + 5 dy(t) + 6y(t) = 2 d'x(t) + 2 d x~f) + 4 dx(t) + 3x(t) dt- dt dt 3 dt- dt Chap. 2 Problems 161 2.57. Consider a causal LTI systemS whose input x[n] and output y[n] are related by the difference equation y[n] = -ay[n- 1] + box[n] + b 1x[n- 1]. (a) Verify that S may be considered a cascade connection of two causal LTI systems sl and s2 with the following input-output relationship: sl : Yl [n] = box I [n] + bl X) [n- 1], s2 : Y2[n] = -ay2[n- 1] + X2[n]. (b) Draw a block diagram representation of S1 • (c) Draw a block diagram representation of S2• (d) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (f) Show that the two unit-delay elements in the block diagram representation of S obtained in part (e) may be collapsed into one unit-delay element. The result- ing block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (d) and (e) are referred to as Direct Form I realizations of S. 2.58. Consider a causal LTI systemS whose input x[n] and output y[n] are related by the difference equation 2y[n] - y[n - 1] + y[n - 3] = x[n] - 5x[n- 4]. (a) Verify that S may be considered a cascade connection of two causal LTI systems S 1 and S2 with the following input-output relationship: S1 : 2yl [n] = X1 [n] - 5XJ [n- 4], 1 1 S2 : Y2[n] = 2Y 2[n- 1] - 2Y 2[n- 3] + x2[n]. (b) Draw a block diagram representation of S1• (c) Draw a block diagram representation of S2 . (d) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (f) Show that the four delay elements in the block diagram representation of S obtained in part (e) may be collapsed to three. The resulting block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (d) and (e) are referred to as Direct Form I realizations of S. 162 Linear Time-Invariant Systems Chap. 2 2.59. Consider a causal LTI system 5 whose input x(t) and output y(t) are related by the differential equation dv(t) dx(t) a1-d· + aov(t) = box(t) + b1 -d-. t - t (a) Show that and express the constants A, B, and C in terms of the constants a0 , a 1, b0 , and b 1 • (b) Show that 5 may be considered a cascade connection of the following two causal LTI systems: sl : )'J(I) ~ Bx\(t) + c Lyx(T)dT, s, : y,(t) ~ A r/ '( T) dT + x,(r). (c) Draw a block diagram representation of 51• (d) Draw a block diagram representation of 52. (e) Draw a block diagram representation of 5 as a cascade connection of the block diagram representation of 51 followed by the block diagram representation of 52. (f) Draw a block diagram representation of 5 as a cascade connection of the block diagram representation of 52 followed by the block diagram of representa- tion 5 1• (g) Show that the two integrators in your answer to part (f) may be collapsed into one. The resulting block diagram is referred to as a Direct Form II realization of 5, while the block diagrams obtained in parts (e) and (f) are referred to as Direct Form I realizations of 5. 2.60. Consider a causal LTI system 5 whose input x(t) and output y(f) are related by the differential equation (a) Show that y( t) ~ A Ly( T) d T + B L ([y( <T) d <T) dT + Cx(t) + DL x(T)dT + EL ([x(<T)d<T )dT, Chap. 2 Problems 163 and express the constants A, B, C, D, and E in terms of the constants a0, a 1, a2 , bo, b,, and b2. (b) Show that S may be considered a cascade connection of the following two causal LTI systems: s, : y,(t) = ex, (I)+ of% x, (r)dT + E L~ u:%XJ(<T)d<T) dT, s2 : Y2(1) = A r%Y2( T) dT + B L% (fxY2(lT) d<T) dT + x2(1). (c) Draw a block diagram representation of S1• (d) Draw a block diagram representation of S2 . (e) Draw a block diagram representation of S as a cascade connection of the block diagram representation of S 1 followed by the block diagram representation of S2. (f) Draw a block diagram representation of S as a cascade connection of the block diagram representation of s2 followed by the block diagram representation of S1• (g) Show that the four integrators in your answer to part (f) may be collapsed into two. The resulting block diagram is referred to as a Direct Form II realization of S, while the block diagrams obtained in parts (e) and (f) are referred to as Direct Form I realizations of S. EXTENSION PROBLEMS 2.61. (a) In the circuit shown in Figure P2.6l(a), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. L=1H 1 y(t) C =1F x(t) l (a) Figure P2.61 a (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form K1 eiw 1 t + K e.iw 21 2 • Specify the values of w 1 and w 2 . (iii) Show that, since the voltage and current are restricted to be real, the natural response of the system is sinusoidal. 164 Linear Time-Invariant Systems Chap.2 (b) In the circuit shown in Figure P2.61 (b), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. R = 10 x(t) + C =1F (b) Figure P2.61 b (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the natural response of this system has the form K e-ar, and spec- ify the value of a. (c) In the circuit shown in Figure P2.61(c), x(t) is the input voltage. The voltage y(t) across the capacitor is considered to be the system output. R = 2!1 L = 1H x(t) + r (c) Figure P2.61 c (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form e- 0 '{K 2 2 1 ei ' + K2e- i '}, and specify the value of a. (iii) Show that, since the voltage and current are restricted to be real, the natural response of the system is a decaying sinusoid. 2.62. (a) In the mechanical system shown in Figure P2.62(a), the force x(t) applied to the mass represents the input, while the displacement y(t) of the mass repre- sents the output. Determine the differential equation relating x(t) and y(t). Show that the natural response of this system is periodic. (b) Consider Figure P2.62(b ), in which the force x(t) is the input and the velocity y(t) is the output. The mass of the car ism, while the coefficient of kinetic fric- tion is p. Show that the natural response of tqis system decays with increasing time. (c) In the mechanical system shown in Figure P2.62(c), the force x(t) applied to the mass represents the input, while the displacement y(t) of the mass represents the output. Chap.2 Problems 165 m = 1,000 Kg p=0.1 N-s/m x(t) (a) (b) K = Spring constant = 2 N/m m =Mass= 1 Kg b = Damping constant= 2 N-s/m ! x(t) (c) Figure P2.62 (i) Determine the differential equation relating x(t) and y(t). (ii) Show that the homogeneous solution of the differential equation from part (i) has the form e-at{K1 ejt + K2e- jt}, and specify the value of a. (iii) Show that, since the force and displacement are restricted to be real, the natural response of the system is a decaying sinusoid. 166 Linear Time-Invariant Systems Chap. 2 2.63. A $100,000 mortgage is to be retired by equal monthly payments of D dollars. In- terest, compounded monthly, is charged at the rate of 12% per annum on the unpaid balance; for example, after the first month, the total debt equals 0 $100,000 + ( ;~2 )$100,000 = $101,000. The problem is to determine D such that after a specified time the mortgage is paid in full, leaving a net balance of zero. (a) To set up the problem, let y[n] denote the unpaid balance after the nth monthly payment. Assume that the principal is borrowed in month 0 and monthly pay- ments begin in month 1. Show that y[n] satisfies the difference equation y[n] - yy[n - 1] = - D n 2: 1 (P2.63-1) with initial condition y[O] = $100,000, where y is a constant. Determine y. (b) Solve the difference equation of part (a) to determine y[n] for n 2: 0. (Hint: The particular solution of eq. (P2.63-1) is a constant Y. Find the value of Y, and express y[n] for n 2: 1 as the sum of particular and homogeneous solutions. Determine the unknown constant in the homogeneous solution by directly calculating y[l] from eq. (P2.63-1) and comparing it to your solution.) (c) If the mortgage is to be retired in 30 years after 360 monthly payments of D dollars, determine the appropriate value of D. (d) What is the total payment to the bank over the 30-year period? (e) Why do banks make loans? 2.64. One important use of inverse systems is in situations in which one wishes to remove distortions of some type. A good example of this is the problem of removing echoes from acoustic signals. For example, if an auditorium has a perceptible echo, then an initial acoustic impulse will be followed by attenuated versions of the sound at regularly spaced intervals. Consequently, an often -used model for this phenomenon is an LTI system with an impulse response consisting of a train of impulses, i.e., h(t) = ~ hko(t - kT). (P2.64-1) k=O Here the echoes occur T seconds apart, and hk represents the gain factor on the kth echo resulting from an initial acoustic impulse. (a) Suppose that x(t) represents the original acoustic signal (the music produced by an orchestra, for example) and that y(t) = x(t) * h(t) is the actual signal that is heard if no processing is done to remove the echoes. In order to remove the distortion introduced by the echoes, assume that a microphone is used to sense y(t) and that the resulting signal is transduced into an electrical signal. We will Chap.2 Problems 167 also use y(t) to denote this signal, as it represents the electrical equivalent of the acoustic signal, and we can go from one to the other via acoustic-electrical conversion systems. The important point to note is that the system with impulse response given by eq. (P2.64-1) is invertible. Therefore, we can find an LTI system with im- pulse response g(t) such that y(t) * g(t) = x(t), and thus, by processing the electrical signal y(t) in this fashion and then con- verting back to an acoustic signal, we can remove the troublesome echoes. The required impulse response g(t) is also an impulse train: g(t) = L gk8(t- kT). k=O Determine the algebraic equations that the successive gk must satisfy, and solve these equations for go, g 1, and g2 in terms of hk· (b) Suppose that h0 = 1, h 1 = 1/2, and hi = 0 for all i ~ 2. What is g(t) in this case? (c) A good model for the generation of echoes is illustrated in Figure P2.64. Hence, each successive echo represents a fed-back version of y(t), delayed by T sec- onds and scaled by a. Typically, 0 < a < 1, as successive echoes are attenu- ated. x(t) _.,.. + y(t) a Delay """"""- T Figure P2. 64 (i) What is the impulse response of this system? (Assume initial rest, i.e., y(t) = 0 fort < 0 if x(t) = 0 fort < 0.) (ii) Show that the system is stable if 0 < a < 1 and unstable if a > 1. (iii) What is g(t) in this case? Construct a realization of the inverse system using adders, coefficient multipliers, and T-second delay elements. (d) Although we have phrased the preceding discussion in terms of continuous-time systems because of the application we have been considering, the same general ideas hold in discrete time. That is, the LTI system with impulse response h[n] = L, hk8[n - kN] k=O is invertible and has as its inverse an LTI system with impulse response g[n] = L, gk8[n- kN]. k=O 168 Linear Time-Invariant Systems Chap.2 It is not difficult to check that the gk satisfy the same algebraic equations as in part (a). Consider now the discrete-time LTI system with impulse response h[n] = L 8[n- kN]. k=-% This system is not invertible. Find two inputs that produce the same output. 2.65. In Problem 1.45, we introduced and examined some of the basic properties of cor- relation functions for continuous-time signals. The discrete-time counterpart of the correlation function has essentially the same properties as those in continuous time, and both are extremely important in numerous applications (as is discussed in Prob- lems 2.66 and 2.67). In this problem, we introduce the discrete-time correlation function and examine several more of its properties. Let x[n] and y[n] be two real-valued discrete-time signals. The autocorrela- tion functions <Pxx[n] and </>yy[n] of x[n] and y[n], respectively, are defined by the expressions +ex; <PxAn] = L x[m + n]x[m] m= -oo and +rye </>yy[n] = L y[m + n]y[m], m=-oc and the cross-correlation functions are given by +rye </>xy[n] = L x[m + n]y[m] m= -oc and +oo </>yx[n] = L y[m + n]x[m]. m= -oo As in continuous time, these functions possess certain symmetry properties. Specif- ically, <PxAn] and </>yy[n] are even functions, while </>xy[n] = </>yx[- n]. (a) Compute the autocorrelation sequences for the signals Xt [n], x2[n], x3[n], and x4 [n] depicted in Figure P2.65. (b) Compute the cross-correlation sequences </>x;x [n], i # j, i, j = 1, 2, 3, 4, 1 for xi[n], i = 1, 2, 3, 4, as shown in Figure P2.65. (c) Let x[n] be the input to an LTI system with unit sample response h[n], and let the corresponding output be y[n]. Find expressions for <f>xy[n] and </>yy[n] in terms of <PxAn] and h[n]. Show how <Pxy[n] and </>yy[n] can be viewed as the output Chap.2 Problems 169 • • • JII. 0 1 2 3 • • n • • • • n x4 [n] • • • • • • • J -1 0 1 n 0 • • • .r . .. 5 n Figure P2.65 of LTI systems with 4>xAn] as the input. (Do this by explicitly specifying the impulse response of each of the two systems.) (d) Let h[n] = x1 [n] in Figure P2.65, and let y[n] be the output of the LTI system with impulse response h[n] when the input x[n] also equals x 1 [n]. Calculate 4>x_v[n] and 4>_v_v[n] using the results of part (c). 2.66. Let h1( t), h2(t), and h3(t), as sketched in Figure P2.66, be the impulse responses of three LTI systems. These three signals are known as Walsh functions and are of considerable practical importance because they can be easily generated by digital logic circuitry and because multiplication by each of them can be implemented in a simple fashion by a polarity-reversing switch. -1 Figure P2.66 (a) Determine and sketch a choice for x 1 (t), a continuous-time signal with the fol- lowing properties: (i) x 1( t) is real. (ii) x1( t) = 0 fort < 0. (iii) \xi (t)\ ::; 1 fort ~ 0. (iv) y 1( t) = x1( t) * h(t) is as large as possible at t = 4. (b) Repeat part (a) for x2(t) and x3(t) by making Y2(t) = x2(t) * h2(t) and y3(t) = x3(t) * h3(t) each as large as possible at t = 4. (c) What is the value of Yij(l) = Xi(t) * h j(l), i =I= j at time t = 4 for i, j = 1, 2, 3? 170 Linear Time-Invariant Systems Chap.2 The system with impulse response hi(t) is known as the matched filter for the signal Xi(t) because the impulse response is tuned to xi(t) in order to produce the maximum output signal. In the next problem, we relate the concept of a matched filter to that of the correlation function for continuous-time signals. 2.67. The cross-correlation function between two continuous-time real signals x(t) and y(t) is c/>xy(t) = J_~ x(t + T)y(T)dT. (P2.67-l) The autocorrelation function of a signal x(t) is obtained by setting y(t) = x(t) in eq. (P2.67-1): c/>xx(t) = r: X(t + T)X(T)dT. (a) Compute the autocorrelation function for each of the two signals x 1 (t) and x2(t) depicted in Figure P2.67(a). 0 2 0 7 -1 (a) I I 4 -1t- -1 (b) Figure P2.67 (b) Let x(t) be a given signal, and assume that x(t) is of finite duration-i.e., that x(t) = 0 for t < 0 and t > T. Find the impulse response of an LTI system so that <f>xxU - T) is the output if x(t) is the input. (c) The system determined in part (b) is a matched filter for the signal x(t). That this definition of a matched filter is identical to the one introduced in Problem 2.66 can be seen from the following: Chap. 2 Problems 171 Let x(t) be as in part (b), and let y(t) denote the response to x(t) of an LTI system with real impulse response h(t). Assume that h(t) = 0 for t < 0 and fort> T. Show that the choice for h(t) that maximizes y(T), subject to the constraint that JoT h2(t)dt = M, a fixed positive number, (P2.67-2) is a scalar multiple of the impulse response determined in part (b). [Hint: Schwartz's inequality states that r Lb 2 ] I /2 [ Lb 2 ] 1/2 u(t)v(t)dt ,; u (t)dt v (t)dt [ for any two signals u(t) and v(t). Use this to obtain a bound on y(T).] (d) The constraint given by eq. (P2.67-2) simply provides a scaling to the impulse response, as increasing M merely changes the scalar multiplier mentioned in part (c). Thus, we see that the particular choice for h(t) in parts (b) and (c) is matched to the signal x(t) to produce maximum output. This is an extremely important property in a number of applications, as we will now indicate. In communication problems, one often wishes to transmit one of a small number of possible pieces of information. For example, if a complex message is encoded into a sequence of binary digits, we can imagine a system that trans- mits the information bit by bit. Each bit can then be transmitted by sending one signal, say, x0(t) , if the bit is a 0, or a different signal x1( t) if a 1 is to be com- municated. In this case, the receiving system for these signals must be capable of recognizing whether x0(t) or x 1 (t) has been received. Intuitively, what makes sense is to have two systems in the receiver, one tuned to x0(t) and one tuned to x1 (t), where, by ""tuned,"" we mean that the system gives a large output after the signal to which it is tuned is received. The property of producing a large output when a particular signal is received is exactly what the matched filter possesses. In practice, there is always distortion and interference in the transmission and reception processes. Consequently, we want to maximize the difference be- tween the response of a matched filter to the input to which it is matched and the response of the filter to one of the other signals that can be transmitted. To illustrate this point, consider the two signals x0 (t) and XI (t) depicted in Fig- ure P2.67(b). Let L0 denote the matched filter for x0(t), and let L1 denote the matched filter for x1( t). (i) Sketch the responses of Lo to x0(t) and x1 (t). Do the same for L1• (ii) Compare the values of these responses at t = 4. How might you modify x0 (t) so that the receiver would have an even easier job of distinguishing between x0(t) and x1 (t) in that the response of L0 to XI (t) and L1 to x0(t) would both be zero at t = 4? 2.68. Another application in which matched filters and correlation functions play an im- portant role is radar systems. The underlying principle of radar is that an electro- 172 Linear Time-Invariant Systems Chap.2 magnetic pulse transmitted at a target will be reflected by the target and will subse- quently return to the sender with a delay proportional to the distance to the target. Ideally, the received signal will simply be a shifted and possibly scaled version of the original transmitted signal. Let p(t) be the original pulse that is sent out. Show that c/>pp(O) = max c/>pp(t). t That is, c/>pp(O) is the largest value taken by c/>pp(t). Use this equation to deduce that, if the waveform that comes back to the sender is x(t) = a p(t - to), where a is a positive constant, then c/>xp(to) = max c/>xp(t). t (Hint: Use Schwartz's inequality.) Thus, the way in which simple radar ranging systems work is based on using a matched filter for the transmitted waveform p(t) and noting the time at which the output of this system reaches its maximum value. 2.69. In Section 2.5, we characterized the unit doublet through the equation +oo x(t) * u1(t) = J-o o x(t- r)u 1(r)dr = x'(t) (P2.69-1) for any signal x(t). From this equation, we derived the relationship +oo J-o o g(r)u 1(r)dr = -g'(O). (P2.69-2) (a) Show that eq. (P2.69-2) is an equivalent characterization of u1( t) by showing that eq. (P2.69-2) implies eq. (P2.69-1). [Hint: Fix t, and define the signal g( r) = x(t - r).] Thus, we have seen that characterizing the unit impulse or unit doublet by how it behaves under convolution is equivalent to characterizing how it be- haves under integration when multiplied by an arbitrary signal g(t). In fact, as indicated in Section 2.5, the equivalence of these operational definitions holds for all signals and, in particular, for all singularity functions. (b) Let f(t) be a given signal. Show that f(t)ul (t) = f(O)ui (t)- f'(O)o(t) by showing that both functions have the same operational definitions. (c) What is the value of Find an expression for f(t)u2(t) analogous to that in part (b) for f(t)u 1( t). Chap.2 Problems 173 2.70. In analogy with continuous-time singularity functions, we can define a set of discrete-time signals. Specifically, let u_ 1[ n] = u[n], uo[n] = o[n], and u, [n] = o[n] - o[n- 1], and define k>O k times and uk[n] = u_, [n] * u_, [n] * · · · * u_, [n], k < 0. lkl times Note that x[n] * 8 [n] = x[n], 00 x[n] * u[n] = L x[m], m= -oo and x[n] * u1 [n] = x[n] - x[n - 1], (a) What is 00 L x[m]u1 [m]? m=oo (b) Show that x[n]u 1 [n] = x[O]u 1 [n] - [x[1] - x[O]]o[n- 1] = x[1]u 1 [n] - [x[l] - x[O]]o[n]. (c) Sketch the signals u2 [n] and u3 [n]. (d) Sketch U-2[n] and U-3 [n]. (e) Show that, in general, for k > 0, (-l)nk! uk[n] = '(k _ ) 1 [u[n] - u[n - k- 1]]. (P2.70-1) n. n. (Hint: Use induction. From part (c), it is evident that uk[n] satisfies eq. (P2.70-1) fork = 2 and 3. Then, assuming that eq. (P2.70-1) satisfies uk[n], write uk+l [n] in terms of uk[n], and show that the equation also satisfies Uk+ I [n].) 174 Linear Time-Invariant Systems Chap.2 (f) Show that, in general, fork> 0, (n + k- 1)! u_k[n] = n!(k _ 1)! u[n]. (P2.70-2) (Hint: Again, use induction. Note that U-(k+ n[n] - U-(k+ l)[n- 1] = u_k[n]. (P2.70-3) Then, assuming that eq. (P2.70-2) is valid for u_k[n], use eq. (P2.70-3) to show that eq. (P2.70-2) is valid for U-(k+l)[n] as well.) 2.71. In this chapter, we have used several properties and ideas that greatly facilitate the analysis of LTI systems. Among these are two that we wish to examine a bit more closely. As we will see, in certain very special cases one must be careful in using these properties, which otherwise hold without qualification. (a) One of the basic and most important properties of convolution (in both con tin- uous and discrete time) is associativity. That is, if x(t), h(t), and g(t) are three signals, then x(t) * [g(t) * h(t)] = [x(t) * g(t)] * h(t) = [x(t) * h(t)] * g(t). (P2.71-l) This relationship holds as long as all three expressions are well defined and finite. As that is usually the case in practice, we will in general use the asso- ciativity property without comments or assumptions. However, there are some cases in which it does not hold. For example, consider the system depicted in Figure P2.71, with h(t) = u1( t) and g(t) = u(t). Compute the response of this system to the input x(t) = 1 for all t. x(t) -8--~E]----. y(t) x(t) -G~--~~ y(t) Figure P2. 71 Do this in the three different ways suggested by eq. (P2.71-l) and by the figure: (i) By first convolving the two impulse responses and then convolving the result with x(t). (ii) By first convolving x(t) with u1( t) and then convolving the result with u(t). (iii) By first convolving x(t) with u(t) and then convolving the result with u 1 (t). Chap.2 Problems 175 (b) Repeat part (a) for x(t) = e-t and h(t) = e -t u(t), g(t) = u1 (t) + o(t). (c) Do the same for x[n] = (U· h[n] = (4 )"" u[n], 1 g[n] = o[n] - 2o[n- 1]. Thus, in general, the associativity property of convolution holds if and only if the three expressions in eq. (P2.71-1) make sense (i.e., if and only if their interpretations in terms of LTI systems are meaningful). For example, in part (a) differentiating a constant and then integrating makes sense, but the process of integrating the constant from t = -oo and then differentiating does not, and it is only in such cases that associativity breaks down. Closely related to the foregoing discussion is an issue involving inverse systems. Consider the LTI system with impulse response h(t) = u(t). As we saw in part (a), there are inputs-specifically, x(t) = nonzero constant-for which the output of this system is infinite, and thus, it is meaningless to consider the question of inverting such outputs to recover the input. However, if we limit ourselves to inputs that do yield finite outputs, that is, inputs which satisfy (P2.71-2) then the system is invertible, and the LTI system with impulse response u1 (t) is its inverse. (d) Show that the LTI system with impulse response u1 (t) is not invertible. (Hint: Find two different inputs that both yield zero output for all time.) However, show that the system is invertible if we limit ourselves to inputs that satisfy eq. (P2.71-2). [Hint: In Problem 1.44, we showed that an LTI system is invertible if no input other than x(t) = 0 yields an output that is zero for all time; are there two inputs x(t) that satisfy eq. (P2.71-2) and that yield identically zero responses when convolved with u1 (t)?] What we have illustrated in this problem is the following: (1) If x(t), h(t), and g(t) are three signals, and if x(t) * g(t), x(t) * h(t), and h(t) * g(t) are all well defined and finite, then the associativity property, eq. (P2. 71-1 ), holds. 176 Linear Time-Invariant Systems Chap.2 (2) Let h(t) be the impulse response of an LTI system, and suppose that the impulse response g(t) of a second system has the property h(t) * g(t) = o(t). (P2.71-3) Then, from (1), for all inputs x(t) for which x(t) * h(t) and x(t) * g(t) are both well defined and finite, the two cascades of systems depicted in Fig- ure P2. 71 act as the identity system, and thus, the two LTI systems can be regarded as inverses of one another. For example, if h(t) = u(t) and g(t) = u1 (t), then, as long as we restrict ourselves to inputs satisfying eq. (P2.71-2), we can regard these two systems as inverses. Therefore, we see that the associativity property of eq. (P2.71-1) and the definition ofLTI inverses as given in eq. (P2.71-3) are valid, as long as all convolutions that are involved are finite. As this is certainly the case in any realistic problem, we will in general use these properties without comment or qualification. Note that, although we have phrased most of our discussion in terms of continuous-time signals and systems, the same points can also be made in discrete time [as should be evident from part (c)]. 2.72. Let 8L1(t) denote the rectangular pulse of height -k for 0 < t ::; Ll. Verify that d 1 dt oLl(t) = K [o(t) - o(t - L1)]. 2.73. Show by induction that tk-1 u_k(t) = (k _ )! u(t) fork = 1, 2, 3 ... 1 3 FouRIER SERIES REPRESENTATION OF PERIODIC SIGNALS 3.0 INTRODUCTION The representation and analysis of LTI systems through the convolution sum as developed in Chapter 2 is based on representing signals as linear combinations of shifted impulses. In this and the following two chapters, we explore an alternative representation for signals and LTI systems. As in Chapter 2, the starting point for our discussion is the development of a representation of signals as linear combinations of a set of basic signals. For this alternative representation we use complex exponentials. The resulting representations are known as the continuous-time and discrete-time Fourier series and transform. As we will see, these can be used to construct broad and useful classes of signals. We then proceed as we did in Chapter 2. That is, because of the superposition prop- erty, the response of an LTI system to any input consisting of a linear combination of basic signals is the same linear combination of the individual responses to each of the basic sig- nals. In Chapter 2, these responses were all shifted versions of the unit impulse response, leading to the convolution sum or integral. As we will find in the current chapter, the re- sponse of an LTI system to a complex exponential also has a particularly simple form, which then provides us with another convenient representation for LTI systems and with another way in which to analyze these systems and gain insight into their properties. In this chapter, we focus on the representation of continuous-time and discrete-time periodic signals referred to as the Fourier series. In Chapters 4 and 5, we extend the anal- ysis to the Fourier transform representation of broad classes of aperiodic, finite energy signals. Together, these representations provide one of the most powerful and important sets of tools and insights for analyzing, designing, and understanding signals and LTI sys- tems, and we devote considerable attention in this and subsequent chapters to exploring the uses of Fourier methods. 177 178 Fourier Series Representation of Periodic Signals Chap. 3 We begin in the next section with a brief historical perspective in order to provide some insight into the concepts and issues that we develop in more detail in the sections and chapters that follow. 3.1 A HISTORICAL PERSPECTIVE The development of Fourier analysis has a long history involving a great many individ- uals and the investigation of many different physical phenomena. 1 The concept of using ""trigonometric sums""-that is, sums of harmonically related sines and cosines or periodic complex exponentials-to describe periodic phenomena goes back at least as far as the Babylonians, who used ideas of this type in order to predict astronomical events.2 The modern history of the subject begins in 1748 with L. Euler, who examined the motion of a vibrating string. In Figure 3.1, we have indicated the first few of what are known as the ""normal modes"" of such a string. If we consider the vertical deflection j(t, x) of the string at time t and at a distance x along the string, then for any fixed instant of time, the normal modes are harmonically related sinusoidal functions of x. What Euler noted was that if the configuration of a vibrating string at some point in time is a linear combination of these normal modes, so is the configuration at any subsequent time. Furthermore, Euler showed that one could calculate the coefficients for the linear combination at the later time in a very straightforward manner from the coefficients at the earlier time. In doing this, Euler performed the same type of calculation as we will in the next section in deriving one of the properties of trigonometric sums that make them so useful for the analysis of LTI systems. Specifically, we will see that if the input to an LTI system is expressed as a linear combination of periodic complex exponentials or sinusoids, the output can also be expressed in this form, with coefficients that are related in a straightforward way to those oftheinput. The property described in the preceding paragraph would not be particularly useful, unless it were true that a large class of interesting functions could be represented by linear combinations of complex exponentials. In the middle of the 18th century, this point was the subject of heated debate. In 1753, D. Bernoulli argued on physical grounds that all physi- cal motions of a string could be represented by linear combinations of normal modes, but he did not pursue this mathematically, and his ideas were not widely accepted. In fact, Eu- ler himself discarded trigonometric series, and in 1759 J. L. Lagrange strongly criticized the use of trigonometric series in the examination of vibrating strings. His criticism was based on his own belief that it was impossible to represent signals with comers (i.e., with discontinuous slopes) using trigonometric series. Since such a configuration arises from 1 The historical material in this chapter was taken from the following references: I. Grattan-Guiness. Joseph Fourier, !76S-JR30 (Cambridge, MA: The MIT Press, 1972): G. F. Simmons, Differential Equations: With Applications and Historical Notes (New York: McGraw-Hill Book Company, 1972): C. Lanczos, Dis- course 011 Fourier Series (London: Oliver and Boyd, 1966): R. E. Edwards, Fourier Series: A Modem Intro- duction (New York: Springer-Verlag, 2nd ed., 1970): and A. D. Aleksandrov, A. N. Kolmogorov, and M.A. Lavrent'ev, Mathematics: Its Content, Methods. and Mean in[?, trans. S. H. Gould, Vol. II: trans. K. Hirsch, Vol. III (Cambridge, MA: The MIT Press, 1969). Of these, Grattan-Guiness' work offers the most complete account of Fourier's life and contributions. Other references are cited in several places in the chapter. ' H. Dym and H. P. McKean, Fourier Series and lntef?rals (New York: Academic Press, 1972). This text and the book of Simmons cited in footnote I also contain discussions of the vibrating-string problem and its role in the development of Fourier analysis. Sec. 3.1 A Historical Perspective 179 >------------x---+- Position along the string 0 ----- ...... ....... Vertical deflection t ........ f(t,x) Figure 3. 1 Normal modes of a vi- brating string. (Solid lines indicate the configuration of each of these modes at some fixed instant of time, t.) the plucking of a string (i.e., pulling it taut and then releasing it), Lagrange argued that trigonometric series were of very limited use. It was in this somewhat hostile and skeptical environment that Jean Baptiste Joseph Fourier (Figure 3.2) presented his ideas half a century later. Fourier was born on March Figure 3.2 Jean Baptiste Joseph Fourier [picture from J. B. J. Fourier, Oeuvres de Fourier, Vol. II (Paris: Gauthier-Villars et Fils, 1980)]. 180 Fourier Series Representation of Periodic Signals Chap. 3 21, 1768, in Auxerre, France, and by the time of his entrance into the controversy con- cerning trigonometric series, he had already had a lifetime of experiences. His many contributions-in particular, those concerned with the series and transform that carry his name-are made even more impressive by the circumstances under which he worked. His revolutionary discoveries, although not completely appreciated during his own life- time, have had a major impact on the development of mathematics and have been and still are of great importance in an extremely wide range of scientific and engineering disci- plines. In addition to his studies in mathematics, Fourier led an active political life. In fact, during the years that followed the French Revolution, his activities almost led to his down- fall, as he narrowly avoided the guillotine on two separate occasions. Subsequently, Fourier became an associate of Napoleon Bonaparte, accompanied him on his expeditions to Egypt (during which time Fourier collected the information he would use later as the basis for his treatises on Egyptology), and in 1802 was appointed by Bonaparte to the position of prefect of a region of France centered in Grenoble. It was there, while serving as prefect, that Fourier developed his ideas on trigonometric series. The physical motivation for Fourier's work was the phenomenon of heat propaga- tion and diffusion. This in itself was a significant step in that most previous research in mathematical physics had dealt with rational and celestial mechanics. By 1807, Fourier had completed a work, Fourier had found series of harmonically related sinusoids to be useful in representing the temperature distribution through a body. In addition, he claimed that ""any"" periodic signal could be represented by such a series. While his treatment of this topic was significant, many of the basic ideas behind it had been discovered by oth- ers. Also, Fourier's mathematical arguments were still imprecise, and it remained for P. L. Dirichlet in 1829 to provide precise conditions under which a periodic signal could be rep- resented by a Fourier series. 3 Thus, Fourier did not actually contribute to the mathematical theory of Fourier series. However, he did have the clear insight to see the potential for this series representation, and it was to a great extent his work and his claims that spurred much of the subsequent work on Fourier series. In addition, Fourier took this type of representa- tion one very large step farther than any of his predecessors: He obtained a representation for aperiodic signals-not as weighted sums of harmonically related sinusoids-but as weighted integrals of sinusoids that are not all harmonically related. It is this extension from Fourier series to the Fourier integral or transform that is the focus of Chapters 4 and 5. Like the Fourier series, the Fourier transform remains one of the most powerful tools for the analysis of LTI systems. Four distinguished mathematicians and scientists were appointed to examine the 1807 paper of Fourier. Three of the four-S. F. Lacroix, G. Monge, and P. S. de Laplace- were in favor of publication of the paper, but the fourth, J. L. Lagrange, remained adamant in rejecting trigonometric series, as he had done 50 years earlier. Because of Lagrange's vehement objections, Fourier's paper never appeared. After several other attempts to have his work accepted and published by the Institut de France, Fourier undertook the writing of another version of his work, which appeared as the text Theorie analytique de Ia chaleur:!, 'Both S.D. Poisson and A. L. Cauchy had obtained results about the convergence of Fourier series before 1829, but Dirichlet's work represented such a significant extension of their results that he is usually credited with being the first to consider Fourier series convergence in a rigorous fashion. 4See J. B. J. Fourier, The Analytical Theory of Heat, trans. A. Freeman (New York: Dover, 1955). Sec. 3.1 A Historical Perspective 181 This book was published in 1822, 15 years after Fourier had first presented his results to the Institut. Toward the end of his life Fourier received some of the recognition he deserved, but the most significant tribute to him has been the enormous impact of his work on so many disciplines within the fields of mathematics, science, and engineering. The theory of integration, point-set topology, and eigenfunction expansions are just a few examples of topics in mathematics that have their roots in the analysis of Fourier series and inte- grals. 5 Furthermore, in addition to the original studies of vibration and heat diffusion, there are numerous other problems in science and engineering in which sinusoidal signals, and therefore Fourier series and transforms, play an important role. For example, sinusoidal signals arise naturally in describing the motion of the planets and the periodic behavior of the earth's climate. Alternating-current sources generate sinusoidal voltages and currents, and, as we will see, the tools of Fourier analysis enable us to analyze the response of an LTI system, such as a circuit, to such sinusoidal inputs. Also, as illustrated in Figure 3.3, waves in the ocean consist of the linear combination of sinusoidal waves with different spatial periods or wavelengths. Signals transmitted by radio and television stations are si- nusoidal in nature as well, and as a quick perusal of any text on Fourier analysis will show, the range of applications in which sinusoidal signals arise and in which the tools of Fourier analysis are useful extends far beyond these few examples. - Wavelength 150 ft ----Wavelenght 500ft - · - • -Wavelength 800 ft Figure 3.3 Ship encountering the superposition of three wave trains, each with a different spatial period. When these waves reinforce one another, a very large wave can result. In more severe seas, a giant wave indicated by the dotted line could result. Whether such a reinforcement occurs at any location depends upon the relative phases of the components that are superposed. [Adapted from an illustration by P. Mion in ""Nightmare Waves Are All Too Real to Deepwater Sailors,"" by P. Britton, Smithsonian 8 (February 1978), pp. 64-65]. While many of the applications in the preceding paragraph, as well as the original work of Fourier and his contemporaries on problems of mathematical physics, focus on phenomena in continuous time, the tools of Fourier analysis for discrete-time signals and systems have their own distinct historical roots and equally rich set of applications. In par- ticular, discrete-time concepts and methods are fundamental to the discipline of numerical analysis. Formulas for the processing of discrete sets of data points to produce numerical approximations for interpolation, integration, and differentiation were being investigated as early as the time of Newton in the 1600s. In addition, the problem of predicting the motion of a heavenly body, given a sequence of observations of the body, spurred the 5For more on the impact of Fourier's work on mathematics, see W. A. Coppel, ""J. B. Fourier-on the occasion of His Two Hundredth Birthday,"" American Mathematical Monthly, 76 ( 1969), 46l:l-83. 182 Fourier Series Representation of Periodic Signals Chap. 3 investigation of harmonic time series in the 18th and 19th centuries by eminent scientists and mathematicians, including Gauss, and thus provided a second setting in which much of the initial work was done on discrete-time signals and systems. In the mid-1960s an algorithm, now known as the fast Fourier transform, or FFT, was introduced. This algorithm, which was independently discovered by Cooley and Tukey in 1965, also has a considerable history and can, in fact, be found in Gauss' notebooks.6 What made its modem discovery so important was the fact that the FFT proved to be perfectly suited for efficient digital implementation, and it reduced the time required to compute transforms by orders of magnitude. With this tool, many interesting but previ- ously impractical ideas utilizing the discrete-time Fourier series and transform suddenly became practical, and the development of discrete-time signal and system analysis tech- niques moved forward at an accelerated pace. What has emerged out of this long history is a powerful and cohesive framework for the analysis of continuous-time and discrete-time signals and systems and an extraordinar- ily broad array of existing and potential applications. In this and the following chapters, we will develop the basic tools of that framework and examine some of its important im- plications. 3.2 THE RESPONSE OF LTI SYSTEMS TO COMPLEX EXPONENTIALS As we indicated in Section 3.0, it is advantageous in the study of LTI systems to represent signals as linear combinations of basic signals that possess the following two properties: 1. The set of basic signals can be used to construct a broad and useful class of signals. 2. The response of an LTI system to each signal should be simple enough in structure to provide us with a convenient representation for the response of the system to any signal constructed as a linear combination of the basic signals. Much of the importance of Fourier analysis results from the fact that both of these prop- erties are provided by the set of complex exponential signals in continuous and discrete time-i.e., signals of the form e'1 in continuous time and z"" in discrete time, where s and z are complex numbers. In subsequent sections of this and the following two chapters, we will examine the first property in some detail. In this section, we focus on the second property and, in this way, provide motivation for the use of Fourier series and transforms in the analysis of LTI systems. The importance of complex exponentials in the study of LTI systems stems from the fact that the response of an LTI system to a complex exponential input is the same complex exponential with only a change in amplitude; that is, continuous time: eH --> H(s)e' 1 , (3.1) discrete time: z""--> H(z)z"", (3.2) where the complex amplitude factor H(s) or H(z) will in general be a function of the complex variable s or z. A signal for which the system output is a (possibly complex) ""M. T. Heideman, D. H. Johnson, and C. S. Burrus, ""Gauss and the History of the Fast Fourier Trans- form,"" The IEEE ASSP Magazi11e I (1984). pp. 14-21. Sec. 3.2 The Response of LTI Systems to Complex Exponentials 183 constant times the input is referred to as an eigenfunction of the system, and the amplitude factor is referred to as the system's eigenvalue. To show that complex exponentials are indeed eigenfunctions of LTI systems, let us consider a continuous-time LTI system with impulse response h(t). For an input x(t), we can determine the output through the use of the convolution integral, so that with x(t) = est y(t) = I+w _ h( r)x(t - r) dr 00 I (3.3) +w = -x h( T)es(t-T) dT. Expressing es<t-T) as es1e-s7 , and noting that est can be moved outside the integral, we see that eq. (3.3) becomes +x y(t) = est I-oo h(r)e- 17 dr. (3.4) Assuming that the integral on the right-hand side of eq. (3.4) converges, the response to est is of the form y(t) = H(s)es1, (3.5) where H(s) is a complex constant whose value depends on sand which is related to the system impulse response by I+w H(s) = -oo h(r)e-s7 dT. (3.6) Hence, we have shown that complex exponentials are eigenfunctions of LTI systems. The constant H (s) for a specific value of s is then the eigenvalue associated with the eigen - function est. In an exactly parallel manner, we can show that complex exponential sequences are eigenfunctions of discrete-time LTI systems. That is, suppose that an LTI system with impulse response h[n] has as its input the sequence (3.7) where z is a complex number. Then the output of the system can be determined from the convolution sum as +x y[n] L h[k]x[n - k] k= -00 (3.8) +oo +oo = L h[k]zn-k = Zn L h[k]z-k. k=-X k= -00 From this expression, we see that if the input x[n] is the complex exponential given by eq. (3.7), then, assuming that the summation on the right-hand side of eq. (3.8) converges, the output is the same complex exponential multiplied by a constant that depends on the 184 Fourier Series Representation of Periodic Signals Chap. 3 value of z. That is, y[n] = H(z)zn, (3.9) where +x H(z) = L h[k]z-k. (3.10) k=-X Consequently, as in the continuous-time case, complex exponentials are eigenfunctions of discrete-time LTI systems. The constant H(z) for a specified value of z is the eigenvalue associated with the eigenfunction zn. For the analysis of LTI systems, the usefulness of decomposing more general signals in terms of eigenfunctions can be seen from an example. Let x(t) correspond to a linear combination of three complex exponentials; that is, (3.11) From the eigenfunction property, the response to each separately is a1esit ~ a1H(s1)es 11, a2es2t ~ a2H(s2)es2t, a3es3t ~ a3H(s3)es31 , and from the superposition property the response to the sum is the sum of the responses, so that (3.12) More generally, in continuous time, eq. (3.5), together with the superposition property, implies that the representation of signals as a linear combination of complex exponentials leads to a convenient expression for the response of an LTI system. Specifically, if the input to a continuous-time LTI system is represented as a linear combination of complex exponentials, that is, if x(t) = L akeskt, (3.13) k then the output will be y(t) = L akH(sk)e""'k1• (3.14) k In an exactly analogous manner, if the input to a discrete-time LTI system is represented as a linear combination of complex exponentials, that is, if (3.15) then the output will be y[n] = L akH(zk)z'k. (3.16) k Sec. 3.2 The Response of LTI Systems to Complex Exponentials 185 In other words, for both continuous time and discrete time, if the input to an LTI system is represented as a linear combination of complex exponentials, then the output can also be represented as a linear combination of the same complex exponential signals. Each coefficient in this representation of the output is obtained as the product of the corre- sponding coefficient a, of the input and the system's eigenvalue H(sk) or H(z.,) associated with the eigenfunction e''' or z.'k, respectively. It was precisely this fact that Euler discov- ered for the problem of the vibrating string, that Gauss and others used in the analysis of time series, and that motivated Fourier and others after him to consider the question of how broad a class of signals could be represented as a linear combination of complex exponentials. In the next few sections we examine this question for periodic signals, first in continuous time and then in discrete time, and in Chapters 4 and 5 we consider the extension of these representations to aperiodic signals. Although in general, the variables sand z in eqs. (3.1 )-(3.16) may be arbitrary complex numbers, Fourier analysis involves restricting our attention to particular forms for these variables. In particular, in continuous time we focus on purely imaginary values of s-i.e., s = }w-and thus, we consider only complex exponentials of the form e.i<ur. Similarly, in discrete time we restrict the range of values of z. to those of unit magnitude-i.e., z. = e.iw_so that we focus on complex exponentials of the form e.iwn. Example 3.1 As an illustration of eqs. (3.5) and (3.6), consider an LTI system for which the input x(t) and output y(t) are related by a time shift of 3, i.e., y(t) = x(t- 3). (3.17) If the input to this system is the complex exponential signal x(t) = ei2', then, from eq. (3.17), (3.18) Equation (3.18) is in the form of eq. (3.5), as we would expect, since ei2' is an eigen- function. The associated eigenvalue is H(j2) = e-16 . It is straightforward to confirm eq. (3.6) for this example. Specifically, from eq. (3.17), the impulse response of the sys- tem is h(t) = 8(t - 3 ). Substituting into eq. (3.6), we obtain H(s) = [,'l D(T- 3)e-"" dT = e- 3', so that H(j2) = e-'6 . As a second example, in this case illustrating eqs. (3.11) and (3.12), consider the input signal x(t) = cos(4t) + cos(7t). From eq. (3.17), y(t) will of course be y(t) = cos(4(t- 3)) + cos(7(t- 3)). (3.19) To see that this will also result from eq. (3.12). we first expand x(t) using Euler's relation: x(t) = !ei~' + !e-i4I + !ei7I +! e-J7I 2 2 (3.20) 2 2 • From eqs. (3.11) and (3.12), 186 Fourier Series Representation of Periodic Signals Chap. 3 or )'(t) = lei4U-3) + le-J411->1 + lei7U-3! + le-J7U-3) 2 2 2 2 = cos(4(t - 3)) + cos(7(t - 3)). For this simple example, multiplication of each periodic exponential component of x(t)-for example, ~ei4'-by the corresponding eigenvalue---e.g., H(j4) = e- i 12- effectively causes the input component to shift in time by 3. Obviously, in this case we can determine y(t) in eq. (3.19) by inspection rather than by employing eqs. (3.11) and (3.12). However, as we will see, the general property embodied in eqs. (3.11) and (3.12) not only allows us to calculate the responses of more complex LTI systems, but also provides the basis for the frequency domain representation and analysis of LTI systems. 3.3 FOURIER SERIES REPRESENTATION OF CONTINUOUS-TIME PERIODIC SIGNALS 3.3.1 Linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a signal is periodic if, for some positive value of T, x(t) = x(t + T) for all t. (3.21) The fundamental period of x(t) is the minimum positive, nonzero value of T far which eq. (3.21) is satisfied, and the value w 0 = 27r!T is referred to as the fundamental fre- quency. In Chapter 1 we also introduced two basic periodic signals, the sinusoidal signal x(t) = cos wot (3.22) and the periodic complex exponential (3.23) Both of these signals are periodic with fundamental frequency w0 and fundamental period T = 27rlwo. Associated with the signal in eq. (3.23) is the set of harmonically related complex exponentials <fJk(t) = eikwot = ejk(2rr!T)t, k = 0, ::±:: 1, ±2, .... (3.24) Each of these signals has a fundamental frequency that is a multiple of w 0 , and therefore, each is periodic with period T (although for lkl 2'.: 2, the fundamental period of <fJk(t) is a fraction of n. Thus, a linear combination of harmonically related complex exponentials of the form +x L L+x x(t) = akejkwot akeik(2rr!T)t (3.25) k=-X k=-X Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 187 is also periodic with period T. In eq. (3.25), the term fork = 0 is a constant. The terms for k = + 1 and k = - I both have fundamental frequency equal to wo and are collectively referred to as the fundamental components or the first harmonic components. The two terms fork = +2 and k = -2 are periodic with half the period (or, equivalently, twice the frequency) of the fundamental components and are referred to as the second harmonic components. More generally, the components for k = + N and k = - N are referred to as the Nth harmonic components. The representation of a periodic signal in the form of eq. (3.25) is referred to as the Fourier series representation. Before developing the properties of this representation, let us consider an example. Example 3.2 Consider a periodic signal x(t), with fundamental frequency 211"", that is expressed in the form of eq. (3.25) as +3 x(t) = L akeik27TI, (3.26) k~-3 where a0 = 1, 4' Rewriting eq. (3.26) and collecting each of the harmonic components which have the same fundamental frequency, we obtain x(t) l + -1( e1·2 = 7T/ + e-1?- '"") + -1( e'·4 1Tt + e-1· 4 1T1) 4 2 (3.27) + 1 .6 .6 3(el 1Tt + e-1 1T'). Equivalently, using Euler's relation, we can write x(t) in the form + 1 2 x(t) = 1 2 cos 211""t + cos 411""t + } cos 611""t. (3.28) In Figure 3.4, we illustrate graphically how the signal x(t) is built up from its harmonic components. 188 Fourier Series Representation of Periodic Signals Chap. 3 x1(t) = ~ cos 2Tit x0(t) + x1 (t) /\/\1/\/\ ~ /\TVV\1~ t Figure 3.4 Construction of the signal x(t) in Example 3.2 as a linear com- bination of harmonically related sinusoidal signals. Equation (3.28) is an example of an alternative form for the Fourier series of real periodic signals. Specifically, suppose that x(t) is real and can be represented in the form of eq. (3.25). Then, since x*(t) = x(t), we obtain t-<f. x(t) = .:2: ale-ik<uot. 1.~-C£ Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 189 Replacing k by - k in the summation, we have +x x(t) = L a*_kejkwor, k=-X which, by comparison with eq. (3.25), requires that ak = a*_k, or equivalently, that (3.29) Note that this is the case in Example 3.2, where the ak's are in fact real and ak = a-k· To derive the alternative forms of the Fourier series, we first rearrange the summation in eq. (3.25) as x(t) = ao + L[akejkwor + a_ke- jkwor]. k=i Substituting a~ for a_k from eq. (3.29), we obtain x(t) = ao + L[akejkwor + ake-jkwor]. k=i Since the two terms inside the summation are complex conjugates of each other, this can be expressed as x(t) = ao + L 2CR-e{akejkwo1 }. (3.30) k=i If ak is expressed in polar form as then eq. (3.30) becomes x(t) = ao + L 2CR-e{Akej(kwot+OA)}. k=i That is, x(t) = ao + 2 L Ak cos(kwot + fh). (3.31) k=i Equation (3.31) is one commonly encountered form for the Fourier series of real periodic signals in continuous time. Another form is obtained by writing ak in rectangular form as ak = Bk + }Ck> where Bk and Ck are both real. With this expression for ak. eq. (3.30) takes the form x(t) = ao + 2 L [Bk cos kwot- ck sin kwot]. (3.32) k=i In Example 3.2 the ak's are all real, so that ak = Ak = Bk. and therefore, both represen- tations, eqs. (3.31) and (3.32), reduce to the same form, eq. (3.28). 190 Fourier Series Representation of Periodic Signals Chap. 3 Thus, for real periodic functions, the Fourier series in terms of complex exponentials, as given in eq. (3.25), is mathematically equivalent to either of the two forms in eqs. (3.31) and (3.32) that use trigonometric functions. Although the latter two are common forms for Fourier series,7 the complex exponential form of eq. (3.25) is particularly convenient for our purposes, so we will use that form almost exclusively. Equation (3.29) illustrates one of many properties associated with Fourier series. These properties are often quite useful in gaining insight and for computational purposes, and in Section 3.5 we collect together the most important of them. The derivation of several of them is considered in problems at the end of the chapter. In Section 4.3, we also will develop the majority of the properties within the broader context of the Fourier transform. 3.3.2 Determination of the Fourier Series Representation of a Continuous-time Periodic Signal Assuming that a given periodic signal can be represented with the series of eq. (3.25), we need a procedure for determining the coefficients ak. Multiplying both sides of eq. (3.25) by e-jn<v""1, we obtain x(t)e i~~<v""r = L ake 1kw""re- 1nw11 r (3.33) k~ -X Integrating both sides from 0 to T = 27T/w0, we have Here, Tis the fundamental period of x(t), and consequently, we are integrating over one period. Interchanging the order of integration and summation yields rT xcr>e-j""w""/ dr = L+x ak [ rT ej(i.-n)'""""1 drl . (3.34) Jo k~-x Jo The evaluation of the bracketed integral is straightforward. Rewriting this integral using Euler's formula, we obtain ( ei!k-nl<'-'""1d t = {T cos(k- n)w0tdt + j {T sin(k- n)wotdt. Jo Jo Jo (3.35) For k ~ n, cos( k- n )w0 t and sin( k -n )wot are periodic sinusoids with fundamental period (Tifk- nf). Therefore, in eq. (3.35), we are integrating over an interval (of length D that is an integral number of periods of these signals. Since the integral may be viewed as measuring the total area under the functions over the interval, we see that for k ~ n, both of the integrals on the right-hand side of eq. (3.35) are zero. For k = n, the integrand on the left-hand side of eq. (3.35) equals I, and thus, the integral equals T. In sum, we then have T { ej!k-n)w11 r dt = { T, k = n Jo 0, k~n· 7 ln fact, in his original work, Fourier used the sine-cosine form of the Fourier series given in eq. (3.32). Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 191 and consequently, the right-hand side of eq. (3.34) reduces to T a11 • Therefore, a = -l iT x(t)e- J.nw ot dt II T () . (3.36) which provides the equation for determining the coefficients. Furthermore, note that in evaluating eq. (3.35), the only fact that we used concerning the interval of integration was that we were integrating over an interval of length T, which is an integral number of periods of cos(k- n)w0t and sin(k- n)w0 t. Therefore, we will obtain the same result if we integrate over any interval of length T. That is, if we denote integration over any interval of length T by JT ' we have I ei<k-ll)w0 t dt = { T, k = n T 0, k=i'n' and consequently, a"" = T1 IT x(t)e-.!'.""v ot dt. (3.37) To summarize, if x(t) has a Fourier series representation [i.e., if it can be expressed as a linear combination of harmonically related complex exponentials in the form of eq. (3.25)], then the coefficients are given by eq. (3.37). This pair of equations, then, defines the Fourier series of a periodic continuous-time signal: +x +x x(t) = L akejkwot = L akejk(27TIT)t, (3.38) k~ -X k~ -X ak = -I I x(t)e-.1"" k wot dt = -1 I x(t)e- 1"" k· (_~7 T IT !t dt. (3.39) T T T T Here, we have written equivalent expressions for the Fourier series in terms of the fun- damnetal frequency w 0 and the fundamental period T. Equation (3.38) is referred to as the synthesis equation and eq. (3.39) as the analysis equation. The set of coefficients {ak} are often called the Fourier series coefficients or the spectral coefficients of x(t). 8 These complex coefficients measure the portion of the signal x(t) that is at each harmonic of the fundamental component. The coefficient a0 is the de or constant component of x(t) and is given by eq. (3.39) with k = 0. That is, ao = ~ Lx (t)dt, (3.40) which is simply the average value of x(t) over one period. Equations (3.38) and (3.39) were known to both Euler and Lagrange in the mid- dle of the 18th century. However, they discarded this line of analysis without having 'The term ""spectral coefficient"" is derived from problems such as the spectroscopic decomposition of light into spectral lines (i.e., into its elementary components at different frequencies). The intensity of any line in such a decomposition is a direct measure of the fraction of the total light energy at the frequency corresponding to the line. 192 Fourier Series Representation of Periodic Signals Chap. 3 examined the question of how large a class of periodic signals could, in fact, be represented in such a fashion. Before we turn to this question in the next section, let us illustrate the continuous-time Fourier series by means of a few examples. Example 3.3 Consider the signal x(t) = sin wot. whose fundamental frequency is w11 • One approach to determining the Fourier series coefficients for this signal is to apply eq. (3.39). For this simple case, however, it is easier to expand the sinusoidal signal as a linear combination of complex exponentials and identify the Fourier series coefficients by inspection. Specifically, we can express sinw 11 t as = I . I . sinW()f -ef(Uof- -e- jtuol. 2j 2j Comparing the right-hand sides of this equation and eq. (3.38), we obtain I a -I = - j' 2 k#+lor-1. Example 3.4 Let x(t) = I + sin wot + 2 cos wot + cos (2wot + i). which has fundamental frequency w11 • As with Example 3.3, we can again expand x(t) directly in terms of complex exponentials, so that I x(t) = I+ 2j[el""'o'- e I. '""""']+ Lel""'o' + e I. '""""']+ I 0 I' . 0 I' 2[el'-""'o'+""~l + e il-wor+7T~']. Collecting terms, we obtain x(t) =I+ (I+ L)e i'""o' +(I- L)e i•""o' + OeJ!7TI4}j2wol + Ge-ji7TI4} Thus, the Fourier series coefficients for this example are ao = I. I . OJ =(I+ L) = I- 21· I . a 1 = (I- 21j) = I+ 21· 02 ~eJI7TI4I h4 (l + 1'). 2 Sec. 3.3 Fourier Series Representation of Continuous-Time Periodic Signals 193 = ~e- ii7T/4i = j2 (I _ 2 4 1') ' ak = 0, lkl > 2. In Figure 3.5, we show a bar graph of the magnitude and phase of ak. -3 -2 -1 0 2 3 k -2 -3 -1 0 2 3 • • k Figure 3.5 Plots of the magnitude and phase of the Fourier coefficients of the signal considered in Example 3.4. Example 3.5 The periodic square wave, sketched in Figure 3.6 and defined over one period as x(t) 1, ltl < T1 = { ltl (3.41) 0, T1 < < T/2' is a signal that we will encounter a number of times throughout this book. This signal is periodic with fundamental period T and fundamental frequency w0 = 27r!T. To determine the Fourier series coefficients for x(t), we use eq. (3.39). Because of the symmetry of x(t) about t = 0, it is convenient to choose - T/2 ::s t < T/2 as the x(t) .. . J [1 [1 ~ [1 [1 [ ... I I -2T -T l -T, T, l T 2T 2 2 Figure 3.6 Periodic square wave. 194 Fourier Series Representation of Periodic Signals Chap. 3 interval over which the integration is performed, although any interval of length T is equally valid and thus will lead to the same result. Using these limits of integration and substituting from eq. (3.41), we have first, fork = 0, ao = _!_ IT 1 dt = 2Ti . (3.42) T -Ti T As mentioned previously, a0 is interpreted to be the average value of x(t), which in this case equals the fraction of each period during which x(t) = 1. For k # 0, we obtain which we may rewrite as (3.43) Noting that the term in brackets is sin kw0 Ti, we can express the coefficients ak as 2 sin(kwoTi) k # 0, (3.44) kwoT where we have used the fact that w 0 T = 27r. Figure 3.7 is a bar graph of the Fourier series coefficients for this example. In particular, the coefficients are plotted for a fixed value of Ti and several values of T. For this specific example, the Fourier coefficients are real, and consequently, they can be depicted graphically with only a single graph. More generally, of course, the Fourier coefficients are complex, so that two graphs, corresponding to the real and imaginary parts, or magnitude and phase, of each coefficient, would be required. For T = 4 Ti, x(t) is a square wave that is unity for half the period and zero for half the period. In this case, woTi = 7r/2, and from eq. (3.44), sin( 1T k/2) ak = k# 0, (3.45) k1T while ao = 2· (3.46) From eq. (3.45), ak = 0 fork even and nonzero. Also, sin( 1T k/2) alternates between :±:: 1 for successive odd values of k. Therefore, ai G-i 1T ' a3 = a_3 = 37r ' as G-5 57r Sec. 3.4 Convergence of the Fourier Series 195 k (a) ..... ..... .1111111. ..... . .... I • I I I I 11-4 0 4 I II I I I • I k (b) • I I . ..111111111111... . 1 I 1 I I Jill ~8 0 8 I I II II I k (c) Figure 3.7 Plots of the scaled Fourier series coefficients Tak for the pe- riodic square wave with T1 fixed and for several values of T: (a) T = 4 T;; (b) T = 8 T1; (c) T = 16 T;. The coefficients are regularly spaced samples of the envelope (2 sin wT; )lw, where the spacing between samples, 2TTIT, de- creases as T increases. 3. 4 CONVERGENCE OF THE FOURIER SERIES Although Euler and Lagrange would have been happy with the results of Examples 3.3 and 3.4, they would have objected to Example 3.5, since x(t) is discontinuous while each of its harmonic components is continuous. Fourier, on the other hand, considered the same example and maintained that the Fourier series representation of the square wave is valid. In fact, Fourier maintained that any periodic signal could be represented by a Fourier series. Although this is not quite true, it is true that Fourier series can be used to represent an extremely large class of periodic signals, including the square wave and all other periodic signals with which we will be concerned in this book and which are of interest in practice. To gain an understanding of the square-wave example and, more generally, of the question of the validity of Fourier series representations, let us examine the problem of approximating a given periodic signal x(t) by a linear combination of a finite number of harmonically related complex exponentials-that is, by a finite series of the form 196 Fourier Series Representation of Periodic Signals Chap. 3 LN XN(t) = ake;kwot (3.47) k=-N Let eN(t) denote the approximation error; that is, +N eN(t) = x(t) - XN(t) = x(t) - L ake;kwot (3.48) k=-N In order to determine how good any particular approximation is, we need to specify a quantitative measure of the size of the approximation error. The criterion that we will use is the energy in the error over one period: (3.49) As shown in Problem 3.66, the particular choice for the coefficients in eq. (3.47) that minimize the energy in the error is (3.50) Comparing eqs. (3.50) and (3.39), we see that eq. (3.50) is identical to the expression used to determine the Fourier series coefficients. Thus, if x(t) has a Fourier series representa- tion, the best approximation using only a finite number of harmonically related complex exponentials is obtained by truncating the Fourier series to the desired number of terms. As N increases, new terms are added and EN decreases. If, in fact, x(t) has a Fourier series representation, then the limit of EN as N ~ oo is zero. Let us tum now to the question of when a periodic signal x(t) does in fact have a Fourier series representation. Of course, for any signal, we can attempt to obtain a set of Fourier coefficients through the use of eq. (3.39). However, in some cases, the integral in eq. (3.39) may diverge; that is, the value obtained for some of the ak may be infinite. Moreover, even if all of the coefficients obtained from eq. (3.39) are finite, when these coefficients are substituted into the synthesis equation (3.38), the resulting infinite series may not converge to the original signal x(t). Fortunately, there are no convergence difficulties for large classes of periodic signals. For example, every continuous periodic signal has a Fourier series representation for which the energy EN in the approximation error approaches 0 as N goes to oo. This is also true for many discontinuous signals. Since we will find it very useful to include discontinuous signals such as square waves in our discussions, it is worthwhile to investigate the issue of convergence in a bit more detail. Specifically, there are two somewhat different classes of conditions that a periodic signal can satisfy to guarantee that it can be represented by a Fourier series. In discussing these, we will not attempt to provide a complete mathematical justification; more rigorous treatments can be found in many texts on Fourier analysis.9 9See, for example, R. V. Churchill, Fourier Series and Boundary Value Problems, 3rd ed. (New York: McGraw-Hill Book Company, 1978); W. Kaplan, Operational Methods for Linear Systems (Reading, MA: Addison-Wesley Publishing Company, 1962); and the book by Dym and McKean referenced in footnote 2 of this chapter. Sec. 3.4 Convergence of the Fourier Series 197 One class of periodic signals that are representable through the Fourier series is those signals which have finite energy over a single period, i.e., signals for which (3.51) When this condition is satisfied, we are guaranteed that the coefficients ak obtained from eq. (3.39) are finite. Furthermore, let XN(t) be the approximation to x(t) obtained by using these coefficients for Ik l :::; N: +N XN(l) = L akeJkwot (3.52) k=-N Then we are guaranteed that the energy EN in the approximation error, as defined in eq. (3.49), converges to 0 as we add more and more terms, i.e., as N ~ oo. That is, if we define +x e(t) = x(t) - L akejkwot, (3.53) k=-% then t le(t)l 2 dt = 0. (3.54) As we will see in an example at the end of this section, eq. (3.54) does not imply that the signal x(t) and its Fourier series representation L+x akejkwot (3.55) k= -ex are equal at every value oft. What it does say is that there is no energy in their difference. The type of convergence guaranteed when x(t) has finite energy over a single pe- riod is quite useful. In this case eq. (3.54) states that the difference between x(t) and its Fourier series representation has zero energy. Since physical systems respond to signal en- ergy, from this perspective x(t) and its Fourier series representation are indistinguishable. Because most of the periodic signals that we consider do have finite energy over a single period, they have Fourier series representations. Moreover, an alternative set of conditions, developed by P. L. Dirichlet and also satisfied by essentially all of the signals with which we will be concerned, guarantees that x(t) equals its Fourier series representation, except at isolated values oft for which x(t) is discontinuous. At these values, the infinite series of eq. (3.55) converges to the average of the values on either side of the discontinuity. The Dirichlet conditions are as follows: Condition 1. Over any period, x(t) must be absolutely integrable; that is, t lx(t)l dt < oo. (3.56) 198 Fourier Series Representation of Periodic Signals Chap.3 As with square integrability, this guarantees that each coefficient ak will be finite, since So if fr [x(t)[ dt < oo, then A periodic signal that violates the first Dirichlet condition is 1 x(t) = t' 0 < t :S l; that is, x(t) is periodic with period 1. This signal is illustrated in Figure 3.8(a). Condition 2. In any finite interval of time, x(t) is of bounded variation; that is, there are no more than a finite number of maxima and minima during any single period of the signal. An example of a function that meets Condition 1 but not Condition 2 is x(t) = sin ( 2;} 0 < t :S 1, (3.57) as illustrated in Figure 3.8(b). For this function, which is periodic with T = 1, L' /x(t)/ dt < 1. The function has, however, an infinite number of maxima and minima in the interval. Condition 3. In any finite interval of time, there are only a finite number of discontinu- ities. Furthermore, each of these discontinuities is finite. An example of a function that violates Condition 3 is illustrated in Figure 3.8(c). The signal, of period T = 8, is composed of an infinite number of sections, each of which is half the height and half the width of the previous section. Thus, the area under one period of the function is clearly less than 8. However, there are an infinite number of discontinuities in each period, thereby violating Condition 3. As can be seen from the examples given in Figure 3.8, signals that do not satisfy the Dirichlet conditions are generally pathological in nature and consequently do not typically arise in practical contexts. For this reason, the question of the convergence of Fourier series will not play a particularly significant role in the remainder of the book. For a periodic signal that has no discontinuities, the Fourier series representation converges and equals the original signal at every value oft. For a periodic signal with a finite number of discontinuities in each period, the Fourier series representation equals the signal every- Sec. 3.4 Convergence of the Fourier Series 199 x(t) ~ I I -1 0 2 (a) x(t) 2 (b) Figure 3.8 Signals that violate the Dirichlet conditions: (a) the signal x(t) = 1/t for 0 <ts 1, a peri- x(t) odic signal with period 1 (this signal violates the first Dirichlet condition); (b) the periodic signal of eq. (3.57), which violates the second Dirichlet condition; (c) a signal periodic with period 8 that violates the third Dirichlet condition [for 0 s t < 8, the value of x( t) decreases by a factor of 2 when- ever the distance from t to 8 decreases by a factor of 2; that is, 8 16 t x(t) = 1, 0 s t < 4, x(t) = 1/2, 4 s t < 6, x(t) = 1/4, 6 s t < 7, (c) x(t) = 1/8, 7 s t < 7.5, etc.]. where except at the isolated points of discontinuity, at which the series converges to the average value of the signal on either side of the discontinuity. In this case the difference between the original signal and its Fourier series representation contains no energy, and consequently, the two signals can be thought of as being the same for all practical pur- 200 Fourier Series Representation of Periodic Signals Chap.3 poses. Specifically, since the signals differ only at isolated points, the integrals of both signals over any interval are identical. For this reason, the two signals behave identically under convolution and consequently are identical from the standpoint of the analysis of LTI systems. To gain some additional understanding of how the Fourier series converges for a periodic signal with discontinuities, let us return to the example of a square wave. In particular, in 1898, 10 an American physicist, Albert Michelson, constructed a harmonic analyzer, a device that, for any periodic signal x(t), would compute the truncated Fourier series approximation of eq. (3.52) for values of N up to 80. Michelson tested his device on many functions, with the expected result that XN(t) looked very much like x(t). However, when he tried the square wave, he obtained an important and, to him, very surprising re- sult. Michelson was concerned about the behavior he observed and thought that his device might have had a defect. He wrote about the problem to the famous mathematical physicist Josiah Gibbs, who investigated it and reported his explanation in 1899. What Michelson had observed is illustrated in Figure 3.9, where we have shown xN(t) for several values of N for x(t), a symmetric square wave (T = 4T1) . In each case, the partial sum is superimposed on the original square wave. Since the square wave satis- fies the Dirichlet conditions, the limit as N -7 oo of XN(t) at the discontinuities should be the average value of the discontinuity. We see from the figure that this is in fact the case, since for any N, XN(t) has exactly that value at the discontinuities. Furthermore, for any other value oft, say, t = t 1, we are guaranteed that lim XN(t1) = x(t1). N -""oo Therefore, the squared error in the Fourier series representation of the square wave has zero area, as in eqs. (3.53) and (3.54). For this example, the interesting effect that Michelson observed is that the behavior of the partial sum in the vicinity of the discontinuity exhibits ripples and that the peak am- plitude of these ripples does not seem to decrease with increasing N. Gibbs.showed that these are in fact the case. Specifically, for a discontinuity of unity height, thep artial sum exhibits a maximum value of 1.09 (i.e., an overshoot of 9% of the height of the discon- tinuity), no matter how large N becomes. One must be careful to interpret this c~ectly, however. As stated before, for any fixed value oft, say, t = t 1, the partial sums will con- verge to the correct value, and at the discontinuity they will converge to one-half the sum of the values of the signal on either side of the discontinuity. However, the closer t1 is cho- sen to the point of discontinuity, the larger N must be in order to reduce the error below a specified amount. Thus, as N increases, the ripples in the partial sums become compressed toward the discontinuity, but for any finite value of N, the peak amplitude of the ripples remains constant. This behavior has come to be known as the Gibbs phenomenon. The im- plication is that the truncated Fourier series approximation xN(t) of a discontinuous signal x(t) will in general exhibit high-frequency ripples and overshoot x(t) near the disconti- nuities. If such an approximation is used in practice, a large enough value of N should be chosen so as to guarantee that the total energy in these ripples is insignificant. In the limit, of course, we know that the energy in the approximation error vanishes and that the Fourier series representation of a discontinuous signal such as the square wave converges. 10The historical information used in this example is taken from the book by Lanczos referenced in foot- note I of this chapter. Sec. 3.4 Convergence of the Fourier Series 201 N=19 0 (d) - T, 0 T, (e) Figure 3. 9 Convergence of the Fourier series representation of a square wave: an illustration of the Gibbs phenomenon. Here, we have depicted the finite series approximation xN(t) = ~~ N ake1k""'o 1 for several values of N. 202 Fourier Series Representation of Periodic Signals Chap. 3 3.5 PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES As mentioned earlier, Fourier series representations possess a number of important prop- erties that are useful for developing conceptual insights into such representations, and they can also help to reduce the complexity of the evaluation of the Fourier series of many sig- nals. In Table 3.1 we have summarized these properties, several of which are considered in the problems at the end of this chapter. In Chapter 4, in which we develop the Fourier transform, we will see that most of these properties can be deduced from corresponding properties of the continuous-time Fourier transform. Consequently we limit ourselves here to the discussion of several of these properties to illustrate how they may be derived, in- terpreted, and used. Throughout the following discussion of selected properties from Table 3.1, we will find it convenient to use a shorthand notation to indicate the relationship between a peri- odic signal and its Fourier series coefficients. Specifically, suppose that x(t) is a periodic signal with period T and fundamental frequency w 0 = 27T!T. Then if the Fourier series coefficients of x(t) are denoted by ab we will use the notation 3'S x(t) <-------+ a k to signify the pairing of a periodic signal with its Fourier series coefficients. 3.5.1 Linearity Let x(t) and y(t) denote atw, o periodic signals with period T and which have Fourier series coefficients denoted by and b"" respectively. That is, 3'S x(t) <-------+ a"" 3'S y (t) <-------+ b k . Since x(t) and y(t) have the same period T, it easily follows that any linear combination of the two scig, nals will also be periodic with period T. Furthermore, the Fourier series coefficients of the linear combination of x(t) and y(t), z(t) = Ax(t) + By(t), are given by the same linear combination of the Fourier series coefficients for x(t) and y(t). That is, ;rs z(t) = Ax(t) + By(t) <-------+ c, = Aa, + Bb,. (3.58) The proof of this follows directly from the application of eq. (3.39). We also note that the linearity property is easily extended to a linear combination of an arbitrary number of signals with period T. 3.5.2 Time Shifting When a time shift is applied to a periodic signal x(t ), the period T of the signal is preserved. The Fourier series coefficients bk of the resulting signal y(t) = x(t- to) may be expressed as b k = -] I x(t - to )e - ·j kw 0 td t. (3.59) T T Sec. 3.5 Properties of Continuous-Time Fourier Series 203 Letting T = t - to in the integral, and noting that the new variable T will also range over an interval of duration T, we obtain (3.60) where ak is the kth Fourier series coefficient of x(t). That is, if ~5 x(t) ~ ak> then One consequence of this property is that, when a periodic signal is shifted in time, the magnitudes of its Fourier series coefficients remain unaltered. That is, lbkl = lakl· 3.5.3 Time Reversal The period T of a periodic signal x(t) also remains unchanged when the signal undergoes time reversal. To determine the Fourier series coefficients of y(t) = x(- t), let us consider the effect of time reversal on the synthesis equation (3.38): x(-t) = L ake-jk2m!T. (3.61) k=-X Making the substitution k = - m, we obtain y(t) = x(- t) = L a-meJm2m!T (3.62) m=-oc We observe that the right-hand side of this equation has the form of a Fourier series syn- thesis equation for x( -t), where the Fourier series coefficients bk are (3.63) That is, if then ~5 x(-t) ~ a-k· In other words time reversal applied to a continuous-time signal results in a time reversal of the corresponding sequence of Fourier series coefficients. An interesting consequence of the time-reversal property is that if x(t) is even-that is, if x( -t) = x(t)-then its Fourier series coefficients are also even-i.e., a_k = ak. Similarly, if x(t) is odd, so that x( -t) = - x(t), then so are its Fourier series coefficients-i.e., a_k = -ak. 204 Fourier Series Representation of Periodic Signals Chap. 3 3.5.4 Time Scaling Time scaling is an operation that in general changes the period of the underlying signal. Specifically, if x(t) is periodic with period T and fundamental frequency w 0 = 27r!T, then x(at), where a is a positive real number, is periodic with period Tla and fundamen- tal frequency aw0 . Since the time-scaling operation applies directly to each of the har- monic components of x(t), we may easily conclude that the Fourier coefficients for each of those components remain the same. That is, if x(t) has the Fourier series representation in eq. (3.38), then +""- x(at) = ~ a,ejk(uwo)l k~-x is the Fourier series representation of x(at). We emphasize that, while the Fourier coef- ficients have not changed, the Fourier series representation has changed because of the change in the fundamental frequency. 3.5.5 Multiplication Suppose that x(t) and y(t) are both periodic with period T and that ~s x(t) <-------? a k> OJ'S y( t) <-------? b k. Since the product x(t)y(t) is also periodic with period T, we can expand it in a Fourier series with Fourier series coefficients h, expressed in terms of those for x(t) and y(t). The result is ~s x( t) y( t) <-------? h k (3.64) One way to derive this relationship (see Problem 3.46) is to multiply the Fourier series representations of x(t) and y(t) and to not~ that the kth harmonic component in the product will have a coefficient which is the sum of terms of the form a1bk-t. Observe that the sum on the right-hand side of eq. (3.64) may be interpreted as the discrete-time convolution of the sequence representing the Fourier coefficients of x(t) and the sequence representing the Fourier coefficients of y(t). 3.5.6 Conjugation and Conjugate Symmetry Taking the complex conjugate of a periodic signal x(t) has the effect of complex conjuga- tion and time reversal on the corresponding Fourier series coefficients. That is, if H x( t) <-------? a k, then (3.65) Sec. 3.5 Properties of Continuous-Time Fourier Series 205 This property is easily proved by applying complex conjugation to both sides of eq. (3.38) and replacing the summation variable k by its negative. Some interesting consequences of this property may be derived for x(t) real-that is, when x(t) = x*(t). In particular, in this case, we see from eq. (3.65) that the Fourier series coefficients will be conjugate symmetric, i.e., a-k = a~, (3.66) as we previously saw in eq. (3.29). This in tum implies various symmetry properties (listed in Table 3.1) for the magnitudes, phases, real parts, and imaginary parts of the Fourier series coefficients of real signals. For example, from eq. (3.66), we see that if x(t) is real, then a0 is real and lakl = la-kl· Also, if x(t) is real and even, then, from Section 3.5.3, ak = a-k· However, from eq. (3.66) we see that a~ = a_b so that ak = a~. That is, if x(t) is real and even, then so are its Fourier series coefficients. Similarly, it can be shown that if x(t) is real and odd, then its Fourier series coefficients are purely imaginary and odd. Thus, for example, a0 = 0 if x(t) is real and odd. This and the other symmetry properties of the Fourier series are examined further in Problem 3.42. 3.5.7 Parseval's Relation for Continuous-Time Periodic Signals As shown in Problem 3.46, Parseval's relation for continuous-time periodic signals is (3.67) where the ak are the Fourier series coefficients of x(t) and Tis the period of the signal. Note that the left-hand side of eq. (3.67) is the average power (i.e., energy per unit time) in one period of the periodic signal x(t). Also, ~ t 2 lakejkwot 1 dt = ~ t lakl 2dt = lakl2 , (3.68) so that lakl2 is the average power in the kth harmonic component of x(t). Thus, what Par- seval's relation states is that the total average power in a periodic signal equals the sum of the average powers in all of its harmonic components. 3.5.8 Summary of Properties of the Continuous-Time Fourier Series In Table 3.1, we summarize these and other important properties of continuous-time Fourier series. 3.5.9 Examples Fourier series properties, such as those listed in Table 3.1, may be used to circumvent some of the algebra involved in determining the Fourier coefficients of a given signal. In the next 206 Fourier Series Representation of Periodic Signals Chap. 3 TABLE 3.1 PROPERTIES OF CONTINUOUS-TIME FOURIER SERIES Property Section Periodic Signal Fourier Series Coefficients x(t)} Periodic with period T and y(t) fundamental frequency w0 = 21TIT Linearity 3.5.1 Ax(t) + By(t) Aak + Bbk Time Shifting 3.5.2 x(t - to) ake- jkwoto = ake- jk(27TIT)to Frequency Shifting efMwot = efM(2rriT)t x(t) Conjugation 3.5.6 x""(t) Time Reversal 3.5.3 x( -t) Time Scaling 3.5.4 x(at), a > 0 (periodic with period T/a) Periodic Convolution t x(r)y(t- r)dr Multiplication 3.5.5 x(t)y(t) dx(t) 2 Differentiation 'k 'k 1T ----;It J woak = J Tak ' d (finite valued and Integration J x(t) t -oo periodic only if a = 0) CLo )ak = Ck( 2~1T) )a, 0 :.:~~~._~ CR.:{a-k} Conjugate Symmetry for 3.5.6 x(t) real 9'm{ak} = -9'm{a_d Real Signals ! \ak\ = \a-1\ <r:ak = -<r:a-k Real and Even Signals 3.5.6 x(t) real and even a1 real and even Real and Odd Signals 3.5.6 x( t) real and odd a1 purely imaginary and odd Even-Odd Decomposition { x,(t) = Sv{x(t)} [x(t) real] CR.:{a1} of Real Signals X0 (t) = 0d{x(t)} [x(t) real] j9'm{ak} Parseval's Relation for Periodic Signals three examples, we illustrate this. The last example in this section then demonstrates how properties of a signal can be used to characterize the signal in great detail. Example 3.6 Consider the signal g(t) with a fundamental period of 4, shown in Figure 3.10. We could determine the Fourier series representation of g(t) directly from the analysis equa- tion (3.39). Instead, we will use the relationship of g(t) to the symmetric periodic square wave x(t) in Example 3.5. Referring to that example, we see that, with T = 4 and T1 = 1, g(t) = x(t- 1)- 1/2. (3.69) Sec. 3.5 Properties of Continuous-Time Fourier Series 207 g(t) 1 2 -2 -1 1 2 - 1 2 Figure 3. 1 o Periodic signal for Example 3.6. The time-shift property in Table 3.1 indicates that, if the Fourier Series coefficients of x(t) are denoted by ako the Fourier coefficients of x(t - 1) may be expressed as (3.70) The Fourier coefficients of the de offset in g(t)-i.e., the term -1/2 on the right-hand side of eq. (3.69)-are given by _ { 0-,k , fork~ 0 fork = 0 . (3.71) Ck - Applying the linearity property in Table 3.1, we conclude that the coefficients for g(t) may be expressed as where each ak may now be replaced by the corresponding expression from eqs. (3.45) and (3.46), yielding (3.72) Example 3.7 Consider the triangular wave signal x(t) with period T = 4 and fundamental frequency w 0 = 7T/2 shown in Figure 3.11. The derivative ofthis signal is the signal g(t) in Exam- x(t) -2 2 Figure 3. 11 Triangular wave signal in Example 3.7. 208 Fourier Series Representation of Periodic Signals Chap. 3 ple 3.6. Denoting the Fourier coefficients of g(t) by d, and those of x(t) by ek, we see that the differentiation property in Table 3.1 indicates that (3.73) This equation can be used to express ek in terms of d"" except when k = 0. Specifically, from eq. (3.72), , _ 2dk _ 2sin(7Tk/2) -jbr/1 ek - jk1T - j(k1T)2 e ' k # 0. (3.74) Fork = 0, e0 can be determined by finding the area under one period of x(t) and dividing by the length of the period: Example 3.8 Let us examine some properties of the Fourier series representation of a periodic train of impulses, or impulse train. This signal and its representation in terms of complex expo- nentials will play an important role when we discuss the topic of sampling in Chapter 7. The impulse train with period T may be expressed as x(t) = L 8(t - kT); (3.75) A~ o. it is illustrated in Figure 3.12(a). To determine the Fourier series coefficients ak, we use eq. (3.39) and select the interval of integration to be -T/2 :<::: t :<::: T/2, avoiding the placement of impulses at the integration limits. Within this interval, x(t) is the same as 8(t), ~it followWhat = -l f T/2 . . 1 ak 8(t)e- 1k2m!T dt = -. (3.76) T -m T In other words, all the Fourier series coefficients of the impulse train are identical. These coefficients are also real valued and even (with respect to the index k). This is to be expected, since, according to Table 3.1, any real and even signal (such as our impulse train) should have real and even Fourier coefficients. The impulse train also has a straightforward relationship to square-wave signals such as g(t) in Figure 3.6, which we repeat in Figure 3.12(b). The derivative of g(t) is the signal q(t) illustrated in Figure 3.12(c). We may interpret q(t) as the difference of two shifted versions of the impulse train x(t). That is, (3.77) Using the properties of Fourier series, we can now compute the Fourier series coeffi- cients of q(t) and g(t) without any further direct evaluation of the Fourier series analysis equation. First, from the time-shifting and linearity properties, we see from eq. (3.77) that the Fourier series coefficients bk of q(t) may be expressed in terms of the Fourier series coefficients a, of x(t); that is, Sec. 3.5 Properties of Continuous-Time Fourier Series 209 x(t) -T T (a) g(t) ' -T/2 -T, T, T/2 T (b) q(t) -T/2 T/2 -1 (c) Figure 3.12 (a) Periodic train of impulses; (b) periodic square wave; (c) derivative of the periodic square wave in (b). where wo = 27TIT. Using eq. (3.76), we then have Finally, since q(t) is the derivative of g(t), we can use the differentiation property in Table 3.1 to write bk = jkwocb (3.78) where the ck are the Fourier series coefficients of g(t). Thus, bk 2} sin(kwoTI) sin(kwoTJ) Ck = -- = ---,---- k ¥: 0, (3.79) jkwo jkwoT k7T 210 Fourier Series Representation of Periodic Signals Chap. 3 where we have used the fact that w 0 T = 27T. Note that eq. (3.79) is valid for k ¥- 0, since we cannot solve for c0 from eq. (3.78) with k = 0. However, since c0 is just the average value of g(t) over one period, we can determine it by inspection from Figure 3.12(b): 2TI Co= T' (3.80) Eqs. (3.80) and (3.79) are identical to eqs. (3.42) and (3.44), respectively, for the Fourier series coefficients of the square wave derived in Example 3.5. The next example is chosen to illustrate the use of many of the properties in Table 3.1. Example 3.9 Suppose we are given the following facts about a signal x(t): 1. x(t) is a real signal. 2. x(t) is periodic with period T = 4, and it has Fourier series coefficients GJ.:. 3. a k = 0 for \k \ > 1. 4. The signal with Fourier coefficients bk = e-Frrkl2a_k is odd. 5. ~ f4 \x(t)\ 2dt = 112. Let us show that this information is sufficient to determine the signal x(t) to within a sign factor. According to Fact 3, x(t) has at most three nonzero Fourier series coefficients at:: a0 , a 1, and a_ 1• Then, since x(t) has fundamental frequency w 0 = 27T/4 = 7T/2, it follows that Since x(t) is real (Fact 1), we can use the symmetry properties in Table 3.1 to conclude that a0 is real and a 1 = a*_ 1 • Consequently, (3.81) Let us now determine the signal corresponding to the Fourier coefficients bt: given in Fact 4. Using the time-reversal property from Table 3.1, we note that G-J.: corresponds to the signal x( -t). Also, the time-shift property in the table indicates that multiplication of the kth Fourier coefficient by e- Jk7rl2 = e- Jkwo corresponds to the underly~ng signal being shifted by 1 to the right (i.e., having t replaced by ~ - 1). We conclude that the coefficients bk correspond to the signal x( -(t- 1)) = x( -t + 1), which, according to Fact 4, must be odd. Since x(t) is real, x( -t + 1) must also be real. From Table 3.1, it then follows that the Fourier coefficients of x( -t + 1) must be purely imaginary and odd. Thus, b0 = 0 and b -I = - b1. Since time-reversal and time-shift operations cannot change the average power per period, Fact 5 holds even if x(t) is replaced by x( -t + 1). That is, ~ L\x ( -t + 1)\2dt = 112. (3.82) Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 211 We can now use Parseval's relation to conclude that (3.83) Substituting b1 = - b_ 1 in this equation, we obtain jb1l = 112. Since b1 is also known to be purely imaginary, it must be either j/2 or- j/2. Now we can translate these conditions on b0 and b1 into equivalent statements on a0 and a 1• First, since b0 = 0, Fact 4 implies that a0 = 0. With k = 1, this condition implies that a 1 = e- j7TI2b_ 1 = - jb_ 1 = jb 1• Thus, if we take b1 = j/2, then a 1 -112, and therefore, from eq. (3.81), x(t) = -cos(7 Tt/2). Alternatively, if we take b1 - j/2, then a 1 = 112, and therefore, x(t) = cos( 7Ttl2). 3.6 FOURIER SERIES REPRESENTATION OF DISCRETE-TIME PERIODIC SIGNALS In this section, we consider the Fourier series representation of discrete-time periodic sig- nals. While the discussion closely parallels that of Section 3.3, there are some important differences. In particular, the Fourier series representation of a discrete-time periodic sig- nal is a .finite series, as opposed to the infinite series representation required for continuous- time periodic signals. As a consequence, there are no mathematical issues of convergence such as those discussed in Section 3.4. 3.6.1 linear Combinations of Harmonically Related Complex Exponentials As defined in Chapter 1, a discrete-time signal x[n] is periodic with period N if x[n] = x[n + N]. (3.84) The fundamental period is the smallest positive integer N for which eq. (3.84) holds, and w 0 = 27T/ N is the fundamental frequency. For example, the complex exponential e1<27TIN)n is periodic with period N. Furthermore, the set of all discrete-time complex exponential signals that are periodic with period N is given by cpk(n] = ejkwon = ejk(27TIN)n, k = 0, ± 1, ±2, .... (3.85) All of these signals have fundamental frequencies that are multiples of27r/N and thus are harmonically related. As mentioned in Section 1.3.3, there are only N distinct signals in the set given by eq. (3.85). This is a consequence of the fact that discrete-time complex exponen- tials which differ in frequency by a multiple of 27r are identical. Specifically, cp0 [n] = </>N[n], <!>1 [n] = <I>N+l [n], and, in general, (3.86) That is, when k is changed by any integer multiple of N, the identical sequence is gener- ated. This differs from the situation in continuous time in which the signals </>k(t) defined in eq. (3 .24) are all different from one another. 212 Fourier Series Representation of Periodic Signals Chap.3 We now wish to consider the representation of more general periodic sequences in terms of linear combinations of the sequences <f>dn] in eq. (3.85). Such a linear combina- tion has the form x[n] = L ak<f>k[n] = L akejkwon = L a~.;ejk(2TTIN)n. (3.87) k k k Since the sequences <f>dn] are distinct only over a range of N successive values of k, the summation in eq. (3.87) need only include terms over this range. Thus, the summation is on k, as k varies over a range of N successive integers, beginning with any value of k. We indicate this by expressing the limits of the summation as k = (N). That is, x[n] = L ak<f>k[n] = L akejkwon = L akejk(2TriNJn. (3.88) k=(N) k=(N) k=(N) For example, k could take on the values k = 0, 1, ... , N- 1, or k = 3, 4, ... , N + 2. In either case, by virtue of eq. (3.86), exactly the same set of complex exponential sequences appears in the summation on the right-hand side of eq. (3.88). Equation (3.88) is referred to as the discrete-time Fourier series and the coefficients a~.; as the Fourier series coeffi- cients. 3.6.2 Determination of the Fourier Series Representation of a Periodic Signal Suppose now that we are given a sequence x[n] that is periodic with fundamental period N. We would like to determine whether a representation of x[n] in the form of eq. (3.88) exists and, if so, what the values of the coefficients ak are. This question can be phrased in terms of finding a solution to a set oflinear equations. Specifically, if we evaluate eq. (3.88) for N successive values of n corresponding to one period of x[n], we obtain x[O] = L a~.;, k=(N) x[l] = L akej2TrkiN, k=(N) (3.89) x[N _ l] = L akej2Trk(N-I)IN. k=(N) Thus, eq. (3.89) represents a set of N linear equations for theN unknown coefficients ak as k ranges over a set of N successive integers. It can be shown that this set of equations is linearly independent and consequently can be solved to obtain the coefficients a~.; in terms of the given values of x[n]. In Problem 3.32, we consider an example in which the Fourier series coefficients are obtained by explicitly solving the set of N equations given in eq. (3.89). However, by following steps parallel to those used in continuous time, it is possible to obtain a closed-form expression for the coefficients ak in terms of the values of the sequence x[n]. Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 213 The basis for this result is the fact, shown in Problem 3.54, that L ejk(2n/N)n = { N, k = 0, ±N, ±2N, ... (3.90) n=(N) O, otherwise Equation (3.90) states that the sum over one period of the values of a periodic complex exponential is zero, unless that complex exponential is a constant. Now consider the Fourier series representation of eq. (3.88). Multiplying both sides bye- jr(2rriN)n and summing over N terms, we obtain L x[n]e- jr(2rr/N)n = L L akej(k-r)(2rr!N)n. (3.91) n=(N) n=(N) k=(N) Interchanging the order of summation on the right-hand side, we have L x[n]e- jr(2rr/N)n = L ak L ej(k-r)(2rr/N)n. (3.92) n=(N) k=(N) n=(N) From the identity in eq. (3.90), the innermost sum on non the right-hand side of eq. (3.92) is zero, unless k - r is zero or an integer multiple of N. Therefore, if we choose values for r over the same range as that over which k varies in the outer summation, the innermost sum on the right-hand side of eq. (3.92) equals N if k = rand 0 if k =I= r. The right-hand side of eq. (3.92) then reduces toNa ,, and we have a, = ~ L x[n]e-jr(2rr!N)n. (3.93) n=(N) This provides a closed-form expression for obtaining the Fourier series coefficients, and we have the discrete-time Fourier series pair: x[n] L akejkwon = L akejk(2rr/N)n, (3.94) k=(N) k=(N) ak = N L x[n]e- jkwon = ~ L x[n]e- jk(2rr!N)n. (3.95) n=(N) n=(N) These equations play the same role for discrete-time periodic signals that eqs. (3.38) and (3.39) play for continuous-time periodic signals, with eq. (3.94) the synthesis equation and eq. (3.95) the analysis equation. As in continuous time, the discrete-time Fourier series coefficients ak are often referred to as the spectral coefficients of x[n]. These coefficients specify a decomposition of x[ n] into a sum of N harmonically related complex exponen- tials. Referring to eq. (3.88), we see that if we take kin the range from 0 toN - 1, we have (3.96) 214 Fourier Series Representation of Periodic Signals Chap.3 Similarly, if k ranges from 1 toN, we obtain (3.97) From eq. (3.86), <f>0[n] = <f>N[n], and therefore, upon comparing eqs. (3.96) and (3.97), we conclude that a0 = aN. Similarly, by letting k range over any set of N consecutive integers and using eq. (3.86), we can conclude that (3.98) That is, if we consider more than N sequential values of k, the values ak repeat periodically with period N. It is important that this fact be interpreted carefully. In particular, since there are only N distinct complex exponentials that are periodic with period N, the discrete- time Fourier series representation is a finite series with N terms. Therefore, if we fix the N consecutive values of k over which we define the Fourier series in eq. (3.94), we will obtain a set of exactly N Fourier coefficients from eq. (3.95). On the other hand, at times it will be convenient to use different sets of N values of k, and consequently, it is useful to regard eq. (3.94) as a sum over any arbitrary set of N successive values of k. For this reason, it is sometimes convenient to think of ak as a sequence defined for all values of k, but where only N successive elements in the sequence will be used in the Fourier series representation. Furthermore, since the <f>k[n] repeat periodically with period N as we vary k [eq. (3.86)], so must the ak [eq. (3.98)]. This viewpoint is illustrated in the next example. Example 3. 1 0 Consider the signal x[n] = sinwon, (3.99) which is the discrete-time counterpart of the signal x(t) = sin w0 t of Example 3.3. x[n] is periodic only if 27T/w0 is an integer or a ratio of integers. For the case when 27T/w0 is an integer N, that is, when 27T wo = N' x[n] is periodic with fundamental period N, and we obtain a result that is exactly analo- gous to the continuous-time case. Expanding the signal as a sum of two complex expo- nentials, we get x[n] = _!_ ej(21T!N)n _ _!_ e- j(21T!N)n (3.100) 2j 2j . Comparing eq. (3.100) with eq. (3.94), we see by inspection that 1 at = j' (3.101) 2 Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 215 and the remaining coefficients over the interval of summation are zero. As described previously, these coefficients repeat with period N; thus, aN+ 1 is also equal to ( 1/2j) and aN-t equals ( -112j). The Fourier series coefficients for this example with N = 5 are illustrated in Figure 3.13. The fact that they repeat periodically is indicated. However, only one period is utilized in the synthesis equation (3.94). 1 2] -6 -1 -8 -7 -5-4-3 -2 0 1 2 3 k Figure 3.13 Fourier coefficients for x[n] = sin(27T/5)n. Consider now the case when 27T/w0 is a ratio of integers-that is, when Assuming that M and N do not have any common factors, x[n] has a fundamental period of N. Again expanding x[n] as a sum of two complex exponentials, we have x[n] = _!_ejM(27TIN)n _ _!_e-jM(27T!N)n 2j 2j ' from which we can determine by inspection that aM = (l/2j), a-M = ( -112j), and the remaining coefficients over one period of length N are zero. The Fourier coefficients for this example with M = 3 and N = 5 are depicted in Figure 3.14. Again, we have indicated the periodicity of the coefficients. For example, for N = 5, a2 = a_ 3, which in our example equals ( -l/2j). Note, however, that over any period of length 5 there are only two nonzero Fourier coefficients, and therefore there are only two nonzero terms in the synthesis equation. 1 21 k Figure 3.14 Fourier coefficients for x[n] = sin 3(27T/5)n. 216 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 11 Consider the signal x[n] = 1 + sm. (2N7T ) n + 3 cos (2N7T ) n + cos (4N7T n + 27T ) . This signal is periodic with period N, and, as in Example 3.10, we can expand x[n] directly in terms of complex exponentials to obtain x[n] = 1 + 1j [ej(27TIN)n _ e- j(27TIN)n] + ~ [ej(27TIN)n + e- j(27TIN)n] 2 + ~ [ej(47Tn/N+7TI2) + e- j(47Tn/N+7T!2)]. Collecting terms, we find that x[n) ~ I + (~ + 21j )ej(2w/N)• + (~ - 2~ )e- j(2w/N)• + (iejwa )ej2(2w/N)• + (ie- jwa )e- j2(2w/Nl•, Thus the Fourier series coefficients for this example are a0 = 1, 3 1 3 1 . a 1 = 2 + 2j = 2- 21' 3 1 3 1 . a-l = 2 - 2j = 2 + 21 ' 1 . a2 = 21· 1 . a-2 = -2], with ak = 0 for other values of kin the interval of summation in the synthesis equa- tion (3.94). Again, the Fourier coefficients are periodic with period N, so, for example, aN = 1, a3N-l = ~ + !i, and a2-N = !i· In Figure 3.15(a) we have plotted the real and imaginary parts of these coefficients for N = 10, while the magnitude and phase of the coefficients are depicted in Figure 3.15(b). Note that in Example 3.11, a-k = a~ for all values of k. In fact, this equality holds whenever x[n] is real. The property is identical to one that we discussed in Section 3.3 for continuous-time periodic signals, and as in continuous time, one implication is that there are two alternative forms for the discrete-time Fourier series of real periodic sequences. These forms are analogous to the continuous-time Fourier series representations given in eqs. (3.31) and (3.32) and are examined in Problem 3.52. For our purposes, the exponential form of the Fourier series, as given in eqs. (3.94) and (3.95), is particularly convenient, and we will use it exclusively. Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 217 .... 111 ....... 111 ....... 11 1.~. ..... 111 ....... 111 ... . -2N -N 0 N 2N k (a) k 1T/2 k -1T/2 (b) Figure 3.1 s (a) Real and imaginary parts of the Fourier series coefficients in Example 3.11; (b) magnitude and phase of the same coefficients. 218 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 1 2 In this example, we consider the discrete-time periodic square wave shown in Fig- ure 3.16. We can evaluate the Fourier series for this signal using eq. (3.95). Because x[ n] = 1 for - N1 ::; n ::; N1, it is particularly convenient to choose the length-N interval of summation in eq. (3.95) so that it includes the range -N1 ::; n ::; N1• In this case, we can express eq. (3.95) as (3.102) .IIIII ..... :IIIII ...... IIIII ..· ·· -N -N1 0 N1 N n Figure 3. 16 Discrete-time periodic square wave. Letting m = n + N1, we observe that eq. (3.102) becomes 1 2Nt = _N L"".'."".'. e- jk(2rr!N)(m-N1l ak m=O (3.103) 1 2Nt Nejk(2rr!N)N1 L e- jk(2TT/N)m. m=O The summation in eq. (3.103) consists of the sum of the first 2N1 + 1 terms in a geometric series, which can be evaluated using the result of Problem 1.54. This yields 1 (1 _e - jk2rr(2N1 + 1)/N) a -ejk(2TTINlNt k = N 1 - e- jk(2rr!N) 1 e- jk(2TT/2N)[ejk2rr(N1+ 1/2)/N _ e- jk2TT(N1 + 1/2)/N] (3.104) N e- jk(2rr/2Nl[ejk(2rr12N) _ e- jk(2rr/2Nl] 1 sin[2?Tk(N1 + 112)/N] k =/= 0, ±N, ±2N, ... N sin( ?T k/N ) and 2NI + 1 k = 0, ±N, ±2N, .... (3.105) N The coefficients ak for 2N1 + 1 = 5 are sketched for N = 10, 20, and 40 in Figures 3.17(a), (b), and (c), respectively. In discussing the convergence of the continuous-time Fourier series in Section 3.4, we considered the example of a symmetric square wave and observed how the finite sum in eq. (3.52) converged to the square wave as the number of terms approached infinity.ln par- Sec. 3.6 Fourier Series Representation of Discrete-Time Periodic Signals 219 (a) (b) 1 olllllllllllllo I 1 I 11!10 I 1 olllllllllllllo 1 I o!llo 1 lolllllllllllllol 'I ill I' 'I 1111_1 8 _ 4 O 4 8 'I Ill I' 'I ill I' 'I k (c) Figure 3.17 Fourier series coefficients for the periodic square wave of Ex- ample 3.12; plots of Nak for 2N1 + 1 = 5 and (a) N = 10 ; (b) N = 20; and (c) N = 40. ticular, we observed the Gibbs phenomenon at the discontinuity, whereby, as the number of terms increased, the ripples in the partial sum (Figure 3.9) became compressed toward the discontinuity, with the peak amplitude of the ripples remaining constant independently of the number of terms in the partial sum. Let us consider the analogous sequence of partial sums for the discrete-time square wave, where, for convenience, we will assume that the period N is odd. In Figure 3.18, we have depicted the signals M x[n] L akejk(2TTIN)n (3.1 06) k=-M for the example of Figure 3.16 with N = 9, 2N1 + 1 = 5, and for several values of M. ForM = 4, the partial sum exactly equals x[n]. We see in particular that in contrast to the continuous-time case, there are no convergence issues and there is no Gibbs phenomenon. In fact, there are no convergence issues with the discrete-time Fourier series in general. The reason for this stems from the fact that any discrete-time periodic sequence x[n] is completely specified by a finite number N of parameters, namely, the values of the se- quence over one period. The Fourier series analysis equation (3.95) simply transforms this set of N parameters into an equivalent set-the values of theN Fourier coefficients-and 220 Fourier Series Representation of Periodic Signals Chap. 3 ~[n] M=1 JJ]IJr .. rJJ!JJr .. rJJ!JJr .. rJl!JJr .. rJJ!JJ -18 -9 0 ·9 18 n (a) ~[n] M=2 JJ[JJt,,tJJ!JJr,,tJJ!JJt,,tiJlJir,,tJJ!JJ -18 -9 0 9 18 n (b) ~[n] M=3 J[J[l ....l llll. ...l llll. ... l!Ill .... l[J[l -18 -9 0 9 18 n (c) ~[n] M=4 Figure 3.18 Partial sums of eqs. lllll .... IIJII ....I IJII ....I IJII ....I IIII (3.1 06) and (3.1 07) for the periodic -18 -9 0 9 18 n square wave of Figure 3.16 with N = 9 and 2N1 + 1 = 5: (a) M = 1; (d) (b) M = 2; (c) M = 3; (d) M = 4. the synthesis equation (3.94) tells us how to recover the values of the original sequence in terms of a .finite series. Thus, if N is odd and we take M = (N - 1) /2 in eq. (3.106), the sum includes exactly N terms, and consequently, from the synthesis equations, we have x[n] = x[n]. Similarly, if N is even and we let M .t[n] = L akejkC'2Tr!Nln, (3.1 07) k=-M+I then with M = N 12, this sum consists of N terms, and again, we can conclude from eq. (3.94) that x[n] = x[n]. In contrast, a continuous-time periodic signal takes on a continuum of values over a single period, and an infinite number of Fourier coefficients are required to represent it. Sec. 3.7 Properties of Discrete-Time Fourier Series 221 Thus, in general, none of the finite partial sums in eq. (3.52) yield the exact values of x(t), and convergence issues, such as those considered in Section 3.4, arise as we consider the problem of evaluating the limit as the number of terms approaches infinity. 3.7 PROPERTIES OF DISCRETE-TIME FOURIER SERIES There are strong similarities between the properties of discrete-time and continuous-time Fourier series. This can be readily seen by comparing the discrete-time Fourier series properties summarized in Table 3.2 with their continuous-time counterparts in Table 3.1. TABLE 3.2 PROPERTIES OF DISCRETE-TIME FOURIER SERIES Property Periodic Signal Fourier Series Coefficients x[n] } Periodic with period Nand ak } Periodic with y[n] fundamental frequency w0 = 2nlN bk period N Linearity Ax[n] + By[n] Aak + Bbk Time Shifting x[n- no] ake-jk(27TIN)n0 Frequency Shifting eiM<27TIN)n x[n] Conjugation x*[n] a*_k Time Reversal x[-n] a_k x[nlm], if n is a multiple of m Time Scaling 1 (viewed as periodic) X(m)[n] = { . . . -ak 0, If n IS not a multiple of m m with period mN (periodic with period mN) Periodic Convolution L x[r]y[n- r] r=(N) Multiplication x[n]y[n] L a1bk-1 l=(N) First Difference x[n] - x[n - 1] (1 - e-Jk(27T!NJ)ak Running Sum """""""" (finite valued and periodic only) L x[k]. . If ao 0 ( (1 - e-Iik(27TINJ) )ak k=-x = ak = a*_k ffi-e{ak} = ffi-e{a-k} Conjugate Symmetry for x[n] real 9m{ad = -9m{a-k} Real Signals liaki = ia-kl <rak = - <ta-k Real and Even Signals x[n] real and even ak real and even Real and Odd Signals x[n] real and odd ak purely imaginary and odd Even-Odd Decomposition Xe[n] = 8v{x[n]} [x[n] real] ffi-e{ak} of Real Signals { X0 [n] = 0d{x[n]} [x[n] real] j9m{ad Parseval's Relation for Periodic Signals ~ L L 2 lx[n]il = lakl n=(N) k=(N) 222 Fourier Series Representation of Periodic Signals Chap. 3 The derivations of many of these properties are very similar to those of the corresponding properties for continuous-time Fourier series, and several such derivations are considered in the problems at the end of the chapter. In addition, in Chapter 5 we will see that most of the properties can be inferred from corresponding properties of the discrete-time Fourier transform. Consequently, we limit the discussion in the following subsections to only a few of these properties, including several that have important differences relative to those for continuous time. We also provide examples illustrating the usefulness of various discrete- time Fourier series properties for developing conceptual insights and helping to reduce the complexity of the evaluation of the Fourier series of many periodic sequences. As with continuous time, it is often convenient to use a shorthand notation to indicate the relationship between a periodic signal and its Fourier series coefficients. Specifically, if x[n] is a periodic signal with period N and with Fourier series coefficients denoted by ak> then we will write 3. 7. 1 Multiplication The multiplication property of the Fourier series representation is one example of a prop- erty that reflects the difference between continuous time and discrete time. From Table 3.1, the product of two continuous-time signals of period T results in a periodic signal with pe- riod T whose sequence of Fourier series coefficients is the convolution of the sequences of Fourier series coefficients of the two signals being multiplied. In discrete time, suppose that and ~s y[n] ~ bk are both periodic with period N. Then the product x[n]y[n] is also periodic with period N, and, as shown in Problem 3.57, its Fourier coefficients, db are given by ~s x[n]y[n] ~ dk = L a,bk-1· (3.108) /=(N) Equation (3 .1 08) is analogous to the definition of convolution, except that the summation variable is now restricted to an interval of N consecutive samples. As shown in Problem 3.57, the summation can be taken over any set of N consecutive values of l. We refer to this tyr~ of operation as a periodic convolution between the two periodic sequences of Fourier coefficients. The usual form of the convolution sum (where the summation variable ranges from - oo to oo) is sometimes referred to as aperiodic convolution, to distinguish it from periodic convolution. 3. 7.2 First Difference The discrete-time parallel to the differentiation property of the continuous-time Fourier series involves the use of the first-difference operation, which is defined as x[n]- x[n -1]. Sec. 3.7 Properties of Discrete-Time Fourier Series 223 If x[n] is periodic with period N, then so is y[n], since shifting x[n] or linearly combining x[n] with another periodic signal whose period is N always results in a periodic signal with period N. Also, if then the Fourier coefficients corresponding to the first difference of x[n] may be expressed as (3.109) which is easily obtained by applying the time-shifting and linearity properties in Table 3.2. A common use of this property is in situations where evaluation of the Fourier series co- efficients is easier for the first difference than for the original sequence. (See Problem 3.31.) 3.7.3 Parseval's Relation for Discrete-Time Periodic Signals As shown in Problem 3.57, Parseval's relation for discrete-time periodic signals is given by (3.110) where the ak are the Fourier series coefficients of x[n] and N is the period. As in the continuous-time case, the left -hand side of Parse val's relation is the average power in one period for the periodic signal x[n]. Similarly, iaki2 is the average power in the kth harmonic component of x[n]. Thus, once again, Parseval's relation states that the average power in a periodic signal equals the sum of the average powers in all of its harmonic components. In discrete time, of course, there are only N distinct harmonic components, and since the ak are periodic with period N, the sum on the right-hand side of eq. (3.110) can be taken over any N consecutive values of k. 3.7.4 Examples In this subsection, we present several examples illustrating how properties of the discrete- time Fourier series can be used to characterize discrete-time periodic signals and to com- pute their Fourier series representations. Specifically, Fourier series properties, such as those listed in Table 3.2, may be used to simplify the process of determining the Fourier series coefficients of a given signal. This involves first expressing the given signal in terms of other signals whose Fourier series coefficients are already known or are simpler to com- pute. Then, using Table 3 .2, we can express the Fourier series coefficients of the given signal in terms of the Fourier series coefficients of the other signals. This is illustrated in Example 3.13. Example 3.14 then illustrates the determination of a sequence from some partial information. In Example 3.15 we illustrate the use of the periodic convolution prop- erty in Table 3.2. 224 Fourier Series Representation of Periodic Signals Chap.3 Example 3. 13 Let us consider the problem of finding the Fourier series coefficients ak of the sequence x[n] shown in Figure 3.19(a). This sequence has a fundamental period of 5. We observe that x[ n] may be viewed as the sum of the square wave x 1 [ n] in Figure 3 .19(b) and the de sequence x2[n] in Figure 3.19(c). Denoting the Fourier series coefficients of x1 [n] by bk and those of x 2 [n] by ck. we use the linearity property of Table 3.2 to conclude that ak=bk+ck. (3.111) 2 x[n] ... I I Ill I i lll I I lll -5 0 5 n (a) I I I . x~[n] • • I I I • • I I I 0 • • n (b) I I I I I I I I I I I I I I I I 0 n (c) Figure 3.19 (a) Periodic sequence x[n] for Example 3.13 and its represen- tation as a sum of (b) the square wave x1 [n] and (c) the de sequence x2[n]. From Example 3.12 (with N1 = 1 and N = 5), the Fourier series coefficients bk corre- sponding to x1 [n] can be expressed as ! sin(37T k/5) _ 5 sin(1rk/5) , fork# 0, ±5, ± 10, ... b k - 3 (3.112) \ 5, fork= 0, ±5, ±10, ... The sequence x2 [n] has only a de value, which is captured by its zeroth Fourier series coefficient: 1 4 co = 5 L x2[n] = 1. (3.113) n=O Since the discrete-time Fourier series coefficients are periodic, it follows that c k = 1 whenver k is an integer multiple of 5. The remaining coefficients of x2 [n] must be zero, because x2 [n] contains only a de component. We can now substitute the expressions for bk and ck into eq. (3.111) to obtain bk = ! sin(37T k/5) _ 5 sin( 1rk/5) , fork# 0, ±5, ± 10, ... ak - (3.114) 8 5, fork= 0, ±5, ±10, ... \ Sec. 3.7 Properties of Discrete-Time Fourier Series 225 Example 3. 1 4 Suppose we are given the following facts about a sequence x[n]: 1. x[n] is periodic with period N = 6. 2. L~=o x[n] = 2. 3. L:= 2( -1) 11 x[n] = 1. 4. x[n] has the minimum power per period among the set of signals satisfying the preceding three conditions. Let us determine the sequence x[n]. We denote the Fourier series coefficients of x[n] by ak. From Fact 2, we conclude that a0 = 1/3. Noting that (-1) 11 = e-}1rn = e-J(21TI6 )3n, we see from Fact 3 that a3 116. From Parseval's relation (see Table 3.2), the average power in x[n] is 5 P = Liakl2 • (3.115) k=O Since each nonzero coefficient contributes a positive amount toP, and since the values of a0 and a3 are prespecified, the value of P is minimized by choosing a 1 = a2 = a4 = a5 = 0. It then follows that x[n] = ao + a3eJm' = (1/3) + (1/6)( -1)"", (3.116) which is sketched in Figure 3.20. x[n] ~ l i l l l I -2 -1 0 2 3 n Figure 3.20 Sequence x[n] that is consistent with the properties specified in Example 3.14. Example 3.15 In this example we determine and sketch a periodic sequence, given an algebraic expres- sion for its Fourier series coefficients. In the process, we will also exploit the periodic convolution property (see Table 3.2) of the discrete-time Fourier series. Specifically, as stated in the table and as shown in Problem 3.58, if x[n] and y[n] are periodic with period N, then the signal w[n] = L x[r]y[n- r] r=(N) is also periodic with period N. Here, the summation may be taken over any set of N consecutive values of r. Furthermore, the Fourier series coefficients of w[n] are equal to Nakbb where ak and bk are the Fourier coefficients of x[n] and y[n], respectively. 226 Fourier Series Representation of Periodic Signals Chap.3 Suppose now that we are told that a signal w[n] is periodic with a fundamental period of N = 7 and with Fourier series coefficients sin2 (3Trkl7) (3.117) ck = 7sin2(Trkl7). We observe that ck = 7d~, where dk denotes the sequence of Fourier series coefficients of a square wave x[n], as in Example 3.12, with N1 = 1 and N = 7. Using the periodic convolution property, we see that 3 w[n] = L x[r]x[n- r] = L x[r]x[n- r], (3.118) r=m r= -3 where, in the last equality, we have chosen to sum over the interval -3 :::; r :::; 3. Except for the fact that the sum is limited to a finite interval, the product-and-sum method for evaluating convolution is applicable here. In fact, we can convert eq. (3.118) to an ordi- nary convolution by defining a signal x[nl that equals x[n] for -3 :::; n :::; 3 and is zero otherwise. Then, from eq. (3.118), 3 +x w[n] = L x[r]x[n- r] = L x[r]x[n- r]. r= -3 r= -'X That is, w[n] is the aperiodic convolution of the sequences x[n] and x[n]. The sequences x[r], x[r], and x[n- r] are sketched in Figure 3.21 (a)-(c ). From the figure we can immediately calculate w[n]. In particular we see that w[O] = 3; w[ -1] = w[1] = 2; w[-2] = w[2] = 1; and w[-3] = w[3] = 0. Since w[n] is periodic with period 7, we can then sketch w[n] as shown in Figure 3.21(d). 3.8 FOURIER SERIES AND LTI SYSTEMS In the preceding few sections, we have seen that the Fourier series representation can be used to construct any periodic signal in discrete time and essentially all periodic continuous-time signals of practical importance. In addition, in Section 3.2 we saw that the response of an LTI system to a linear combination of complex exponentials takes a particularly simple form. Specifically, in continuous time, if x(t) = e-~1 is the input to a continuous-time LTI system, then the output is given by y(t) = H(s)e·11 , where, from eq. (3.6), (3.119) in which h(t) is the impulse response of the LTI system. Similarly, if x[n] = zn is the input to a discrete-time LTI system, then the output is given by y[n] = H(z)zn, where, from eq. (3.10), +x H(z) = L h[k]z-k, (3.120) k= -Cfj in which h[n] is the impulse response of the LTI system. Sec. 3.8 Fourier Series and LTI Systems 227 x[r] • I I I • • • • I I r • • • • I I I • -3 -2 -1 0 2 3 (a) Q[r] I 1 • • • • • • • • I I • • • • • • • • ' -1 0 (b) x[n-r] I I I I I I1 • • • • • • • • • • • I I ' n-7 n-1 n n+1 (c) 3 w[n] 2 -7 -3 -2 -1 0 2 3 7 n (d) Figure 3.21 (a) The square-wave sequence x[r] in Example 3.15; (b) the sequence x[r] equal to x[r] for -3 :::; r :::; 3 and zero otherwise; (c) the sequence x[n- r]; (d) the sequence w[n] equal to the periodic convolution of x[n] with itself and to the aperiodic convolution of x[n] with x[n]. When s or z are general complex numbers, H(s) and H(z) are referred to as the system functions of the corresponding systems. For continuous-time signals and systems in this and the following chapter, we focus on the specific case in which CRe{s} = 0, so that s = jw, and consequently, est is of the form ejwr. This input is a complex exponential at frequency w. The system function of the forms = jw-i.e., H(jw) viewed as a function of w-is referred to as the frequency response of the system and is given by +oc H(jw) = J-o c h(t)e-jwtdt. (3.121) 228 Fourier Series Representation of Periodic Signals Chap.3 Similarly, for discrete-time signals and systems, we focus in this chapter and in Chapter 5 on values of z for which lzl = 1, so that z = eiw and z"" is of the form eJwn. Then the system function H(z) for z restricted to the form z = eiw is referred to as the frequency response of the system and is given by +x H(eiw) = L h[n]e- jwn. (3.122) n= -oo The response of an LTI system to a complex exponential signal of the form eiwt (in continuous time) or eJwn (in discrete time) is particularly simple to express in terms of the frequency response of the system. Furthermore, as a result of the superposition property for LTI systems, we can express the response of an L TI system to a linear combination of complex exponentials with equal ease. In Chapters 4 and 5, we will see how we can use these ideas together with continuous-time and discrete-time Fourier transforms to an- alyze the response of LTI systems to aperiodic signals. In the remainder of this chapter, as a first look at this important set of concepts and results, we focus on interpreting and understanding this notion in the context of periodic signals. Consider first the continuous-time case, and let x(t) be a periodic signal with a Fourier series representation given by +oo x(t) = L akejkwot. (3.123) k=-X Suppose that we apply this signal as the input to an LTI system with impulse response h(t). Then, since each of the complex exponentials in eq. (3.123) is an eigenfunction of the system, as in eq. (3.13) with sk = jkw0 , it follows that the output is +oc y(t) = L akHUkwo)eJkwot. (3.124) k = -oc Thus, y(t) is also periodic with the same fundamental frequency as x(t). Furthermore, if {ak} is the set of Fourier series coefficients for the input x(t), then {akH(j kwo)} is the set of coefficients for the output y(t). That is, the effect of the LTI system is to modify individually each of the Fourier coefficients of the input through multiplication by the value of the frequency response at the corresponding frequency. Example 3. 16 Suppost: that the periodic signal x(t) discussed in Example 3.2 is the input signal to an LTI system with impulse response h(t) = e-r u(t). Sec. 3.8 Fourier Series and LTI Systems 229 To calculate the Fourier series coefficients of the output y(t), we first compute the fre- quency response: H(jw) = Le -re- Jwr dT ---1- e- r e -;·wr I""' (3.125) 1 + jw 0 1 1 + jw"" Therefore, using eqs. (3.124) and (3.125), together with the fact that w 0 = 21T in this example, we obtain +3 y(t) = L bkejk2m, (3.126) k= -3 with bk = akH(j k21T), so that bo = 1, bl ~ H,+,j27T). b-1 H~-~j27T). b, ~ HI +lj 47T). h-2 ~ HI -lj47T). (3.127) Note that y(t) must be a real-valued signal, since it is the convolution of x(t) and h(t), which are both real. This can be verified by examining eq. (3.127) and observing that b~ = b- k. Therefore, y(t) can also be expressed in either ofthe forms given in eqs. (3 .31) and (3.32); that is, 3 y(t) = 1 + 2 L Dk cos (21Tkt + fh), (3.128) k=l or 3 y(t) = 1 + 2 L [Ek cos 21Tkt- Fk sin 21Tkt], (3.129) k=l where (3.130) These coefficients can be evaluated directly from eq. (3.127). For example, 1 1 D1 = lbii = 01 = <tb1 = - tan- (27T), 4Jl+47T2 ' 1 1T E1 = cR-e{bi} = 4(1 + 47T2)' F1 = tfm{bi} = 2(1 + 47T2)"" 230 Fourier Series Representation of Periodic Signals Chap.3 In discrete time, the relationship between the Fourier series coefficients of the input and output of an LTI system exactly parallels eqs. (3.123) and (3.124). Specifically, let x[n] be a periodic signal with Fourier series representation given by x[n] = L akejk(2rr!Nln. k=(N) If we apply this signal as the input to an LTI system with impulse response h[n], then, as in eq. (3 .16) with Zk = ei k(2rr1N ), the output is y[n] = ~ akH(ej2rrk!N)ejk(2rr!N)n. (3.131) k=(N) Thus, y[n] is also periodic with the same period as x[n], and the kth Fourier coefficient of y[n] is the product of the kth Fourier coefficient of the input and the value of the frequency response of the LTI system, H(ei2rrk!N), at the corresponding frequency. Example 3.17 Consider an LTI system with impulse response h[n] = a 11 u[n], -1 <a< 1, and with the input x[n] =cos (N27 Tn) . (3.132) As in Example 3.10, x[n] can be written in Fourier series form as Also, from eq. (3.122), H(eiw) = fane- Jwn = f (ae- Jw r. (3.133) n=O n=O This geometric series can be evaluated using the result of Problem 1.54, yielding (3.134) Using eq. (3.131), we then obtain the Fourier series for the output: (3.135) Sec. 3.9 Filtering 231 If we write then eq. (3.135) reduces to y[n] ~ rcos(~ n + 8). (3.136) For example, if N = 4, and thus, y[n] = ~c1 os (7Tn - tan- 1(a) ). We note that for expressions such as eqs. (3.124) and (3.131) to make sense, the frequency responses H(jw) and H(eiw) in eqs. (3.121) and (3.122) must be well defined and finite. As we will see in Chapters 4 and 5, this will be the case if the LTI systems under consideration are stable. For example, the LTI system in Example 3 .16, with impulse response h(t) = e-1 u(t), is stable and has a well-defined frequency response given by eq. (3.125). On the other hand, an LTI system with impulse response h(t) = e1 u(t) is unstable, and it is easy to check that the integral in eq. (3.121) for H(jw) diverges for any value of w. Similarly, the LTI system in Example 3.17, with impulse response h[n] = anu[n], is stable for jaj < 1 and has frequency response given by eq. (3.134). However, if jaj > 1, the system is unstable, and then the summation in eq. (3.133) diverges. 3. 9 FILTERING In a variety of applications, it is of interest to change the relative amplitudes of the fre- quency components in a signal or perhaps eliminate some frequency components entirely, a process referred to as .filtering. Linear time-invariant systems that change the shape of the spectrum are often referred to as frequency-shaping filters. Systems that are designed to pass some frequencies essentially undistorted and significantly attenuate or eliminate oth- ers are referred to as frequency-selective filters. As indicated by eqs. (3.124) and (3.131), the Fourier series coefficients of the output of an LTI system are those of the input multi- plied by the frequency response of the system. Consequently, filtering can be conveniently accomplished through the use of LTI systems with an appropriately chosen frequency re- sponse, and frequency-domain methods provide us with the ideal tools to examine this very important class of applications. In this and the following two sections, we take a first look at filtering through a few examples. 232 Fourier Series Representation of Periodic Signals Chap.3 3.9.1 Frequency-Shaping Filters One application in which frequency-shaping filters are often encountered is audio sys- tems. For example, LTI filters are typically included in such systems to permit the listener to modify the relative amounts of low-frequency energy (bass) and high-frequency en- ergy (treble). These filters correspond to LTI systems whose frequency responses can be changed by manipulating the tone controls. Also, in high-fidelity audio systems, a so-called equalizing filter is often included in the preamplifier to compensate for the frequency- response characteristics of the speakers. Overall, these cascaded filtering stages are fre- quently referred to as the equalizing or equalizer circuits for the audio system. Figure 3.22 illustrates the three stages of the equalizer circuits for one particular series of audio speak- ers. In this figure, the magnitude of the frequency response for each of these stages is shown on a log-log plot. Specifically, the magnitude is in units of 20 log 10 )H(jw )), referred to as decibels or dB. The frequency axis is labeled in Hz (i.e., w/27T) along a logarithmic scale. As will be discussed in more detail in Section 6.2.3, a logarithmic display of the magnitude of the frequency response in this form is common and useful. Taken together, the equalizing circuits in Figure 3.22 are designed to compensate for the frequency response of the speakers and the room in which they are located and to allow the listener to control the overall frequency response. In particular, since the three systems are connected in cascade, and since each system modifies a complex exponential input K e.iwt by multiplying it by the system frequency response at that frequency, it follows that the overall frequency response of the cascade of the three systems is the product of the three frequency responses. The first two filters, indicated in Figures 3.22(a) and (b), together make up the control stage of the system, as the frequency behavior of these filters can be adjusted by the listener. The third filter, illustrated in Figure 3.22(c), is the equalizer stage, which has the fixed frequency response indicated. The filter in Figure 3.22(a) is a low- frequency filter controlled by a two-position switch, to provide one of the two frequency responses indicated. The second filter in the control stage has two continuously adjustable slider switches to vary the frequency response within the limits indicated in Figure 3.22(b). Another class of frequency-shaping filters often encountered is that for which the filter output is the derivative of the filter input, i.e., y(t) = d x(t)ldt. With x(t) of the form x(t) = e.iwt, y(t) will be y(t) = jwe.iwt, from which it follows that the frequency response is H(jw) = jw. (3.137) The frequency response characteristics of a differentiating filter are shown in Figure 3.23. Since H(jw) is complex in general, and in this example in particular, H(jw) is frequently displayed (as in the figure) as separate plots of )H(jw )) and <XH(jw ). The shape ofthis fre- quency response implies that a complex exponential input e.iwr will receive greater ampli- fication for larger values of w. Consequently, differentiating filters are useful in enhancing rapid variations or transitions in a signal. One purpose for which differentiating filters are often used is to enhance edges in picture processing. A black-and-white picture can be thought of as a two-dimensional ""continuous-time"" signal x(t1, t2), where t1 and t2 are the horizontal and vertical coordi- nates, respectively, and x(t1, t2) is the brightness of the image. If the image is repeated periodically in the horizontal and vertical directions, then it can be represented by a two- dimensional Fourier series (see Problem 3.70) consisting of sums of products of complex +25 ~~~--~~~--~----~~--~--~--~~~~~--~ +20 +15 :m3. +10 r------------====-------------------------------~ 3l +5 ~-----~ c g_ 0 Switch position 2 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 100 200 400 600 1kHz 2 3 4 6 8 10 20 Frequency (a) +25 --~~--~~----~------~--~--~--~~~~~--~ +20 +15 m +10 :3. 3l +5 c g_ 0 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 1 00 200 400 600 1 kHz 2 3 4 6 8 10 20 Frequency (b) +25 .--.-.--.-.-.----.----.--.--.----.--.-.--.-.-.--~ +20 +15 m +1o :3. 3l +5 c g_ 0 (/) Q) a: -5 -10 -15 ~~~--~~~--~----~~--~--~--~~~~~--~ 20Hz 30 40 60 1 00 200 400 600 1kHz 2 3 4 6 8 10 20 Frequency (c) Figure 3.22 Magnitudes of the frequency responses of the equalizer circuits for one particular series of audio speakers, shown on a scale of 201og10 IH(jw)l, which is referred to as a decibel (or dB) scale. (a) Low- frequency filter controlled by a two-positior~ switch; (b) upper and lower frequency limits on a continuously adjustable shaping filter; (c) fixed frequency response of the equalizer stage. 233 234 Fourier Series Representation of Periodic Signals Chap.3 I HOw) I w ~r------ Figure 3.23 Characteristics of the w frequency response of a filter for which 1T 2 the output is the derivative of the in- put. exponentials, ejw 1 ' 1 and ejw 2t2 , that oscillate at possibly different frequencies in each of the two coordinate directions. Slow variations in brightness in a particular direction are represented by the lower harmonics in that direction. For example, consider an edge cor- responding to a sharp transition in brightness that runs vertically in an image. Since the brightness is constant or slowly varying along the edge, the frequency content of the edge in the vertical direction is concentrated at low frequencies. In contrast, since there is an abrupt variation in brightness across the edge, the frequency content of the edge in the horizontal direction is concentrated at higher frequencies. Figure 3.24 illustrates the effect on an image of the two-dimensional equivalent of a differentiating filter. 11 Figure 3.24(a) shows two original images and Figure 3.24(b) the result of processing those images with the filter. Since the derivative at the edges of a picture is greater than in regions where the brightness varies slowly with distance, the effect of the filter is to enhance the edges. Discrete-time LTI filters also find a broad array of applications. Many of these in- volve the use of discrete-time systems, implemented using general- or special-purpose digital processors, to process continuous-time signals, a topic we discuss at some length in Chapter 7. In addition, the analysis of time series information, including demographic data and economic data sequences such as the stock market average, commonly involves the use of discrete-time filters. Often the long-term variations (which correspond to low fre- quencies) have a different significance than the short-term variations (which correspond to high frequencies), and it is useful to analyze these components separately. Reshaping the relative weighting of the components is typically accomplished using discrete-time filters. As one example of a simple discrete-time filter, consider an LTI system that succes- sively takes a two-point average of the input values: 1 y[n] = 2(x[n] + x[n - 1] ). (3.138) 11 Specifically each image in Figure 3.24(b) is the magnitude of the two-dimensional gradient of its counterpart image in Figure 3.24(a) where the magnitude of the gradient off (x, y) is Sec. 3.9 Filtering 235 (a) (b) Figure 3.24 Effect of a differentiating filter on an image: (a) two original images; (b) the result of processing the original images with a differentiating filter. In this case h[n] = ~(8[n] + 8[n- 1]), and from eq. (3.122), we see that the frequency response of the system is . 1 + . . n H(e1w) = [1 e- 1w] = e-1w ~ cos(w/2). 2 (3.139) The magnitude of H(e.iw) is plotted in Figure 3.25(a), and <t-H(e.iw) is shown in Figure 3.25(b ). As discussed in Section 1.3.3, low frequencies for discrete-time complex expo- nentialsoccurnearw = 0, ±27T, ±47T, ... , andhighfrequenciesnearw = ±1r, ±37T, .... This is a result of the fact that e.i(w + 21r)n = e.iwn, so that in discrete time we need only con- sider a 27T interval of values of w in order to cover a complete range of distinct discrete- time frequencies. As a consequence, any discrete-time frequency responses H(e.iw) must be periodic with period 27T, a fact that can also be deduced directly from eq. (3.122). For the specific filter defined in eqs. (3.138) and (3.139), we see from Figure 3.25(a) that IH(e.iw)l is large for frequencies near w = 0 and decreases as we increase lwl toward 7T, indicating that higher frequencies are attentuated more than lower ones. For exam- ple, if the input to this system is constant-i.e., a zero-frequency complex exponential 236 Fourier Series Representation of Periodic Signals Chap. 3 IH(ei""')l 0 'lT (!) (a) Figure 3.25 (a) Magnitude and (b) phase for the frequency response of the discrete-time LTI system (b) y[n] = 1/2(x[n] + x[n- 1] ). x[n] = KejO·n = K-then the output will be y[n] = H(ej·O)KejwO·n = K = x[n]. On the other hand, if the input is the high-frequency signal x[n] Kej1rn then the output will be y[n] = H(ej1r)K ej1r·n = 0. Thus, this system separates out the long-term constant value of a signal from its high- frequency fluctuations and, consequently, represents a first example of frequency-selective filtering, a topic we look at more carefully in the next subsection. 3. 9. 2 Frequency-Selective Filters Frequency-selective filters are a class of filters specifically intended to accurately or approximately select some bands of frequencies and reject others. The use of frequency- selective filters arises in a variety of situations. For example, if noise in an audio recording is in a higher frequency band than the music or voice on the recording is, it can be removed by frequency-selective filtering. Another important application of frequency- selective filters is in communication systems. As we discuss in detail in Chapter 8, the basis for amplitude modulation (AM) systems is the transmission of information from many different sources simultaneously by putting the information from each channel into a separate frequency band and extracting the individual channels or bands at the receiver using frequency-selective filters. Frequency-selective filters for separating the individual Sec. 3.9 Filtering 237 channels and frequency-shaping filters (such as the equalizer illustrated in Figure 3.22) for adjusting the quality of the tone form a major part of any home radio and television receiver. While frequency selectivity is not the only issue of concern in applications, its broad importance has led to a widely accepted set of terms describing the characteristics of frequency-selective filters. In particular, while the nature of the frequencies to be passed by a frequency-selective filter varies considerably from application to application, several basic types of filter are widely used and have been given names indicative of their func- tion. For example, a lowp ass filter is a filter that passes low frequencies-i.e., frequencies around w = 0-and attenuates or rejects higher frequencies. A highpass filter is a filter that passes high frequencies and attentuates or rejects low ones, and a bandpass filter is a filter that passes a band of frequencies and attenuates frequencies both higher and lower than those in the band that is passed. In each case, the cutofff requencies are the frequen- cies defining the boundaries between frequencies that are passed and frequencies that are rejected-i.e., the frequencies in the passband and stopband. Numerous questions arise in defining and assessing the quality of a frequency- selective filter. How effective is the filter at passing frequencies in the passband? How effective is it at attentuating frequencies in the stopband? How sharp is the transition near the cutoff frequency-i.e., from nearly free of distortion in the passband to highly attenuated in the stopband? Each of these questions involves a comparison of the charac- teristics of an actual frequency-selective filter with those of a filter with idealized behavior. Specifically, an ideal frequency-selective filter is a filter that exactly passes complex ex- ponentials at one set of frequencies without any distortion and completely rejects signals at all other frequencies. For example, a continuous-time ideal lowpass filter with cutoff frequency we is an LTI system that passes complex exponentials efwt for values of win the range -we :::; w :::; we and rejects signals at all other frequencies. That is, the frequency response of a continuous-time ideal lowpass filter is 1 H(jw) = { ' (3.140) 0, as shown in Figure 3.26. H(jw) 11 0 w -+---Stopband-!-Passband-!-Stopband----+- Figure 3.26 Frequency response of an ideal lowpass filter. Figure 3.27(a) depicts the frequency response of an ideal continuous-time highpass filter with cutoff frequency We, and Figure 3.27(b) illustrates an ideal continuous-time bandpass filter with lower cutoff frequency wc1 and upper cutoff frequency wc2 . Note that each of these filters is symmetric about w = 0, and thus, there appear to be two pass bands for the highpass and bandpass filters. This is a consequence of our having adopted the 238 Fourier Series Representation of Periodic Signals Chap.3 H(jw} 1! -we We w (a) H(jw) 1! Figure 3.27 (a) Frequency re- -we2 -we1 We1 w sponse of an ideal highpass filter; (b) frequency response of an ideal (b) bandpass filter. use of the complex exponential signal e.iwt, rather than the sinusoidal signals sin wt and cos wt, at frequency w. Since e.iwt = cos wt + j sin wt and e-.iwt = cos wt- j sin wt, both of these complex exponentials are composed of sinusoidal signals at the same frequency w. For this reason, we usually define ideal filters so that they have the symmetric frequency response behavior seen in Figures 3.26 and 3.27. In a similar fashion, we can define the corresponding set of ideal discrete-time frequency-selective filters, the frequency responses for which are depicted in Figure 3.28. H(eiw) ,=] 11 -2TI -we 0 We 2TI W (a) H(eiw) I -2TI 2TI W (b) H(eiw) I II -2TI 2TI Figure 3.28 Discrete-time ideal W frequency-selective filters: (a) lowpass; (c) (b) highpass; (c) bandpass. Sec. 3.10 Examples of Continuous-Time Filters Described by Differential Equations 239 In particular, Figure 3.28(a) depicts an ideal discrete-time lowpass filter, Figure 3.28(b) is an ideal highpass filter, and Figure 3.28(c) is an ideal bandpass filter. Note that, as discussed in the preceding section, the characteristics of the continuous-time and discrete- time ideal filters differ by virtue of the fact that, for discrete-time filters, the frequency response H(ejw) must be periodic with period 27T, with low frequencies near even multi- ples of 1T and high frequencies near odd multiples of 1T. As we will see on numerous occasions, ideal filters are quite useful in describing ide- alized system configurations for a variety of applications. However, they are not realizable in practice and must be approximated. Furthermore, even if they could be realized, some of the characteristics of ideal filters might make them undesirable for particular applications, and a nonideal filter might in fact be preferable. In detail, the topic of filtering encompasses many issues, including design and imple- mentation. While we will not delve deeply into the details of filter design methodologies, in the remainder of this chapter and the following chapters we will see a number of other examples of both continuous-time and discrete-time filters and will develop the concepts and techniques that form the basis of this very important engineering discipline. 3. 1 0 EXAMPLES OF CONTINUOUS-TIME FILTERS DESCRIBED BY DIFFERENTIAL EQUATIONS In many applications, frequency-selective filtering is accomplished through the use of LTI systems described by linear constant-coefficient differential or difference equations. The reasons for this are numerous. For example, many physical systems that can be inter- preted as performing filtering operations are characterized by differential or difference equations. A good example of this that we will examine in Chapter 6 is an automobile suspension system, which in part is designed to filter out high-frequency bumps and ir- regularities in road surfaces. A second reason for the use of filters described by differen- tial or difference equations is that they are conveniently implemented using either analog or digital hardware. Furthermore, systems described by differential or difference equa- tions offer an extremely broad and flexible range of designs, allowing one, for example, to produce filters that are close to ideal or that possess other desirable characteristics. In this and the next section, we consider several examples that illustrate the implementation of continuous-time and discrete-time frequency-selective filters through the use of dif- ferential and difference equations. In Chapters 4-6, we will see other examples of these classes of filters and will gain additional insights into the properties that make them so use- ful. 3.1 0.1 A Simple RC Lowpass Filter Electrical circuits are widely used to implement continuous-time filtering operations. One of the simplest examples of such a circuit is the first-order RC circuit depicted in Fig- ure 3.29, where the source voltage vs(t) is the system input. This circuit can be used to perform either a lowpass or highpass filtering operation, depending upon what we take as the output signal. In particular, suppose that we take the capacitor voltage vc(t) as the output. In this case, the output voltage is related to the input voltage through the linear 240 Fourier Series Representation of Periodic Signals Chap. 3 + v,(t)- R + c Figure 3.29 First-order RC filter. constant-coefficient differential equation (3.141) Assuming initial rest, the system described by eq. (3.141) is LTI. In order to determine its frequency response H(jw ), we note that, by definition, with input voltage v,(t) = e.i'"" 1, we must have the output voltage vc(t) = H(jw )e.iwt. If we substitute these expressions into eq. (3.141), we obtain d . . . RC- [H(jw )e1'""1) + H(jw )ei'"" 1 = el'"" 1, (3.142) dt or RC jwH(jw )ejvJt + H(jw )ejcvt = ejwt, (3.143) from which it follows directly that H(jw )ejwt 1 ejcvl (3.144) 1 + RC}w ' or H(jw) = 1 + RC . . (3.145) JW The magnitude and phase of the frequency response H(jw) for this example are shown in Figure 3.30. Note that forfrequencies near w = 0, IH (jw )I = 1, while for larger values of w (positive or negative), IH(}w)i is considerably smaller and in fact steadily decreases as lwl increases. Thus, this simple RC filter (with Vc(t) as output) is a nonideal lowpass filter. To provide a first glimpse at the trade-offs involved in filter design, let us briefly consider the time-domain behavior of the circuit. In particular, the impulse response of the system described by eq. (3.141) is (3.146) Sec. 3.10 Examples of Continuous-Time Filters Described by Differential Equations 241 !H(jw)l -1/RC 0 1/RC w (a) <tH(jw) Tr/2 w (b) Figure 3.30 (a) Magnitude and (b) phase plots for the frequency response for the RC circuit of Figure 3.29 with output Vc(t). and the step response is s(t) = [1 - e -r!RC]u(t), (3.147) both of which are plotted in Figure 3.31 (where T = RC). Comparing Figures 3.30 and 3.31, we see a fundamental trade-off. Specifically, suppose that we would like our filter to pass only very low frequencies. From Figure 3.30(a), this implies that 1/ RC must be small, or equivalently, that RC is large, so that frequencies other than the low ones of interest will be attentuated sufficiently. However, looking at Figure 3.3l(b), we see that if RC is large, then the step response will take a considerable amount of time to reach its long-term value of 1. That is, the system responds sluggishly to the step input. Conversely, if we wish to have a faster step response, we need a smaller value of RC, which in tum implies that the filter will pass higher frequencies. This type of trade-off between behavior in the frequency domain and in the time domain is typical of the issues arising in the design and analysis of LTI systems and filters and is a subject we will look at more carefully in Chapter 6. 3.1 0.2 A Simple RC Highpass Filter As an alternative to choosing the capacitor voltage as the output in our RC circuit, we can choose the voltage across the resistor. In this case, the differential equation relating input 242 Fourier Series Representation of Periodic Signals Chap. 3 h(t) (a) s(t) 1- .!_ e Figure 3.31 (a) Impulse response of the first-order RC lowpass filter with r = RC; (b) step response of RC low- (b) pass filter with r = RC. and output is Rc dv,(t) () = RCdvs(t) dt + Vr t dt . (3.148) We can find the frequency response G(jw) of this system in exactly the same way we did in the previous case: If Vs(t) = eJwt, then we must have v,(t) = G(jw )eiwt; substituting these expressions into eq. (3.148) and performing a bit of algebra, we find that G(. ) = jwRC JW 1 + (3.149) jwRC The magnitude and phase of this frequency response are shown in Figure 3.32. From the figure, we see that the system attenuates lower frequencies and passes higher frequencies- i.e., those for which Jwl >> 11 RC-with minimal attenuation. That is, this system acts as a nonideal highpass filter. As with the lowpass filter, the parameters of the circuit control both the frequency response of the high pass filter and its time response characteristics. For example, consider the step response for the filter. From Figure 3.29, we see that v,(t) = v,(t) - Vc(t). Thus, if v,(t) = u(t), Vc(t) must be given by eq. (3.147). Consequently, the step response of the highpass filter is (3.150) which is depicted in Figure 3.33. Consequently, as RC is increased, the response becomes more sluggish-i.e., the step response takes a longer time to reach its long-term value Sec.3.10 Examples of Continuous-Time Filters Described by Differential Equations 243 IG(jw)l -1/RC 0 1/RC (a) <l:G(jw) (b) Figure 3.32 (a) Magnitude and (b) phase plots for the frequency response of the RC circuit of Figure 3.29 with output v,(t). v,(t) Figure 3.33 Step response of the RC first-order RC highpass filter with T = RC. of 0. From Figure 3.32, we see that increasing RC (or equivalently, decreasing 11 RC) also affects the frequency response, specifically, it extends the passband down to lower frequencies. We observe from the two examples in this section that a simple RC circuit can serve as a rough approximation to a high pass or a lowpass filter, depending upon the choice of the physical output variable. As illustrated in Problem 3.71, a simple mechanical system using a mass and a mechanical damper can also serve as a lowpass or highp ass filter described by 244 Fourier Series Representation of Periodic Signals Chap.3 analogous first-order differential equations. Because of their simplicity, these examples of electrical and mechanical filters do not have a sharp transition from passband to stopband and, in fact, have only a single parameter (namely, RC in the electrical case) that con- trols both the frequency response and time response behavior of the system. By designing more complex filters, implemented using more energy storage elements (capacitances and inductances in electrical filters and springs and damping devices in mechanical filters), we obtain filters described by higher order differential equations. Such filters offer con- siderably more flexibility in terms of their characteristics, allowing, for example, sharper passband-stopband transition or more control over the trade-offs between time response and frequency response. 3.11 EXAMPLES OF DISCRETE-TIME FILTERS DESCRIBED BY DIFFERENCE EQUATIONS As with their continuous-time counterparts, discrete-time filters described by linear constant-coefficient difference equations are of considerable importance in practice. In- deed, since they can be efficiently implemented in special- or general-purpose digital systems, filters described by difference equations are widely used in practice. As in al- most all aspects of signal and system analysis, when we examine discrete-time filters described by difference equations, we find both strong similarities and important differ- ences with the continuous-time case. In particular, discrete-time LTI systems described by difference equations can either be recursive and have impulse responses of infinite length (IIR systems) or be nonrecursive and have finite-length impulse responses (FIR systems). The former are the direct counterparts of continuous-time systems described by differential equations illustrated in the previous section, while the latter are also of considerable practical importance in digital systems. These two classes have distinct sets of advantages and disadvantages in terms of ease of implementation and in terms of the order of filter or the complexity required to achieve particular design objectives. In this section we limit ourselves to a few simple examples of recursive and nonrecursive filters, while in Chapters 5 and 6 we develop additional tools and insights that allow us to analyze and understand the properties of these systems in more detail. 3.11.1 First-Order Recursive Discrete-Time Filters The discrete-time counterpart of each of the first -order filters considered in Section 3.10 is the LTI system described by the first-order difference equation y[n] - ay[n- 1] = x[n]. (3.151) From the eigenfunction property of complex exponential signals, we know that if x[n] = eiwn, then y[n] = H(ei""')eiwn, where H(ei""') is the frequency response of the system. Substituting into eq. (3.151), we obtain (3.152) or (3.153) Sec.3.11 Examples of Discrete-Time Filters Described by Difference Equations 245 so that H(eiw) = (3.154) 1-ae~jw' The magnitude and phase of H(eiw) are shown in Figure 3.34(a) for a = 0.6 and in Figure 3.34(b) for a = -0.6. We observe that, for the positive value of a, the difference equation (3.151) behaves like a lowpass filter with minimal attenuation of low frequencies near w = 0 and increasing attenuation as we increase w toward w = 7T. For the negative value of a, the system is a highpass filter, passing frequencies near w = 7T and attenuating lower frequencies. In fact, for any positive value of a < 1, the system approximates a lowpass filter, and for any negative value of a > -1, the system approximates a highpass filter, where Ia I controls the size of the filter passband, with broader pass bands as Ia I is decreased. As with the continuous-time examples, we again have a trade-off between time do- main and frequency domain characteristics. In particular, the impulse response of the sys- tem described by eq. (3.151) is h[n] = a 11 u[n]. (3.155) The step response s[n] u[n] * h[n] is 1 -an+ I s[n] = 1 u[n]. (3.156) -a From these expressions, we see that lal also controls the speed with which the impulse and step responses approach their long-term values, with faster responses for smaller values of lai, and hence, for filters with smaller passbands. Just as with differential equations, higher order recursive difference equations can be used to provide sharper filter charac- teristics and to provide more flexibility in balancing time-domain and frequency-domain constraints. Finally, note from eq. (3.155) that the system described by eq. (3.151) is unstable if lal ::::: 1 and thus does not have a finite response to complex exponential inputs. As we stated previously, Fourier-based methods and frequency domain analysis focus on systems with finite responses to complex exponentials; hence, for examples such as eq. (3.151), we restrict ourselves to stable systems. 3. 11.2 Nonrecursive Discrete-Time Filters The general form of an FIR nonrecursive difference equation is M y[n] = L bkx[n - k]. (3.157) k= ~N That is, the output y[n] is a weighted average of the (N + M + 1) values of x[n] from x[n - M] through x[n + N], with the weights given by the coefficients bk. Systems of this form can be used to meet a broad array of filtering objectives, including frequency- selective filtering. One frequently used example of such a filter is a moving-average filter, where the output y[n] for any n-say, n0-is an average of values of x[n] in the vicinity of no. The -1T 1T w 7r 2 w 7r 2 (a) I H(ei""') I -1T Figure 3.34 Frequency response 7r 2 of the first-order recursive discrete- time filter of eq. (3.151 ): (a) a = 0.6; (b) (b) a = -0.6. 246 Sec. 3.11 Examples of Discrete-Time Filters Described by Difference Equations 247 basic idea is that by averaging values locally, rapid high-frequency components of the in- put will be averaged out and lower frequency variations will be retained, corresponding to smoothing or lowpass filtering the original sequence. A simple two-point moving-average filter was briefly introduced in Section 3.9 [eq. (3.138)]. An only slightly more complex example is the three-point moving-average filter, which is of the form 1 y[n] = 3(x[n- 1] + x[n] + x[n + 1]), (3.158) so that each output y[n] is the average of three consecutive input values. In this case, 1 h[n] = '3[8[n + 1] + 8[n] + 8[n- 1]], and thus, from eq. (3.122), the corresponding frequency response is (3.159) The magnitude of H(eiw) is sketched in Figure 3.35. We observe that the filter has the general characteristics of a lowpass filter, although, as with the first-order recursive filter, it does not have a sharp transition from passband to stopband. Figure 3.35 Magnitude of the fre- 0 :rr 1T 21T w quency response of a three-point 2 moving-average lowpass filter. The three-point moving-average filter in eq. (3.158) has no parameters that can be changed to adjust the effective cutoff frequency. As a generalization of this moving- average filter, we can consider averaging over N + M + 1 neighboring points-that is, using a difference equation of the form 1 M y[n] = N + M + 1 L x[n- k]. (3.160) k=-N The corresponding impulse response is a rectangular pulse (i.e., h[n] = li(N + M + 1) for -N ::::; n ::::; M, and h[n] = 0 otherwise). The filter's frequency response is M H(eiw) = 1 L e- }wk. (3.161) N + M + 1 k=-N 248 Fourier Series Representation of Periodic Signals Chap.3 The summation in eq. (3.161) can be evaluated by performing calculations similar to those in Example 3.12, yielding H( jw) = 1 jw[(N-M)/2] sin[w(M + N + 1)/2] + + (3.162) e N M 1 e sin(w/2) · By adjusting the size, N + M + 1, of the averaging window we can vary the cutoff fre- quency. For example, the magnitude of H(ejw) is shown in Figure 3.36for M +N + 1 = 33 and M + N + 1 = 65. 0 11'12 w (a) 0 w (b) Figure 3.36 Magnitude of the frequency response for the lowpass moving- average filter of eq. {3.162): {a) M = N = 16; {b) M = N = 32. Nonrecursive filters can also be used to perform highpass filtering operations. To illustrate this, again with a simple example, consider the difference equation y [ n ] _ x[n] - x[n - 1] - 2 . (3.163) For input signals that are approximately constant, the value of y[n] is close to zero. For input signals that vary greatly from sample to sample, the values of y[n] can be ex- Sec. 3.12 Summary 249 pected to have larger amplitude. Thus, the system described by eq. (3.163) approximates a highpass filtering operation, attenuating slowly varying low-frequency components and passing rapidly varying higher frequency components with little attenuation. To see this more precisely we need to look at the system's frequency response. In this case, h[n] ~{S[n]- o[n- 1]}, so that direct application of eq. (3.122) yields (3.164) In Figure 3.37 we have plotted the magnitude of H(ejw), showing that this simple system approximates a highpass filter, albeit one with a very gradual transition from pass- band to stopband. By considering more general nonrecursive filters, we can achieve far sharper transitions in lowpass, highpass, and other frequency-selective filters. w Figure 3.37 Frequency response of a simple highpass filter. Note that, since the impulse response of any FIR system is of finite length (i.e., from eq. (3.157), h[n] = bn for -N ::::: n ::::: M and 0 otherwise), it is always absolutely summable for any choices of the bn. Hence, all such filters are stable. Also, if N > 0 in eq. (3.157), the system is noncausal, since y[n] then depends on future values of the input. In some applications, such as those involving the processing of previously recorded signals, causality is not a necessary constraint, and thus, we are free to use filters with N > 0. In others, such as many involving real-time processing, causality is essential, and in such cases we must take N ::::: 0. 3.12 SUMMARY In this chapter, we have introduced and developed Fourier series representations for both continuous-time and discrete-time systems and have used these representations to take a first look at one of the very important applications of the methods of signal and system analysis, namely, filtering. In particular, as we discussed in Section 3.2, one of the primary motivations for the use of Fourier series is the fact that complex exponential signals are eigenfunctions of LTI systems. We have also seen, in Sections 3.3-3.7, that any periodic signal of practical interest can be represented in a Fourier series-i.e., as a weighted sum 250 Fourier Series Representation of Periodic Signals Chap. 3 of harmonically related complex exponentials that share a common period with the signal being represented. In addition, we have seen that the Fourier series representation has a number of important properties which describe how different characteristics of signals are reflected in their Fourier series coefficients. One of the most important properties of Fourier series is a direct consequence of the eigenfunction property of complex exponentials. Specifically, if a periodic signal is ap- plied to an LTI system, then the output will be periodic with the same period, and each of the Fourier coefficients of the output is the corresponding Fourier coefficient of the input multiplied by a complex number whose value is a function of the frequency corre- sponding to that Fourier coefficient. This function of frequency is characteristic of the LTI system and is referred to as the frequency response of the system. By examining the fre- quency response, we were led directly to the idea of filtering of signals using LTI systems, a concept that has numerous applications, including several that we have described. One important class of applications involves the notion of frequency-selective filtering-i.e., the idea of using an LTI system to pass certain specified bands of frequencies and stop or significantly attentuate others. We introduced the concept of ideal frequency-selective filters and also gave several examples of frequency-selective filters described by linear constant-coefficient differential or difference equations. The purpose of this chapter has been to begin the process of developing both the tools of Fourier analysis and an appreciation for the utility of these tools in applications. In the chapters that follow, we continue with this agenda by developing the Fourier transform representations for aperiodic signals in continuous and discrete time and by taking a deeper look not only at filtering, but also at other important applications of Fourier methods. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 3.1. A continuous-time periodic signal x(t) is real valued and has a fundamental period T = 8. The nonzero Fourier series coefficients for x(t) are a1 = a_ 1 = 2,a3 = a*_ 3 = 4j. Express x(t) in the form x(t) = L Ak cos(wkt + cPk). k=O 3.2. A discrete-time periodic signal x[n] is real valued and has a fundamental period N = 5. The nonzero Fourier series coefficients for x[n] are Chap. 3 Problems 251 Express x[n] in the form x[n] = Ao + ~ Ak sin(wkn + cfJk). k~l 3.3. For the continuous-time periodic signal x(t) = 2 +cos (327T t ) + 4sm. (537T t ), determine the fundamental frequency w0 and the Fourier series coefficients ak such that x(t) = ~ akeJkwot k~-x 3.4. Use the Fourier series analysis equation (3.39) to calculate the coefficients ak for the continuous-time periodic signal x(t) = { 1.5, Ost<l -1.5, l:St<2 with fundamental frequency w0 = 1T. 3.5. Let x1 (t) be a continuous-time periodic signal with fundamental frequency w 1 and Fourier coefficients ak. Given that X2(t) = X1(1- t) + X1(t- 1), how is the fundamental frequency w2 of x 2(t) related tow 1? Also, find a relationship between the Fourier series coefficients bk of x 2(t) and the coefficients ak. You may use the properties listed in Table 3.1. 3.6. Consider three continuous-time periodic signals whose Fourier series representa- tions are as follows: Use Fourier series properties to help answer the following questions: (a) Which of the three signals is/are real valued? (b) Which of the three signals is/are even? 3.7. Suppose the periodic signal x(t) has fundamental period T and Fourier coefficients ak. In a variety of situations, it is easier to calculate the Fourier series coefficients 252 Fourier Series Representation of Periodic Signals Chap.3 bk for g(t) = dx(t)/dt, as opposed to calculating ak directly. Given that I:J T x(t)dt = 2, find an expression for ak in terms of bk and T. You may use any of the properties listed in Table 3.1 to help find the expression. 3.8. Suppose we are given the following information about a signal x(t): 1. x(t) is real and odd. 2. x(t) is periodic with period T = 2 and has Fourier coefficients ak. 3. ak = 0 for lkl > I. 4. Ho2 ix(t)j2 dt = 1. Specify two different signals that satisfy these conditions. 3.9. Use the analysis equation (3.95) to evaluate the numerical values of one period of the Fourier series coefficients of the periodic signal x[n] = L {48[n- 4m] + 88[n- 1 - 4m]}. In= -X 3.10. Let x[n] be areal and odd periodic signal with period N = 7 and Fourier coefficients ak· Given that a,s = j, at6 = 2}, a17 = 3}, determine the values of ao, a_,, a-2, and a-3· 3.11. Suppose we are given the following information about a signal x[n]: 1. x[ n] is a real and even signal. 2. x[n] has period N = 10 and Fourier coefficients ak. 3. a11 = 5. 9 4. Yo 2:: jx[nJI2 = 50. n=O Show that x[ n] = A cos(Bn + C), and specify numerical values for the constants A, B, and C. 3.12. Each of the two sequences Xt [n] and x2[n] has a period N = 4, and the correspond- ing Fourier series coefficients are specified as Xt [n] ~ ah x2[n] ~ bh where 1 ao = a3 = -a, 2 Using the multiplication property in Table 3.1, determine the Fourier series coeffi- cients ck for the signal g[n] = Xt [n]x2[n]. Chap. 3 Problems 253 3.13. Consider a continuous-time LTI system whose frequency response is Ix sin(4w) H(jw) = -x h(t)e-jwtdt = w If the input to this system is a periodic signal ~ O:s:t<4 x(t) = { l, 4:St<8 with period T = 8, determine the corresponding system output y(t). 3.14. When the impulse train x[n] = L o[n - 4k] k=-X is the input to a particular LTI system with frequency response H(eiw), the output of the system is found to be y[n] = cos (257T n + 47T ) . Determine the values of H(eikrr/2 ) fork = 0, 1, 2, and 3. 3.15. Consider a continuous-time ideallowpass filterS whose frequency response is H(jw) 1, lwl :s: 100 = { 0, lwl > 100 · When the input to this filter is a signal x(t) with fundamental period T = 1r/6 and Fourier series coefficients ak. it is found that s x(t) ~ y(t) = x(t). For what values of k is it guaranteed that ak = 0? 3.16. Determine the output of the filter shown in Figure P3.16 for the following periodic inputs: (a) x 1[n] = (-1)"" (b) x2[n] = 1 + sin(3; n + 'i-) (c) x3[n] = ~~=-oc(~r-4ku[n- 4k] ""IT 57r ""IT 3 12 Figure P3.16 254 Fourier Series Representation of Periodic Signals Chap.3 3.17. Consider three continuous-time systems S1, S2, and S3 whose responses to a complex exponential input ei51 are specified as sl : ej5t --7 tej5t, S2 : ej5t ----7 ejS(t-1), S3 : ei51 ----7 cos(St). For each system, determine whether the given information is sufficient to conclude that the system is definitely not LTI. 3.18. Consider three discrete-time systems S1, S2, and S3 whose respective responses to a complex exponential input ei""""12 are specified as sl : ej7rnl2 ----7 ejmz/2u[n], s2 : ej7rn/2 ----7 ej37rnl2, s3 : ej7rn/2 --7 2ej57rn/2. For each system, determine whether the given information is sufficient to conclude that the system is definitely not LTI. 3.19. Consider a causal LTI system implemented as the RL circuit shown in Figure P3.19. A current source produces an input current x(t), and the system output is considered to be the current y(t) flowing through the inductor. 1f1 Figure P3. 19 (a) Find the differential equation relating x(t) and y(t). (b) Determine the frequency response of this system by considering the output of the system to inputs of the form x(t) = eiwt. (c) Determine the output y(t) if x(t) = cos(t). 3.20. Consider a causal LTI system implemented as the RLC circuit shown in Figure P3.20. In this circuit, x(t) is the input voltage. The voltage y(t) across the capac- itor is considered the system output. R=1H L=1H + x(t) Figure P3.20 Chap. 3 Problems 255 (a) Find the differential equation relating x(t) and y(t). (b) Determine the frequency response of this system by considering the output of the system to inputs of the form x(t) = ejwr. (c) Determine the output y(t) if x(t) = sin(t). BASIC PROBLEMS 3.21. A continuous-time periodic signal x(t) is real valued and has a fundamental period T = 8. The nonzero Fourier series coefficients for x(t) are specified as a,= a*_ 1 = j,as =a-s= 2. Express x(t) in the form x(t) = L Ak cos(wkt + cf>k). k=O 3.22. Determine the Fourier series representations for the following signals: (a) Each x(t) illustrated in Figure P3.22(a)-(f). (b) x(t) periodic with period 2 and x(t) = e -r for - 1 < t < 1 x(t) x(t) ~ I / , I ~ / -5 -4 -3 -2 -1 2 3 4 5 (b) x(t) ~ (c) Figure P3.22 256 Fourier Series Representation of Periodic Signals Chap.3 x(t) t ~3 t ~1 1 t 3 t 5 r t ~4 l ~2 l l~ 2 2 l 4 l 6 (d) x(t) l I ~7 ~6 u ~~ ~ 2n ~ 1 ~ 1' t u I n I 3 4 5 6 u (e) x(t) ~-QJ,:~ r--, r 3 4 5 6 7 (f) Figure P3.22 Continued (c) x(t) periodic with period 4 and x(t) = { sin 7rt, 0, 3.23. In each of the following, we specify the Fourier series coefficients of a continuous- time signal that is periodic with period 4. Determine the signal x(t) in each case. { 0 k = 0 (a) ak = ( '·)k sin hr/4 otherwise J k1r ' (b) ak = (-l)ksi;~;/8, ao = /6 ( ) - { jk, lkl < 3 c ak - 0, otherwise (d) ak = { 1' k even 2, k odd 3.24. Let x(t) = { i-t, be a periodic signal with fundamental period T = 2 and Fourier coefficients ak. (a) Determine the value of ao. (b) Determine the Fourier series representation of dx(t)ldt. (c) Use the result of part (b) and the differentiation property of the continuous-tim{ Fourier series to help determine the Fourier series coefficients of x(t). Chap. 3 Problems 257 3.25. Consider the following three continuous-time signals with a fundamental period of T = 112: x(t) = cos( 41Tt), y(t) = sin(47Tt), z(t) = x(t)y(t). (a) Determine the Fourier series coefficients of x(t). (b) Determine the Fourier series coefficients of y(t). (c) Use the results of parts (a) and (b), along with the multiplication property of the continuous-time Fourier series, to determine the Fourier series coefficients of z(t) = x(t)y(t). (d) Determine the Fourier series coefficients of z(t) through direct expansion of z(t) in trigonometric form, and compare your result with that of part (c). 3.26. Let x(t) be a periodic signal whose Fourier series coefficients are k = 0 otherwise· Use Fourier series properties to answer the following questions: (a) Is x(t) real? (b) Is x(t) even? (c) Is dx(t)ldt even? 3.27. A discrete-time periodic signal x[n] is real valued and has a fundamental period N = 5. The nonzero Fourier series coefficients for x[n] are Express x[n] in the form x[n] = Ao + L Ak sin(wkn + cflk). k=l 3.28. Determine the Fourier series coefficients for each of the following discrete-time periodic signals. Plot the magnitude and phase of each set of coefficients ak· (a) Each x[n] depicted in Figure P3.28(a)-(c) (b) x[n] = sin(27Tn/3)cos(7Tn/2) (c) x[n] periodic with period 4 and x[n] = 1 - sin :n for 0 :5 n :5 3 (d) x[n] periodic with period 12 and x [ n ] = 1 - sm· 41r n f or 0 :5 n :5 11 258 Fourier Series Representation of Periodic Signals Chap.3 x[n] 111 .. 11111 .. 11111.~lllll .. lllll .. lllll .. ll -14 -7 0 7 14 21 n (a) x[n] .llll .. llll .. llll.~llll .. 1111 .. 1111 .. 1111. -18 -12 -6 0 6 12 18 n (b) x[n] 2 n (c) Figure P3.28 3.29. In each of the following, we specify the Fourier series coefficients of a signal that is periodic with period 8. Determine the signal x[n] in each case. (a) ak = cos(k;) + sin(3!7T) (b) ak = { ~~n(k:;7), ~:; ~ 6 (c) ak as in Figure P3.29(a) (d) ak as in Figure P3.29(b) 1 . 111 . 111 . 111 . 111 . 111 . 111 .111 . 11 -8 0 8 16 k (a) ak 2 I 1,il 1I11 ... 11li1:.i11111 ... 11 I -8 0 8 16 k (b) Figure P3.29 3.30. Consider the following three discrete-time signals with a fundamental period of 6: x[n] = 1 +cos (627T n ), y[n] = s.m (267T n + 4'TT ) , z[n] = x[n]y[n]. Chap.3 Problems 259 (a) Determine the Fourier series coefficients of x[n]. (b) Determine the Fourier series coefficients of y[n]. (c) Use the results of parts (a) and (b), along with the multiplication property of the discrete-time Fourier series, to determine the Fourier series coefficients of z[n] = x[n]y[n]. (d) Determine the Fourier series coefficients of z[n] through direct evaluation, and compare your result with that of part (c). 3.31. Let x[n] = { ~: be a periodic signal with fundamental period N = 10 and Fourier series coefficients ak. Also, let g[n] = x[n] - x[n - 1]. (a) Show that g[n] has a fundamental period of 10. (b) Determine the Fourier series coefficients of g[n]. (c) Using the Fourier series coefficients of g[n] and the First-Difference property in Table 3.2, determine ak for k -:/= 0. 3.32. Consider the signal x[ n] depicted in Figure P3.32. This signal is periodic with period N = 4. The signal can be expressed in terms of a discrete-time Fourier series as 3 x[n] = L akeik(27T/4)n. (P3.32-1) k=O x[n] f12 1-8 1-4 _Jo 14 18 112 116 n Figure P3.32 As mentioned in the text, one way to determine the Fourier series coefficients is to treat eq. (P3.32-1) as a set of four linear equations (for n = 0, 1, 2, 3) in four unknowns (ao, a1, a2, and a3). (a) Write out these four equations explicitly, and solve them directly using any stan- dard technique for solving four equations in four unknowns. (Be sure first to reduce the foregoing complex exponentials to the simplest form.) (b) Check your answer by calculating the ak directly, using the discrete-time Fourier series analysis equation 3 ak = ~ L x[n]e- jk(21T/4)n. n=O 260 Fourier Series Representation of Periodic Signals Chap.3 3.33. Consider a causal continuous-time LTI system whose input x(t) and output y(t) are related by the following differential equation: d dty(t) + 4y(t) = x(t). Find the Fourier series representation of the output y(t) for each of the following inputs: (a) x(t) = cos 27rt (b) x(t) = sin 47rt + cos( 67rt + 7r/4) 3.34. Consider a continuous-time LTI system with impulse response h(t) = e-4lrl. Find the Fourier series representation of the output y(t) for each of the following inputs: (a) x(t) = L::_xo(t - n) (b) x(t) = L:: _x(- l)no(t - n) (c) x(t) is the periodic wave depicted in Figure P3.34. x(t) 1 H ··l n n n ~ n n n n -3 -2 -1 0 2 3 4 Figure P3.34 3.35. Consider a continuous-time LTI system S whose frequency response is H(jw) = { I, lwl 2: 250 0, otherwise · When the input to this system is a signal x(t) with fundamental period T = 7r/7 and Fourier series coefficients ah it is found that the output y(t) is identical to x(t). For what values of k is it guaranteed that ak = O? 3.36. Consider a causal discrete-time LTI system whose input x[n] and output y[n] are related by the following difference equation: I y[n] - 4y[n - I] = x[nl Find the Fourier series representation of the output y[n] for each of the following inputs: (a) x[n] = sinc3; n) (b) x[n] = cos(*n) + 2cos(¥n) 3.37. Consider a discrete-time LTI system with impulse response l ~nl h[n] = (2) Chap.3 Problems 261 Find the Fourier series representation of the output y[n] for each of the following inputs: (a) x[n] = L~= -xD[n - 4k] (b) x[ n] is periodic with period 6 and b: n = 0, :tl x[n] = { n = :±::2, :±::3 3.38. Consider a discrete-time LTI system with impulse response 1, 0:Sn:52 h[n] = { -1, -2:5n:5-l. 0, otherwise Given that the input to this system is +oc x[n] = 2= o[n - 4k], k=-X determine the Fourier series coefficients of the output y[n]. 3.39. Consider a discrete-time LTI system S whose frequency response is H(elw. ) = [ I ' lwl :5 ~ 0, ¥ < lwl <'1T'' Show that if the input x[n] to this system has a period N = 3, the output y[n] has only one nonzero Fourier series coefficient per period. ADVANCED PROBLEMS 3.40. Let x(t) be a periodic signal with fundamental period T and Fourier series coeffi- cients ak. Derive the Fourier series coefficients of each of the following signals in terms of ak: (a) x(t - to) + x(t + to) (b) &v{x(t)} (c) CRc{x(t)} (d) d 2 x(t) dt2 (e) x(3t - 1) [for this part, first determine the period of x(3t - I)] 3.41. Suppose we are given the following information about a continuous-time periodic signal with period 3 and Fourier coefficients ak: 1. ak = ak+2· 2. ak = a-k· 3. J~ a55 x(t)dt = 1. 4. f x(t)dt = 2. Determine x(t). 262 Fourier Series Representation of Periodic Signals Chap.3 3.42. Let x(t) be a real-valued signal with fundamental period T and Fourier series coef- ficients ak· (a) Show that ak = a*_k and a0 must be real. (b) Show that if x(t) is even, then its Fourier series coefficients must be real and even. (c) Show that if x(t) is odd, then its Fourier series coefficients are imaginary and odd and ao = 0. (d) Show that the Fourier coefficients of the even part of x(t) are equal to ffi-e{ak}. (e) Show that the Fourier coefficients of the odd part of x(t) are equal to jdm{ak}· 3.43. (a) A continuous-time periodic signal x(t) with period Tis said to be odd hannonic if, in its Fourier series representation +x x(t) = L akejk(27TIT)t, (P3.43-1) k= -CG ak = 0 for every non -zero even integer k. (i) Show that if x(t) is odd harmonic, then x(t) = -x~ + ~). (P3.43-2) (ii) Show that if x(t) satisfies eq. (P3.43-2), then it is odd harmonic. (b) Suppose that x(t) is an odd-harmonic periodic signal with period 2 such that x(t) = t for 0 < t < 1. Sketch x(t) and find its Fourier series coefficients. (c) Analogously, to an odd-harmonic signal, we could define an even-harmonic signal as a signal for which ak = 0 fork odd in the representation in eq. (P3.43- 1). Could T be the fundamental period for such a signal? Explain your answer. (d) More generally, show that Tis the fundamental period of x(t) in eq. (P3.43-1) if one of two things happens: (1) Either a 1 or a-1 is nonzero; or (2) There are two integers k and l that have no common factors and are such that both ak and a, are nonzero. 3.44. Suppose we are given the following information about a signal x(t): 1. x(t) is a real signal. 2. x(t) is periodic with period T = 6 and has Fourier coefficients ak. 3. ak = 0 for k = 0 and k > 2. 4. x(t) = - x(t - 3). s. H!3 lx<t)l2 dt = 4. 6. a1 is a positive real number. Show that x(t) = A cos(Bt + C), and determine the values of the constants A, B, and C. Chap.3 Problems 263 3.45. Let x(t) be a real periodic signal with Fourier series representation given in the sine-cosine form of eq. (3.32); i.e., x(t) = ao + 2 L[Bk cos kwot - ck sin kwot]. (P3.45-1) k=I (a) Find the exponential Fourier series representation of the even and odd parts of x(t); that is, find the coefficients ak and f3k in terms of the coefficients in eq. (P3.45-1) so that +co Sv{x(t)} = L akeJkwot, k= -00 +co Od{x(t)} = L f3kejkwot. k= -00 (b) What is the relationship between a k and a-kin part (a)? What is the relationship between f3 k and f3- k? (c) Suppose that the signals x(t) and z(t) shown in Figure P3.45 have the sine-cosine series representations x(t) = a0 + 2 t~i[ Bk cos (2- 1T-kt) - Ck sm. (-21Tkt)~ 3 3- r z(t) = do+ 2 t~i[ Ek cos (2- 1T-kt) - Fk sm. (-21Tk-t)~ 3 3 'J' x(t) \ t -5 -3 -2 0 1 3 4 6 7 9 z(t) 2 -6 -4 -3 -1 2 34 5 67 8 9 -2 Figure P3.45 264 Fourier Series Representation of Periodic Signals Chap. 3 Sketch the signal y(t) = 4(a0 + d0 ) + 2 t~:Jr rB k+ )l. Ek 1c os (2-7T-kt) + F, sm. (2-7Tk-t)11j° 3 3 3.46 In this problem, we derive two important properties of the continuous-time Fourier series: the multiplication property and Parseval's relation. Let x(t) and y(t) both be continuous-time periodic signals having period T0 and with Fourier series represen- tations given by +x +x x(r) = 2= akejkw""', y(t) = 2= bkejkw""'. (P3.46-l) k=-x k= -x cos 20nt {a) X2{t) z(t) cos 20nt, / where z(t) is as in Figure P3.22(f) (b) -3 -1 (c) Figure P3.46 Chap.3 Problems 265 (a) Show that the Fourier series coefficients of the signal +oc z(t) = x(t)y(t) = L ckejkwot k= -00 are given by the discrete convolution +oo ck = 2:: anbk-n· n= -oo (b) Use the result of part (a) to compute the Fourier series coefficients of the signals x1 (t), x2(t), and x3(t) depicted in Figure P3.46. (c) Suppose that y(t) in eq. (P3.46-1) equals x*(t). Express the bk in the equation in terms of ak. and use the result of part (a) to prove Parseval's relation for periodic signals-that is, 3.47 Consider the signal x(t) = cos 27T't. Since x(t) is periodic with a fundamental period of 1, it is also periodic with a period of N, where N is any positive integer. What are the Fourier series coefficients of x(t) if we regard it as a periodic signal with period 3? 3.48. Let x[n] be a periodic sequence with period N and Fourier series representation x[n] = L akejk(27r!N)n_ (P3.48-1) k=<N> The Fourier series coefficients for each of the following signals can be expressed in terms of akin eq. (P3.48-l). Derive the expressions. (a) x[n - no] (b) x[n] - x[n - 1] (c) x[n] - x[n - ~] (assume that N is even) (d) x[n] + x[n + ~] (assume that N is even; note that this signal is periodic with period N/2) (e) x*[-n] (f) (- l)n x[n] (assume that N is even) (g) (- l)n x[n] (assume that N is odd; note that this signal is periodic with period 2N) (h) y[n] = { x[n], n even 0, n odd 3.49. Let x[n] be a periodic sequence with period N and Fourier series representation x[n] = L akejk(27TIN)n. (P3.49-1) k=<N> 266 Fourier Series Representation of Periodic Signals Chap. 3 (a) Suppose that N is even and that x[n] in eq. (P3.49-1) satisfies x[n] = - x [ n + ~] for all n. Show that ak = 0 for all even integers k. (b) Suppose that N is divisible by 4. Show that if x[n] = -x[n +~]for all n, then ak = 0 for every value of k that is a multiple of 4. (c) More generally, suppose that N is divisible by an integer M. Show that if (N/M)-1 [ N] ~ x n + r M = 0 for all n, then ak = 0 for every value of k that is a multiple of M. 3.50. Suppose we are given the following information about a periodic signal x[n] with period 8 and Fourier coefficients ak: 1. ak = -ak-4· 2. x[2n + 1] = (-l)ll. Sketch one period of x[n]. 3.51. Let x[n] be a periodic signal with period N = 8 and Fourier series coefficients ak = -ak-4· A signal y[n] = ( 1+(-l)ll) 2 x[n - 1] with period N = 8 is generated. Denoting the Fourier series coefficients of y[n] by bk. find a function J[k] such that 3.52. x[n] is a real periodic signal with period N and complex Fourier series coefficients ak. Let the Cartesian form for ak be denoted by where bk and ck are both real. (a) Show that a-k = a'k. What is the relation between bk and b-k? What is the relation between ck and c - k? (b) Suppose that N is even. Show that aN12 is real. Chap.3 Problems 267 (c) Show that x[n] can also be expressed as a trigonometric Fourier series of the form + (N~-l) / 2 (2 k ) (2 k ) x[n] = ao 2 bk cos : n - ck sin : n if N is odd or as (N-2l12 + + 2 ~ (2 k ) (2 k ) x[n] = (ao aN12(-lt) bk cos : n - ck sin : n if N is even. (d) Show that ifthe polar form of ak is Akejek, then the Fourier series representation for x[n] can also be written as (N-1)/2 ( 2 k ) x[n] = ao + 2 L Akcos _!!._!!_+Ok k=l N if N is odd or as if N is even. (e) Suppose that x[n] and z[n], as depicted in Figure P3.52, have the sine-cosine series representations x[n] 3 2 1 l -7 l 1 14 l n z[n] 3 2 n Figure P3.52 268 Fourier Series Representation of Periodic Signals Chap.3 x[n] = ao + 2 t~i1 bk cos (2- 77T-kn) - ck sm. (2-77T-kn)j , z[n] =do+ 2 t~i1 dkcos (2-77T-kn) - fksm. (2-77T-kn)j . Sketch the signal 3 y[n] = a0 - do+ 2 t;I d kcos (2- 77T-kn) +Uk - ck)sm. (2-7T7 -kn)j . 3.53. Let x[n] be a real periodic signal with period N and Fourier coefficients ak. (a) Show that if N is even, at least two of the Fourier coefficients within one period of ak are real. (b) Show that if N is odd, at least one of the Fourier coefficients within one period of a k is real. 3.54. Consider the function N-1 a[k] = L ej(21TIN)kn. n=O (a) Show that a[k] = N for k = 0, ±N, ±2N, ±3N, .... (b) Show that a[k] = 0 whenever k is not an integer multiple of N. (Hint: Use the finite sum formula.) (c) Repeat parts (a) and (b) if a[k] = L ej(21TIN)kn. n=<N> 3.55. Let x[n] be a periodic signal with fundamental period N and Fourier series coeffi- cients ak. In this problem, we derive the time-scaling property [ n ] -_ { x[ .!!. ], n = 0, ±m, ±2m, · · · X(m) m 0, elsewhere listed in Table 3.2. (a) Show that X(m)[n] has period of mN. (b) Show that if x[n] = v[n] + w[n], then Chap.3 Problems 269 (c) Assuming that x[n] = ej27rkon!N for some integer k0, verify that l m-1 X(m)[n] = - L ej27r(ko+lN)nl(mN)_ m l=O That is, one complex exponential in x[n] becomes a linear combination of m complex exponentials in X(m)[n]. (d) Using the results of parts (a), (b), and (c), show that if x[n] has the Fourier coefficients ak> then X(m)[n] must have the Fourier coefficients ~ak. 3.56. Let x[n] be a periodic signal with period N and Fourier coefficients ak. (a) Express the Fourier coefficients bk of Jx[nJl2 in terms of ak. (b) If the coefficients ak are real, is it guaranteed that the coefficients bk are also real? 3.57. (a) Let N-1 x[n] = L akejk(27r!N)n (P3.57-1) k=O and NL-1 y[n] = bkejk(27r/N)n k=O be periodic signals. Show that N-1 x[n]y[n] = L ckejk(2TrlN)n, k=O where N-1 N-1 ck = L azbk-l = L ak-1b1. l=O l=O (b) Generalize the result of part (a) by showing that ck = L azbk-l = L ak-1b1. l=<N> l=<N> (c ) Use the result of part (b) to find the Fourier series representation of the following signals, where x[n] is given by eq. (P3.57-l). (i) x[n]cos(6~n) (ii) x[n]L:::,:'_ 00 8[n - rN] (iii) x[n] (L:::,:°_ 008 [n - '~])(assume thatNis divisible by 3) (d) Find the Fourier series representation for the signal x[n]y[n], where x[n] = cos(rrn/3) 270 Fourier Series Representation of Periodic Signals Chap.3 and { o1: lnl :5 3 y[n] = 4 :5 lnl :5 6 is periodic with period 12. (e ) Use the result of part (b) to show that L x[n]y[n] = N L a1b-1, n=<N> l=<N> and from this expression, derive Parseval's relation for discrete-time periodic signals. 3.58. Let x[n] and y[n] be periodic signals with common period N, and let z[n] = L x[r]y[n - r] r=<N> be their periodic convolution. (a) Show that z[n] is also periodic with period N. (b) Verify that if ah bh and ck are the Fourier coefficients of x[n], y[n], and z[n], respectively, then (c) Let x[n] = sin (3~n) and [n] = { 1, 0 :5 n :5 3 y 0, 4 :5 n :5 7 be two signals that are periodic with period 8. Find the Fourier series represen- tation for the periodic convolution of these signals. (d) Repeat part (c) for the following two periodic signals that also have period 8: x[n] [ s.m (34'l Tn) = ' 0 <- n <- 3 ' 0, 4 :5 n :5 7 y[n] = Gro :5 n :5 7. 3.59. (a) Suppose x[n] is a periodic signal with period N. Show that the Fourier series coefficients of the periodic signal 00 g(t) = L x[k] 8(t - kT) k= -00 are periodic with period N. Chap.3 Problems 271 (b) Suppose that x(t) is a periodic signal with period T and Fourier series coeffi- cients ak with period N. Show that there must exist a periodic sequence g[n] such that 00 x(t) = L g[k] 5(t - kTIN). k= -00 (c) Can a continuous periodic signal have periodic Fourier coefficients? 3.60. Consider the following pairs of signals x[n] and y[n]. For each pair, determine whether there is a discrete-time LTI system for which y[n] is the output when the corresponding x[n] is the input. If such a system exists, determine whether the sys- tem is unique (i.e., whether there is more than one LTI system with the given input- output pair). Also, determine the frequency response of an LTI system with the desired behavior. If no such LTI system exists for a given x[n], y[n] pair, explain why. In In (a) x[n] = (2 ), y[n] = (4 ) (b) x[n] = <! n)u[n], y[n] = (~)nu[n] (c) x[n] = <! n)u[n], y[n] = 4nu[-n] (d) x[n] = ein!S, y[n] = 2ein!S (e) x[n] = ein18 u[n], y[n] = 2ein18 u[n] (t) x[n] = r. y[n] = 2jll(l - j) (g) x[n] = cos(1Tn/3),y[n] = cos(7Tn/3) + J3 sin(7Tn/3) (h) x[n] and YI [n] as in Figure P3.60 (i) x[n] and y2 [n] as in Figure P3.60 x[n] ••• 111 •••••••• ~llt ••••••••• tlt ••••••••• IJJ -12 0 12 24 n 111 ••• 111 ••• 111 ••• TTT ••• llt ••• TTT ••• Tll ••• -15 -9 -3 0 3 9 15 21 n Y2[n) •••••• Tlt •••••• JJT •••••• TJJ •••••• 111 •••••• -9 0 9 18 n Figure P3.60 3.61. As we have seen, the techniques of Fourier analysis are of value in examining continuous-time LTI systems because periodic complex exponentials are eigenfunc- tions for LTI systems. In this problem, we wish to substantiate the following state- ment: Although some LTI systems may have additional eigenfunctions, the complex exponentials are the only signals that are eigenfunctions of every LTI system. 272 Fourier Series Representation of Periodic Signals Chap.3 (a) What are the eigenfunctions of the LTI system with unit impulse response h(t) = B(t)? What are the associated eigenvalues? (b) Consider the LTI system with unit impulse response h(t) = B(t - T). Find a signal that is not of the form est, but that is an eigenfunction of the system with eigenvalue 1. Similarly, find the eigenfunctions with eigenvalues 1/2 and 2 that are not complex exponentials. (Hint: You can find impulse trains that meet these requirements.) (c) Consider a stable LTI system with impulse response h(t) that is real and even. Show that cos w t and sin w t are eigenfunctions of this system. (d) ConsidertheLTI system with impulse response h(t) = u(t). Suppose that cf>(t) is an eigenfunction of this system with eigenvalue A. Find the differential equation that cf>(t) must satisfy, and solve the equation. This result, together with those of parts (a) through (c), should prove the validity of the statement made at the beginning of the problem. 3.62. One technique for building a de power supply is to take an ac signal and full-wave rectify it. That is, we put the ac signal x(t) through a system that produces y(t) = lx(t)l as its output. (a) Sketch the input and output waveforms if x(t) = cost. What are the fundamen- tal periods of the input and output? (b) If x(t) = cost, determine the coefficients of the Fourier series for the output y(t). (c) What is the amplitude of the de component of the input signal? What is the amplitude of the de component of the output signal? 3.63. Suppose that a continuous-time periodic signal is the input to an LTI system. The signal has a Fourier series representation 00 x(t) = L alklejk(1TI4)r, k= -00 where a is a real number between 0 and 1, and the frequency response of the system is lwl $ W H(jw) = { 1' 0, lwi>W. How large must W be in order for the output of the system to have at least 90% of the average energy per period of x(t)? 3.64. As we have seen in this chapter, the concept of an eigenfunction is an extremely important tool in the study ofLTI systems. The same can be said for linear, but time- varying, systems. Specifically, consider such a system with input x(t) and output i y(t). We say that a signal t/J(t) is an eigenfunction of the system if l j cf>(t) ---+ Acf>(t). 'j 1 That is, if x(t) = cf>(t), then y(t) = At/J(t), where the complex constant A is called J the eigenvalue associated with t/J(.t). l Chap.3 Problems 273 (a) Suppose that we can represent the input x(t) to our system as a linear combina- tion of eigenfunctions cfJk(t), each of which has a corresponding eigenvalue Ak; that is, x(t) = L ckcfJk(t). k= -00 Express the output y(t) of the system in terms of {ck}, {c/Jk(t)}, and {Ak}. (b) Consider the system characterized by the differential equation - 2 d 2 () x(t) dx(t) y t - t ----;_jfl + t--;[(. Is this system linear? Is it time invariant? (c) Show that the functions are eigenfunctions of the system in part (b). For each cfJk(t), determine the cor- responding eigenvalue Ak. (d) Determine the output of the system if 1 x(t) = lOt- 10 + 3t + 2t4 + 7T. EXTENSION PROBLEMS 3.65. Two functions u(t) and v(t) are sraid to be orthogonal over the interval (a,b) if u(t)v*(t) dt = 0. (P3.65-1) If, in addition, rlu (t)l2 dt = 1 = rlv (t)l2 dt, the functions are said to be normalized and hence are called orthonormal. A set of functions {cfJk(t)} is called an orthogonal (orthonormal) set if each pair of functions in the set is orthogonal (orthonormal). (a) Consider the pairs of signals u(t) and v(t) depicted in Figure P3.65. Determine whether each pair is orthogonal over the interval (0, 4). (b) Are the functions sin mwot and sin nwot orthogonal over the interval (0, T), where T = 27Tiwo? Are they alsd orthonormal? (c) Repeat part (b) for the functions c/Jm(t) and c/Jn(t), where cfJk(t) = Jr [cos kwot + sin kw0t]. 274 Fourier Series Representation of Periodic Signals Chap. 3 u(t) v(t) 2 3 4 1 2 3 41 -1 -1 (a) Exponentials with Exponentials with time constant = 1 time constant = 1 l j 4 -3 (b) u(t) v(t) 4 -1 (c) u(t) v(t) 7T 1-----, 7T - - 2 3 4 1 2 3 4 (d) Figure P3.65 (d) Show that the functions cf>t(t) = eikwot are orthogonal over any interval of length T = 27rlwo. Are they orthonormal? (e) Let x(t) be an arbitrary signal, and let X0 (t) and Xe(t) be, respectively, the odd and even parts of x(t). Show that X0 (t) and Xe(t) are orthogonal over the interval ( -T, T) for any T. Chap. 3 Problems 275 (f) Show that if {<f>k(t)} is a set of orthogonal signals over the interval (a, b), then the set {(1/ jA~)<f>k(t)}, where Ak = f h l<f>k(t)l2 dt, a is orthonormal. (g) Let {</>;(t)} be a set of orthonormal signals on the interval (a, b), and consider a signal of the form x(t) = ~ a;<f>;(t), where the a; are complex constants. Show that rl x(t)l2 dt = ~ la;l2. l (h) Suppose that <f> 1( t), ... , <f>N(t) are nonzero only in the time interval 0 ::::: t ::::: T and that they are orthonormal over this time interval. Let L; denote the LTI system with impulse response hJt) = </>;(T - t). (P3.65-2) Show that if <f>J(t) is applied to this system, then the output at time T is 1 if i = j and 0 if i =P j. The system with impulse response given by eq. (P3.65-2) was referred to in Problems 2.66 and 2.67 as the matched filter for the signal </>;(t). 3.66. The purpose of this problem is to show that the representation of an arbitrary pe- riodic signal by a Fourier series or, more generally, as a linear combination of any set of orthogonal functions is computationally efficient and in fact very useful for obtaining good approximations of signals. 12 Specifically, let {<f>Jt)}, i = 0, ± 1, ±2, ... be a set of orthonormal functions on the interval a ::::: t ::::: b, and let x(t) be a given signal. Consider the follow- ing approximation of x(t) over the interval a ::::: t ::::: b: +N Xn(t) = ~ a;</>;(t). (P3.66-1) i~-N Here, the a; are (in general, complex) constants. To measure the deviation between x(t) and the series approximation xN(t), we consider the error eN(t) defined as (P3.66-2) A reasonable and widely used criterion for measuring the quality of the approxima- tion is the energy in the error signal over the interval of interest-that is, the integral 12See Problem 3.65 for the definitions of orthogonal and orthonormal functions. 276 Fourier Series Representation of Periodic Signals Chap. 3 of the square of the magnitude of the error over the interval a ::5 t ::5 b: (P3.66-3) (a) Show that E is minimized by choosing a; = rx (t)c/J;(t)dt. (P3.66-4) [Hint: Use eqs. (P3.66-1)-(P3.66-3) to express E in terms of a;, cl>i(t), and x(t). Then express a; in rectangular coordinates as a; = b; + jc;, and show that the equations aE aE . -ab = 0 and -ac; = 0, z = 0, :±: 1, :±:2, ... , N i are satisfied by the a; as given by eq. (P3.66-4).] (b) How does the result of part (a) change if and the {cP;(t)} are orthogonal but not orthonormal? (c) Let cPn(t) = ejnwot, and choose any interval of length To = 2nlw0 . Show that the a; that minimize E are as given in eq. (3.50). (d) The set of Walsh functions is an often-used set of orthonormal functions. (See Problem 2.66.) The set of five Walsh functions, cPo(t), cP1 (t), ... , cP4(t), is illus- trated in Figure P3.66, where we have scaled time so that the cP;(t) are nonzero and orthonormal over the interval 0 ::::; t ::5 1. Let x(t) = sin 7Tt. Find the ap- proximation of x(t) of the form 4 x(t) = ,L a;cP;(t) i=O such that is minimized. (e) Show that XN(t) in eq. (P3.66-1) and eN(t) in eq. (P3.66-2) are orthogonal if the ai are chosen as in eq. (P3.66-4). The results of parts (a) and (b) are extremely important in that they show that each coefficient ai is independent of all the other aj's, i :;i: j. Thtis, if we add more terms to the approximation [e.g., if we compute the approxi- mation .XN+t(t)], the coefficients of cPi(t), i = 1, .. . ,N, that were previously deterinined will not change. In contrast to ·this, consider another type of se- Chap. 3 Problems 277 <Po(t) (a) 1 2 -1 (b) 1 1 4 2 -1 (c) -1 (d) -1 (e) Figure P3.66 ries expansion, the polynomial Taylor series. The infinite Taylor series for et is et = 1 + t + t2/2! + ... , but as we shall show, when we consider a finite polynomial series and the error criterion of eq. (P3.66-3), we get a very different result. Specifically, let cf>o(t) = 1, </11( t) = t, </12(t) = t2, and so on. (f) Are the </Ji(t) orthogonal over the interval 0 s t s 1? 278 Fourier Series Representation of Periodic Signals Chap.3 (g) Consider an approximation of x(t) = e1 over the interval 0 s t s 1 of the form io(t) = aocf>o(t). Find the value of a0 that minimizes the energy in the error signal over the in- terval. (h) We now wish to approximate e1 by a Taylor series using two terms-i.e., i 1 (t) = a0 + a1t . Find the optimum values for ao and a1. [Hint: Compute E in terms of a0 and a 1, and then solve the simultaneous equations aE = 0 and aao Note that your answer for a0 has changed from its value in part (g), where there was only one term in the series. Further, as you increase the number of terms in the series, that coefficient and all others will continue to change. We can thus see the advantage to be gained in expanding a function using orthogonal terms.] 3.67 As we discussed in the text, the origins of Fourier analysis can be found in problems of mathematical physics. In particular, the work of Fourier was motivated by his investigation of heat diffusion. In this problem, we illustrate how the Fourier series enter into the investigation. 13 Consider the problem of determining the temperature at a given depth beneath the surface of the earth as a function of time, where we assume that the temperature at the surface is a given function of time T(t) that is periodic with period I. (The unit of time is one year.) Let T(x, t) denote the temperature at a depth x below the surface at time t. This function obeys the heat diffusion equation aT(x, t) ~k2 a2 T(x, t) at (P3.67-l) 2 ax2 with auxiliary condition T(O, t) = T(t). (P3.67-2) Here, k is the heat diffusion constant for the earth (k > 0). Suppose that we expand T(t) in a Fourier series: +x T(t) = L anejnZm. (P3.67-3) n= -:o Similarly, let us expand T(x, t) at any given depth x in a Fourier series in t. We obtain +oo T(x, t) = L bn(x)ejnZm, (P3.67- 4) n= -oc where the Fourier coefficients bn(x} depend upon the depth x. 13The problem has been adapted from A. Sommerfeld, Partial Differential Equations in Physics (Nt York: Academic Press, 1949), pp 68-71. Chap. 3 Problems 279 (a) Use eqs. (P3.67-1)-(P3.67-4) to show that h11 (x) satisfies the differential equa- tion 47T""jn ---yzr-b""(x) (P3.67-5a) with auxiliary condition (P3.67-5b) Since eq. (P3.67-5a) is a second-order equation, we need a second auxiliary condition. We argue on physical grounds that, far below the earth's surface, the variations in temperature due to surface fluctuations should disappear. That is, lim T(x, t) = a constant. (P3.67-5c) x~x (b) Show that the solution of eqs. (P3.67-5) is bn(x) = [ an exp[- J27Tinl(l + j)xl k], n 2:: 0 an exp[- J27Tinl(l - j)xlk], n s 0 (c) Thus, the temperature oscillations at depth x are damped and phase-shifted ver- sions of the temperature oscillations at the surface. To see this more clearly, let T(t) = ao + a, sin 2m (so that a0 represents the mean yearly temperature). Sketch T(t) and T(x, t) over a one-year period for a0 = 2, and a 1 = 1. Note that at this depth not only are the temperature os- cillations significantly damped, but the phase shift is such that it is warmest in winter and coldest in summer. This is exactly the reason why vegetable cellars are constructed! 3.68. Consider the closed contour shown in Figure P3.68. As illustrated, we can view this curve as being traced out by the tip of a rotating vector of varying length. Let r(O) denote the length ofthe vector as a function of the angle (J. Then r( (J) is periodic in (J with period 27T and thus has a Fourier series representation. Let {ad denote the Fourier coefficients of r(O). (a) Consider now the projection x(0 ) of the vector r(0 ) onto the x-axis, as indicated in the figure. Determine the Fourier coefficients for x(O) in terms of the ak's. (b) Consider the sequence of coefficients bk = akejk7rt4. Sketch the figure in the plane that corresponds to this set of coefficients. 280 Fourier Series Representation of Periodic Signals Chap. 3 -1 -1 Figure P3.68 (c) Repeat part (b) with (d) Sketch figures in the plane such that r(O) is not constant, but does have each of the following properties: (i) r(O) is even. (ii) The fundamental period of r(O) is 7T. (iii) The fundamental period of r(O) is 7T/2. 3.69. In this problem, we consider the discrete-time counterpart of the concepts introduced in Problems 3.65 and 3.66. In analogy with the continuous-time case, two discrete- time signals <Pdn] and <Pm[n] are said to be orthogonal over the interval (N1, N 2) if k = m (P3.69-1) k# m · If the value of the constants Ak and Am are both 1, then the signals are said to be orthonormal. (a) Consider the signals ¢k[n] = o[n - k], k = 0, :':: l, :'::2, ... ' ±N. Show that these signals are orthonormal over the interval (-N, N). (b) Show that the signals cPk[n] = eik(21TIN)n, k = 0, 1, ... , N- 1, are orthogonal over ~y interval of length N. (c) Showthatif · M x[n] = ,La;cf>;[n], i=l Chap. 3 Problems 281 where the cf>i[n] are orthogonal over the interval (N1, N 2 ), then N? M ::;2 lx[n]i2 = L lail2 Ai. i= I (d) Let cf>i[n], i = 0, 1, ... , M, be a set of orthogonal functions over the interval (N1, N2), and let x[n] be a given signal. Suppose that we wish to approximate x[n] as a linear combination of the cf>i[n]; that is, M i[n] = L aicf>i[n], i=O where the ai are constant coefficients. Let e[n] = x[n] - i[n], and show that if we wish to minimize N, E = ::;2 le[n]i2, n=N1 then the ai are given by (P3.69-2) [Hint: As in Problem 3.66, express E in terms of ai, cf>i[n], Ai, and x[n], write ai = bi + }ci, and show that the equations aE - = 0 and abi are satisfied by the ai given by eq. (P3.69-2). Note that applying this result when the cf>i[n] are as in part (b) yields eq. (3.95) for ak.] (e) Apply the result of part (d) when the cf>i[n] are as in part (a) to determine the coefficients ai in terms of x[n]. 3.70. (a) In this problem, we consider the definition of the two-dimensional Fourier se- ries for periodic signals with two independent variables. Specifically, consider a signal x(t1, t2) that satisfies the equation This signal is periodic with period T1 in the t 1 direction and with period T2 in the t2 direction. Such a signal has a series representation of the form +oo +oo x(t], t2) = L, L, amnej<I1UJ),t, +1Ut.lztz), n= -oo m= -oo 282 Fourier Series Representation of Periodic Signals Chap.3 where Wt = 2w1Tt, w2 = 2w!T2. Find an expression for amn in terms of x(tt, t2). (b) Determine the Fourier series coefficients amn for the following signals: (i) cos(27ftt + 2t2) (ii) the signal illustrated in Figure P3.70 x(t1 t2) = 1 in shaded areas and · 0 elsewhere [] [··· 3T2/2 r-~-' T2 f---...., D [··· T2/2 f---""'-' -3T,f2 - T,J2 T,/2 3T1/2 --~r----.~~~----1-~~~----~--~-----.-- t1 J D [ Figure P3.70 3.71. Consider the mechanical system shown in Figure P3.71. The differential equation relating velocity v(t) and the input force f(t) is given by Bv(t) + K I v(t) dt = j(t). v(t) ~ f(t) B Figure P3.71 Chap. 3 Problems 283 (a) Assuming that the output is f,(t), the compressive force acting on the spring, write the differential equation relating f,(t) and f(t). Obtain the frequency re- sponse of the system, and argue that it approximates that of a lowpass filter. (b) Assuming that the output is /J(t), the compressive force acting on the dash- pot, write the differential equation relating /J(t) and f(t). Obtain the frequency response of the system, and argue that it approximates that of a highpass filter. 4 THE CONTINUOUS-TIME FOURIER TRANSFORM 4.0 INTRODUCTION In Chapter 3, we developed a representation of periodic signals as linear combinations of complex exponentials. We also saw how this representation can be used in describing the effect of LTI systems on signals. In this and the following chapter, we extend these concepts to apply to signals that are not periodic. As we will see, a rather large class of signals, including all signals with finite energy, can also be represented through a linear combination of complex exponentials. Whereas for periodic signals the complex exponential building blocks are harmonically related, for aperiodic signals they are infinitesimally close in frequency, and the represen- tation in terms of a linear combination takes the form of an integral rather than a sum. The resulting spectrum of coefficients in this representation is called the Fourier transform, and the synthesis integral itself, which uses these coefficients to represent the signal as a linear combination of complex exponentials, is called the inverse Fourier transform. The development of this representation for aperiodic signals in continuous time is one of Fourier's most important contributions, and our development of the Fourier trans- form follows very closely the approach he used in his original work. In particular, Fourier reasoned that an aperiodic signal can be viewed as a periodic signal with an infinite pe- riod. More precisely, in the Fourier series representation of a periodic signal, as the period increases the fundamentfll frequency decreases and the harmonically related components become closer in frequency. As the period becomes infinite, the frequency components form a continuum and the Fourier series sum becomes an integral. In the next section we develop the Fourier series representation for continuous-time periodic signals, and in the sections that follow we build on this foundation as we explore many of the important 284 Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 285 properties of the continuous-time Fourier transform that form the foundation of frequency- domain methods for continuous-time signals and systems. In Chapter 5, we parallel this development for discrete-time signals. 4. 1 REPRESENTATION OF APERIODIC SIGNALS: THE CONTINUOUS-TIME FOURIER TRANSFORM 4. 1 . 1 Development of the Fourier Transform Representation of an Aperiodic Signal To gain some insight into the nature of the Fourier transform representation, we begin by revisiting the Fourier series representation for the continuous-time periodic square wave examined in Example 3.5. Specifically, over one period, 6: JtJ < T1 x(t) = { T1 < JtJ < T/2 and periodically repeats with period T, as shown in Figure 4.1. As determined in Example 3.5, the Fourier series coefficients ak for this square wave are 2 sin(kwoTI) [eq. (3.44)] (4.1) kwoT where w 0 = 211""/T. In Figure 3.7, bar graphs ofthese coefficients were shown for a fixed value of T 1 and several different values ofT. An alternative way of interpreting eq. (4.1) is as samples of an envelope function, specifically, T ak -_ 2sinwT11 . (4.2) W w=kw0 That is, with w thought of as a continuous variable, the function (2 sin wT1 )lw represents the envelope of Tab and the coefficients ak are simply equally spaced samples of this envelope. Also, for fixed T 1, the envelope of Tak is independent ofT. In Figure 4.2, we again show the Fourier series coefficients for the periodic square wave, but this time as samples of the envelope of Tab as specified in eq. (4.2). From the figure, we see that as x(t) ... J 1.1 1.1 ~ 1.1 1.1 [ ... I I -2T -T _.I -T1 T1 T T 2T 2 2 Figure 4. 1 A continuous-time periodic square wave. 286 The Continuous-Time Fourier Transform Chap.4 (a) (b) Figure 4.2 The Fourier series co- efficients and their envelope for the periodic square wave in Figure 4.1 for several values of T (with T; fixed): (a) T = 47;; (b) T = 8T1: (c) T = 16T1. T increases, or equivalently, as the fundamental frequency w 0 = 27r/T decreases, the envelope is sampled with a closer and closer spacing. As T becomes arbitrarily large, the original periodic square wave approaches a rectangular pulse (i.e., all that remains in the time domain is an aperiodic signal corresponding to one period of the square wave). Also, the Fourier series coefficients, multiplied by T, become more and more closely spaced samples of the envelope, so that in some sense (which we will specify shortly) the set of Fourier series coefficients approaches the envelope function as T --7 oo. This example illustrates the basic idea behind Fourier's development of a represen- tation for aperiodic signals. Specifically, we think of an aperiodic signal as the limit of a periodic signal as the period becomes arbitrarily large, and we examine the limiting be- . havior of the Fourier series representation for this signal. In particular, consider a signal x(t) that is of finite duration. That is, for some number Tt. x(t) = 0 if ltl > T1, as illus- trated in Figure 4.3(a). From this aperiodic signal, we can construct a periodic signal i(t) for which x(t) is one period, as indicated in Figure 4.3(b). As we choose the period T to"" be larger, i(t) is identical to x(t) over a longer interval, and as T ---+ oo, i(t) is equal to x(t) for any finite value oft. Let us now examine the effect of this on the Fourier series representation of i(t). Rewriting eqs. (3.38) and (3.39) here for convenience, with the integral in eq. (3.39) Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 287 x(t) 0b -T1 T1 (a) -2T -T -T1 0 T1 T 2T (b) Figure 4.3 (a) Aperiodic signal x(t); (b) periodic signal x(t), constructed to be equal to x(t) over one period. carried out over the interval - T 12 :5 t :5 T 12, we have +oo x<r) = :L akejkwot, (4.3) k= -oc ak = -1 IT/2 . i(t)e-Jkwotdt, (4.4) T -T/2 where w 0 = 27TIT. Since i(t) = x(t) for ltl < T/2, and also, since x(t) = 0 outside this interval, eq. (4.4) can be rewritten as 1 IT /2 . } I+ oo . ak = - x(t)e-Jkwotdt = - x(t)e-Jkwotdt. T -T/2 T -00 Therefore, defining the envelope X(jw) of Tak as X(jw) = L+oooo x(t)e-jwtdt, (4.5) we have, for the coefficients ako ak = ~X(j kwo). (4.6) Combining eqs. (4.6) and (4.3), we can express i(t) in terms of X(jw) as +co 1 . i(t) = L TX(jkwo)elkcoot, k=-co or equivalently, since 27TIT = w0, 1 +co . i(t) = 27T L X(jkw0 )eJkcootw0• (4.7) k=-CO 288 The Continuous-Time Fourier Transform Chap.4 As T ----? oo, i(t) approaches x(t), and consequently, in the limit eq. (4.7) becomes a rep- resentation of x(t). Furthermore, w 0 ----? 0 as T ----? oo, and the right-hand side of eq. (4.7) passes to an integral. This can be seen by considering the graphical interpretation of the equation, illustrated in Figure 4.4. Each term in the summation on the right-hand side is the area of a rectangle of height X(j kw0 )eikwot and width w0 . (Here, tis regarded as fixed.) As w0 ----? 0, the summation converges to the integral of X(jw )eiwt. Therefore, using the fact that i(t) ----? x(t) as T ----7 oo, we see that eqs. (4.7) and (4.5) respectively become 1 f+x . x(t) = 27T -oo X(jw )e1w1d w (4.8) and + X(jw) = I- oc x x(t)e-Jwtdt. (4.9) X(jw)eiwt (k + 1)w0 Figure 4.4 Graphical interpretation w of eq. (4.7). Equations (4.8) and (4.9) are referred to as the Fourier transform pair, with the func- tion X(jw) referred to as the Fourier Transform or Fourier integral of x(t) and eq. (4.8) as the inverse Fourier transform equation. The synthesis equation (4.8) plays a role for aperiodic signals similar to that of eq. (3.38) for periodic signals, since both represent a signal as a linear combination of complex exponentials. For periodic signals, these com- plex exponentials have amplitudes {ak}, as given by eq. (3.39), and occur at a discrete set of harmonically related frequencies kw0, k = 0, .± 1, ±2, .... For aperiodic signals, the complex exponentials occur at a continuum of frequencies and, according to the synthesis equation (4.8), have ""amplitude"" X(jw )(dw/27T). In analogy with the terminology used for the Fourier series coefficients of a periodic signal, the transform X(jw) of an aperiodic signal x(t) is commonly referred to as the spectrum of x(t), as it provides us with the in- formation needed for describing x(t) as a linear combination (specifically, an integral) of sinusoidal signals at different frequencies. Based on the above development, or equivalently on a comparison of eq. (4.9) and eq. (3.39), we also note that the Fourier coefficients ak of a periodic signal i(t) can be expressed in terms of equally spaced samples of the Fourier transform of one period of i(t). Specifically, suppose that i(t) is a periodic signal with period T and Fourier coefficients Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 289 ak. Let x(t) be a finite-duration signal that is equal to i(t) over exactly one period-say, for s ::s: t ::s: s + T for some value of s-and that is zero otherwise. Then, since eq. (3.39) allows us to compute the Fourier coefficients of i(t) by integrating over any period, we can write Since x(t) is zero outside the range s ::s: t ::s: s + T we can equivalently write Comparing with eq. (4.9) we conclude that ak = ~X(jw)l , (4.10) w=kwo where X(jw) is the Fourier transform of x(t). Equation 4.10 states that the Fourier coef- ficients of i(t) are proportional to samples of the Fourier transform of one period of i(t). This fact, which is often of use in practice, is examined further in Problem 4.37. 4.1.2 Convergence of Fourier Transforms Although the argument we used in deriving the Fourier transform pair assumed that x(t) was of arbitrary but finite duration, eqs. (4.8) and (4.9) remain valid for an extremely broad class of signals of infinite duration. In fact, our derivation of the Fourier transform suggests that a set of conditions like those required for the convergence of Fourier series should also apply here, and indeed, that can be shown to be the case. 1 Specifically, consider X(jw) evaluated according to eq. (4.9), and let x(t) denote the signal obtained by using X(jw) in the right-hand side of eq. (4.8). That is, = -12 I+ oc . x(t) X(jw )eJWt dw. 7T -ac What we would like to know is when eq. (4.8) is valid [i.e., when is x(t) a valid represen- tation of the original signal x(t)?]. If x(t) has finite energy, i.e., if it is square integrable, so that (4.11) then we are guaranteed that X(jw) is finite [i.e., eq. (4.9) converges] and that, with e(t) denoting the error between x(t) and x(t) [i.e., e(t) = x(t) - x(t)], 1F or a mathematically rigorous discussion of the Fourier transform and its properties and applications, seeR. Bracewell, The Fourier Transform and Its Applications, 2nd ed. (New York: McGraw-Hill Book Com- pany, 1986); A. Papoulis, The Fourier Integral and Its Applications (New York: McGraw-Hill Book Company, 1987); E. C. Titchmarsh, Introduction to the Theory ofF ourier Integrals (Oxford: Clarendon Press, 1948); and the book by Dym and McKean referenced in footnote 2 of Chapter 3. · 290 The Continuous-Time Fourier Transform Chap. 4 (4.12) Equations (4.11) and (4.12) are the aperiodic counterparts of eqs. (3.51) and (3.54) for periodic signals. Thus, in a manner similar to that for periodic signals, if x(t) has finite energy, then, although x(t) and its Fourier representation x(t) may differ significantly at individual values oft, there is no energy in their difference. Just as with periodic signals, there is an alternative set of conditions which are suffi- cient to ensure that i(t) is equal to x(t) for any t except at a discontinuity, where it is equal to the average of the values on either side of the discontinuity. These conditions, again referred to as the Dirichlet conditions, require that: 1. x(t) be absolutely integrable; that is, J_+xx lx(t)ldt < rx. (4 .13) 2. x(t) have a finite number of maxima and minima within any finite interval. 3. x(t) have a finite number of discontinuities within any finite interval. Futhermore, each of these discontinuities must be finite. Therefore, absolutely integrable signals that are continuous or that have a finite number of discontinuities have Fourier transforms. Although the two alternative sets of conditions that we have given are sufficient to guarantee that a signal has a Fourier transform, we will see in the next section that peri- odic signals, which are neither absolutely integrable nor square integrable over an infinite interval, can be considered to have Fourier transforms if impulse functions are permitted in the transform. This has the advantage that the Fourier series and Fourier transform can be incorporated into a common framework, which we will find to be very convenient in subsequent chapters. Before examining the point further in Section 4.2, however, let us consider several examples of the Fourier transform. 4. 1 .3 Examples of Continuous-Time Fourier Transforms Example 4.1 Consider the signal x(t) = e-""1 u(t) a> 0. From eq. (4.9), X(jw) = {""' e-ate-i""''dt = ___1_ _ -e-<a+jw)tl""'. Jo a+ JW o That is, X(jw) = - 1-.-, a > 0. a+ JW Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 291 Since this Fourier transform is complex valued, to plot it as a function of w, we express X(jw) in terms of its magnitude and phase: IX(jw)i= 1 , <r.X(jw)=-tan~ 1 (~a)· Ja2 + w2 Each of these components is sketched in Figure 4.5. Note that if a is complex rather than real, then x(t) is absolutely integrable as long as (Jl.e{a} > 0, and in this case the preceding calculation yields the same form for X(jw ). That is, X(jw) = - 1-.-, <Re{a} > 0. a+ JW IX(jw)l 1/a -a a w (a) <J:X(jw) TI/2 w (b) Figure 4.5 Fourier transform of the signal x(t) = e~at u(t), a> 0, consid- ered in Example 4.1. Example 4.2 Let x(t) = e-aftl, a> 0. 292 The Continuous-Time Fourier Transform Chap.4 This signal is sketched in Figure 4.6. The Fourier transform of the signal is 1 1 ---+--- a- jw a+ jw 2a a2 + w2"" In this case X(jw) is real, and it is illustrated in Figure 4.7. x(t) Figure 4.6 Signal x(t) = e-altl of Example 4.2. XGw) 2/a Figure 4.7 Fourier transform of the signal considered in Example 4.2 and depicted in Figure 4.6. Example 4.3 Now let us determine the Fourier transform of the unit impulse x(t) = 8(t). Substituting into eq. (4.9) yields +oo X(jw) = f-oo 8(t)e-i<»tdt = 1. That is, the unit impulse bas a Fourier transform consisting of equal contributions at frequencies. Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 293 Example 4.4 Consider the rectangular pulse signal { o1: ltl < Tt x(t) = ltl > T1 ' (4.16) as shown in Figure 4.8(a). Applying eq. (4 .9), we find that the Fourier transform of this signal is TI . T X( J.W ) -- f e - jwt d t-- 2 -Sil-l W- ,l (4.17) -TI W as sketched in Figure 4.8(b). x(t) I II X(jw) Figure 4.8 (a) The rectangular pulse signal of Example 4.4 and (b) its Fourier transform. As we discussed at the beginning of this section, the signal given by eq. (4.16) can be thought of as the limiting form of a periodic square wave as the period becomes arbitrarily large. Therefore, we might expect that the convergence of the synthesis equation for this signal would behave in a manner similar to that observed in Example 3.5 for the square wave. This is, in fact, the case. Specifically, consider the inverse Fourier transform for the rectangular pulse signal: xA (t ) -_ -1 I+ co 2 sinwT, e Jwtd w. 21T -co W Then, since x(t) is square integrable, 294 The Continuous-Time Fourier Transform Chap.4 +oc 2 J-o o lx(t) - .X(t)l dt = 0. Furthermore, because x(t) satisfies the Dirichlet conditions, x(t) = x(t), except at the points of discontinuity, t = ±T1, where x(t) converges to 112, which is the average of the values of x(t) on both sides of the discontinuity. In addition, the convergence of x(t) to x(t) exhibits the Gibbs phenomenon, much as was illustrated for the periodic square wave in Figure 3.9. Specifically, in analogy with the finite Fourier series approximation, eq. (3.47), consider the following integral over a finite-length interval of frequencies: _I_ Jw 2sinwTt eiwt dw. 271' -w w As W ~ oo, this signal converges to x(t) everywhere, except at the discontinuities. More- over, the signal exhibits ripples near the discontinuities. The peak amplitude of these rip- ples does not decrease as W increases, although the ripples do become compressed toward the discontinuity, and the energy in the ripples converges to zero. Example 4.5 Consider the signal x(t) whose Fourier transform is X(1' w)={l, iwi<W. 0, lwl (4.18) > W This transform is illustrated in Figure 4.9(a). Using the synthesis equation (4.8), we can X(jw) I ,11 -w w w (a) x(t) --rr/W -rr/W (b) Figure 4. 9 Fourier transform pair of Example 4.5: (a) Fourier transform for Example 4.5 and (b) the corresponding time function. Sec. 4.1 Representation of Aperiodic Signals: The Continuous-Time Fourier Transform 295 then determine x(t ) -_ -1 f w e Jwtd w_ -si-n W- t (4.19) 27T -w 7Tt ' which is depicted in Figure 4.9(b). Comparing Figures 4.8 and 4.9 or, equivalently, eqs. (4.16) and (4.17) with eqs. (4 .18) and (4 .19), we see an interesting relationship. In each case, the Fourier transform pair consists of a function of the form (sin a8)/b8 and a rectangular pulse. However, in Example 4.4, it is the signal x(t) that is a pulse, while in Example 4.5, it is the transform X(jw ). The special relationship that is apparent here is a direct consequence of the duality property for Fourier transforms, which we discuss in detail in Section 4.3.6. Functions of the form given in eqs. (4.17) and (4.19) arise frequently in Fourier analysis and in the study of LTI systems and are referred to as sine functions. A commonly used precise form for the sine function is . (O) smc = -s-i-n- -1;;ro8 . (4.20) The sine function is plotted in Figure 4.1 0. Both of the signals in eqs. (4 .17) and (4 .19) can be expressed in terms of the sine function: 2 sinwT1 = 2T1 sm. c (w-T-1) w 7T sin Wt = W . (Wt) 7T smc ---;: . 7Tt sinc(O) Figure 4. 1 0 The sine function. Finally, we can gain insight into one other property of the Fourier transform by examining Figure 4.9, which we have redrawn as Figure 4.11 for several different values of W. From this figure, we see that as W increases, X(jw) becomes broader, while the main peak of x(t) at t = 0 becomes higher and the width of the first lobe of this sig- nal (i.e., the part of the signal for ltl < 1riW) becomes narrower. In fact, in the limit as W ~ oo, X(jw) = 1 for all w, and consequently, from Example 4.3, we see that x(t) in eq. (4.19) converges to an impulse as W ~ oo. The behavior depicted in Figure 4.11 is an example of the inverse relationship that exists between the time and frequency domains, 296 The Continuous-Time Fourier Transform Chap.4 X1(jw) X2(jw) 11 11 I I -w1 w1 w -w2 w2 w (a) (b) X3(jw) rn -w3 w3 w (c) Figure 4. 11 Fourier transform pair of Figure 4.9 for several different values of W. and we can see a similar effect in Figure 4.8, where an increase in T1 broadens x(t) but makes X(jw) narrower. In Section 4.3.5, we provide an explanation of this behavior in the context of the scaling property of the Fourier transform. 4.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS In the preceding section, we introduced the Fourier transform representation and gave several examples. While our attention in that section was focused on aperiodic signals, we can also develop Fourier transform representations for periodic signals, thus allowing us to Sec. 4.2 The Fourier Transform for Periodic Signals 297 consider both periodic and aperiodic signals within a unified context. In fact, as we will see, we can construct the Fourier transform of a periodic signal directly from its Fourier series representation. The resulting transform consists of a train of impulses in the frequency domain, with the areas of the impulses proportional to the Fourier series coefficients. This will turn out to be a very useful representation. To suggest the general result, let us consider a signal x(t) with Fourier transform X(jw) that is a single impulse of area 27T at w = w0 ; that is, X(jw) = 27T8(w - wo). (4.21) To determine the signal x(t) for which this is the Fourier transform, we can apply the inverse transform relation, eq. (4.8), to obtain 1 f+x x(t) = - 27T8(w - wo)eJwt dw 2 7T -X More generally, if X (jw) is of the form of a linear combination of impulses equally spaced in frequency, that is, +x X(jw) = ~ 27Tak8(w - kwo), (4.22) k=-010 then the application of eq. (4.8) yields +x x(t) = ~ akejkwot (4.23) k= -00 We see that eq. (4.23) corresponds exactly to the Fourier series representation of a periodic signal, as specified by eq. (3.38). Thus, the Fouri~f transform of a periodic signal with Fourier series coefficients {ak} can be interpr~ted as a train of impulses occurring at the harmonically related frequencies and for which the area of the impulse at the kth harmonic frequency kw 0 is 27T times the kth Fourier series coefficient ak. Example 4.6 Consider again the square wave illustrated in Figure 4.1. The Fourier series coefficients for this signal are and the Fourier transform of the signal is X(jw) = f 2 sin ~woTI 8(w - kwo), k=-X 298 The Continuous-Time Fourier Transform Chap.4 which is sketched in Figure 4.12 forT = 4T1• In comparison with Figure 3.7(a), the only differences are a proportionality factor of 27T and the use of impulses rather than a bar graph. XOw) ,.,1T / \ I \ I \ I I I I 2 I I 2 Figure 4.12 Fourier transform of a symmetric periodic square wave. Example 4.7 Let x(t) = sin wot. The Fourier series coefficients for this signal are 2j' k # 1 or -1. Thus, the Fourier transform is as shown in Figure 4.13(a). Similarly, for x(t) = cos wot, the Fourier series coefficients are k # 1 or -1. The Fourier transform of this signal is depicted in Figure 4.13(b). These two transforms will be of considerable importance when we analyze sinusoidal modulation systems in Chapter 8. Sec. 4.2 The Fourier Transform for Periodic Signals 299 XO w) 1Tij 0 w (a) XO w) 1T 1T t I t 0 w (b) Figure 4.13 Fourier transforms of (a) x(t) = sin wat; (b) x(t) = cos wat. Example 4.8 A signal that we will find extremely useful in our analysis of sampling systems in Chap- ter 7 is the impulse train +oc x(t) = L 8(t - kT), k=-00 which is periodic with period T, as indicated in Figure 4.14(a). The Fourier series coef- ficients for this signal were computed in Example 3.8 and are given by 1 I+ T/2 . 1 ak = - 8(t)e- Jkwot dt = -. T -T/2 T That is, every Fourier coefficient of the periodic impulse train has the same value, liT. Substituting this value for akin eq. (4.22) yields X(jw) ~ 2; k~oo I+- 2;k). Thus, the Fourier transform of a periodic impulse train in the time domain with pe- riod Tis a periodic impulse train in the frequency domain with period 27T/T, as sketched in Figure 4.14(b ). Here again, we see an illustration of the inverse relationship between the time and the frequency domains. As the spacing between the impulses in the time domain (i.e., the period) gets longer, the spacing between the impulses in the frequency domain (namely, the fundamental frequency) gets smaller. 300 The Continuous-Time Fourier Transform Chap.4 x(t) ... 1 1 1! 1 1 -2T -T 0 T 2T (a) X(jw) 2 ... t t T I t t 41T 21T 0 21T 41T w T T T T (b) Figure 4.14 (a) Periodic impulse train; (b) its Fourier transform. 4.3 PROPERTIES OF THE CONTINUOUS-TIME FOURIER TRANSFORM In this and the following two sections, we consider a number of properties of the Fourier transform. A detailed listing of these properties is given in Table 4.1 in Section 4.6. As was the case for the Fourier series representation of periodic signals, these properties provide us with a significant amount of insight into the transform and into the relationship between the time-domain and frequency-domain descriptions of a signal. In addition, many of the properties are often useful in reducing the complexity of the evaluation of Fourier trans- forms or inverse transforms. Furthermore, as described in the preceding section, there is a close relationship between the Fourier series and Fourier transform representations of a periodic signal, and using this relationship, we can translate many of the Fourier transform properties into corresponding Fourier series properties, which we discussed independently in Chapter 3. (See, in particular, Section 3.5 and Table 3.1.) Throughout the discussion in this section, we will be referring frequently to functions of time and their Fourier transforms, and we will find it convenient to use a shorthand notation to indicate the pairing of a signal and its transform. As developed in Section 4.1, a signal x(t) and its Fourier transform X(jw) are related by the Fourier transform synthesis and analysis equations, 1 J+ % . [eq. (4.8)] x(t) = 7T -% X(jw )e1wt dw (4.24) 2 and [eq. (4.9)] (4.25) Sec. 4.3 Properties of the Continuous-Time Fourier Transform 301 We will sometimes find it convenient to refer to X(jw) with the notation g:{x(t)} and to x(t) with the notation g:- 1{X(jw)}. We will also refer to x(t) and X(jw) as a Fourier transform pair with the notation ~ x(t) ~ X(jw ). Thus, with reference to Example 4.I, -- = g:{e-at u(t)}, a+ jw e-at u(t) = g:-I { I . } ' a+ JW and ~ I e-atu(t) ~ .. a+ JW 4.3. 1 Linearity If ~ x(t) ~ X(jw) and ~ y(t) ~ Y(jw ), then ~ ax(t) + by(t) ~ aX(jw) + bY(jw ). (4.26) The proof of eq. (4.26) follows directly by application of the analysis eq. (4.25) to ax(t) + by(t). The linearity property is easily extended to a linear combination of an arbitrary number of signals. 4.3.2 Time Shifting If ~ x(t) ~ X(jw), then (4.27) 302 The Continuous-Time Fourier Transform Chap.4 To establish this property, consider eq. (4.24): x(t) = -1 Jx X(jw)e 1.w tdw. 27T -X Replacing t by t - to in this equation, we obtain - 1 J+ x x(t- to) = X(jw)e1.w U-to)dw 27T -X - 1 J+ x . . (e-JwtoX(jw))e1wtdw. 27T -X Recognizing this as the synthesis equation for x(t- t0), we conclude that ~{x(t- to)} = e- jwto X(jw ). One consequence of the time-shift property is that a signal which is shifted in time does not have the magnitude of its Fourier transform altered. That is, if we express X(jw) in polar form as ~{x(t)} = X(jw) = IX(}w )iej<tX(jw), then ~{x(t- to)}= e-jwtoX(jw) = IX(jw)lej[<l:X(jw)-wtol. Thus, the effect of a time shift on a signal is to introduce into its transform a phase shift, namely, -wt0 , which is a linear function of w. Example 4.9 To illustrate the usefulness of the Fourier transform linearity and time-shift proper- ties, let us consider the evaluation of the Fourier transform of the signal x(t) shown in Figure 4.15(a). First, we observe that x(t) can be expressed as the linear combination 1 x(t) = 2X 1 (t - 2.5) + x 2(t - 2.5), where the signals x 1 (t) and x 2(t) are the rectangular pulse signals shown in Figure 4.15(b) and (c). Then, using the result from Example 4.4, we obtain X ( . ) _ 2sin(w/2) d X (. ) _ 2sin(3w/2) I }W- ~ 2]W- . w w Finally, using the linearity and time-shift properties of the Fourier transform yields X(jw) = e- jSw/2 { sin(w/2) +~ sin(3w/2)} . Sec. 4.3 Properties of the Continuous-Time Fourier Transform 303 ~(t) 1.~ 1 2 3 4 (a) _1 1 2 2 (b) I I I -1. 1. 2 2 (c) Figure 4. 1 5 Decomposing a signal into the linear combination of two sim- pler signals. (a) The signal x(t) for Example 4.9; (b) and (c) the two compo- nent signals used to represent x(t). 4.3.3 Conjugation and Conjugate Symmetry The conjugation property states that if ~ x(t) ~ X(jw ), then x * (t) ~~ X * (- jw ). (4.28) This property follows from the evaluation of the complex conjugate of eq. (4.25): X'(jw) = [r: x(t)e-iwt dtr +oo = f- oo x*(t)ejwt dt. Replacing w by -w, we see that +oo X*(- jw) = f- oo x*(t)e- jwt dt. (4.29) 304 The Continuous-Time Fourier Transform Chap.4 Recognizing that the right-hand side of eq. (4 .29) is the Fourier transform analysis equation for x*(t), we obtain the relation given in eq. (4.28). The conjugation property allows us to show that if x(t) is real, then X(jw) has con- jugate symmetry; that is, I X(- jw) = X'(jw) [x(t) real]. I (4.30) Specifically, if x(t) is real so that x*(t) = x(t), we have, from eq. (4.29), +oo X*(- jw) = J-o o x(t)ejwt dt = X(jw ), and eq. (4.30) follows by replacing w with -w. From Example 4.1, with x(t) = e-atu(t), 1 X(jw) = . a+ JW and X(- jw) = = X*(jw). a- jw As one consequence of eq. (4.30), if we express X(jw) in rectangular form as X(jw) = ffi-e{X(jw )} + jdm{X(jw )}, then if x(t) is real, ffi-e{X(jw )} = ffi-e{X(- jw )} and dm{X(jw)} = -dm{X(- jw )}. That is, the real part of the Fourier transform is an even function of frequency, and the imaginary part is an odd function of frequency. Similarly, if we express X(jw) in polar form as X(jw) = IX(jw )jej<tX(jw), then it follows from eq. (4.30) that IX(jw )I is an even function of w and 4:.X(jw) is an odd function of w. Thus, when computing or displaying the Fourier transform of a real- valued signal, the real and imaginary parts or magnitude and phase of the transform need only be specified for positive frequencies, as the values for negative frequencies can be determined directly from the values for w > 0 using the relationships just derived. As a further consequence of eq. (4.30), if x(t) is both real and even, then X(jw) will also be real and even. To see this, we write +oo X(- jw) = J- oo x(t)ejwt dt, Sec. 4.3 Properties of the Continuous-Time Fourier Transform 305 or, with the substitution T = -t, +oo X(- jw) = I-o o x( -T)e- jwT dT. Since x( -T) = x(T), we have +oo X(- jw) = I-o o x(T)e-JwtdT = X(jw). Thus, X(jw) is an even function. This, together with eq. (4.30), also requires that X*(jw) = X(jw) [i.e., that X(jw) is real]. Example 4.2 illustrates this property for the real, even signal e-altl. In a similar manner, it can be shown that if x(t) is a real and odd function of time, so that x(t) = - x( -t), then X(jw) is purely imaginary and odd. Finally, as was discussed in Chapter 1, a real function x(t) can always be expressed in terms of the sum of an even function Xe(t) = Sv{x(t)} and an odd function x 0 (t) = <9d{x(t)}; that is, x(t) = Xe(t) + X0 (t). From the linearity of the Fourier transform, ~{x(t)} = ~{xe(t)} + ~{x0 (t)}, and from the preceding discussion, ~{xe(t)} is a real function and ~{x0 (t)} is purely imag- inary. Thus, we can conclude that, with x(t) real, ~ x(t) ~ X(jw ), ~ Sv{x(t)} ~ (Re{X(jw )}, ~ <9d{x(t)} ~ j!Jm{X(jw)}. One use of these symmetry properties is illustrated in the following example. Example 4. 1 0 Consider again the Fourier transform evaluation of Example 4.2 for the signal x(t) = e-alrl, where a > 0. This time we will utilize the symmetry properties of the Fourier transform to aid the evaluation process. From Example 4.1, we have ~ 1 e-atu(t) ~ --.-. a+ JW Note that for t > 0, x(t) equals e-ar u(t), while for t < 0, x(t) takes on mirror image values. That is, 306 The Continuous-Time Fourier Transform Chap.4 x(t) = e -altl = e-at u(t) + eat u(- t) = 2 [ e-atu(t) ~ eatu(-t)] = 28v{e-at u(t)}. Since e-at u(t) is real valued, the symmetry properties of the Fourier transform lead us to conclude that 1 Bv{e-atu(t)} ~ ffi-e{ - -.-}. a+ JW It follows that 1 2 X(jw) = 2ffi-e{ - -.-} = 2 +a 2 , a+ JW a w which is the same as the answer found in Example 4.2. 4.3.4 Differentiation and Integration Let x(t) be a signal with Fourier transform X(jw ). Then, by differentiating both sides of the Fourier transform synthesis equation (4 .24) , we obtain dx(t) 1 J+ oo 1· = - jwX(jw )e wt dw. dt 27T -00 Therefore, ddx(tf) ~ . X( . ) ~JW JW. (4.31) This is a particularly important property, as it replaces the operation of differentiation in the time domain with that of multiplication by jw in the frequency domain. We will find the substitution to be extremely useful in our discussion in Section 4. 7 on the use of Fourier transforms for the analysis of LTI systems described by differential equations. Since differentiation in the time domain corresponds to multiplication by jw in the frequency domain, one might conclude that integration should involve division by jw in the frequency domain. This is indeed the case, but it is only one part of the picture. The precise relationship is t ~ 1 J x(r)dr ~ -.X (jw) + 7TX(O)o(w). (4.32) -oo JW The impulse term on the right-hand side of eq. (4.32) reflects the de or average value that can result from integration. The use of eqs. (4.31) and (4.32) is illustrated in the next two examples. Sec. 4.3 Properties of the Continuous-Time Fourier Transform 307 Example 4. 11 Let us determine the Fourier transform X(jw) of the unit step x(t) = u(t), making use of eq. (4.32) and the knowledge that ~ g(t) = S(t) ~ G(jw) = 1. Noting that x(t) = Loo g( r)dr and taking the Fourier transform of both sides, we obtain X(jw) = G(_Jw) + 7TG(O)S(w), )W where we have used the integration property listed in Table 4.1. Since G(jw) = 1, we conclude that X(jw) = ~ + 7TS(w). (4.33) JW Observe that we can apply the differentiation property of eq. (4.31) to recover the transform of the impulse. That is, S(t) = du(t) ~ jw [~ + 7TS(w )] = 1, dt JW where the last equality follows from the fact that wS(w) = 0. Example 4. 1 2 Suppose that we wish to calculate the Fourier transform X(jw) for the signal x(t) dis- played in Figure 4.16(a). Rather than applying the Fourier integral directly to x(t), we instead consider the signal d g(t) = dt x(t). x(t) 1 -1 +A ~ (a) g(t) = d~it) (b) -1 -1 Figure 4. 16 (a) A signal x(t) for which the Fourier transform is to be eval- uated; (b) representation of the derivative of x(t) as the sum of two components. 308 The Continuous-Time Fourier Transform Chap.4 As illustrated in Figure 4.16(b ), g(t) is the sum of a rectangular pulse and two impulses. The Fourier transforms of each of these component signals may be determined from Table 4.2: G(jw) = -2swinw- ) - . . ( e.Jw- e-Jw. Note that G(O) = 0. Using the integration property, we obtain . G(jw) X(;w) = -.- + 7TG(0)8(w). )W With G(O) = 0 this becomes . 2sinw 2cosw X(JW) = -.2- - -.-. ;w JW The expression for X(jw) is purely imaginary and odd, which is consistent with the fact that x(t) is real and odd. 4.3.5 Time and Frequency Scaling If ~ x(t) ~ X(jw ), then ~ 1 (jw) x(at) ~ rarx a ' (4.34) where a is a nonzero real number. This property follows directly from the definition of the Fourier transform-specifically, +(X) 5""{x(at)} = f-(X) x(at)e- fwtdt. Using the substitution T = at, we obtain a>O a<O which corresponds to eq. (4.34). Thus, aside from the amplitude factor llial, a linear scal- ing in time by a factor of a corresponds to a linear scaling in frequency by a factor of 11 a, and vice versa. Also, letting a = -1, we see from eq. (4.34) that Sec. 4.3 Properties of the Continuous-Time Fourier Transform 309 (4.35) That is, reversing a signal in time also reverses its Fourier transform. A common illustration of eq. (4.34) is the effect on frequency content that results when an audiotape is recorded at one speed and played back at a different speed. If the playback speed is higher than the recording speed, corresponding to compression in time (i.e., a > 1) , then the spectrum is expanded in frequency (i.e., the audible effect is that the playback frequencies are higher). Conversely, the signal played back will be scaled down in frequency if the playback speed is slower than the recording speed (0 < a < 1). For example, if a recording of the sound of a small bell ringing is played back at a reduced speed, the result will sound like the chiming of a larger and deeper sounding bell. The scaling property is another example of the inverse relationship between time and frequency that we have already encountered on several occasions. For example, we have seen that as we increase the period of a sinusoidal signal, we decrease its frequency. Also, as we saw in Example 4.5 (see Figure 4.11 ), if we consider the transform 1 X(jw) = { ' \w\<W 0, \w\>W' then as we increase W, the inverse transform of X(jw) becomes narrower and taller and approaches an impulse as W ~ oo. Finally, in Example 4.8, we saw that the spacing in the frequency domain between impulses in the Fourier transform of a periodic impulse train is inversely proportional to the spacing in the time domain. The inverse relationship between the time and frequency domains is of great im- portance in a variety of signal and systems contexts, including filtering and filter design, and we will encounter its consequences on numerous occasions in the remainder of the book. In addition, the reader may very well come across the implications of this property in studying a wide variety of other topics in science and engineering. One example is the uncertainty principle in physics; another is illustrated in Problem 4.49. 4.3.6 Duality By comparing the transform and inverse transform relations given in eqs. (4.24) and (4.25), we observe that these equations are similar, but not quite identical, in form. This symmetry leads to a property of the Fourier transform referred to as duality. In Example 4.5, we alluded to duality when we noted the relationship that exists between the Fourier transform pairs of Examples 4.4 and 4.5. In the former example we derived the Fourier transform pair \t\ < T 1 g: 2 sinwT1 XJ(I) = { 6: \t\ ~ Xt(jW) = (4.36) > Tt w while in the latter we considered the pair sin Wt g: • { 1 x2(t) = --~ X2C1w) = ' \w\<W (4.37) ~t 0' \w\>W"" 310 The Continuous-Time Fourier Transform Chap.4 x1 (t) ~ ~ -T1 T1 x2(t) X2(jw) /Whr 11 I -w w w Figure 4.17 Relationship between the Fourier transform pairs of eqs. (4.36) and (4.37). The two Fourier transform pairs and the relationship between them are depicted in Figure 4.17. The symmetry exhibited by these two examples extends to Fourier transforms in general. Specifically, because of the symmetry between eqs. (4 .24) and (4 .25), for any transform pair, there is a dual pair with the time and frequency variables interchanged. This is best illustrated through an example. Example 4. 1 3 Let us consider using duality to find the Fourier transform G(jw) of the signal 2 g(t) = -1-2. +t In Example 4.2 we encountered a Fourier transform pair in which the Fourier transform, as a function of w, had a form similar to that of the signal x(t). Specifically, suppose we consider a signal x(t) whose Fourier transform is X(jw) = -+21w 2' Then, from Example 4.2, x(t) = e-ltl ~ X(jw) = .; w . 1 2 Sec. 4.3 Properties of the Continuous-Time Fourier Transform 311 The synthesis equation for this Fourier transform pair is e-iri = - 1 IX (- -2 2 ) e;.w tdw. 21T -X 1 + w Multiplying this equation by 21T and replacing t by -t, we obtain 27Te-lt1 = Ix (~)e- Jwtdw. -X 1 + w Now, interchanging the names of the variables t and w, we find that (4.38) The right-hand side of eq. (4.38) is the Fourier transform analysis equation for 2/(1 + t2 ), and thus, we conclude that ~{ _2_} = 21Te-lwi. 1 + t2 The duality property can also be used to determine or to suggest other properties of Fourier transforms. Specifically, if there are characteristics of a function of time that have implications with regard to the Fourier transform, then the same characteristics associated with a function of frequency will have dual implications in the time domain. For example, in Section 4.3.4, we saw that differentiation in the time domain corresponds to multiplica- tion by jw in the frequency domain. From the preceding discussion, we might then suspect that multiplication by jt in the time domain corresponds roughly to differentiation in the frequency domain. To determine the precise form of this dual property, we can proceed in a fashion exactly analogous to that used in Section 4.3.4. Thus, if we differentiate the analysis equation (4 .25) with respect to w, we obtain dX(jw) f +oo dw = -oo -jtx(t)e-Jwtdt. (4.39) That is, . () ~ dX(jw) -}tXt ~ dw . (4.40) Similarly, we can derive the dual properties of eqs. (4.27) and (4.32): . ~ elwot x(t) ~ X(j(w - wo)) (4 .41) and 1 ~ f(J) - -:-X(t) + 7TX(0)8(t) ~ X(T])dTJ. (4.42) }t -00 312 The Continuous-Time Fourier Transform Chap.4 4.3.7 Parseval's Relation If x(t) and X(jw) are a Fourier transform pair, then (4.43) This expression, referred to as Parseval's relation, follows from direct application of the Fourier transform. Specifically, Reversing the order of integration gives The bracketed term is simply the Fourier transform of x(t); thus, The term on the left-hand side of eq. (4.43) is the total energy in the signal x(t). Parse val's relation says that this total energy may be determined either by computing the energy per unit time (jx(t)j2) and integrating over all time or by computing the energy per unit frequency (jX(jw )j2/27T) and integrating over all frequencies. For this reason, jX(jw )1 2 is often referred to as the energy-density spectrum of the signal x(t). (See also Problem 4.45.) Note that Parseval's relation for finite-energy signals is the direct counterpart of Parseval's relation for periodic signals (eq. 3.67), which states that the average power of a periodic signal equals the sum of the average powers of its individual harmonic compo- nents, which in tum are equal to the squared magnitudes of the Fourier series coefficients. Parseval's relation and other Fourier transform properties are often useful in deter- mining some time domain characteristics of a signal directly from the Fourier transform. The next example is a simple illustration of this. Example 4. 14 For each of the Fourier transforms shown in Figure 4.18, we wish to evaluate the follow- ing time-domain expressions: E f""YO /x(t)/2 = dt D = !!_x(t)l dt t=O Sec. 4.3 Properties of the Continuous-Time Fourier Transform 313 X(jw) -1 -0.5 0 0.5 w (a) X(jw) jfiT -1 0 w ....__ ___ , -j fiT (b) Figure 4. 18 The Fourier transforms considered in Example 4.14. To evaluate E in the frequency domain, we may use Parseval's relation. That is, E 1 Joe = 27T _""' IX(jw)i1dw (4.44) which evaluates to ~ for Figure 4.18( a) and to 1 for Figure 4.18(b ). To evaluate D in the frequency domain, we first use the differentiation property to observe that g(t) = :t x(t) ~ jwX(jw) = G(jw ). Noting that D = g(O) = -1 fx G(jw)dw (4.45) 27T -X we conclude: D = f_""'x jwX(jw) dw (4.46) which evaluates to zero for figure 4.18( a) and to - ~ for Figure 4.18(b ). (2 ~'Tr ) There are many other properties of the Fourier transform in addition to those we have already discussed. In the next two sections, we present two specific properties that play 314 The Continuous-Time Fourier Transform Chap.4 particularly central roles in the study of LTI systems and their applications. The first of these, discussed in Section 4.4, is referred to as the convolution property, which is central to many signals and systems applications, including filtering. The second, discussed in Section 4.5, is referred to as the multiplication property, and it provides the foundation for our discussion of sampling in Chapter 7 and amplitude modulation in Chapter 8. In Section 4.6, we summarize the properties of the Fourier transform. 4.4 THE CONVOLUTION PROPERTY As we saw in Chapter 3, if a periodic signal is represented in a Fourier series-i.e., as a linear combination of harmonically related complex exponentials, as in eq. (3.38)- then the response of an LTI system to this input can also be represented by a Fourier series. Because complex exponentials are eigenfunctions of LTI systems, the Fourier series coefficients of the output are those of the input multiplied by the frequency response of the system evaluated at the corresponding harmonic frequencies. In this section, we extend this result to the situation in which the signals are aperiodic. We first derive the property somewhat informally, to build on the intuition we developed for periodic signals in Chapter 3, and then provide a brief, formal derivation starting directly from the convolution integral. Recall our interpretation of the Fourier transform synthesis equation as an expression for x(t) as a linear combination of complex exponentials. Specifically, referring back to eq. (4.7), x(t) is expressed as the limit of a sum~ that is, 1 I+ :rc . 1 +00 . x(t) = - X(jw)e 1w1dw = lim - L X(jkw )eJkw01 0 w 0. (4.47) 2 7f -00 W() --?() 2 7f k =-X As developed in Sections 3.2 and 3.8, the response of a linear system with impulse response h(t) to a complex exponential e.ikwol is H(j kw 0 )e.ikwol, where +% H(j kwo) = I-00 h(t)e -jkwol dt. (4.48) We can recognize the frequency response H(jw), as defined in eq. (3.121), as the Fourier transform of the system impulse response. In other words, the Fourier transform of the impulse response (evaluated at w = kw0 ) is the complex scaling factor that the LTI system applies to the eigenfunction e.i kwo1 • From superposition [see eq. (3 .124)], we then have 1 +x 1 +x 7T L"""" '""' X (J"" k wo ) e jkw 0 I wo ~ 7T L"""" '""' X (J"" k w0 )H( J"" k w ) e jkw 0 0 I w0, 2 2 k=-X k=-X and thus, from eq. (4.47), the response of the linear system to x(t) is 1 y(t) = lim - f X(jkwo)H(jkwo)ejkwo1w0 W()--7{) 27f k= -X (4.49) = -1 I +00 X(jw)H(jw)e1. w1dw. 27f -X Sec. 4.4 The Convolution Property 315 Since y(t) and its Fourier transform Y(jw) are related by y(t) = -1 J+x Y(jw )e1.w t dw, (4.50) 27 T -cc we can identify Y(jw) from eq. (4.49), yielding Y(jw) = X(jw)H(jw). (4.51) As a more formal derivation, we consider the convolution integral +x y(t) = f-cc X( T)h(t - T)dT. (4.52) We desire Y(jw ), which is (4.53) Interchanging the order of integration and noting that x( r) does not depend on t, we have (4.54) By the time-shift property, eq. (4.27), the bracketed term is e- jwr H(jw ). Substituting this into eq. (4.54) yields +x f +oo Y(jw) = f- cc x(r)e-jwrH(jw)dT = H(jw) -cc x(r)e-jwrdT. (4.55) The integral is X(jw ), and hence, Y(jw) = H(jw)X(jw). That is, g: y(t) = h(t) * x(t) ~ Y(jw) = H(jw )X(jw ). (4.56) Equation (4 .56) is of major importance in signal and system analysis. As expressed in this equation, the Fourier transform maps the convolution of two signals into the product of their Fourier transforms. H(jw ), the Fourier transform of the impulse response, is the frequency response as defined in eq. (3.121) and captures the change in complex amplitude of the Fourier transform of the input at each frequency w. For example, in frequency- selective filtering we may want to have H (jw) = 1 over one range of frequencies, so that the frequency components in this band experience little or no attenuation or change due to the system, while over another range of frequencies we may want to have H(jw) = 0, so that components in this range are eliminated or significantly attenuated. 316 The Continuous-Time Fourier Transform Chap.4 The frequency response H(jw) plays as important a role in the analysis of LTI sys- tems as does its inverse transform, the unit impulse response. For one thing, since h(t) completely characterizes an LTI system, then so must H(jw ). In addition, many of the properties of LTI systems can be conveniently interpreted in terms of H(jw ). For exam- ple, in Section 2.3, we saw that the impulse response of the cascade of two LTI systems is the convolution of the impulse responses of the individual systems and that the over- all impulse response does not depend on the order in which the systems are cascaded. Using eq. (4.56), we can rephrase this in terms of frequency responses. As illustrated in Figure 4.19, since the impulse response of the cascade of two LTI systems is the con- volution of the individual impulse responses, the convolution property then implies that the overall frequency response of the cascade of two systems is simply the product of the individual frequency responses. From this observation, it is then clear that the overall frequency response does not depend on the order of the cascade. x(t) H1(jw) H2(jw) y(t) (a) x(t) •I H1(jw)H2(jw) I • y(t) (b) x(t) y(t) Figure 4. 19 Three equivalent LTI systems. Here, each block represents an LTI system with the indicated (c) frequency response. As discussed in Section 4.1.2, convergence of the Fourier transform is guaranteed only under certain conditions, and consequently, the frequency response cannot be defined for every LTI system. If, however, an LTI system is stable, then, as we saw in Section 2.3.7 and Problem 2.49, its impulse response is absolutely integrable; that is, r: ,h(t),dt < oo. (4.57) Equation (4 .57) is one of the three Dirichlet conditions that together guarantee the exis- tence of the Fourier transform H(jw) of h(t). Thus, assuming that h(t) satisfies the other two conditions, as essentially all signals of physical or practical significance do, we see that a stable LTI system has a frequency response H(jw ). In using Fourier analysis to study LTI systems, we will be restricting ourselves to systems whose impulse responses possess Fourier transforms. In order to use trans- form techniques to examine unstable LTI systems we will develop a generalization of Sec. 4.4 The Convolution Property 317 the continuous-time Fourier transform, the Laplace transform. We defer this discussion to Chapter 9, and until then we will consider the many problems and practical applications that we can analyze using the Fourier transform. 4.4. 1 Examples To illustrate the convolution property and its applications further, let us consider several examples. Example 4. 1 5 Consider a continuous-time LTI system with impulse response h(t) = o(t - to). (4.58) The frequency response of this system is the Fourier transform of h(t) and is given by H(jw) = e-Jwto. (4.59) Thus, for any input x(t) with Fourier transform X(jw ), the Fourier transform of the output is Y(jw) = H(jw)X(jw) (4.60) = e-Jwtox(jw). This result, in fact, is consistent with the time-shift property of Section 4.3.2. Specifi- cally, a system for which the impulse response is o(t- to) applies a time shift of to to the input-that is, y(t) = x(t - to). Thus, the shifting property given in eq. (4.27) also yields eq. (4.60). Note that, either from our discussion in Section 4.3.2 or directly from eq. (4.59), the frequency response of a system that is a pure time shift has unity magnitude at all frequencies (i.e., ie-Jwro I = 1) and has a phase characteristic -wt0 that is a linear function of w. Example 4. 1 6 As a second example, let us examine a differentiator-that is, an LTI system for which the input x(t) and the output y(t) are related by y(t) = d~;t). From the differentiation property of Section 4.3.4, Y(jw) = jwX(jw). (4.61) Consequently, from eq. (4.56), it follows that the frequency response of a differentiator is H(jw) = jw. (4.62) 318 The Continuous-Time Fourier Transform Chap.4 Example 4. 1 7 Consider an integrator-that is, an LTI system specified by the equation y(t) = fx X(T)dT. The impulse response for this system is the unit step u(t), and therefore, from Exam- ple 4.11 and eq. (4.33), the frequency response of the system is H(jw) = _;_ + 7TO(w). JW Then using eq. (4.56), we have Y(jw) = H(jw)X(jw) _;_ X(jw) + 1T X(jw );(w) JW _;_X(jw) + 1TX(O)o(w), JW which is consistent with the integration property of eq. (4.32). Example 4. 18 As we discussed in Section 3.9.2, frequency-selective filtering is accomplished with an LTI system whose frequency response H (jw) passes the desired range of frequencies and significantly attenuates frequencies outside that range. For example, consider the ideal lowpass filter introduced in Section 3.9.2, which has the frequency reponse illustrated in Figure 4.20 and given by H(jw) = { 1 /w/ <We. (4.63) 0 /w/ >we Now that we have developed the Fourier transform representation, we know that the impulse response h(t) of this ideal filter is the inverse transform of eq. (4 .63). Using the result in Example 4.5, we then have h(t) = sin Wet (4.64) 1Tf which is plotted in Figure 4.21. 0 -stopband+ Passband --I--stopband-- Figure 4.20 Frequency response of an ideal lowpass filter. Sec. 4.4 The Convolution Property 319 h(t) Figure 4.21 Impulse response of an ideal lowpass filter. From Example 4.18, we can begin to see some of the issues that arise in filter design that involve looking in both the time and frequency domains. In particular, while the ideal lowpass filter does have perfect frequency selectivity, its impulse response has some char- acteristics that may not be desirable. First, note that h(t) is not zero fort < 0. Consequently, the ideal lowpass filter is not causal, and thus, in applications requiring causal systems, the ideal filter is not an option. Moreover, as we discuss in Chapter 6, even if causality is not an essential constraint, the ideal filter is not easy to approximate closely, and non- ideal filters that are more easily implemented are typically preferred. Furthermore, in some applications (such as the automobile suspension system discussed in Section 6. 7.1 ), oscil- latory behavior in the impulse response of a lowpass filter may be undesirable. In such applications the time domain characteristics of the ideal lowpass filter, as shown in Fig- ure 4.21, may be unacceptable, implying that we may need to trade off frequency-domain characteristics such as ideal frequency selectivity with time-domain properties. For example, consider the LTI system with impulse response h(t) = e -r u(t). (4.65) The frequency response of this system is H(jw) = + (4.66) jw 1· Comparing eqs. (3.145) and (4.66), we see that this system can be implemented with the simple RC circuit discussed in Section 3.10. The impulse response and the magnitude of the frequency response are shown in Figure 4.22. While the system does not have the strong frequency selectivity of the ideal lowpass filter, it is causal and has an impulse response that decays monotonically, i.e., without oscillations. This filter or somewhat more complex ones corresponding to higher order differential equations are quite frequently preferred to ideal filters because of their causality, ease of implementation, and flexibility in allowing trade-offs, among other design considerations such as frequency selectivity and oscillatory behavior in the time domain. Many of these issues will be discussed in more detail in Chapter 6. The convolution property is often useful in evaluating the convolution integral-i.e., in computing the response of LTI systems. This is illustrated in the next example. 320 The Continuous-Time Fourier Transform Chap.4 h(t) e (a) IH(jw)l -1 (b) Figure 4.22 (a) Impulse response of the LTI system in eq. (4.65); (b) magnitude of the frequency response of the system. Example 4. 19 Consider the response of an LTI system with impulse response h(t) = e-at u(t), a > 0, to the input signal x(t) = e-bt u(t), b > 0. Rather than computing y(t) = x(t) * h(t) directly, let us transform the problem into the frequency domain. From Example 4.1, the Fourier transforms of x(t) and h(t) are 1 X(jw) = - -.- b+ JW and 1 H(jw) = - -.-. a+ JW Therefore, Y(jw) = ( (4.67) a+ . )l(b + . ) JW )W To determine the output y(t), we wish to obtain the inverse transform of Y(jw ). This is most simply done by expanding Y(jw) in a partial-fraction expansion. Such expansions are extremely useful in evaluating inverse transforms, and the general method for performing a partial-fraction expansion is developed in the appendix. For this Sec. 4.4 The Convolution Property 321 example, assuming that b =P a, the partial fraction expansion for Y(jw) takes the form Y(jw) = _A_._+ -Bb. ' (4.68) a+ )W + JW where A and B are constants to be determined. One way to find A and B is to equate the right-hand sides of eqs. (4.67) and (4.68), multiply both sides by (a+ jw )(b + jw ), and solve for A and B. Alternatively, in the appendix we present a more general and efficient method for computing the coefficients in partial-fraction expansions such as eq. (4.68). Using either of these approaches, we find that 1 A= b- a = -B, and therefore, Y(jw) = _1_ [-1- __1 _]. (4.69) b - a a + jw b + jw The inverse transform for each of the two terms in eq. (4.69) can be recognized by inspection. Using the linearity property of Section 4.3.1, we have 1 y(t) = --[e-at u(t)- e-bt u(t)]. b-a When b = a, the partial fraction expansion of eq. (4.69) is not valid. However, with b = a, eq. (4.67) becomes Y(jw) = (a + jw )2. Recognizing this as . d [ 1 ] (a+jw)2 =Jdw a+jw' we can use the dual of the differentiation property, as given in eq. (4.40). Thus, ~ 1 e-at u(t) ~ --.- a+ JW ~ j_!}__ [-1 te-atu(t) -.-] = dw a+ JW (a+ jw)2' and consequently, y(t) = te-at u(t). Example 4.20 As another illustration of the usefulness of the convolution property, let us consider the problem of determining the response of an ideallowpass filter to an input signal x(t) that has the form of a sine function. That is, sinwit x(t ) = --. 7Tt Of course, the impulse response of the ideallowpass filter is of a similar form, namely, 322 The Continuous-Time Fourier Transform Chap.4 h(t) = sin wet. 1T't The filter output y(t) will therefore be the convolution of two sine functions, which, as we now show, also turns out to be a sine function. A particularly convenient way of deriving this result is to first observe that Y(jw) = X(jw )H(jw ), where X(jw) = { 01 lwl ::::;; w; elsewhere and H(jw) = { 1 lwl ::::;; We 0 elsewhere Therefore, Y(jw) = { 1 lwl ::::;; wo , 0 elsewhere where w0 is the smaller of the two numbers w; and we. Finally, the inverse Fourier trans- form of Y(jw) is given by sinwet 'f l· -- 1 We::::;; W; y(t) = . 1T't smw;t 'f -- 1 W;::::;; We 1T't That is, depending upon which of we and w; is smaller, the output is equal to either x(t) or h(t). 4.5 THE MULTIPLICATION PROPERTY The convolution property states that convolution in the time domain corresponds to mul- tiplication in the frequency domain. Because of duality between the time and frequency domains, we would expect a dual property also to hold (i.e., that multiplication in the time domain corresponds to convolution in the frequency domain). Specifically, = = -1 f+x r(t) s(t)p(t) ~ R(jw) S(j8)P(j(w- 8))d() (4.70) 27T -X This can be shown by exploiting duality as discussed in Section 4.3.6, together with the convolution property, or by directly using the Fourier transform relations in a manner anal- ogous to the procedure used in deriving the convolution property. Multiplication of one signal by another can be thought of as using one signal to scale or modulate the amplitude of the other, and consequently, the multiplication of two sig- nals is often referred to as amplitude modulation. For this reason, eq. (4.70) is sometimes Sec. 4.5 The Multiplication Property 323 referred to as the modulation property. As we shall see in Chapters 7 and 8, this property has several very important applications. To illustrate eq. (4.70), and to suggest one of the applications that we will discuss in subsequent chapters, let us consider several examples. Example 4.21 Let s(t) be a signal whose spectrum S(jw) is depicted in Figure 4.23(a). Also, consider the signal p(t) = cos wot. Then P(jw) = 1r8(w - wo) + 1r8(w + wo), as sketched in Figure 4.23(b), and the spectrum R(jw) of r(t) = s(t)p(t) is obtained by w (a) PGw) 1T t I t w (b) RGw) = _1_ [SGw) * PGw)] M ~:i (c) Figure 4.23 Use of the multiplication property in Example 4.21: (a) the Fourier transform of a signal s(t); (b) the Fourier transform of p(t) = cos wot; (c) the Fourier transform of r(t) = s(t)p(t). 324 The Continuous-Time Fourier Transform Chap.4 an application of eq. (4.70), yielding +oo 1 R(jw) = 27T f-oosue)PU(w - ()))d() = ~S(j(w - wo)) + ~S(j(w + wo)), (4.71) which is sketched in Figure 4.23(c). Here we have assumed that w0 > w 1, so that the two nonzero portions of R(jw) do not overlap. Clearly, the spectrum of r(t) consists of the sum of two shifted and scaled versions of S(jw ). From eq. (4.71) and from Figure 4.23, we see that all of the information in the signal s(t) is preserved when we multiply this signal by a sinusoidal signal, although the information has been shifted to higher frequencies. This fact forms the basis for sinu- soidal amplitude modulation systems for communications. In the next example, we learn how we can recover the original signal s(t) from the amplitude-modulated signal r(t). Example 4.22 Let us now consider r(t) as obtained in Example 4.21, and let g(t) = r(t)p(t), where, again, p(t) = cos w0t. Then, R(jw ), P(jw ), and G(jw) are as shown in Figure 4.24. From Figure 4.24(c) and the linearity of the Fourier transform, we see that g(t) is the sum of (l/2)s(t) and a signal with a spectrum that is nonzero only at higher frequen- R(jw) & tN2 & -wo wo w (a) 1T P(jw) 1T t I 1 -wo wo w (b) G(jw) A/4 ,;k A/4 ~ ~ -2w0 -w1 w1 w (c) Figure 4.24 Spectra of signals considered in Example 4.22: (a) R(jw ); (b) P(jw ); (c) G(jw ). Sec. 4.5 The Multiplication Property 325 cies (centered around ±2w0 ). Suppose then that we apply the signal g(t) as the input to a frequency-selective lowpass filter with frequency response H(jw) that is constant at low frequencies (say, for lwl < w1) and zero at high frequencies (for lwl > wt). Then the output of this system will have as its spectrum H(jw )G(jw ), which, because of the particular choice of H(jw ), will be a scaled replica of S(jw ). Therefore, the output itself will be a scaled version of s(t). In Chapter 8, we expand significantly on this idea as we develop in detail the fundamentals of amplitude modulation. Example 4.23 Another illustration of the usefulness of the Fourier transform multiplication property is provided by the problem of determining the Fourier transform of the signal ( ) _ sin(t) sin(t/2) X t - 1T""t2 • The key here is to recognize x(t) as the product of two sine functions: x(t) = ,. ( si:~) )('in~;2)). Applying the multiplication property of the Fourier transform, we obtain X(jw) = !~ { sin(t)} * ~ { sin(t/2)} . 2 'TT""t 'TT""t Noting that the Fourier transform of each sine function is a rectangular pulse, we can proceed to convolve those pulses to obtain the function X(jw) displayed in Figure 4.25. ~1~:w) v<::, ~ -3 _1 1 3 w 2 2 2 2 Figure 4.25 The Fourier transform of x(t) in Example 4.23. 4.5.1 Frequency-Selective Filtering with Variable Center Frequency As suggested in Examples 4.21 and 4.22 and developed more fully in Chapter 8, one of the important applications of the multiplication property is amplitude modulation in commu- nication systems. Another important application is in the implementation of frequency- selective bandpass filters with tunable center frequencies that can be adjusted by the simple tum of a dial. In a frequency-selective bandpass filter built with elements such as resistors, operational amplifiers, and capacitors, the center frequency depends on a number of element values, all of which must be varied simultaneously in the correct way if the center frequency is to be adjusted directly. This is generally difficult and cumber- some in comparison with building a filter whose characteristics are fixed. An alternative to directly varying the filter characteristics is to use a fixed frequency-selective filter and 326 The Continuous-Time Fourier Transform Chap.4 shift the spectrum of the signal appropriately, using the principles of sinusoidal amplitude modulation. For example, consider the system shown in Figure 4.26. Here, an input signal x(t) is multiplied by the complex exponential signal ejwct. The resulting signal is then passed through a lowpass filter with cutoff frequency w0 , and the output is multiplied by e- jwct. The spectra of the signals x(t), y(t), w(t), and f(t) are illustrated in Figure 4.27. e iwct Ideal lowpass filter .~ H{jw) y(t) w(t) x(t) • 11 1-----+-t X J-------1~ f(t) -wo wo w Figure 4.26 Implementation of a bandpass filter using amplitude modula- tion with a complex exponential carrier. X(jw) ~ w Y(jw) Frequency response of 1-- ideal lowpass filter ~ I I W(jw) A1 w F(jw) w Figure 4.27 Spectra of the signals in the system of Figure 4.26. Sec. 4.5 The Multiplication Property 327 Specifically, from either the multiplication property or the frequency-shifting property it follows that the Fourier transform of y(t) = eiwct x(t) is Y(jw) = r: ll(O- wc)X(w- O)dO so that Y(jw) equals X(jw) shifted to the right by We and frequencies in X(jw) near w = we have been shifted into the passband of the lowpass filter. Similarly, the Fourier transform ofj(t) = e-Jwc1w(t) is F(jw) = W(j(w + wo)), so that the Fourier transform ofF Uw) is W Uw) shifted to the left by We. From Figure 4.27, we observe that the overall system of Figure 4.26 is equivalent to an ideal bandpass fil- ter with center frequency -we and bandwidth 2w0 , as illustrated in Figure 4.28. As the frequency We of the complex exponential oscillator is varied, the center frequency of the bandpass filter varies. I I I (1) Figure 4.28 Bandpass filter equiva- lent of Figure 4.26. In the system of Figure 4.26 with x(t) real, the signals y(t), w(t), and f(t) are all complex. If we retain only the real part of f(t), the resulting spectrum is that shown in Figure 4.29, and the equivalent bandpass filter passes bands of frequencies centered around We and -we, as indicated in Figure 4.30. Under certain conditions, it is also possi- ble to use sinusoidal rather than complex exponential modulation to implement the system of the latter figure. This is explored further in Problem 4.46. (1) Figure 4.29 Spectrum of ffi-e{f(t)} associated with Figure 4.26. H(jw) I I I tf I I (1) Figure 4.30 Equivalent bandpass filter for ffi-e{f(t)} in Figure 4.29. 328 The Continuous-Time Fourier Transform Chap.4 4.6 TABLES OF FOURIER PROPERTIES AND OF BASIC FOURIER TRANSFORM PAIRS In the preceding sections and in the problems at the end of the chapter, we have consid- ered some of the important properties of the Fourier transform. These are summarized in Table 4.1, in which we have also indicated the section of this chapter in which each prop- erty has been discussed. In Table 4.2, we have assembled a list of many of the basic and important Fourier transform pairs. We will encounter many of these repeatedly as we apply the tools of TABLE 4. 1 PROPERTIES OF THE FOURIER TRANSFORM Section Property Aperiodic signal Fourier transform x(t) X(jw) y(t) Y(jw) ----------------------------------------------------------- 4.3.1 Linearity ax(t) + by(t) aX(jw) + bY(jw) 4.3.2 Time Shifting x(t- to) e-Jwto X(jw) 4.3.6 Frequency Shifting eiwof x(t) X(j(w- wo)) 4.3.3 Conjugation x*(t) X*(-jw) 4.3.5 Time Reversal x( -t) X(- jw) 4.3.5 Time and Frequency x(at) _!_xCW) Scaling lal a 4.4 Convolution x(t) * y(t) X(jw)Y(jw) 4.5 Multiplication x(t)y(t) ~ J:!ue);(j(w- 8))de d 4.3.4 Differentiation in Time dt x(t) jwX(jw) 1 . 4.3.4 Integration f>' x(t)dt -J:W- X(]w) + 7TX(O)o(w) 4.3.6 Differentiation in tx(t) jd~X(jw) Frequency r(jw) ~X'(- jw) <Re{X(jw )} = (fk{X(- jw )} 4.3.3 Conjugate Symmetry x(t) real 9m{X(jw )} = -9m{X(- jw)} for Real Signals /X(jw)/ = /X(-jw)/ <r.X(jw) = -<r.X(- jw) 4.3.3 Symmetry for Real and x(t) real and even X(jw) real and even Even Signals 4.3.3 Symmetry for Real and x(t) real and odd X(jw) purely imaginary and odd Odd Signals Xe(t) = 8v{x(t)} [x(t) real] <Re{X(jw)} 4.3.3 Even-Odd Decompo- sition for Real Sig- X0 (t) = 0d{x(t)} [x(t) real] j9m{X(jw)} nals 4.3.7 Parseval's Relation for IA periodic Signals I+ oo 1 +oo -x /x(t)i1dt = 27T -x /X(jw)!Zdw Sec. 4.6 Tables of Fourier Properties and of Basic Fourier Transform Pairs 329 TABLE 4.2 BASIC FOURIER TRANSFORM PAIRS Fourier series coefficients Signal Fourier transform (if periodic) L+oo +oo akejkwot 27T L akB(w - kw0 ) k=-00 k=-00 a1 = 1 ak = 0, otherwise a1 = a-1 = 4 coswot 1r[8(w - wo) + B(w + wo)] ak = 0, otherwise I ~ a1 = -a-1 = 2] sinwot [B(w - wo) - B(w + wo)] 1 <lk = 0, otherwise ao = 1, ak = 0, k =F 0 x(t) = 1 27T8(w) this is the Fourier series representation for) (any choice of T > 0 Periodic square wave !1, ltl < T1 x(t) = 0, T1 < ltl ~ ~ ~ 2 sin kwoT1 !:.'( _ k ) woT1 . (kwoT1 ) _ sin kwoT1 L u w w 0 -- smc -- - k1r and k=-00 k 7T 7T x(t + T) = x(t) +oo L B(t- nT) 27T f 8 (w- 27Tk) ak = T1 for all k T k=-00 T 1 ltl < T1 2sinwTJ x(t) { ' 0, ltl > T1 w sin Wt X(jw)= 1' lwl < W 7Tt { 0, lwi>W B(t) 1 u(t) ~ + 1r8(w) JW B(t- to) a+ jw te-at u(t), CRe{a} > 0 (a+jw) 2 tn-1 -at () (n-l)!e u t, CRe{a} > 0 (a+jw)n 330 The Continuous-Time Fourier Transform Chap.4 Fourier analysis in our examination of signals and systems. All of the transform pairs, except for the last one in the table, have been considered in examples in the preceding sections. The last pair is considered in Problem 4.40. In addition, note that several of the signals in Table 4.2 are periodic, and for these we have also listed the corresponding Fourier series coefficients. 4.7 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENTIAL EQUATIONS As we have discussed on several occasions, a particularly important and useful class of continuous-time LTI systems is those for which the input and output satisfy a linear constant-coefficient differential equation of the form ±a k dky;t) = ±bk dkx;o. (4.72) k=O dt k=O dt In this section, we consider the question of determining the frequency response of such an LTI system. Throughout the discussion we will always assume that the frequency response of the system exists, i.e., that eq. (3.121) converges. There are two closely related ways in which to determine the frequency response H(jw) for an LTI system described by the differential equation (4.72). The first of these, which relies on the fact that complex exponential signals are eigenfunctions of LTI systems, was used in Section 3.10 in our analysis of several simple, nonideal filters. Specifically, if x(t) = eJwt, then the output must be y(t) = H (jw )eJwt. Substituting these expressions into the differential equation (4.72) and performing some algebra, we can then solve for H(jw ). In this section we use an alternative approach to arrive at the same answer, making use of the differentiation property, eq. (4 . 31 ), of Fourier transforms. Consider an LTI system characterized by eq. (4.72). From the convolution property, Y(jw) = H(jw)X(jw), or equivalently, H( 1' w) = Y(jw) (4.73) X(jw)' where X(jw ), Y(jw ), and H(jw) are the Fourier transforms of the input x(t), output y(t), and impulse response h(t), respectively. Next, consider applying the Fourier transform to both sides of eq. (4.72) to obtain (4.74) From the linearity property, eq. (4.26), this becomes ~ ~{ dky(t)} = ~ b ~{ dkx(t)} L ak dtk L k dtk ' (4.75) k=O k=O Sec. 4.7 Systems Characterized by Linear Constant-Coefficient Differential Equations 331 and from the differentiation property, eq. (4.31), N M L ak(jw)kY(jw) = L bk(jw)kX(jw), k=O k=O or equivalently, Thus, from eq. (4.73), (4.76) Observe that H(jw) is thus a rational function; that is, it is a ratio of polynomials in (jw ). The coefficients of the numerator polynomial are the same coefficients as those that appear on the right-hand side of eq. (4.72), and the coefficients of the denominator polynomial are the same coefficients as appear on the left side of eq. (4.72). Hence, the frequency response given in eq. (4.76) for the LTI system characterized by eq. (4.72) can be written down directly by inspection. The differential equation (4 . 72) is commonly referred to as an Nth-order differen- tial equation, as the equation involves derivatives of the output y(t) up through the Nth derivative. Also, the denominator of H(jw) in eq. (4.76) is an Nth-order polynomial in (jw). Example 4.24 Consider a stable LTI system characterized by the differential equation (dy[((t) + ay(t) = x(t), (4.77) with a> 0. From eq. (4.76), the frequency response is 1 H(jw) = -.- -. (4.78) JW +a Comparing this with the result of Example 4.1, we see that eq. (4.78) is the Fourier transform of e-at u(t). The impulse response of the system is then recognized as h(t) = e-at u(t). Example 4.25 Consider a stable LTI system that is characterized by the differential equation 332 The Continuous-Time Fourier Transform Chap. 4 From eq. (4.76), the frequency response is H( . ) _ (jw) + 2 (4.79) JW - (jw)2 + 4(jw) + 3 · To determine the corresponding impulse response, we require the inverse Fourier trans- form of H(jw ). This can be found using the technique of partial-fraction expansion em- ployed in Example 4.19 and discussed in detail in the appendix. (In particular, see Ex- ample A.1, in which the details of the calculations for the partial-fraction expansion of eq. (4.79) are worked out.) As a first step, we factor the denominator of the right-hand side of eq. (4.79) into a product of lower order terms: H(. ) jw + 2 (4.80) JW = (jw + 1)(jw + 3) Then, using the method of partial-fraction expansion, we find that I I H( . ) - 2 )W - -.-+-1 + -.-2- 3. )W )W + The inverse transform of each term can be recognized from Example 4.24, with the result that The procedure used in Example 4.25 to obtain the inverse Fourier transform is gen- erally useful in inverting transforms that are ratios of polynomials in jw. In particular, we can use eq. (4.76) to determine the frequency response of any LTI system described by a linear constant-coefficient differential equation and then can calculate the impulse response by performing a partial-fraction expansion that puts the frequency response into a form in which the inverse transform of each term can be recognized by inspection. In addition, if the Fourier transform X(jw) of the input to such a system is also a ratio of polynomials in jw, then so is Y(jw) = H(jw )X(jw ). In this case we can use the same technique to solve the differential equation-that is, to find the response y(t) to the input x(t). This is illustrated in the next example. Example 4.26 Consider the system of Example 4.25, and suppose that the input is x(t) = e~ 1 u(t). Then, using eq. (4.80), we have 1 Y(jw) = H(jw)X(jw) = [(jw !~)~~ + 3)] [jw + 1] jw + 2 (jw + 1)2 (jw + (4.81) 3) · Sec. 4.8 Summary 333 As discussed in the appendix, in this case the partial-fraction expansion takes the form (4.82) where A 11 , A 12 , and A21 are constants to be determined. In Example A.2 in the appendix, the technique of partial-fraction expansion is used to determine these constants. The values obtained are 4' so that I I I Y( · ) 4 2 4 JW = jw + 1 + (jw + 1) 2 - jw + 3 · (4.83) Again, the inverse Fourier transform for each term in eq. (4.83) can be obtained by in- spection. The first and third terms are of the same type that we have encountered in the preceding two examples, while the inverse transform of the second term can be obtained from Table 4.2 or, as was done in Example 4.19, by applying the dual of the differenti- ation property, as given in eq. (4.40), to ll(jw + 1). The inverse transform of eq. (4.83) is then found to be From the preceding examples, we see how the techniques of Fourier analysis allow us to reduce problems concerning LTI systems characterized by differential equations to straightforward algebraic problems. This important fact is illustrated further in a number of the problems at the end of the chapter. In addition (see Chapter 6), the algebraic structure of the rational transforms encountered in dealing with LTI systems described by differen- tial equations greatly facilitate the analysis of their frequency-domain properties and the development of insights into both the time-domain and frequency-domain characteristics of this important class of systems. 4.8 SUMMARY In this chapter, we have developed the Fourier transform representation for continous-time signals and have examined many of the properties that make this transform so useful. In particular, by viewing an aperiodic signal as the limit of a periodic signal as the period becomes arbitrarily large, we derived the Fourier transform representation for aperiodic signals from the Fourier series representation for periodic signals developed in Chapter 3. In addition, periodic signals themselves can be represented using Fourier transforms con- sisting of trains of impulses located at the harmonic frequencies of the periodic signal and with areas proportional to the corresponding Fourier series coefficients. The Fourier transform possesses a wide variety of important properties that de- scribe how different characteristics of signals are reflected in their transforms, and in 334 The Continuous-Time Fourier Transform Chap.4 this chapter we have derived and examined many of these properties. Among them are two that have particular significance for our study of signals and systems. The first is the convolution property, which is a direct consequence of the eigenfunction property of com- plex exponential signals and which leads to the description of an LTI system in terms of its frequency response. This description plays a fundamental role in the frequency- domain approach to the analysis of LTI systems, which we will continue to explore in subsequent chapters. The second property of the Fourier transform that has extremely important implications is the multiplication property, which provides the basis for the frequency-domain analysis of sampling and modulation systems. We examine these sys- tems further in Chapters 7 and 8. We have also seen that the tools of Fourier analysis are particularly well suited to the examination of LTI systems characterized by linear constant- coefficient differential equations. Specifically, we have found that the frequency response for such a system can be determined by inspection and that the technique of partial-fraction expansion can then be used to facilitate the calculation of the impulse response of the system. In subsequent chapters, we will find that the convenient algebraic structure of the frequency responses of these systems allows us to gain considerable insight into their characteristics in both the time and frequency domains. The first section of problems belongs to the basic category and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 4.1. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) e-2U-l)u(t- 1) (b) e-2lt-ll Sketch and label the magnitude of each Fourier transform. 4.2. Use the Fourier transform analysis equation (4.9) to calculate the Fourier transforms of: (a) B(t + 1) + B(t- 1) (b) fr{u( -2- t) + u(t- 2)} Sketch and label the magnitude of each Fourier transform. 4.3. Determine the F*o)u rier transform of each of the following periodic signals: (a) sin(21Tt + (b) 1 + cos(61rt + ¥) 4.4. Use the Fourier transform synthesis equation (4.8) to determine the inverse Fourier transforms of: (a) X1( jw) = 21T B(w) + 1r B(w - 41T) + 1r B(w + 41T) Chap. 4 Problems 335 2, O:s;w:s;2 (b) X2(jw) = -2, -2 ::; w < 0 { 0, lwl >2 4.5. Use the Fourier transform synthesis equation (4.8) to determine the inverse Fourier transform of X(jw) = IX(jw )lei<t:X(Jw), where IX(jw )I = 2{u(w + 3) - u(w - 3)}, <.X(jw) = -~w + 7T. Use your answer to determine the values oft for which x(t) = 0. 4.6. Given that x(t) has the Fourier transform X(jw ), express the Fourier transforms of the signals listed below in terms of X(jw ). You may find useful the Fourier transform properties listed in Table 4.1. (a) x 1 (t) = x(1 - t) + x( -1 - t) (b) x2(t) = x(3t- 6) 2 (c) x3(t) = ;r2 x(t - 1) 4. 7. For each of the following Fourier transforms, use Fourier transform properties (Table 4.1) to determine whether the corresponding time-domain signal is (i) real, imaginary, or neither and (ii) even, odd, or neither. Do this without evaluating the inverse of any of the given transforms. (a) X1(jw) = u(w)- u(w- 2) (b) X2(jw) = cos(2w) sin(~) (c) X3(jw) = A(w)eiB(w), where A(w) = (sin2w)/w and B(w) = 2w + ~ (d) X(jw) = L~= _00 (~)1kl 5(w - k;) 4.8. Consider the signal 0, t < -.!. 2 x(t) = !t + ~' -.!2. <- t <- .!2."" 1, t >.!. 2 (a) Use the differentiation and integration properties in Table 4.1 and the Fourier transform pair for the rectangular pulse in Table 4.2 to find a closed-form ex- pression for X(jw ). (b) What is the Fourier transform of g(t) = x(t)- ~? 4.9. Consider the signal ltl > 1 x(t) = { ~; + 1)/2, -1 ::; t::; r (a) With the help of Tables 4.1 and 4.2, determine the closed-form expression for X(jw). (b) Take the real part of your answer to part (a), and verify that it is the Fourier transform of the even part of x(t). (c) What is the Fourier transform of the odd part of x(t)? 336 The Continuous-Time Fourier Transform Chap.4 4.10. (a) Use Tables 4.1 and 4.2 to help determine the Fourier transform of the following signal: 2 x(t) = t ( ·~n (b) Use Parse val's relation and the result of the previous part to determine the nu- merical value of A = +oo t2 ( s•m f t )4 dt -00 7Tt 4.11. Given the relationships y(t) = x(t) * h(t) and g(t) = x(3t) * h(3t), and given that x(t) has Fourier transform X(jw) and h(t) has Fourier transform H(jw ), use Fourier transform properties to show that g(t) has the form g(t) = Ay(Bt). Determine the values of A and B. 4.12. Consider the Fourier transform pair -ltl ~ 2 e ~ 1 +w2"" (a) Use the appropriate Fourier transform properties to find the Fourier transform of te-ltl. (b) Use the result from part (a), along with the duality property, to determine the Fourier transform of 4t Hint: See Example 4.13. 4.13. Let x(t) be a signal whose Fourier transform is X(jw) = 5(w) + 5(w- 7T) + 5(w- 5), and let h(t) = u(t) - u(t - 2). (a) Is x(t) periodic? (b) Is x(t) * h(t) periodic? (c) Can the convolution of two aperiodic signals be periodic? Chap. 4 Problems 337 4.14. Consider a signal x(t) with Fourier transform X(jw ). Suppose we are given the following facts: 1. x(t) is real and nonnegative. 2. ~- 1 {(1 + jw )X(jw )} = Ae-2 t u(t), where A is independent oft. 00 3. J_ iX(jw )1 2 dw = 21T. 00 Determine a closed-form expression for x(t). 4.15. Let x(t) be a signal with Fourier transform X(jw ). Suppose we are given the fol- lowing facts: 1. x(t) is real. 2. x(t) = 0 fort ~ 0. 3. 2~ J_ :ooo ffie{X(jw )}eiwt dw = ltle-ltl. Determine a closed-form expression for x(t). 4.16. Consider the signal oo sin(kE:) 1T x(t) = k~oo (k*) D(t- k4 ). (a) Determine g(t) such that x(t) = sin t) ( 1Tt g(t). (b) Use the multiplication property of the Fourier transform to argue that X(jw) is periodic. Specify X(jw) over one period. 4.17. Determine whether each of the following statements is true or false. Justify your answers. (a) An odd and imaginary signal always has an odd and imaginary Fourier trans- form. (b) The convolution of an odd Fourier transform with an even Fourier transform is always odd. 4.18. Find the impulse response of a system with the frequency response 2 H(jw) = (sin (3w )) cos w w2 4.19. Consider a causal LTI system with frequency response H(jw) = . 1 3 JW + For a particular input x(t) this system is observed to produce the output y(t) = e-3tu(t)-e-4tu(t). Determine x(t). 4.20. Find the impulse response of the causal LTI system represented by the RLC circuit considered in Problem 3.20. Do this by taking the inverse Fourier transform of the circuit's frequency response. You may use Tables 4.1 and 4.2 to help evaluate the inverse Fourier transform. 338 The Continuous-Time Fourier Transform Chap. 4 BASIC PROBLEMS 4.21. Compute the Fourier transform of each of the following signals: (a) [e-at cos wot]u(t), a > 0 (b) e-31tl sin 2t (c) x(t) = { 1 +cos 7T't, lltll :::; 1 (d) 2:~-o ak o(t- kT), Ia I < 1 0, t > 1 - (e) [te-2tsin4t]u(t) (f) [sin7Tt][sin27T(t-l)] 1Tt 1T(t-l) (g) x(t) as shown in Figure P4.21(a) (h) x(t) as shown in Figure P4.21(b) 2 (i) x(t) = { 1 - t , 0 < t ~ 1 (j) """"""""+oo -lt-2nl 0, otherwise Ln=-ao e X (t) X (t) ... t t t 2 t t t !f t t -6 -5 -4 -3 -2 -1 0 1 2 3 (a) (b) Figure P4.21 4.22. Determine the continuous-time signal corresponding to each of the following transforms. <t X Ow) IXOw)l -1 w (a) [7 1 2 3 w -1 (b) Figure P4.22 Chap. 4 Problems 339 (a) X(jw) = 2sin[3(w-27T)] (w-27T) (b) X(jw) = cos(4w + 7r/3) (c) X(jw) as given by the magnitude and phase plots of Figure P4.22(a) (d) X(jw) = 2[o(w - 1) - o(w + 1)] + 3[o(w - 27r) + o(w + 27r)] (e) X(jw) as in Figure P4.22(b) 4.23. Consider the signal -t 0 < t < 1 x0 (t) = e ' - - { 0, elsewhere · Determine the Fourier transform of each of the signals shown in Figure P4.23. You should be able to do this by explicitly evaluating only the transform of x0(t) and then using properties of the Fourier transform. x0(t) -1 0 1 (a) (b) -1 0 1 0 1 (c) (d) Figure P4.23 4.24. (a) Determine which, if any, of the real signals depicted in Figure P4.24 have Fourier transforms that satisfy each of the following conditions: (1) CR-e{X(jw)} = 0 (2) dm{X(jw )} = 0 (3) There exists a real a such that ejaw X(jw) is real (4) J_::'ooX(jw)dw = 0 (5) J_::'oowX(jw)dw = 0 (6) X(jw) is periodic (b) Construct a signal that has properties (1), (4), and (5) and does not have the others. 340 The Continuous-Time Fourier Transform Chap.4 X (t) (a) (b) X (t) I 2ij3ij vv v v (c) X t) (d) x(t) = e -t2/2 (e) (f) Figure P4.24 Chap. 4 Problems 341 4.25. Let X(jw) denote the Fourier transform of the signal x(t) depicted in Figure P4.25. (a) Find 1:X(jw ). (b) Find X(jO). (c) Findf:oox(jw)dw. (d) Evaluate J: oo X(jw ) 2s:w ejlw dw. (e) Evaluate J: oo iX(Jw )1 2 dw. (f) Sketch the inverse Fourier transform of CRe{X(jw )}. Note: You should perform all these calculations without explicitly evaluating X(jw ). X (t) -1 0 2 3 Figure P4.25 4.26. (a) Compute the convolution of each of the following pairs of signals x(t) and h(t) by calculating X(jw) and H(jw ), using the convolution property, and inverse transforming. (i) x(t) = te-lt u(t), h(t) = e-4t u(t) (ii) x(t) = te-lt u(t), h(t) = te-4t u(t) (iii) x(t) = e-tu(t), h(t) = etu(-t) (b) Suppose that x(t) = e-(t-l)u(t- 2) and h(t) is as depicted in Figure P4.26. Ver- ify the convolution property for this pair of signals by showing that the Fourier transform of y(t) = x(t) * h(t) equals H(jw )X(jw ). -1 3 Figure P4.26 4.27. Consider the signals x(t) = u(t - 1) - 2u(t- 2) + u(t - 3) and 00 i(t) = L, x(t- kT), k= -00 342 The Continuous-Time Fourier Transform Chap.4 where T > 0. Let ak denote the Fourier series coefficients of i(t), and let X(jw) denote the Fourier transform of x(t). (a) Determine a closed-form expression for X(jw ). (b) Determine an expression for the Fourier coefficients ak and verify that ak = tx(i2;k ). 4.28. (a) Let x(t) have the Fourier transform X(jw ), and let p(t) be periodic with funda- mental frequency wo and Fourier series representation +oo p(t) = 2.: anejnwot. n=-oo Determine an expression for the Fourier transform of y(t) = x(t)p(t). (P4.28-1) (b) Suppose that X(jw) is as depicted in Figure P4.28(a). Sketch the spectrum of y(t) in eq. (P4.28-1) for each of the following choices of p(t): (i) p(t) = cos(t/2) (ii) p(t) = cost (iii) p(t) = cos 2t (iv) p(t) = (sin t)(sin 2t) (v) p(t) = cos 2t- cost (vi) p(t) = 2:;: _00 8(t- 1Tn) (vii) p(t) = 2:;: _00 8(t - 27Tn) (viii) p(t) = 2:;: _00 8(t - 41Tn) (ix) p(t) = 2:;: -oo 8(t- 27Tn) - i 2:;: _00 8(t- 1Tn) (x) p(t) = the periodic square wave shown in Figure P4.28(b ). X(jw) -1 w (a) p (t) J D D D rn D D D D D... (b) Figure P4.28 Chap. 4 Problems 343 4.29. A real-valued continuous-time function x(t) has a Fourier transform X(jw) whose magnitude and phase are as illustrated in Figure P4.29(a). The functions Xa(t), xb(t), Xc(t), and xd(t) have Fourier transforms whose magnitudes are identical to X(jw ), but whose phase functions differ, as shown in Figures P4.29(b)-(e). The phase functions 1:.Xa(jw) and 1:.Xb(jw) are formed by adding a linear phase to 1:.X(jw ). The function 1:Xc(jw) is formed by reflecting 1:X(jw) about w = 0, and 1:Xd(jw) is obtained by a combination of a reflection and an addition of a linear phase. Using the properties of Fourier transforms, deter- mine the expressions for xa(t), xb(t), Xc(t), and xd(t) in terms of x(t). IXGw)l 1: X Ow) (a) ..................... w ........................................... .............. ....... Slope =-a (b) Figure P4.29 344 The Continuous-Time Fourier Transform Chap.4 ...... .............................. Slope =b ............ .................. (c) w ----------- -TI/2 (d) ____ ---Slope =d ---- w ------ ---- ---- --- (e) Figure P4.29 Continued 4.30. Suppose g(t) = x(t) cost and the Fourier transform of the g(t) is G(jw) = { l, lwl ~ 2 0, otherwise· (a) Determine x(t). (b) Specify the Fourier transform X1( jw) of a signal x1( t) such that g(t) = Xt (I) COS (~1). Chap. 4 Problems 345 4.31. (a) Show that the three LTI systems with impulse responses h1 (t) = u(t), h2(t) = -28(t) + 5e-2tu(t), and all have the same response to x(t) = cost. (b) Find the impulse response of another LTI system with the same response to cost. This problem illustrates the fact that the response to cos t cannot be used to specify an LTI system uniquely. 4.32. Consider an LTI system S with impulse response h(t) = sin(4(t - 1)). 7T(t - 1) Determine the output of S for each of the following inputs: (a) x1( t) = cos(6t + ~) (b) x2(t) = 2:;= 0(~)k sin(3kt) (c) X (t) = sin(4(t+ I)) 3 7T(t+ 1) (d) X4(t) = cin2t)2 7Tt 4.33. The input and the output of a stable and causal L TI system are related by the dif- ferential equation -d2-y(t+) 6d-y-(t) 2 + 8 y( t) -- 2 x(t ) dt dt (a) Find the impulse response of this system. (b) What is the response of this system if x(t) = te-2t u(t)? (c) Repeat part (a) for the stable and causal LTI system described by the equation d 2y(t) r;:;. dy(t) ( ) = d 2 x(t) _ ( ) dt2 + .y 2L dt + y t 2 dt2 2 X t 4.34. A causal and stable LTI system S has the frequency response . jw +4 H(jw)=6 -w 2 + sJW·. 346 The Continuous-Time Fourier Transform Chap.4 (a) Determine a differential equation relating the input x(t) and output y(t) of S. (b) Determine the impulse response h(t) of S. (c) What is the output of S when the input is x(t) = e-4tu(t)-te-4tu(t)? 4.35. In this problem, we provide examples of the effects of nonlinear changes in phase. (a) Consider the continuous-time LTI system with frequency response H( ·w) = a- jw J a + J.W ' where a> 0. What is the magnitude of H(jw )? What is <r:..H(jw )? What is the impulse response of this system? (b) Determine the output of the system of part (a) with a = 1 when the input is cos(t/ J3) + cost + cos J3t. Roughly sketch both the input and the output. 4.36. Consider an LTI system whose response to the input x(t) = [e-t + e- 3t]u(t) is y(t) = [2e-t- 2e-4t]u(t). (a) Find the frequency response of this system. (b) Determine the system's impulse response. (c) Find the differential equation relating the input and the output of this system. ADVANCED PROBLEMS 4.37. Consider the signal x(t) in Figure P4.37. (a) Find the Fourier transform X(jw) of x(t). (b) Sketch the signal 00 x(t) = x(t) * L o(t - 4k). k= -00 (c) Find another signal g(t) such that g(t) is not the same as x(t) and 00 x(t) = g(t) * L o(t- 4k). k= -00 Chap. 4 Problems 347 (d) Argue that, although G(jw) is different from X(jw), G(jn;k) = X(jn;k) for all integers k. You should not explicitly evaluate G(jw) to answer this question. X (t) Figure P4.37 4.38. Let x(t) be any signal with Fourier transform X(jw ). The frequency-shift property of the Fourier transform may be stated as . ~ e1wot x(t) ~ X(j(w - wo)). (a) Prove the frequency-shift property by applying the frequency shift to the anal- ysis equation X(jw) = r~ x(t)e- Jwt dt. (b) Prove the frequency-shift property by utilizing the Fourier transform of eiwot in conjunction with the multiplication property of the Fourier transform. 4.39. Suppose that a signal x(t) has Fourier transform X(jw ). Now consider another signal g(t) whose shape is the same as the shape of X(jw ); that is, g(t) = X(jt). (a) Show that the Fourier transform G(jw) of g(t) has the same shape as 21Tx( -t); that is, show that G(jw) = 21Tx(-w). (b) Using the fact that g:{o(t + B)} = efBw in conjunction with the result from part (a), show that g:{ejBt} = 21T o(w - B). 4.40. Use properties of the Fourier transform to show by induction that the Fourier trans- form of tn-1 x(t) = (n _ I)! e-at u(t), a > 0, 348 The Continuous-Time Fourier Transform Chap.4 is 1 (a+jw)n' 4.41. In this problem, we derive the multiplication property ofthe continuous-time Fourier transform. Let x(t) and y(t) be two continuous-time signals with Fourier transforms X(jw) and Y(jc.q ), respectively. Also, let g(t) denote the inverse Fourier transform of 2~ {X(jw) * Y(jw )}. (a) Show that g(t) = 1 f +oo [ 1 f +oo . ] 7T -oo X(j8) 7T -oo Y(j(w - O))eJwt dw dO. 2 2 (b) Show that 1 f +oo . . - Y(j(w - 8))e1wt dw = el8t y(t). 27T -00 (c) Combine the results of parts (a) and (b) to conclude that g(t) = x(t)y(t). 4.42. Let g1 (t) = {[cos(wot)]x(t)} * h(t) and g2(t) = {[sin(wot)]x(t)} * h(t), where 00 x(t) = L akejkiOOt k= -DO is a real-valued periodic signal and h(t) is the impulse response of a stable LTI system. (a) Specify a value for w 0 and any necessary constraints on H(jw) to ensure that g1 (t) = (Jl.e{as} and (b) Give an example of h(t) such that H(jw) satisfies the constraints you specified in part (a). 4.43. Let sint g(t) = x(t) cos 2 t * -. 7Tt Assuming that x(t) is real and X(jw) = 0 for lwl 2: 1, show that there exists an LTI system S such that s x(t) ~ g(t). Chap. 4 Problems 349 4.44. The output y(t) of a causal LTI system is related to the input x(t) by the equation dy(t) + J +oc -d- lOy(t) = x( r)z(t- r) dr - x(t), f -oc where z(t) = e-t u(t) + 3 o(t). (a) Find the frequency response H(jw) = Y(jw )IX(jw) of this system. (b) Determine the impulse response of the system. 4.45. In the discussion in Section 4.3.7 ofParseval's relation for continuous-time signals, we saw that +oc 2 1 J +oc 2 J- oc lx(t)l dt = 21T -oo IX(jw )1 dw. This says that the total energy of the signal can be obtained by integrating IX(jw )1 2 over all frequencies. Now consider a real-valued signal x(t) processed by the ideal bandpass filter H(jw) shown in Figure P4.45. Express the energy in the output sig- nal y(t) as an integration over frequency of IX(Jw )1 2 • For~ sufficiently small so that IX(Jw )I is approximately constant over a frequency interval of width~. show that the energy in the output y(t) of the bandpass filter is approximately proportional to ~IX(Jwo)l2 • On the basis of the foregoing result, ~IX(jw0 )1 2 is proportional to the energy in the signal in a bandwidth~ around the frequency w0• For this reason, IX(jw )12 is often referred to as the energy-density spectrum of the signal x(t). ---1 a 1- 0 w Figure P4.45 4.46. In Section 4.5 .1, we discussed the use of amplitude modulation with a complex exponential carrier to implement a bandpass filter. The specific system was shown in Figure 4.26, and if only the real part of f(t) is retained, the equivalent bandpass filter is that shown in Figure 4.30. In Figure P4.46, we indicate an implementation of a bandpass filter using sinusoidal modulation and lowpass filters. Show that the output y(t) of the sys- tem is identical to that which would be obtained by retaining only CRe{f(t)} in Fig- ure 4.26. 350 The Continuous-Time Fourier Transform Chap.4 x(t) y(t) t EJ---<r-1 I t w Figure P4.46 4.47. An important property of the frequency response H(jw) of a continuous-time LTI system with a real, causal impulse response h(t) is that H(jw) is completely spec- ified by its real part, (Jl.e{H(jw )}. The current problem is concerned with deriving and examining some of the implications of this property, which is generally referred to as real-part sufficiency. (a) Prove the property of real-part sufficiency by examining the signal he(t), which is the even part of h(t). What is the Fourier transform of he(t)? Indicate how h(t) can be recovered from he(t). (b) If the real part of the frequency response of a causal system is (Jl.e{H(jw )} = cos w, what is h(t)? (c) Show that h(t) can be recovered from h 0 (t), the odd part of h(t), for every value of t except t = 0. Note that if h(t) does not contain any singularities [o(t), u1( t), u2(t), etc.] at t = 0, then the frequency response +oc H(jw) = J-o c h(t)e- jwt dt will not change if h(t) is set to some arbitrary finite value at the single point t = 0. Thus, in this case, show that H(jw) is also completely specified by its imaginary part. Chap. 4 Problems 351 EXTENSION PROBLEMS 4.48. Let us consider a system with a real and causal impulse response h(t) that does not have any singularities at t = 0. In Problem 4.47, we saw that either the real or the imaginary part of H(jw) completely determines H(jw ). In this problem we derive an explicit relationship between HR(jw) and H1(jw ), the real and imaginary parts of H(jw). (a) To begin, note that since h(t) is causal, h(t) = h(t)u(t), (P4.48-l) except perhaps at t = 0. Now, since h(t) contains no singularities at t = 0, the Fourier transforms of both sides of eq. (P4.48-l) must be identical. Use this fact, together with the multiplication property, to show that H( J.W ) -- -1.- J+x -H(-j'Y-J)d 'YJ. (P4.48-2) j7T -X w- 'YJ Use eq. (P4.48-2) to determine an expression for HR(jw) in terms of H 1(jw) and one for H1(jw) in terms of HR(jw ). (b) The operation y( )t _- -1 J +x X( T) d --T (P4.48-3) 7T -X t- T is called the Hilbert transform. We have just seen that the real and imaginary parts of the transform of a real, causal impulse response h(t) can be determined from one another using the Hilbert transform. Now considereq. (P4.48-3), and regard y(t) as the output of an LTI system with input x(t). Show that the frequency response of this system is H( . ) = { - j, w > 0 JW ]., w < o· (c) What is the Hilbert transform of the signal x(t) = cos 3t? 4.49. Let H(jw) be the frequency response of a continuous-time LTI system, and suppose that H(jw) is real, even, and positive. Also, assume that max{H(jw )} = H(O). w (a) Show that: (i) The impulse response, h(t), is real. (ii) max{ih(t)i} = h(O). Hint: If f(t. w) is a complex function of two variables, then 352 The Continuous-Time Fourier Transform Chap.4 (b) One important concept in system analysis is the bandwidth of an LTI system. There are many different mathematical ways in which to define bandwidth, but they are related to the qualitative and intuitive idea that a system with fre- quency response G(jw) essentially ""stops"" signals of the form eiwt for values of w where G(jw) vanishes or is small and ""passes"" those complex exponentials in the band of frequency where G(jw) is not small. The width of this band is the bandwidth. These ideas will be made much clearer in Chapter 6, but for now we will consider a special definition of bandwidth for those systems with frequency responses that have the properties specified previously for H(jw ). Specifically, one definition of the bandwidth Bw of such a system is the width of the rect- angle of height H(jO) that has an area equal to the area under H(jw ). This is illustrated in Figure P4.49(a). Note that since H(jO) = maxw H(jw ), the fre- quencies within the band indicated in the figure are those for which H (jw) is largest. The exact choice of the width in the figure is, of course, a bit arbitrary, but we have chosen one definition that allows us to compare different systems and to make precise a very important relationship between time and frequency. What is the bandwidth of the system with frequency response . = { 1, lwl < w? H(jw) 0, lwi>W. H(jw) H(O) ---: Area of rectangle = 1- area under H (jw) w (a) Figure P4.49a (c) Find an expression for the bandwidth Bw in terms of H(jw ). (d) Let s(t) denote the step response of the system set out in part (a). An important measure of the speed of response of a system is the rise time, which, like the bandwidth, has a qualitative definition, leading to many possible mathematical definitions, one of which we will use. Intuitively, the rise time of a system is a measure of how fast the step response rises from zero to its final value, s(oo) = lim s(t). (-H:IJ Thus, the smaller the rise time, the faster is the response of the system. For the system under consideration in this problem, we will define the rise time as s(oo) tr = h(O) Chap. 4 Problems 353 Since s' (t) = h(t), and also because of the property that h(O) = maxt h(t), tr is the time it would take to go from zero to s( oo) while maintaining the maximum rate of change of s(t). This is illustrated in Figure P4.49(b ). Find an expression for tr in terms of H(jw ). s(t) (b) Figure P4.49b (e) Combine the results of parts (c) and (d) to show that Bwtr = 27r. (P4.49-l) Thus, we cannot independently specify both the rise time and the bandwidth of our system. For example, eq. (P4.49-l) implies that, if we want a fast system (tr small), the system must have a large bandwidth. This is a fundamental trade-off that is of central importance in many problems of system design. 4.50. In Problems 1.45 and 2.67, we defined and examined several of the properties and uses of correlation functions. In the current problem, we examine the properties of such functions in the frequency domain. Let x(t) and y(t) be two real signals. Then the cross-correlation function of x(t) and y(t) is defined as c/>xy(t) = r~~ X(t + T)y( T) dT. Similarly, we can define cf>_vx(t), cf>xx(t), and c/>yy(t). [The last two of these are called the autocorrelation functions of the signals x(t) and y(t), respectively.] Let <I> xy(jw ), <l>yx(jw ), <l>xxUw ), and <l>yy(jw) denote the Fourier transforms of cf>xy(t), cf>.vx(t), cf>xx(t), and cf>.v_v(t), respectively. (a) What is the relationship between <l>xy(jw) and <l>yx(}w )? (b) Find an expression for <l>xy(jw) in terms of X(jw) and Y(jw ). (c) Show that <l>_u(jw) is real and nonnegative for every w. (d) Suppose now that x(t) is the input to an LTI system with a real-valued impulse response and with frequency response H(jw) and that y(t) is the output. Find expressions for <l>xy(jw) and <l>yy(}w) in terms of <l>xxUw) and H(jw ). 354 The Continuous-Time Fourier Transform Chap.4 (e) Let x(t) be as is illustrated in Figure P4.50, and let the LTI system impulse response be h(t) = e-at u(t), a > 0. Compute <I> xxCjw ), <I> xy(jw ), and <l>yy(jw) using the results of parts (a)-( d). (f) Suppose that we are given the following Fourier transform of a function cf>(t): . w 2 + 100 <l>(jw) = w 2 + 25 · Find the impulse responses of two causal, stable LTI systems that have autocor- relation functions equal to cp(t). Which one of these has a causal, stable inverse? x(t) Figure P4.50 4.51. (a) Consider two LTI systems with impulse responses h(t) and g(t), respectively, and suppose that these systems are inverses of one another. Suppose also that the systems have frequency responses denoted by H(jw) and G(jw ), respectively. What is the relationship between H(jw) and G(jw )? (b) Consider the continuous-time LTI system with frequency response 1 2 < iwl < 3 H(jw) = { 0,· otherwise (i) Is it possible to find an input x(t) to this system such that the output is as depicted in Figure P4.50? If so, find x(t). If not, explain why not. (ii) Is this system invertible? Explain your answer. (c) Consider an auditorium with an echo problem. As discussed in Problem 2.64, we can model the acoustics of the auditorium as an LTI system with an im- pulse response consisting of an impulse train, with the kth impulse in the train corresponding to the kth echo. Suppose that in this particular case the impulse response is h(t) = L e-kT 8(t- kT), k=O where the factor e-kT represents the attenuation of the kth echo. In order to make a high-quality recording from the stage, the effect of the echoes must be removed by performing some processing of the sounds sensed by the recording equipment. In Problem 2.64, we used convolutional techniques to consider one example of the design of such a processor (for a different acous- tic model). In the current problem, we will use frequency-domain techniques. Specifically, let G(jw) denote the frequency response of the LTI system to be Chap. 4 Problems 355 used to process the sensed acoustic signal. Choose G(jw) so that the echoes are completely removed and the resulting signal is a faithful reproduction of the original stage sounds. (d) Find the differential equation for the inverse of the system with impulse re- sponse h(t) = 2 O(t) + UJ (t). (e) Consider the LTI system initially at rest and described by the differential equa- tion d 2y(t) dy(t) ( ) _ d 2 x(t) dx(t) ( ) d t2 + 6 d t + 9 y t - ~ + 3 ----;[( + 2 X t . The inverse of this system is also initially at rest and described by a differen- tial equation. Find the differential equation describing the inverse, and find the impulse responses h(t) and g(t) of the original system and its inverse. 4.52. Inverse systems frequently find application in problems involving imperfect mea- suring devices. For example, consider a device for measuring the temperature of a liquid. It is often reasonable to model such a device as an LTI system that, because of the response characteristics of the measuring element (e.g., the mercury in ather- mometer), does not respond instantaneously to temperature changes. In particular, assume that the response of this device to a unit step in temperature is s(t) = (1 - e -r12 )u(t). (P4.52-1) (a) Design a compensatory system that, when provided with the output of the mea- suring device, produces an output equal to the instantaneous temperature of the liquid. (b) One of the problems that often arises in using inverse systems as compensators for measuring devices is that gross inaccuracies in the indicated temperature may occur if the actual output of the measuring device produces errors due to small, erratic phenomena in the device. Since there always are such sources of error in real systems, one must take them into account. To illustrate this, consider a measuring device whose overall output can be modeled as the sum of the response of the measuring device characterized by eq. (P4.52-1) and an interfering ""noise"" signal n(t). Such a model is depicted in Figure P4.52(a), where we have also included the inverse system of part (a), which now has as its input the overall output of the measuring device. Suppose that n(t) = sin wt. What is the contribution of n(t) to the output of the inverse system, and how does this output change as w is increased? (c) The issue raised in part (b) is an important one in many applications of LTI system analysis. Specifically, we are confronted with the fundamental trade- off between the speed of response of the system and the ability of the system to attenuate high-frequency interference. In part (b) we saw that this trade- off implied that, by attempting to speed up the response of a measuring device (by means of an inverse system), we produced a system that would also amplify 356 The Continuous-Time Fourier Transform Chap.4 ~--------------------1 I Actual measuring device n(t) I I I I I I I LTI model of Inverse system ....I. ... I measuring device + _l to LTI model I s(t) = (1- e-t/2 u(t) I of measuring ) I I device l ____________________ j (a) r--------------------1 I n(t) I I Perfect measuring I ~ device ...,.._1--...,.~l Compensating ~ I s (t) = u (t) 1 system IL ___________________ JI (b) Figure P4.52 corrupting sinusoidal signals. To illustrate this concept further, consider a mea- suring device that responds instantaneously to changes in temperature, but that also is corrupted by noise. The response of such a system can be modeled, as depicted in Figure P4.52(b ), as the sum of the response of a perfect measuring qevice and a corrupting signal n(t). Suppose that we wish to design a compen- satory system that will slow down the response to actual temperature variations, but also will attenuate the noise n(t). Let the impulse response of this system be h(t) = ae -at u(t). Choose a so that the overall system of Figure P4.52(b) responds as quickly as possible to a step change in temperature, subject to the constraint that the amplitude of the portion of the output due to the noise n(t) = sin 6t is no larger than 114. 4.53. As mentioned in the text, the techniques of Fourier analysis can be extended to signals having two independent variables. As their one-dimensional counterparts do in some applications, these techniques play an important role in other applications, such as image processing. In this problem, we introduce some of the elementary ideas of two-dimensional Fourier analysis. Let x(t1, t2) be a signal that depends upon two independent variables t1 and t2 . The two-dimensional Fourier transform of x(t1, t2 ) is defined as X(jw,, jw2) = L+: L+xx x(t,, t2)e- j(w,t, +w,t,) dt, dt2. (a) Show that this double integral can be performed as two successive one- dimensional Fourier transforms, first in t 1 with t2 regarded as fixed and then in t2. Chap. 4 Problems 357 (b) Use the result of part (a) to determine the inverse transform-that is, an expres- sion for x(tJ, t2) in terms of X(jw1, jw2). (c) Determine the two-dimensional Fourier transforms of the following signals: (i) x(t1, t2) = e-t1 +2t2 u(t1 - 1)u(2 - t2) •• { e-1t~l-lt2l, if -1 < t1 ::; 1 and -1 ::; t ::; 1 (n) x(t1, t2) = O, 2 otherwise (iii) x(t , t2) = { e-lt1Ht21, ifhO ::; .t1 ::; 1 or 0 ::; t2 ::; 1 (or both) 1 0, ot erw1se (iv) x(t1, t2) as depicted in Figure P4.53. (v) e-ltl +t2l-lt1-t2l x(t1, t2) = 1 in shaded area and 0 outside -1 -1 Figure P4.53 (d) Determine the signal x(t1, t2) whose two-dimensional Fourier transform is 2 X(jw1, jw2) = ~ l>(w2- 2wJ). 4 + )WJ (e) Let x(t1, t2) and h(t1, t2) be two signals with two-dimensional Fourier trans- forms X(jw 1, jw2) and H(jw 1, jw2), respectively. Determine the transforms of the following signals in terms of X(jw 1, jw2) and H(jw 1, jw2): (i) x(t1 - T1, t2 - T2) (ii) x(at1, bt2) 00 00 (iii) y(tJ, t2) = J_+ J_+ X(T], T2)h(t1 - T], f2- T2)dT1 d T2 00 00 5 THE DISCRETE-TIME FOURIER TRANSFORM 5.0 INTRODUCTION In Chapter 4, we introduced the continuous-time Fourier transform and developed the many characteristics of that transform which make the methods of Fourier analysis of such great value in analyzing and understanding the properties of continuous-time signals and systems. In the current chapter, we complete our development of the basic tools of Fourier analysis by introducing and examining the discrete-time Fourier transform. In our discussion of Fourier series in Chapter 3, we saw that there are many similari- ties and strong parallels in analyzing continuous-time and discrete-time signals. However, there are also important differences. For example, as we saw in Section 3.6, the Fourier series representation of a discrete-time periodic signal is a .finite series, as opposed to the infinite series representation required for continuous-time periodic signals. As we will see in this chapter, there are corresponding differences between continuous-time and discrete- time Fourier transforms. In the remainder of the chapter, we take advantage of the similarities between continuous-time and discrete-time Fourier analysis by following a strategy essentially identical to that used in Chapter 4. In particular, we begin by extending the Fourier se- ries description of periodic signals in order to develop a Fourier transform representation for discrete-time aperiodic signals, and we follow with an analysis of the properties and characteristics of the discrete-time Fourier transform that parallels that given in Chap- ter 4. By doing this, we not only will enhance our understanding of the basic concepts of Fourier analysis that are common to both continuous and discrete time, but also will con- trast their differences in order to deepen our understanding of the distinct characteristics of each. 358 Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 359 5.1 REPRESENTATION OF APERIODIC SIGNALS: THE DISCRETE-TIME FOURIER TRANSFORM 5. 1. 1 Development of the Discrete-Time Fourier Transform In Section 4.1 [eq. (4.2) and Figure 4.2], we saw that the Fourier series coefficients for a continuous-time periodic square wave can be viewed as samples of an envelope function and that, as the period of the square wave increases, these samples become more and more finely spaced. This property suggested representing an aperiodic signal x(t) by first constructing a periodic signal x(t) that equaled x(t) over one period. Then, as this period approached infinity, x(t) was equal to x(t) over larger and larger intervals of time, and the Fourier series representation for x(t) converged to the Fourier transform representation for x(t). In this section, we apply an analogous procedure to discrete-time signals in order to develop the Fourier transform representation for discrete-time aperiodic sequences. Consider a general sequence x[n] that is of finite duration. That is, for some integers N 1 andN2,x[n] = Ooutsidetherange -N1 ::; n::; N2.Asignalofthistypeisillustrated in Figure 5.l(a). From this aperiodic signal, we can construct a periodic sequence x[n] for which x[n] is one period, as illustrated in Figure 5.l(b). As we choose the period N to be larger, x[n] is identical to x[n] over a longer interval, and as N ~ oo, x[n] = x[n] for any finite value of n. Let us now examine the Fourier series representation of x[n]. Specifically, from eqs. (3.94) and (3.95), we have x[n] = L akejk(27r!N)n, (5.1) k=(N) x[n] ......... riiirriillflir ......... n (a) x[n] ...I IIIIriilfffir ...... IIIIrriilffJir ......r lllrrtilffJir ... . (b) Figure 5.1 (a) Finite-duration signal x[n]; (b) periodic signal x[n] con- structed to be equal to x[n] over one period. 360 The Discrete-Time Fourier Transform Chap.5 ak = ~ ~ i[n]e- jk(27T!N)n. (5.2) n=(N) Since x[n] = i[n] over a period that includes the interval -N1 ~ n ~ N2, it is convenient to choose the interval of summation in eq. (5.2) to include this interval, so that i[n] can be replaced by x[n] in the summation. Therefore, 1 N2 . 1 +cc . ak = N ~ x[n]e- Jk(27TIN)n = N ~ x[n]e- Jk(27TIN)n, (5.3) n=-N, n=-cc where in the second equality in eq. (5.3) we have used the fact that x[n] is zero outside the interval - N 1 ~ n ~ N2. Defining the function +cc X(ejw) = ~ x[n]e- jwn, (5.4) 11= -cc we see that the coefficients ak are proportional to samples of X(ejw), i.e., (5.5) where w0 = 21TIN is the spacing of the samples in the frequency domain. Combining eqs. (5.1) and (5.5) yields i[n] = ~ _!__X(ejkw0 )ejkwon. (5.6) k=(N) N Since w0 = 21TIN, or equivalently, liN = w 0121T, eq. (5.6) can be rewritten as (5.7) As with eq. (4.7), as N increases w 0 decreases, and as N ~ oo eq. (5.7) passes to an integral. To see this more clearly, consider X(ejw)ejwn as sketched in Figure 5.2. From Figure 5.2 Graphical interpretation of eq. (5.7). Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 361 eq. (5.4), X(eiw) is seen to be periodic in w with period 21T, and so is eiwn. Thus, the product X(eiw)eiwn will also be periodic. As depicted in the figure, each term in the summation in eq. (5.7) represents the area of a rectangle of height X(efkwo)eiwon and width w 0 . As w 0 ~ 0, the summation becomes an integral. Furthermore, since the summation is carried out over N consecutive intervals of width w 0 = 21TIN, the total interval of integration will always have a width of 21T. Therefore, as N ~ oo, x[n] = x[n], and eq. (5.7) becomes x[n] = - 1 J X(e1.w )eJw. n dw, 21T 27T where, since X(eiw)efwn is periodic with period 21T, the interval of integration can be taken as any interval of length 21T. Thus, we have the following pair of equations: x[n] = -1 J X(e1.w )e1.w n dw, (5.8) 21T 27T +oc X(eiw) = L x[n]e-Jwn. (5.9) n= -oc Equations (5.8) and (5.9) are the discrete-time counterparts of eqs. (4.8) and (4.9). The function X(eiw) is referred to as the discrete-time Fourier transform and the pair of equations as the discrete-time Fourier transform pair. Equation (5.8) is the synthesis equa- tion, eq. (5.9) the analysis equation. Our derivation of these equations indicates how an aperiodic sequence can be thought of as a linear combination of complex exponentials. In particular, the synthesis equation is in effect a representation of x[n] as a linear com- bination of complex exponentials infinitesimally close in frequency and with amplitudes X(eiw)(dwi21T). For this reason, as in continuous time, the Fourier transform X(eiw) will often be referred to as the spectrum of x[n], because it provides us with the information on how x[n] is composed of complex exponentials at different frequencies. Note also that, as in continuous time, our derivation of the discrete-time Fourier transform provides us with an important relationship between discrete-time Fourier series and transforms. In particular, the Fourier coefficients ak of a periodic signal x[n] can be expressed in terms of equally spaced samples of the Fourier transform of a finite-duration, aperiodic signal x[n] that is equal to x[n] over one period and is zero otherwise. This fact is of considerable importance in practical signal processing and Fourier analysis, and we look at it further in Problem 5.41. As our derivation indicates, the discrete-time Fourier transform shares many sim- ilarities with the continuous-time case. The major differences between the two are the periodicity of the discrete-time transform X(eiw) and the finite interval of integration in the synthesis equation. Both of these stem from a fact that we have noted several times be- fore: Discrete-time complex exponentials that differ in frequency by a multiple of 21T are identical. In Section 3.6 we saw that, for periodic discrete-time signals, the implications of this statement are that the Fourier series coefficients are periodic and that the Fourier series representation is a finite sum. For aperiodic signals, the analogous implications are that X(eiw) is periodic (with period 27T) and that the synthesis equation involves an inte- gration only over a frequency interval that produces distinct complex exponentials (i.e., any interval of length 27T). In Section 1.3.3, we noted one further consequence of the pe- 362 The Discrete-Time Fourier Transform Chap.5 riodicity of eiwn as a function of w: w = 0 and w = 27T yield the same signal. Signals at frequencies near these values or any other even multiple of 1T are slowly varying and therefore are all appropriately thought of as low-frequency signals. Similarly, the high frequencies in discrete time are the values of w near odd multiples of 1T. Thus, the signal x1 [n] shown in Figure 5.3(a) with Fourier transform depicted in Figure 5.3(b) varies more slowly than the signal x2 [n] in Figure 5.3(c) whose transform is shown in Figure 5.3(d). 0 n (a) (b) j j ~ I ~ I \ I J I \ J I \ I I I \ I n w (c) (d) Figure 5.3 (a) Discrete-time signal x1 [n]. (b) Fourier transform of x1 [n]. Note that X1 (ejw) is concentrated near w = 0, ±2?T, ±4?T, .... (c) Discrete- time signal x2[n]. (d) Fourier transform of x2[n]. Note that X2(ejw) is concen- trated near w = ± 1r, ±3?T, .... 5.1.2 Examples of Discrete-Time Fourier Transforms To illustrate the discrete-time Fourier transform, let us consider several examples. Example 5.1 Consider the signal x[n] = a""u[n], lal < 1. Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 363 In this case, +co X(ejw) = L anu[n]e- jwn n=-oo GO = """"'(ae-jwt = ----:-- L 1- ae-jw' n=O The magnitude and phase of X(ejw) are shown in Figure 5.4(a) for a> 0 and in Fig- ure 5.4(b) for a< 0. Note that all of these functions are periodic in w with period 27T. w (a) <l:X(Eh tan- 1 (lal//1 - a2 )-......... (b~ -tan-1 (lal/!1 - a2 ) Figure 5.4 Magnitude and phase of the Fourier transform of Example 5.1 for (a) a> 0 and (b) a< 0. 364 The Discrete-Time Fourier Transform Chap.5 Example 5.2 Let x[n] = alnl, iai < 1. This signal is sketched for 0 < a < 1 in Figure 5.5(a). Its Fourier transform is obtained from eq. (5.9): +x X(ejw) = L alnle- jwn n=-'X; X -1 = Lane-jwn + L a-""e-Jwn. n=O n=-x x[n] 0 n (a) X(eiw) (Ha)/(1-a) (b) Figure 5.5 (a) Signal x[n] = alnl of Example 5.2 and (b) its Fourier trans- form (0 < a < 1) . Sec. 5.1 Representation of Aperiodic Signals: The Discrete-Time Fourier Transform 365 Making the substitution of variables m = - n in the second summation, we obtain X 00 X(eiw) = '2_(ae-Jwt + '2_(aeiw)m. n=O m=I Both of these summations are infinite geometric series that we can evaluate in closed form, yielding aeiw X ( eiw) = + ----,---- 1- ae-Jw 1- aeiw 1 - a2 1 - 2a cos w + a2 • In this case, X(eiw) is real and is illustrated in Figure 5.5(b), again for 0 <a < 1. Example 5.3 Consider the rectangular pulse x[n] = { ~: (5.10) which is illustrated in Figure 5.6(a) for N1 = 2. In this case, Nl X(eiw) = 2_ e- Jwn. (5.11) n=-Nl x[n] .................. II]II ................. . -N1 0 N1 n (a) (b) Figure 5.6 (a) Rectangular pulse signal of Example 5.3 for N1 = 2 and (b) its Fourier transform. 366 The Discrete-Time Fourier Transform Chap.5 Using calculations similar to those employed in obtaining eq. (3.104) in Example 3.12, we can write . sinw (N1 + ~) X(eJw) = sin(w/2) · (5.12) This Fourier transform is sketched in Figure 5.6(b) for N 1 = 2. The function in eq. (5.12) is the discrete-time counterpart of the sine function, which appears in the Fourier trans- form of the continuous-time rectangular pulse (see Example 4.4). An important differ- ence between these two functions is that the function in eq. (5.12) is periodic with period 27T, whereas the sine function is aperiodic. 5.1.3 Convergence Issues Associated with the Discrete-Time Fourier Transform Although the argument we used to derive the discrete-time Fourier transform in Sec- tion 5.1.1 was constructed assuming that x[n] was of arbitrary but finite duration, eqs. (5.8) and (5.9) remain valid for an extremely broad class of signals with infinite duration (such as the signals in Examples 5.1 and 5.2). In this case, however, we again must con- sider the question of convergence of the infinite summation in the analysis equation (5.9). The conditions on x[n] that guarantee the convergence of this sum are direct counterparts of the convergence conditions for the continuous-time Fourier transform.' Specifically, eq. (5.9) will converge either if x[n] is absolutely summable, that is, +oc L ix[n]i < oo, (5.13) n= -oo or if the sequence has finite energy, that is, (5.14) n= -oo In contrast to the situation for the analysis equation (5.9), there are generally no convergence issues associated with the synthesis equation (5.8), since the integral in this equation is over a finite interval of integration. This is very much the same situation as for the discrete-time Fourier series synthesis equation (3.94), which involves a finite sum and consequently has no issues of convergence associated with it either. In particular, if we approximate an aperiodic signal x[n] by an integral of complex exponentials with frequencies taken over the intervallwl :s W, i.e., x[n] = -1 Jw X(e1.w )eJW. n dw, (5.15) 27T -w 1F or discussions of the convergence issues associated with the discrete-time Fourier transform, see A. V. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1989), and L. R. Rabiner and B. Gold, Theory and Application ofD igital Signal Processing (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1975). Sec. 5.2 The Fourier Transform for Periodic Signals 367 then x[n] = x[n] for W = 1r. Thus, much as in Figure 3.18, we would expect not to see any behavior like the Gibbs phenomenon in evaluating the discrete-time Fourier transform synthesis equation. This is illustrated in the following example. Example 5.4 Let x[ n] be the unit impulse; that is, x[n] = 8[n]. In this case the analysis equation (5.9) is easily evaluated, yielding In other words, just as in continuous time, the unit impulse has a Fourier transform repre- sentation consisting of equal contributions at all frequencies. If we then apply eq. (5.15) to this example, we obtain x""[ n] _ --1 Iw e jwn d w_- -sin- W n. (5.16) 27T -w 7Tn This is plotted in Figure 5. 7 for several values of W. As can be seen, the frequency of the oscillations in the approximation increases as W is increased, which is similar to what we observed in the continuous-time case. On the other hand, in contrast to the continuous- time case, the amplitude of these oscillations decreases relative to the magnitude of .X[O] as W is increased, and the oscillations disappear entirely for W = 7T. 5.2 THE FOURIER TRANSFORM FOR PERIODIC SIGNALS As in the continuous-time case, discrete-time periodic signals can be incorporated within the framework of the discrete-time Fourier transform by interpreting the transform of a periodic signal as an impulse train in the frequency domain. To derive the form of this representation, consider the signal (5.17) In continuous time, we saw that the Fourier transform of eiwot can be interpreted as an impulse at w = w 0 . Therefore, we might expect the same type of transform to result for the discrete-time signal of eq. (5.17). However, the discrete-time Fourier transform must be periodic in w with period 21r. This then suggests that the Fourier transform of x[n] in eq. (5.17) should have impulses at w 0, w 0 ± 21T, w 0 ± 41T, and so on. In fact, the Fourier transform of x[n] is the impulse train +oo X(eiw) = ~ 21T o(w - Wo - 21Tl), (5.18) l= -00 x[n] x[n] w = 11""/4 w = 311""/8 -~ 1 t ...... ...... .:rr_rr,. . .......... . 0 00 • ••• 0 ••• • n u •n•• ••••••.,)i!Ir.,.•••••• h 000 ••h 0 n (a) (b) w = 11""/2 3 1 4 2 n (c) (d) x[n] 7 8 0 n (e) (f) Figure s. 7 Approximation to the unit sample obtained as in eq. (5.16) using complex exponentials with frequencies lwl ~ W: (a) W = 'TT'/4; (b) W = 37r/8; (c) W = 1r!2; (d) W = 31T'/4; (e) W = ?1r/8; (f) W = 1r. Note that for W = 1r, x[n] = S[n]. Sec. 5.2 The Fourier Transform for Periodic Signals 369 which is illustrated in Figure 5.8. In order to check the validity of this expression, we must evaluate its inverse transform. Substituting eq. (5.18) into the synthesis equation (5.8), we find that w Figure 5.8 Fourier transform of x[n] = eiwan. Note that any interval of length 27T includes exactly one impulse in the summation given in eq. (5.18). Therefore, if the interval of integration chosen includes the impulse located at w 0 + 21Tr, then Now consider a periodic sequence x[n] with period Nand with the Fourier series representation x[n] = ~ akejk(2TTIN)n. (5.19) k=(N) In this case, the Fourier transform is (5.20) so that the Fourier transform of a periodic signal can be directly constructed from its Fourier coefficients. To verify that eq. (5.20) is in fact correct, note that x[n] in eq. (5.19) is a linear combination of signals of the form in eq. (5.17), and thus the Fourier transform of x[n] must be a linear combination of transforms of the form of eq. (5.18). In particular, suppose that we choose the interval of summation in eq. (5.19) ask = 0, 1, ... , N- 1, so that x[n] = ao + al ej(2TT/N)n + a2ej2(2TT!N)n (5.21) + ... + aN-lej(N-l)(2TTIN)n. 370 The Discrete-Time Fourier Transform Chap.5 Thus, x[n] is a linear combination of signals, as in eq. (5.17), with w 0 = 0, 27T/N, 47T/N, ... , (N - 1)27r/N. The resulting Fourier transform is illustrated in Figure 5.9. In Figure 5.9(a), we have depicted the Fourier transform of the first term on the right-hand side of eq. (5.21): The Fourier transform of the constant signal a 0 = a 0 ejO·n is a periodic impulse train, as in eq. (5.18), with w 0 = 0 and a scaling of27Tao on each of the impulses. Moreover, from Chapter 4 we know that the Fourier series coefficients a k are periodic with period N, so that 27Ta0 = 27TaN = 27Ta-N. In Figure 5.9(b) we have illustrated the Fourier transform of the second term in eq. (5.21), where we have again used eq. (5.18), 21ra0 = 21Ta_N 21ra0 21ra0 = 21raN 1 1 1 21T 0 21T w (a) 21ra1 = 21ra_N + 1 21ra1 21ra1 = 21raN + 1 t t t w ( (-N + 1) 2~) (2~) 2 ((N + 1) ~) (b) 2 ( (-N- 1) ~ ) 2 (- 2~) ( (N- 1) ~ ) l l l w 21TaN_1 = 21Ta_N _ 1 21TaN_1 = 21Ta_1 21TaN- 1 (c) 21Ta_N 21ra0 21raN 21Ta_N+1 21ra1 21TaN+1 (d) Figure 5. 9 Fourier transform of a discrete-time periodic signal: (a) Fourier transform of the first term on the right-hand side of eq. (5.21 ); (b) Fourier transform of the sec- ond term in eq. (5.21 ); (c) Fourier transform of the last term in eq. (5.21 ); (d) Fourier transform of x[n] in eq (5.21 ). Sec. 5.2 The Fourier Transform for Periodic Signals 371 in this case for a1ei(27TIN)n, and the fact that 21Ta1 = 21TaN+I = 21Ta-N+I· Similarly, Figure 5.9(c) depicts the final term. Finally, Figure 5.9(d) depicts the entire expression for X(eiw). Note that because of the periodicity of the ah X(eiw) can be interpreted as a train of impulses occurring at multiples of the fundamental frequency 21T/N, with the area of the impulse located at w = 21TkiN being 21Tab which is exactly what is stated in eq. (5.20). Example 5.5 Consider the periodic signal 27T with w 0 = 5 . (5.22) From eq. (5.18), we can immediately write (5.23) That is, X( e1.w ) = 7T 8 (w - 27T) + 7T 8 (w + 27T) 5 5 , (5.24) and X(ejw) repeats periodically with a period of 27T, as illustrated in Figure 5.10. X(eiw) 1T r t t I t I -21T t -wo 0 wo r 21T r w (-21T-w0) (-21T+w0) (21r-w0) (21T+w0) Figure 5. 1 0 Discrete-time Fourier transform of x[n] = cos won. Example 5.6 The discrete-time counterpart of the periodic impulse train of Example 4.8 is the se- quence +oo x[n] = L 8[n- kN], (5.25) k=-X as sketched in Figure 5.11(a). The Fourier series coefficients for this signal can be cal- culated directly from eq. (3.95): ak = ~ L x[n]e- jk(27TIN)n. n=(N) 372 The Discrete-Time Fourier Transform Chap.5 Choosing the interval of summation as 0 :::; n :::; N- 1, we have (5.26) Using eqs. (5.26) and (5.20), we can then represent the Fourier transform of the signal as (5.27) which is illustrated in Figure 5.1l(b). x[n] ... J 1J J J ... •••• ••••••••• ••••••••• ••••••••• ••••••• -N 0 N 2N n (a) X(eij 21r1N w (b) Figure 5.11 (a) Discrete-time periodic impulse train; (b) its Fourier transform. 5.3 PROPERTIES OF THE DISCRETE-TIME FOURIER TRANSFORM As with the continuous-time Fourier transform, a variety of properties of the discrete-time Fourier transform provide further insight into the transform and, in addition, are often useful in reducing the complexity in the evaluation of transforms and inverse transforms. In this and the following two sections we consider these properties, and in Table 5.1 we present a concise summary of them. By comparing this table with Table 4.1, we can get a clear picture of some of the similarities and differences between continuous-time and discrete-time Fourier transform properties. When the derivation or interpretation of a discrete-time Fourier transform property is essentially identical to its continuous-time counterpart, we will simply state the property. Also, because of the close relationship between the Fourier series and the Fourier transform, many of the transform properties Sec. 5.3 Properties of the Discrete-Time Fourier Transform 373 translate directly into corresponding properties for the discrete-time Fourier series, which we summarized in Table 3.2 and briefly discussed in Section 3.7. In the following discussions, it will be convenient to adopt notation similar to that used in Section 4.3 to indicate the pairing of a signal and its transform. That is, X(ejw) = ~{x[n]}, x[n] = ~- 1 {X(ejw)}, ~ . x[n] ~ X(e 1w). 5.3.1 Periodicity of the Discrete-Time Fourier Transform As we discussed in Section 5.1, the discrete-time Fourier transform is always periodic in w with period 27T; i.e., (5.28) This is in contrast to the continuous-time Fourier transform, which in general is not peri- odic. 5.3.2 Linearity of the Fourier Transform If and then (5.29) 5.3.3 Time Shifting and Frequency Shifting If ~ . x[n] ~ X(e1w), then (5.30) 374 The Discrete-Time Fourier Transform Chap.5 and (5.31) Equation (5.30) can be obtained by direct substitution of x[n- n0 ] into the analysis equa- tion (5.9), while eq. (5.31) is derived by substituting X(ej(w-wo>) into the synthesis equa- tion (5.8). As a consequence ofthe periodicity and frequency-shifting properties of the discrete- time Fourier transform, there exists a special relationship between ideallowpass and ideal highpass discrete-time filters. This is illustrated in the next example. Example 5.7 In Figure 5.12(a) we have depicted the frequency response H1p(eiw) of a lowpass filter with cutoff frequency We. while in Figure 5.12(b) we have displayed H1p(ei<w-7T))-that is, the frequency response H1p(eiw) shifted by one-half period, i.e., by 7T. Since high frequencies in discrete time are concentrated near 7T (and other odd multiples of 7T), the filter in Figure 5 .12(b) is an ideal high pass filter with cutoff frequency 7T - w c. That is, (5.32) As we can see from eq. (3.122), and as we will discuss again in Section 5.4, the frequency response of an LTI system is the Fourier transform of the impulse response of the system. Thus, if h1p[n] and hhp[n] respectively denote the impulse responses of ~ H,p(<,iw) Q I I Q -27T -7T -we We 7T 27T w (a) Hlp(ei(w-1T)) q ±1 I -27T -7T \ p I 27T w -(7T-we) (7T-we) (b) Figure 5.12 (a) Frequency response of a lowpass filter; (b) frequency re- sponse of a highpass filter obtained by shifting the frequency response in (a) by w = 1r corresponding to one-half period. Sec. 5.3 Properties of the Discrete-Time Fourier Transform 375 Figure 5.12, eq. (5.32) and the frequency-shifting property imply that the lowpass and highpass filters in hhp[n] = eJ7Tn hip[n] (5.33) = ( -lY hip[n]. (5.34) 5.3.4 Conjugation and Conjugate Symmetry If then x * [n] ~~ X * (e- 1.w ). (5.35) Also, if x[n] is real valued, its transform X(ejw) is conjugate symmetric. That is, (5.36) From this, it follows that ffie{X(ejw)} is an even function of w and 9m{X(ejw)} is an odd function of w. Similarly, the magnitude of X(ejw) is an even function and the phase angle is an odd function. Furthermore, ~ . 8v{x[n]} ~ ffie{X(elw)} and where 8v and fJd denote the even and odd parts, respectively, of x[n]. For example, if x[n] is real and even, its Fourier transform is also real and even. Example 5.2 illustrates this symmetry for x[n] = alnl. 5.3.5 Differencing and Accumulation In this subsection, we consider the discrete-time counterpart of integration-that is, accumulation-and its inverse, first differencing. Let x[n] be a signal with Fourier trans- form X(ejw). Then, from the linearity and time-shifting properties, the Fourier transform pair for the first-difference signal x[n] - x[n - 1] is given by ~ . . x[n] - x[n- 1] ~ (1 - e-JW)X(elw). (5.37) 376 The Discrete-Time Fourier Transform Chap.5 Next, consider the signal n y[n] = ~ x[m]. (5.38) m= -oo Since y[n] - y[n - 1] = x[n], we might conclude that the transform of y[n] should be related to the transform of x[n] by division by (1 - e-Jw). This is partly correct, but as with the continuous-time integration property given by eq. (4.32), there is more involved. The precise relationship is (5.39) The impulse train on the right-hand side of eq. (5.39) reflects the de or average value that can result from summation. Example 5.8 Let us derive the Fourier transform X(ejw) of the unit step x[n] = u[n] by making use of the accumulation property and the knowledge that ~ . g[n] = 8[n] ~ G(e1w) = 1. From Section 1.4.1 we know that the unit step is the running sum of the unit impulse. That is, n x[n] = L g[m]. m=-x Taking the Fourier transform of both sides and using accumulation yields X(ejw) = (1 _ ~- jw) G(ejw) + 7TG(ej0 ) i 8(w - 27Tk) k=-OC 1 X 1 _ e-jw + 7T L 8(w- 27Tk). k=-X 5.3.6 Time Reversal Let x[n] be a signal with spectrum X(eiw), and consider the transform Y(eiw) of y[n] = x[ -n]. From eq. (5.9), +oo +oo Y(eiw) = ~ y[n]e- jwn = ~ x[-n]e-Jwn. (5.40) n= -oo n= -oo Substituting m = -n into eq. (5.40), we obtain +oo Y(eiw) = ~ x[m]e- j(-w)m = X(e- jw). (5.41) m= -oo Sec. 5.3 Properties of the Discrete-Time Fourier Transform 377 That is, (5.42) 5. 3. 7 Time Expansion Because of the discrete nature of the time index for discrete-time signals, the relation be- tween time and frequency scaling in discrete time takes on a somewhat different form from its continuous-time counterpart. Specifically, in Section 4.3.5 we derived the continuous- time property g: r1 (jw) x(at) ~ arx ---;;- . (5.43) However, if we try to define the signal x[an], we run into difficulties if a is not an integer. Therefore, we cannot slow down the signal by choosing a < 1. On the other hand, if we let a be an integer other than± 1-for example, if we consider x[2n]-we do not merely speed up the original signal. That is, since n can take on only integer values, the signal x[2n] consists of the even samples of x[n] alone. There is a result that does closely parallel eq. (5.43), however. Let k be a positive integer, and define the signal _ { x[nl k], if n is a multiple of k x(k) [ n ] - 0, (5.44) if n is not a multiple of k. As illustrated in Figure 5.13 fork = 3, X(k)[n] is obtained from x[n] by placing k- 1 zeros between successive values of the original signal. Intuitively, we can think of x(k)[n] as a slowed-down version of x[n]. Since x(k)[n] equals 0 unless n is a multiple of k, i.e., unless n = rk~ we see that the Fourier transform of x(k)[n] is given by +oo +oo x(k)(e1(J)) = L X(k)[n]e- jwn = L X(k)[rk]e- jwrk. n= -oo r= -oo x[n] Ill!!llriii n -1 1 X~~ I .l .... I. . I l. . I. Figure 5.13 The signal x(3)[n] ob- I r• • I. ... r• • I. ... tained from x[n] by inserting two zeros between successive values of the -6 -3 0 3 6 n original signal. 378 The Discrete-Time Fourier Transform Chap.5 Furthermore, since x<k>[rk] = x[r], we find that +x x(k)(ejw) = L x[r]e- j(kw)r = X(e.ikw). r=-x That is, (5.45) Note that as the signal is spread out and slowed down in time by taking k > 1, its Fourier transform is compressed. For example, since X ( e.iw) is periodic with period 27T, X(e.ikw) is periodic with period 27T/ k. This property is illustrated in Figure 5.14 for a rectangular pulse. x[n] __.. _______ :::-_ 0 n 0 n w JL 7T 2 -------- -- -- 0 -- -- -------- n w Figure 5.14 Inverse relationship between the time and frequency domains: As k in- creases, x(k) [n] spreads out while its transform is compressed. Sec. 5.3 Properties of the Discrete-Time Fourier Transform 379 Example 5.9 As an illustration of the usefulness of the time-expansion property in determining Fourier transforms, let us consider the sequence x[n] displayed in Figure 5.15(a). This sequence can be related to the simpler sequence y[n] depicted in Figure 5.15(b). In particular x[n] = Y(2)[n] + 2y(2)[n- 1], where _ { y[n/2], if n is even Y(2) [ n ] - 0 , 1"" f n I·S 0 dd and y(2)[n - 1] represents y(2)[n] shifted one unit to the right. The signals y(2)[n] and 2y(2)[n- 1] are depicted in Figures 5.15(c) and (d), respectively. Next, note that y[n] = g[n - 2], where g[n] is a rectangular pulse as considered in Example 5.3 (with N 1 = 2) and as depicted in Figure 5.6(a). Consequently, from Example 5.3 and the time-shifting property, we see that Y( jw) = _ j 2w sin(5w/2) e e sin(w/2) · x[n] 11 21 • I I I I I 1 I I 0 2 3 4 5 6 7 8 9 • • n (a) y[n] 11 • I I I I 0 2 3 4 •5 •6 •7 •8 •9 • • n (b) Y(2)[n] 11 • • I I I I 0 1 2 3• 4 •5 6 •7 8 9• • • n (c) 2y(2)[n-1] 2I • •0 • I • I • I I 2 3 4 5 6 7 8• 9 • • n (d) Figure 5.15 (a) The signal x[n] in Example 5.9; (b) the signal y[n]; (c) the signal Y(2)[n] obtained by inserting one zero between successive values of y[n]; and (d) the signal 2y(2)[n- 1] . 380 The Discrete-Time Fourier Transform Chap.5 Using the time-expansion property, we then obtain ~ _ J4w sin(5w) y(2)[n] ~ e -.- (-) , smw and using the linearity and time-shifting properties, we get 2 , [ _ 1] ~ 2 _ JSw sin(5w) )( 2J n e sin(w) · Combining these two results, we have X(eiw) = e-J4w(l + 2e-Jw)(si~(5w))· sm(w) 5.3.8 Differentiation in Frequency Again, let If we use the definition of X(ejw) in the analysis equation (5.9) and differentiate both sides, we obtain L+x - jnx[n]e- jwn. n= -x The right-hand side of this equation is the Fourier transform of- jnx[n]. Therefore, mul- tiplying both sides by j, we see that g: dX(ejw) nx[n] ~ j dw . (5.46) The usefulness of this property will be illustrated in Example 5.13 in Section 5 .4. 5.3.9 Parseval's Relation If x[n] and X(ejw) are a Fourier transform pair, then (5.47) We note that this is similar to eq. (4.43), and the derivation proceeds in a similar man- ner. The quantity on the left-hand side of eq. (5.47) is the total energy in the signal x[n], and Sec. 5.3 Properties of the Discrete-Time Fourier Transform 381 Parse val's relation states that this energy can also be determined by integrating the energy per unit frequency, iX(eiw)i 2127T, over a full27T interval of distinct discrete-time frequen- cies. In analogy with the continuous-time case, iX(eiw)i 2 is referred to as the energy-density spectrum of the signal x[n]. Note also that eq. (5.47) is the counterpart for aperiodic sig- nals of Parseval's relation, eq. (3.110), for periodic signals, which equates the average power in a periodic signal with the sum of the average powers of its individual harmonic components. Given the Fourier transform of a sequence, it is possible to use Fourier transform properties to determine whether a particular sequence has a number of different properties. To illustrate this idea, we present the following example. Example 5. 1 0 Consider the sequence x[n] whose Fourier transform X(eiw) is depicted for -7r ::::; w ::::; 7r in Figure 5.16. We wish to determine whether or not, in the time domain, x[n] is periodic, real, even, and/or of finite energy. IX(eiw)l _....____L~:l""'""------+-~----""--- w _1I. 1I. 2 (a) 2 (b) Figure 5. 16 Magnitude and phase of the Fourier transform for Exam- ple5.10. 382 The Discrete-Time Fourier Transform Chap.5 Accordingly, we note first that periodicity in the time domain implies that the Fourier transform is zero, except possibly for impulses located at various integer multi- ples of the fundamental frequency. This is not true for X(eiw). We conclude, then, that x[n] is not periodic. Next, from the symmetry properties for Fourier transforms, we know that a real- valued sequence must have a Fourier transform of even magnitude and a phase function that is odd. This is true for the given IX(eiw)l and 1-:X(eiw). We thus conclude that x[n] is real. Third, if x[n] is an even function, then, by the symmetry properties forreal signals, X(eiw) must be real and even. However, since X(eiw) = IX(eiw)le- i 2w, X(eiw) is not a real-valued function. Consequently, x[n] is not even. Finally, to test for the finite-energy property, we may use Parseval's relation, It is clear from Figure 5.16 that integrating IX(eiw)l2 from -w to 1r will yield a finite quantity. We conclude that x[n] has finite energy. In the next few sections, we consider several additional properties. The first two of these are the convolution and multiplication properties, similar to those discussed in Sections 4.4 and 4.5. The third is the property of duality, which is examined in Section 5.7, where we consider not only duality in the discrete-time domain, but also the duality that exists between the continuous-time and discrete-time domains. 5.4 THE CONVOLUTION PROPERTY In Section 4.4, we discussed the importance of the continuous-time Fourier transform with regard to its effect on the operation of convolution and its use in dealing with continuous- time LTI systems. An identical relation applies in discrete time, and this is one of the principal reasons that the discrete-time Fourier transform is of such great value in repre- senting and analyzing discrete-time LTI systems. Specifically, if x[n], h[n], and y[n] are the input, impulse response, and output, respectively, of an LTI system, so that y[n] = x[n] * h[n], then (5.48) where X(ejw), H(ejw), and Y(ejw) are the Fourier transforms of x[n], h[n], and y[n], re- spectively. Furthermore, comparing eqs. (3.122) and (5.9), we see that the frequency re- sponse of a discrete-time LTI system, as first defined in Section 3.8, is the Fourier transform of the impulse response of the system. The derivation of eq. (5.48) exactly parallels that carried out in Section 4.4. In par- ticular, as in continuous time, the Fourier synthesis equation (5.8) for x[n] can be inter- Sec. 5.4 The Convolution Property 383 preted as a decomposition of x[n] into a linear combination of complex exponentials with infinitesimal amplitudes proportional to X(eiw). Each of these exponentials is an eigen- function of the system. In Chapter 3, we used this fact to show that the Fourier series coefficients of the response of an LTI system to a periodic input are simply the Fourier coefficients of the input multiplied by the system's frequency response evaluated at the corresponding harmonic frequencies. The convolution property (5.48) represents the ex- tension of this result to aperiodic inputs and outputs by using the Fourier transform rather than the Fourier series. As in continuous time, eq. (5.48) maps the convolution of two signals to the simple algebraic operation of multiplying their Fourier transforms, a fact that both facilitates the analysis of signals and systems and adds significantly to our understanding of the way in which an LTI system responds to the input signals that are applied to it. In particular, from eq. (5.48), we see that the frequency response H(eiw) captures the change in complex amplitude of the Fourier transform of the input at each frequency w. Thus, in frequency- selective filtering, for example, we want H(eiw) = 1 over the range of frequencies cor- responding to the desired passband and H(eiw) = 0 over the band of frequencies to be eliminated or significantly attenuated. 5.4. 1 Examples To illustrate the convolution property, along with a number of other properties, we consider several examples in this section. Example 5. 11 Consider an LTI system with impulse response h[n] = 8[n- no]. The frequency response is +""' H(eiw) = L 8[n- no]e- jwn = e- jwno. n=-x Thus, for any input x[n] with Fourier transform X(eiw), the Fourier transform of the output is (5.49) We note that, for this example, y[n] = x[n - n0 ] and eq. (5.49) is consistent with the time-shifting property. Note also that the frequency response H(eiw) = e-Jwno of a pure time shift has unity magnitude at all frequencies and a phase characteristic -wn0 that is linear with frequency. Example 5. 12 Consider the discrete-time ideal lowpass filter introduced in Section 3.9.2. This sys- tem has the frequency response H(eiw) illustrated in Figure 5.17(a). Since the impulse 384 The Discrete-Time Fourier Transform Chap.5 response and frequency response of an LTI system are a Fourier transform pair, we can determine the impulse response of the ideallowpass filter from the frequency response using the Fourier transform synthesis equation (5.8). In particular, using -7r ~ w ::::; 1r as the interval of integration in that equation, we see from Figure 5.17(a) that (5.50) which is shown in Figure 5.17(b). H(eiw) ,I , I II -2'IT -'IT -wo 0 wo 'IT 2'IT w (a) h[n] •••••••••••••••.,,Ttltt,,.•••••••••••••••n (b) Figure 5. 17 (a) Frequency response of a discrete-time ideal lowpass filter; (b) impulse response of the ideal lowpass filter. In Figure 5.17, we come across many of the same issues that surfaced with the continuous-time ideallowpass filter in Example 4.18. First, since h[n] is not zero for n < 0, the ideallowpass filter is not causal. Second, even if causality is not an important is- sue, there are other reasons, including ease of implementation and preferable time domain characteristics, that nonideal filters are generally used to perform frequency-selective fil- tering. In particular, the impulse response of the ideallowpass filter in Figure 5.17 (b) is oscillatory, a characteristic that is undesirable in some applications. In such cases, a trade- off between frequency-domain objectives such as frequency selectivity and time-domain properties such as nonoscillatory behavior must be made. In Chapter 6, we will discuss these and related ideas in more detail. As the following example illustrates, the convolution property can also be of value in facilitating the calculation of convolution sums. Sec. 5.4 The Convolution Property 385 Example 5. 1 3 Consider an LTI system with impulse response with Ia I < 1, and suppose that the input to this system is x[n] = f3nu[n], with l/31 < 1. Evaluating the Fourier transforms of h[n] and x[n], we have H(ejw) = 1 . (5.51) 1-ae-Jw and (5.52) so that (5.53) As with Example 4.19, determining the inverse transform of Y(ejw) is most easily done by expanding Y(ejw) by the method of partial fractions. Specifically, Y(ejw) is a ratio of polynomials in powers of e- jw, and we would like to express this as a sum of simpler terms of this type so that we can find the inverse transform of each term by inspection (together, perhaps, with the use of the frequency differentiation property of Section 5.3.8). The general algebraic procedure for rational transforms is described in the appendix. For this example, if a ¥- f3, the partial fraction expansion of Y ( ejw) is of the form A Y(ejw) = + B .. 1- (5.54) ae-jw 1- f3e-1w Equating the right-hand sides of eqs (5.53) and (5.54), we find that A= _a_ B = __/3 _. a- f3' a-{3 Therefore, from Example 5.1 and the linearity property, we can obtain the inverse trans- form of eq. (5.54) by inspection: y[n] = _a_anu[n]- _f3_f3 11 u[n] a-{3 a-{3 (5.55) 1 = --[an+I u[n]- f3n+I u[n]]. a-{3 For a = f3, the partial-fraction expansion in eq. ( 5.54) is not valid. However, in this case, Y ( ejw) = ( 1 1 - . )2 ' - ae JW 386 The Discrete-Time Fourier Transform Chap.5 which can be expressed as Y(eiw) = j jw d ( 1 ) ~e 1 - (5.56) dw ae-Jw · As in Example 4.19, we can use the frequency differentiation property, eq. (5.46), together with the Fourier transform pair ~ anu[n] ~ ---,---- 1-ae-Jw' to conclude that nanu[n]~ J-.d( 1) w - ae _ .. 1 JW To account for the factor eiw, we use the time-shifting property to obtain (n + 1)an+ 1 u[n + 1] ~~ je.lwd_( 1.) , dw 1- ae-Jw and finally, accounting for the factor 1/a, in eq. (5.56), we obtain y[n] = (n + l)anu[n + 1]. (5.57) It is worth noting that, although the right-hand side is multiplied by a step that begins at n = -1, the sequence (n + 1)anu[n + 1] is still zero prior ton = 0, since the factor n + 1 is zero at n = -1. Thus, we can alternatively express y[n] as y[n] = (n + 1)anu[n]. (5.58) As illustrated in the next example, the convolution property, along with other Fourier transform properties, is often useful in analyzing system interconnections. Example 5. 14 Consider the system shown in Figure 5.18(a) with input x[n] and output y[n]. The LTI systems with frequency response H 1p(eiw) are ideallowpass filters with cutoff frequency TT/4 and unity gain in the passband. Let us first consider the top path in Figure 5.18(a). The Fourier transform of the signal w 1 [n] can be obtained by noting that ( -l)n = eJ7Tn so that w 1 [n] = eJ7Tn x[n]. Using the frequency-shifting property, we then obtain WI (eiw) = X(ei(w-7T)). The convolution property yields Wz(eiw) = Hzp(eiw)X(ei(w-7T)). Since w3 [n] = ei7Tnw2 [n], we can again apply the frequency-shifting property to obtain W3(e1w) = Wz(eJ(w-7T)) = Hzp(ei(w-7T))X(ei(w-21T)). Sec. 5.4 The Convolution Property 387 x[n] y[n] w4[n] (a) H(eiw) 1 I I I 1 I I I -'IT _371"" 71"" 2!. 3TI 'IT w 4 4 4 4 (b) Figure 5. 18 (a) System interconnection for Example 5.14; (b) the overall frequency response for this system. Since discrete-time Fourier transforms are always periodic with period 27T, Applying the convolution property to the lower path, we get W4(ejw) = H,p(eiw)X(eiw). From the linearity property of the Fourier transform, we obtain Y(eiw) = W3(ejw) + W4(ejw) = [HJp(ei(w-TT)) + Htp(eiw)]X(eiw). Consequently, the overall system in Figure 5.18(a) has the frequency response H(eiw) = [HJp(ei(w-TT)) + H,p(eiw)] which is shown in Figure 5.18(b). As we saw in Example 5.7, H 1p(ei<w-'TT)) is the frequency response of an ideal highpass filter. Thus, the overall system passes both low and high frequencies and stops frequencies between these two pass bands. That is, the filter has what is often referred to as an ideal bandstop characteristic, where the stopband is the region 7T/4 < lwl < 37T/4. It is important to note that, as in continuous time, not every discrete-time LTI system has a frequency response. For example, the LTI system with impulse response h[n] = 2nu[n] does not have a finite response to sinusoidal inputs, which is reflected in the fact 388 The Discrete-Time Fourier Transform Chap.5 that the Fourier transform analysis equation for h[n] diverges. However, if an LTI system is stable, then, from Section 2.3.7, its impulse response is absolutely summable; that is, +oo L, lh[n]l < oo. (5.59) n=-oo Therefore, the frequency response always converges for stable systems. In using Fourier methods, we will be restricting ourselves to systems with impulse responses that have well- defined Fourier transforms. In Chapter 10, we will introduce an extension of the Fourier transform referred to as the z-transform that will allow us to use transform techniques for LTI systems for which the frequency response does not converge. 5.5 THE MULTIPLICATION PROPERTY In Section 4.5, we introduced the multiplication property for continuous-time signals and indicated some of its applications through several examples. An analogous property exists for discrete-time signals and plays a similar role in applications. In this section, we derive this result directly and give an example of its use. In Chapters 7 and 8, we will use the multiplication property in the context of our discussions of sampling and communications. Consider y[n] equal to the product of x1 [n] and x2[n], with Y(eiw), X1 (eiw), and X2(eiw) denoting the corresponding Fourier transforms. Then +oo +oo Y(eiw) = L y[n]e- Jwn = L XI [n]x2[n]e- jwn, n= -oo n= -oo or since (5.60) it follows that (5.61) Interchanging the order of summation and integration, we obtain Y(ej""') = 2~ L, X, (ej9) [n~oo X2[n]e- j(w-9)n ]dll. (5.62) The bracketed summation is X2(ei(w-O)), and consequently, eq. (5.62) becomes (5.63) Sec. 5.5 The Multiplication Property 389 Equation (5.63) corresponds to a periodic convolution of X1( eiw) and X2(eiw), and the integral in this equation can be evaluated over any interval of length 27T. The usual form of convolution (in which the integral ranges from -oo to +oo) is often referred to as ape- riodic convolution to distinguish it from periodic convolution. The mechanics of periodic convolution are most easily illustrated through an example. Example 5.1 5 Consider the problem of finding the Fourier transform X(eiw) of a signal x[n] which is the product of two other signals; that is, x[n] = x1 [n]x2[n], where sin(37Tn/4) x 1[n] = ---- and x [n] = sin(7Tn/2)_ 2 1Tn From the multiplication property given in eq. (5.63), we know that X(eiw) is the periodic convolution of X1 (eiw) and X2(eiw), where the integral in eq. (5.63) can be taken over any interval of length 27T. Choosing the interval -7r < (}::::; 7T, we obtain (5.64) Equation (5.64) resembles aperiodic convolution, except for the fact that the inte- gration is limited to the interval -7r < (} ::::; 7T. However, we can convert the equation into an ordinary convolution by defining for -7r < w ::::; 7T otherwise Then, replacing X1 (ei0 ) in eq. (5.64) by X1( ei0 ), and using the fact that X1(ei0 ) is zero for I 8 I> 7T, we see that 1 X(eiw) = - - f7T Xl(el0 )X2(ei<w-IJ))d8 27T -7T = - 1- foo X1 (el0 )X2(ei<w-IJ))d8. 27T -00 Thus, X(eiw) is 1/27T times the aperiodic convolution of the rectangular pulse X1 (eiw) and the periodic square wave X2(eiw), both of which are shown in Figure 5.19. The result of this convolution is the Fourier transform X(eiw) shown in Figure 5.20. 390 The Discrete-Time Fourier Transform Chap.5 I -21T 21T w -21T w Figure 5.19 X1 (ei""') representing one period of X1(ei""'), and X2(ei""'). The linear convolution of X1 ( ei""') and X2 ( ei""') corresponds to the periodic convolu- tion of X1 (ei""') and X2(ei""'). .1L .1L 31T 1T w 4 2 4 Figure 5.20 Result of the periodic convolution in Example 5.15. 5.6 TABLES OF FOURIER TRANSFORM PROPERTIES AND BASIC FOURIER TRANSFORM PAIRS In Table 5 .l, we summarize a number of important properties of the discrete-time Fourier transform and indicate the section of the text in which each is discussed. In Table 5.2, we summarize some of the basic and most important discrete-time Fourier transform pairs. Many of these have been derived in examples in the chapter. 5.7 DUALITY In considering the continuous-time Fourier transform, we observed a symmetry or duality between the analysis equation (4.9) and the synthesis equation (4.8). No corresponding duality exists between the analysis equation (5.9) and the synthesis equation (5.8) for the discrete-time Fourier transform. However, there is a duality in the discrete-time Fourier series equations (3.94) and (3.95), which we develop in Section 5.7.1. In addition, there is Sec. 5.7 Duality 391 TABLE 5.1 PROPERTIES OF THE DISCRETE-TIME FOURIER TRANSFORM Section Property Aperiodic Signal Fourier Transform x[n] X(elw)} periodic with y[n] Y(elw) period 277' 5.3.2 Linearity ax[n] + by[n] aX(eiw) + bY(elw) 5.3.3 Time Shifting x[n- no] e- Jw""o X(eiw) 5.3.3 Frequency Shifting e;wo"" x[n] X(ej(w-wol) 5.3.4 Conjugation x*[n] X*(e- ;w) 5.3.6 Time Reversal x[-n] X(e jw) { x[nlk], if n = multiple of k 5.3.7 Time Expansion X(k [n] = X(eikw) 1 0, if n =I= multiple of k 5.4 Convolution x[n] * y[n] X(e1w)Y(e1w) 5.5 Multiplication x[n]y[n] __!___I X(eili)Y(ej(w-IIJ)d(J 277' 27T 5.3.5 Differencing in Time x[n] - x[n- 1] ( 1 - e- ;w)X(e1w) II 5.3.5 Accumulation ~ x[k] __1 _. X(eiw) 1 - e- ;w k = -cc +7TX(ei0 ) ~ o(w- 27Tk) k= -X .dX(eiw) 5.3.8 Differentiation in Frequency nx[n] 1 ------;[;;;- X(eiw) = X*(e- Jw) lffi,e{X(eiw)} = ffi,e{X(e- Jw)} 5.3.4 Conjugate Symmetry for x[n] real 9m{X(eiw)} = -9m{X. (e-Jw)} Real Signals [X (e1w)[ = [X(e- jw)[ <tX(eiw) = -<tX(e-;w) 5.3.4 Symmetry for Real, Even x[n] real an even X ( eiw) real and even Signals 5.3.4 Symmetry for Real, Odd x[n] real and odd X(elw) purely imaginary and Signals odd 5.3.4 Even -odd Decomposition Xe[n] = 8v{x[n]} [x[n] real] ffi,e{X(eiw)} of Real Signals Xo[n] = 0d{x[n]} [x[n] real] j9n-t{X(eiw)} 5.3.9 Parseval's Relation for Aperiodic Signals ,""'!?;"" 2 2 [x[n][ = 2~ Lr [X(eiw)[ dw a duality relationship between the discrete-time Fourier transform and the continuous-time Fourier series. This relation is discussed in Section 5.7.2. 5. 7. 1 Duality in the Discrete-Time Fourier Series Since the Fourier series coefficients ak of a periodic signal x[n] are themselves a periodic sequence, we can expand the sequence ak in a Fourier series. The duality property for discrete-time Fourier series implies that the Fourier series coefficients for the periodic se- quence ak are the values of (1/N)x[- n] (i.e., are proportional to the values of the original TABLE 5.2 BASIC DISCRETE-TIME FOURIER TRANSFORM PAIRS Signal Fourier Transform Fourier Series Coefficients (if periodic) L akejk(2n/N)n k~+eoc ( 2 k) 271' x- aJj w - ~ ak k=(N) (a) _ 2Trm wo -/II +x ejwon L k = m, m ::':: N, m ::':: 2N, ... 271' l>(w - w0 - 271'/) ak { I. 1=-00 = 0, otherwise (b) wo 277 irrational ::? The signal is aperiodic (a) wo = 21Tln /II L+""' l: k = ::'::m, ::':: m ::':: N, ::':: m ::':: 2N, ... cosw0 n 1T {l>(w - wo - 211'1) + l>(w + w0 - 27T/)} ak I= = { -'X- otherwise (b) ~ 2-n irrational ::? The signal is aperiodic (a) wo _ 21Tr -N k = r, r ::':: N, r ::':: 2N, ... sinw0 n ~ f {l>(w - wo - 271'1) - l>(w + w0 - 271'/)} ak = {_ ...t!_ 2j. k = - r, - r ::':: N, - r ::':: 2N, ... j I= -oc 0. otherwise (b) wu 211 irrational ::? The signal is aperiodic +oo L { I, k = 0, ±N, ±2N. ... x[n] =I 271' l>(w - 271'/) (/k = I= 0, otherwise -X Periodic square wave { I, lnl s N, sin[(27Tk/ N)(N 1 + ~ )] x[n] = 2 k) ak = . k =P 0, ±N, ±2N, ... 0, N, < lnl s N/2 +x ( N sin[27Tk/2N] 271' k~x akl> w - ~ and 2N1 +I ak = -N--, k = 0, ±N, ±2N. ... x[n + N] = x[n] +<X L I l>[n- kN] 2: kr;oc l>(w- 2~k) (/k = N for all k k= -oc I anu[n], ial <I - I - ae- jw x[n] c0· lnl s N, sin[w(N1 + ~)] - , lnl >N, sin(w/2) 0 :S lwl :S W sin Wn = ~sine (Wn) X(w) = { I, TTn 1T 1T 0, W < lwl s 1T - O<W<1T X(w) periodic with period 271' l>[n] I - I +""' u[n] - -- e--;w. + L 1Ti>(w - 21Tk) - 1 k= -X l>[n- no] e- jwno - I (n + l)anu[n], ial <I - (I - ae- jw)2 (n+r-1)' I n!(r- 1)!. anu[n], lal <I - (I- ar jwy 392 Sec. 5.7 Duality 393 signal, reversed in time). To see this in more detail, consider two periodic sequences with period N, related through the summation f[m] = ~ L g[r]e- jr(27T!N)m. (5.65) r=(N) If we let m = k and r = n, eq. (5.65) becomes f[k] = ~ L g[n]e- jk(27TIN)n. n=(N) Comparing this with eq. (3.95), we see that the sequence f[k] corresponds to the Fourier series coefficients of the signal g[n]. That is, if we adopt the notation introduced in Chapter 3 for a periodic discrete-time signal and its set of Fourier coeffi- cients, the two periodic sequences related through eq. (5.65) satisfy ~s g[n] ~ f[k]. (5.66) Alternatively, if we let m = nand r = - k, eq. (5.65) becomes f[n] = L _!._ g[- k]ejk(27T!N)n. k=<N>N Comparing this with eq. (3.94), we find that (1/N)g[- k] corresponds to the sequence of Fourier series coefficients of f [n ]. That is, ~s 1 f[n] ~ Ng[ -k]. (5.67) As in continuous time, this duality implies that every property of the discrete-time Fourier series has a dual. For example, referring to Table 3.2, we see that the pair of prop- erties (5.68) and ejm(27TIN)n x[n] ~ ak-m (5.69) are dual. Similarly, from the same table, we can extract another pair of dual properties: L ~s x[r]y[n- r] ~ Nakbk (5.70) r=(N) and ~s x[n]y[n] ~ L azbk-l· (5.71) l=(N) 394 The Discrete-Time Fourier Transform Chap.5 In addition to its consequences for the properties of discrete-time Fourier series, du- ality can often be useful in reducing the complexity of the calculations involved in deter- mining Fourier series representations. This is illustrated in the following example. Example 5.16 Consider the following periodic signal with a period of N = 9: 1 sin(57Tn/9) n =1=- multiple of 9 x[n] ~ !~ sin(>Tn/9) ' (5.72) n = multiple of 9 9' In Chapter 3, we found that a rectangular square wave has Fourier coefficients in a form much as in eq. (5.72). Duality, then, suggests that the coefficients for x[n] must be in the form of a rectangular square wave. To see this more precisely, let g[n] be a rectangular square wave with period N = 9 such that g[n] = { ~: lnl ~ 2 2 < lnl ~ 4. The Fourier series coefficients bk for g[n] can be determined from Example 3.12 as ! sin(57Tk/9) l k =1=- multiple of 9 _ 9 sin( 7Tk/9) ' b k - 5 9' k = multiple of 9 The Fourier series analysis equation (3.95) for g[n] can now be written as 2 bk = ~ ~ (l)e- J27Tnk19. n= -2 Interchanging the names of the variables k and nand noting that x[n] = b11 , we find that 1 2 . x[n] = _ ~ (l)e- 127Tnkt9. 9 k=-2 Letting k' = - k in the sum on the right side, we obtain 2 x[n] = ~ ~ e+ J27Tnk'l9. k'=-2 Finally, moving the factor 119 inside the summation, we see that the right side of this equation has the form of the synthesis equation (3.94) for x[n]. We thus conclude that the Fourier coefficients of x[n] are given by 119 lkl ~ 2 ak = { ' 0, 2 < lkl ~ 4, and, of course, are periodic with period N = 9. Sec. 5.7 Duality 395 5. 7. 2 Duality between the Discrete-Time Fourier Transform and the Continuous-Time Fourier Series In addition to the duality for the discrete Fourier series, there is a duality between the discrete-time Fourier transform and the continuous-time Fourier series. Specifically, let us compare the continuous-time Fourier series equations (3.38) and (3.39) with the discrete- time Fourier transform equations (5.8) and (5.9). We repeat these equations here for con- venience: [eq. (5.8)] x[n] = _1_ f X(eiw)ejwndw, (5.73) 27T 27T +oo [eq. (5.9)] X(eiw) = L x[n]e- Jwn, (5.74) n=-oo +oo [eq. (3.38)] x(t) = L akejkwot, (5.75) k=-00 [eq. (3.39)] ak = _!__ J x(t)e- Jkwotdt. (5.76) T T Note that eqs. (5.73) and (5.76) are very similar, as are eqs. (5.74) and (5.75), and in fact, we can interpret eqs. (5.73) and (5.74) as a Fourier series representation of the periodic frequency response X(eiw). In particular, since X(eiw) is a periodic function of w with period 27T, it has a Fourier series representation as a weighted sum of harmonically related periodic exponential functions of w, all of which have the common period of 27T. That is, X ( eiw) can be represented in a Fourier series as a weighted sum of the signals eiwn, n = 0, ± 1, ±2, .... From eq. (5.74), we see that the nth Fourier coefficient in this expansion-i.e., the coefficient multiplying eJwn_is x[- n]. Furthermore, since the period of X(eiw) is 27T, eq. (5.73) can be interpreted as the Fourier series analysis equation for the Fourier series coefficient x[n]-i.e., for the coefficient multiplying e-Jwn in the expression for X(eiw) in eq. (5.74). The use of this duality relationship is best illustrated with an example. Example 5. 1 7 The duality between the discrete-time Fourier transform synthesis equation and the continuous-time Fourier series analysis equation may be exploited to determine the discrete-time Fourier transform of the sequence x[n] = sin( 7Tn/2). 1Tn To use duality, we first must identify a continuous-time signal g(t) with period T = 21T and Fourier coefficients ak = x[k]. From Example 3.5, we know that if g(t) is a periodic square wave with period 21T (or, equivalently, with fundamental frequency w 0 = 1) and with ~: It! :::; TI g(t) = { TI <It! :::; 1T' then the Fourier series coefficients of g(t) are 396 The Discrete-Time Fourier Transform Chap.5 sin(kTt) k7T Consequently, if we take T1 = 7T/2, we will have ak = x[k]. In this case the analysis equation for g(t) is sin(7rk/2) 1 I7T _ ·kr 1 I7TI2 --- = - g(t)e 1 dt = - (1)e-Jktdt. 7Tk 27T -TT 27T -TT/2 Renaming k as n and t as w, we have sin (7rnl2) 1 I7 T12 . ---- = - (1)e-;nwdw. (5.77) 7Tn 27T -TT/2 Replacing n by-non both sides of eq. (5.77) and noting that the sine function is even, we obtain sin (7rn/2) 1 I7 T12 = - . (l)e;nwdw. 7Tn 27T -TT/2 The right-hand side of this equation has the form of the Fourier transform synthesis equation for x[n], where . { 01 lwl ~ 7T/2 X(elw) = 7T/2 < lwl ~ 7T. In Table 5.3, we present a compact summary of the Fourier series and Fourier trans- form expressions for both continuous-time and discrete-time signals, and we also indicate the duality relationships that apply in each case. TABLE 5.3 SUMMARY OF FOURIER SERIES AND TRANSFORM EXPRESSIONS Continuous time Discrete time Time domain Frequency domain Time domain Frequency domain I x(t) = ak = x[n] = I ak = L.t:-oo I akejkwot fo JT o x(t)e- jkwot Lk~(N) akejk(2TTIN)n ~ Lk~(N) x[n]e- jk(2Tr/N)n I Fourier I Series continuous time di""rete frequency ~ di""<Orele time discrete frequency periodic in time aperiodic in frequency %7/-· periodic in time ~ periodic in frequency I ""'~ I x(t) = X(jw) = x[n) = I X(ejw) = ~ J_ 00 + X(jw )ejwt dw 00 [_+..,"""" x(t)e- jw'dt I -1w 2.;: _ x[n]e- jwn b1T X( eiw )ejwn 00 I Fourier I Transform continuous time ~ I continuous frequency discrete time continuous frequency I aperiodic in time . aperiodic in frequency aperiodic in time I periodic in frequency I I 5.8 SYSTEMS CHARACTERIZED BY LINEAR CONSTANT-COEFFICIENT DIFFERENCE EQUATIONS A general linear constant-coefficient difference equation for an LTI system with input x[n] and output y[n] is of the form Sec. 5.8 Systems Characterized by Linear Constant-Coefficient Difference Equations 397 N M .:Z aky[n - k] = .:Z bkx[n - k]. (5.78) k=O k=O The class of systems described by such difference equations is quite an important and useful one. In this section, we take advantage of several of the properties of the discrete- time Fourier transform to determine the frequency response H ( ejw) for an LTI system described by such an equation. The approach we follow closely parallels the discussion in Section 4.7 for continuous-time LTI systems described by linear constant-coefficient differential equations. There are two related ways in which to determine H(ejw). The first of these, which we illustrated in Section 3.11 for several simple difference equations, explicitly uses the fact that complex exponentials are eigenfunctions of LTI systems. Specifically, if x[n] = ejwn is the input to an LTI system, then the output must be of the form H(ejw)ejwn. Substi- tuting these expressions into eq. (5.78) and performing some algebra allows us to solve for H(ejw). In this section, we follow a second approach making use of the convolution, linear- ity, and time-shifting properties of the discrete-time Fourier transform. Let X ( ejw ), Y ( ejw ), and H(ejw) denote the Fourier transforms of the input x[n], output y[n], and impulse re- sponse h[n], respectively. The convolution property, eq. (5.48), of the discrete-time Fourier transform then implies that H(ejw) = Y(el:w). (5.79) X(eJw) Applying the Fourier transform to both sides of eq. (5.78) and using the linearity and time-shifting properties, we obtain the expression .:NZ ake~ jkw Y(ejw) = .:MZ bke~ jkw X(ejw), k=O k=O or equivalently, ""'M b e~ jkw Lk=O k (5.80) Comparing eq. (5.80) with eq. (4.76), we see that, as in the case of continuous time, H(ejw) is a ratio of polynomials, but in discrete time the polynomials are in the variable e~ jw. The coefficients of the numerator polynomial are the same coefficients as appear on the right side of eq. (5.78), and the coefficients of the denominator polynomial are the same as appear on the left side of that equation. Therefore, the frequency response of the LTI system specified by eq. (5.78) can be written down by inspection. The difference equation (5.78) is generally referred to as an Nth-order difference equation, as it involves delays in the output y[n] of uptoN time steps. Also, the denomi- nator of H(ejw) in eq. (5.80) is an Nth-order polynomial in e~ jw. Example 5. 18 Consider the causal LTI system that is characterized by the difference equation y[n] - ay[n- 1] = x[n], (5.81) 398 The Discrete-Time Fourier Transform Chap.5 with \a\ < 1. From eq. (5.80), the frequency response of this system is 1 H(elw) = ---- (5.82) 1-ae-Jw Comparing this with Example 5.1, we recognize it as the Fourier transform of the se- quence a""u[n]. Thus, the impulse response of the system is (5.83) Example 5. 1 9 Consider a causal LTI system that is characterized by the difference equation 3 1 y[n] - 4 y[n- 1] + Sy [n- 2] = 2x[n]. (5.84) From eq. (5.80), the frequency response is (5.85) As a first step in obtaining the impulse response, we factor the denominator of eq. (5.85): jw - 2 H(e ) - I . I .. (5.86) (1 - 2e- Jw)(l - 4e- Jw) H(eiw) can be expanded by the method of partial fractions, as in Example A.3 in the appendix. The result of this expansion is (5.87) The inverse transform of each term can be recognized by inspection, with the result that h[n] = 4 (21: )"" u[n] - 2 (14) "" u[n]. (5.88) The procedure followed in Example 5.19 is identical in style to that used in contin- uous time. Specifically, after expanding H(e.iw) by the method of partial fractions, we can find the inverse transform of each term by inspection. The same approach can be applied to the frequency response of any LTI system described by a linear constant-coefficient dif- ference equation in order to determine the system impulse response. Also, as illustrated in the next example, if the Fourier transform X ( e.iw) of the input to such a system is a ratio of polynomials in e- .iw, then Y(e.iw) is as well. In this case, we can use the same technique to find the response y[n] to the input x[n]. Example 5.20 Consider the LTI system of Example 5.19, and let the input to this system be 11 x[n] = (~ ) u[n]. Sec. 5.9 Summary 399 Then, using eq. (5.80) and Example 5.1 or 5.18, we obtain (5.89) (1 - ~e- jw)(1 - ~e-ew)2 · As described in the appendix, the form of the partial-fraction expansion in this case is Y(ejw) = B11 + BI2 + ~21 ' (5.90) 1- ~e-jw (1- ~e-jw)2 1- 2e-jw where the constants Btt. B12, and B2 1 can be determined using the techniques described in the appendix. This particular expansion is worked out in detail in Example A.4, and the values obtained are B11 = -4, B12 = -2, B21 = 8, so that . 4 2 8 Y(elw) = - - + . (5.91) 1- ~e-jw (1- ~e-jw)2 1- ~e-jw The first and third terms are of the same type as those encountered in Example 5.19, while the second term is of the same form as one seen in Example 5.13. Either from these examples or from Table 5.2, we can invert each of the terms in eq. (5.91) to obtain the inverse transform y[n] ~ { -4(U- 2(n + l)(U + s(U} u[n]. (5.92) 5.9 SUMMARY In this chapter, we have paralleled Chapter 4 as we developed the Fourier transform for discrete-time signals and examined many of its important properties. Throughout the chap- ter, we have seen a great many similarities between continuous-time and discrete-time Fourier analysis, and we have also seen some important differences. For example, the re- lationship between Fourier series and Fourier transforms in discrete time is exactly anal- ogous to that in continuous time. In particular, our derivation of the discrete-time Fourier transform for aperiodic signals from the discrete-time Fourier series representations is very much the same as the corresponding continuous-time derivation. Furthermore, many of the properties of continuous-time transforms have exact discrete-time counterparts. On the other hand, in contrast to the continuous-time case, the discrete-time Fourier transform of an aperiodic signal is always periodic with period 27T. In addition to similarities and differences such as these, we have described the duality relationships among the Fourier representations of continuous-time and discrete-time signals. The most important similiarities between continuous- and discrete-time Fourier anal- ysis are in their uses in analyzing and representing signals and LTI systems. Specifically, the convolution property provides us with the basis for the frequency-domain analysis of LTI systems. We have already seen some of the utility of this approach in our discussion of 400 The Discrete-Time Fourier Transform Chap.5 filtering in Chapters 3-5 and in our examination of systems described by linear constant- coefficient differential or difference equations, and we will gain a further appreciation for its utility in Chapter 6, in which we examine filtering and time-versus-frequency issues in more detail. In addition, the multiplication properties in continuous and discrete time are essential to our development of sampling in Chapter 7 and communications in Chapter 8. Chapter 5 Problems The first section of problems belongs to the basic category and the answers are provided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 5.1. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) (~) 11 - 1 u[n- 1] (b) (~)ln-II Sketch and label one period of the magnitude of each Fourier transform. 5.2. Use the Fourier transform analysis equation (5.9) to calculate the Fourier transforms of: (a) o [ n - 1] + o [ n + 1] (b) o [ n + 2] - o [ n - 2] Sketch and label one period of the magnitude of each Fourier transform. 5. 3. Determine the Fourier transform for -1T :::; w < 1T in the case of each of the fol- lowing periodi*c )s ignals: (a) sin( }n + (b) 2 +cos( *n + i) 5.4. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transforms of: (a) X1(ejw) = L~=-oo{21To(w- 21Tk) + 1TO(w- ~- 21Tk) + 1TO(w + ~- 27Tk)} (b) X2(ejw) = { 2), . 0 < w :::; 1T -2], -1T < w :::; 0 5.5. Use the Fourier transform synthesis equation (5.8) to determine the inverse Fourier transform of X(ejw) = iX(ejw)iej-tX(ejw>, where 0*::::: ;: ; lwl < * 3w lwl :::; 1T T· Use your answer to determine the values of n for which x[n] = 0. 5.6. Given that x[n] has Fourier transform X(ejw), express the Fourier transforms of the following signals in terms of X(ejw). You may use the Fourier transform properties listed in Table 5 .1. (a) x 1[ n] = x[l - n] + x[ -1 - n] (b) X2[n] = x*[-ni+x[nj (c) x3[n] = (n - 1)2 x[n] Chap. 5 Problems 401 5.7. For each of the following Fourier transforms, use Fourier transform properties (Table 5.1) to determine whether the corresponding time-domain signal is (i) real, imagi- nary, or neither and (ii) even, odd, or neither. Do this without evaluating the inverse of any of the given transforms. (a) X1 (eiw) = e- jw) l~ 1 (sin kw) (b) X2(ei<v) = j sin(w) cos(5w) (c) X3(eiw) = A(w) + eJB(w) where 1, 0 :::; lwl :::; i 3 A(w) = { O, i lwl :::; and B(w) = - ; + 'TT. < 7T 5.8. Use Tables 5.1 and 5.2 to help determine x[n] when its Fourier transform is . 1 s.m ;3, w ) X(e 1w) = . -.- ~- + 57TO(w), 1- e-.1w ( sm ~ 2 5.9. The following four facts are given about a real signal x[n] with Fourier transform X(ej111 ): 1. x[n] = 0 for n > 0. 2. x[O] > 0. 3. 9n~{X(eiw)} = sinw- sin2w. 4. 2~ J_7T7T IX(eiw)l2 dw = 3. Determine x[n]. 5.10. Use Tables 5.1 and 5.2 in conjunction with the fact that n=-x to determine the numerical value of A= in(~)n· n=O 5.11. Consider a signal g[n] with Fourier transform G(eiw). Suppose g[n] = X(2)[n], where the signal x[n] has a Fourier transform X(eiw). Determine a real number a such that 0 < a < 27T and G(eiw) = G(eJ<w-cd). 5.12. Let 2 y[n] = -sin *-n) ( * (sin wen) ' 7Tn 7Tn where * denotes convolution and lwei :::; 7T. Determine a stricter constraint on We 402 The Discrete-Time Fourier Transform Chap.5 which ensures that s.m 7-T n )2 y[n] = ( 7T~ . 5.13. An LTI system with impulse response h 1[n] = (*)nu[n] is connected in parallel with another causal LTI system with impulse response h2 [n]. The resulting parallel interconnection has the frequency response H jw _ -12 + 5e- jw (e)- 12-?e-jw+e-j2w' Determine h2 [n]. 5.14. Suppose we are given the following facts about an LTI systemS with impulse re- sponse h[n] and frequency response H(ejw): 1. (~)nu[n] ~ g[n], where g[n] = 0 for n 2: 2 and n < 0. 2. H(ej7TI2) = 1. 3. H(ejw) = H(ej(w-7T)). Determine h[n]. 5.15. Let the inverse Fourier transform of Y(ejw) be y[n] = ('i::,n r where 0 <We< 7T. Determine the value of We which ensures that 5.16. The Fourier transform of a particular signal is X(ejw) = ~ (1/2)k . L 1 - .!e- j(w-7T!2k) k=O 4 It can be shown that x[n] = g[n]q[n], where g[n] is of the form anu[n] and q[n] is a periodic signal with period N. (a) Determine the value of a. (b) Determine the value of N. (c) Is x[n] real? 5.17. The signal x[ n] = (- 1) n has a fundamental period of 2 and corresponding Fourier series coefficients ak. Use duality to determine the Fourier series coefficients bk of the signal g[ n] = an with a fundamental period of 2. 5.18. Given the fact that ~ 1- a2 alnl ~ 1- 2acosw + a2' lal < 1, Chap. 5 Problems 403 use duality to determine the Fourier series coefficients of the following continuous- time signal with period T = 1: x(t) = 5 - 4 cos(27Tt) · 5.19. Consider a causal and stable LTI systemS whose input x[n] and output y[n] are related through the second-order difference equation 1 1 y[n]- (;y[n- 1]- (;y[n- 2] = x[n]. (a) Determine the frequency response H(eiw) for the systemS. (b) Determine the impulse response h[n] for the systemS. 5.20. A causal and stable LTI system S has the property that (a) Determine the frequency response H(eiw) for the systemS. (b) Determine a difference equation relating any input x[n] and the corresponding output y[n]. BASIC PROBLEMS 5.21. Compute the Fourier transform of each of the following signals: (a) x[n] = u[n - 2] - u[n - 6] (b) x [ n] = ( ~) -n u [- n - 1] (c) x[n] = (l)l 111u[-n- 2] 3 (d) x[n] = 211 sin(*n)u[ -n] (e) x[n] = (~)lnl cos(-~(n- 1)) (f) x[n] = { n, -3 ~ _n ~ 3 0, otherwise (g) x[n] = sin(~n) + cos(n) (h) x[n] = sinCS; n) + cosC; n) (i) x[n] = x[n - 6], and x[n] = u[n] - u[n - 5] for 0 ~ n ~ 5 (j) x[n] = (n - 1)( l )lnl 3 (k) x[nl = cinC:,~15>) cosC; n) 5.22. The following are the Fourier transforms of discrete-time signals. Determine the signal corresponding to each transform. 1 7!. < lw I < 37T (a) X(eiw) = ' 4 - - 4 { 0, 3 ; ~ lw I ~ 7T, 0 ~ lw I < * (b) X(eiw) = 1 + 3e-.iw + 2e- i 2w - 4e- i 3w + e-JIOw (c) X(eiw) = e-Jw/2 for -7r ~ w ~ 7T (d) X(eiw) = cos2 w + sin2 3w 404 The Discrete-Time Fourier Transform Chap.5 (e) X(ejw) = L~= _00 ( -1)kB(w - ¥k ) -jw I (f) X(ejw) = ~ 1-~rJw (g) X(ejw) = 1- ~e- jw 1-±e-jw_~e-2jw (h) X(ejw) = 1-(~)6e-J6w 1-~e-Jw 5.23. Let X(ejw) denote the Fourier transform of the signal x[n] depicted in Figure P5.23. Perform the following calculations without explicitly evaluating X(ejw): (a) Evaluate X(ej0 ). (b) Find <t.X(ejw). (c) Evaluate J~ 7TX(ejw)dw. (d) Find X(ej7T). (e) Determine and sketch the signal whose Fourier transform is CR..e{x(w )}. (f) Evaluate: (i) J~ 7TIX(ejw)l 2dw (ii) J_ 7T7T Id Xd~w) 12 dw x[n] n Fig P5.23 5.24. Determine which, if any, of the following signals have Fourier transforms that sat- isfy each of the following conditions: 1. ffi..e{X(ejw)} = 0. 2. dm{X(ejw)} = 0. 3. There exists a real a such that ejaw X ( ejw) is real. 4. J ~7TX(ejw)dw = 0. 5. X(ejw) periodic. 6. X(ej0 ) = 0. (a) x[n] as in Figure P5.24(a) (b) x[n] as in Figure P5.24(b) (c) x[n] = (~)nu[n] (d) x[n] = (~)lnl (e) x[n] = B[n- 1] + B[n + 2] (f) x[n] = B[n- 1] + B[n + 3] (g) x[n] as in Figure P5.24(c) (h) x[n] as in Figure P5.24(d) (i) x[n] = B[n- 1] - B[n + 1] Chap. 5 Problems 405 x[n] 1 • • • • .;I t(l II I -1 0 1 2 3 4 5 •6 • • • • • n (a) x[n] .I. .I. . 1I . 1 • I. . I . . I. 2 I I-2-1 ~J I I I n (b) x[n] 2 n (c) x[n] 2 n (d) Fig P5.24 5.25. Consider the signal depicted in Figure P5 .25. Let the Fourier transform of this signal be written in rectangular form as X(ejw) = A(w) + jB(w ). Sketch the function of time corresponding to the transform 406 The Discrete-Time Fourier Transform Chap.5 x[n] 3 2 n -2 Fig P5.25 5.26. Let x1 [n] be the discrete-time signal whose Fourier transform X1( eiw) is depicted in Figure P5.26(a). (a) Consider the signal x2[n] with Fourier transform X2(eiw), as illustrated in Fig- ure P5.26(b). Express x2[n] in terms of x 1 [n]. [Hint: First express X2(eiw) in terms of X1( eiw), and then use properties of the Fourier transform.] (b) Repeat part (a) for x3 [n] with Fourier transform X3(eiw), as shown in Figure P5.26(c). (c) Let n=-x a=---- X L Xt[n] n=-'XJ This quantity, which is the center of gravity of the signal x 1 [n], is usually re- ferred to as the delay time of x 1 [n]. Find a. (You can do this without first deter- mining x 1 [n] explicitly.) 1T 1T w 3 (a) Fig P5.26a Chap. 5 Problems 407 w (b) w (c) Fig P5.26b,c (d) Consider the signal x4 [n] = x 1 [n] * h[n], where sin( rrn/6) h[n] = --- rrn Sketch X4(ejw). 5.27. (a) Let x[n] be a discrete-time signal with Fourier transform X(ejw), which is il- lustrated in Figure P5 .27. Sketch the Fourier transform of w[n] = x[n]p[n] for each of the following signals p[n]: (i) p[n] = cos rrn (ii) p[n] = cos( rrn/2) (iii) p[n] = sin( rrn/2) 00 (iv) p[n] = L: o[n- 2k] k= -00 00 (v) p[n] = L: o[n- 4k] k= -00 '1T '1T w 2 Fig P5.27 408 The Discrete-Time Fourier Transform Chap.5 (b) Suppose that the signal w[n] of part (a) is applied as the input to an LTI system with unit sample response h[n] = sin( '7Tn/2). 1Tn Determine the output y[n] for each of the choices of p[n] in part (a). 5.28. The signals x[n] and g[n] are known to have Fourier transforms X(ejw) and G(ejw), respectively. Furthermore, X(ejw) and G(ejw) are related as follows: - 1 J +1r X(ej8 )G(ej(w-O))d() = 1 + e- jw (P5.28-1) 21T -7T (a) If x[n] = ( -l)n, determine a sequence g[n] such that its Fourier transform G(ejw) satisfies eq. (P5.28-1). Are there other possible solutions for g[n]? (b) Repeat the previous part for x[n] = (~)nu[n]. 5.29. (a) Consider a discrete-time LTI system with impulse response h[n] = G)"" u[n]. Use Fourier transforms to determine the response to each of the following input signals: (i) x[n] = (~)nu[n] (ii) x[n] = (n + l)(~)nu[n] (iii) x[n] = ( -l)n (b) Suppose that h[n] = [(H cos(~n)]u[n]. Use Fourier transforms to determine the response to each of the following in- puts: (i) x[n] = (~ )nu[n] (ii) x[n] = cos( '7Tn/2) (c) Let x[n] and h[n] be signals with the following Fourier transforms: X(ejw) = 3ejw + 1 - e- jw + 2e-j3w, H(ejw) = -ejw + 2e-2jw + ej4w. Determine y[n] = x[n] * h[n]. 5.30. In Chapter 4, we indicated that the continuous-time LTI system with impulse re- sponse h(t) = -Ws m. c (-Wt) = -sin W-t 1T 1T 1Tt plays a very important role in LTI system analysis. The same is true of the discrete- time LTI system with impulse response h[n] = -Ws m. c (W- n) = -sin- Wn. 1T 1T 1Tn Chap. 5 Problems 409 (a) Determine and sketch the frequency response for the system with impulse re- sponse h[n]. (b) Consider the signal x[n] = sm. (7ST n) - 2 cos (74T n) . Suppose that this signal is the input to LTI systems with the following impulse responses. Determine the output in each case. (i) h[n] sin(1711/6) 1711 (ii) h[n] = sin(1711/6) + sin(171112) 1711 1711 (iii) h[n] = sin(1711/6)sin(1711/3) 172112 (iv) h[n] = sin(17n/6)sin(17n/3) 1711 (c) Consider an LTI system with unit sample response h[n] = sin( 7Tn/3). 7Tn Determine the output for each of the following inputs: (i) x[n] = the square wave depicted in Figure P5.30 00 (ii) x[n] = L o[n- 8k] k= -00 (iii) x[n] = ( -1)11 times the square wave depicted in Figure P5.30 (iv) x[n] = o[n + 1] + o[n- 1] x[n] III ... IIIII .. ~IIIII ... IIIII ... IIIII ... II -8 0 8 16 n Fig P5.30 5.31. An LTI system S with impulse response h[n] and frequency response H(ejw) is known to have the property that, when -7T ::::; w0 ::::; 7T, cos won ~ wo cos won. (a) Determine H(ejw). (b) Determine h[n]. 5.32. Let h1 [n] and h2[n] be the impulse responses of causal LTI systems, and let H 1 (ejw) and H 2(ejw) be the corresponding frequency responses. Under these conditions, is the following equation true in general or not? Justify your answer. 410 The Discrete-Time Fourier Transform Chap.S 5.33. Consider a causal LTI system described by the difference equation 1 y[n] + 2y [n - 1] = x[n]. (a) Determine the frequency response H(ejw) of this system. (b) What is the response of the system to the following inputs? (i) x[n] = <!)nu[n] (ii) x[n] = (-!)nu[n] (iii) x[n] = o[n] + !o[n - 1] (iv) x[n] = o[n] - !o[n - 1] (c) Find the response to the inputs with the following Fourier transforms: (i) X(ejw) = 1- ±e-JW I+ ~e-.1w ("""") X( e jw) -- l+.!e-jw II I - 4I e-.Jw. (iii) X(ejw) = 1 ( 1- £r jw)(l + ~e- jw) (iv) X(ejw) = 1 + 2e- 3jw 5.34. Consider a system consisting of the cascade of two LTI systems with frequency responses and (a) Find the difference equation describing the overall system. (b) Determine the impulse response of the overall system. 5.35. A causal LTI system is described by the difference equation y[n] - ay[n - 1] = bx[n] + x[n - 1], where a is real and less than 1 in magnitude. (a) Find a value of b such that the frequency response of the system satisfies IH(ejw)l = 1, for all w. This kind of system is called an all-pass system, as it does not attenuate the input ejwn for any value of w. Use the value of b that you have found in the rest of the problem. (b) Roughly sketch <r:.H(ejw), 0 :::; w :::; 7T, when a = !· (c) Roughly sketch <r:.H(ejw), 0 :::; w :::; 7T, when a = -!. Chap. 5 Problems 411 (d) Find and plot the output of this system with a = - ~ when the input is x[n] = (~ ru [n]. From this example, we see that a nonlinear change in phase can have a signif- icantly different effect on a signal than the time shift that results from a linear phase. 5.36. (a) Let h[n] and g[n] be the impulse responses of two stable discrete-time LTI sys- tems that are inverses of each other. What is the relationship between the fre- quency responses of these two systems? (b) Consider causal LTI systems described by the following difference equations. In each case, determine the impulse response of the inverse system and the difference equation that characterizes the inverse. (i) y[n] = x[n] - *x[n- 1] (ii) y[n] + ~y[n- 1] = x[n] (iii) y[n] + ~y[n- 1] = x[n] - *x[n- 1] (iv) y[n] + ~y[n- 1]- ~y[n- 2] = x[n]- *x[n- 1]- ~x[n- 2] (v) y[n] + ~y[n- 1]- ~y[n- 2] = x[n]- ~x[n- 1] (vi) y[n] + ~y[n- 1]- ~y[n- 2] = x[n] (c) Consider the causal, discrete-time LTI system described by the difference equa- tion 1 1 y[n] + y[n- 1] + 4y[n- 2] = x[n- 1] - 2x[n- 2]. (P5.36-1) What is the inverse of this system? Show that the inverse is not causal. Find an- other causal LTI system that is an ""inverse with delay"" of the system described by eq. (P5.36-1). Specifically, find a causal LTI system such that the output w[n] in Figure P5.36 equals x[n - 1]. LTI system y[n] Causal x[n] described by LTI w[n] eq. (P5.36-1) system Fig P5.36 ADVANCED PROBLEMS 5.37. Let X(efw) be the Fourier transform of x[n]. Derive expressions in terms of X(efw) for the Fourier transforms of the following signals. (Do not assume that x[n] is real.) (a) CRe{x[n]} (b) x*[ -n] (c) 8v{x[n]} 412 The Discrete-Time Fourier Transform Chap.5 5.38. Let X(eiw) be the Fourier transform of a real signal x[n]. Show that x[n] can be written as x[n] = r{B(w)cosw + C(w)sinw}dw by finding expressions for B(w) and C(w) in terms of X(eiw). 5.39. Derive the convolution property ~ . . x[n] * h[n] ~ X(elw)H(elw). 5.40. Let x[n] and h[n] be two signals, and let y[n] = x[n] * h[n]. Write two expressions for y[O], one (using the convolution sum directly) in terms of x[n] and h[n], and one (using the convolution property of Fourier transforms) in terms of X(eiw) and H(eiw). Then, by a judicious choice of h[n], use these two expressions to derive Parsev al's relation-that is, In a similar fashion, derive the following generalization of Parseval's relation: +oo 1 J7T . . ~ x[n]z*[n] = 7T _ X(elw)Z*(elw)dw. n-oo 2 7T 5.41 Let i[n] be a periodic signal with period N. A finite-duration signal x[n] is related to i[n] through x[n] = { i[n], no :::sn:::sno+N-1 0, otherwise for some integer no. That is, x[n] is equal to i[n] over one period and zero elsewhere. (a) If i[n] has Fourier series coefficients ak and x[n] has Fourier transform X(eiw), show that regardless of the value of no. (b) Consider the following two signals: x[n] = u[n] - u[n - 5] 00 i[n] = ~ x[n- kN] k= -00 where N is a positive integer. Let ak denote the Fourier coefficients of i[n] and let X(eiw) denote the Fourier transform of x[n]. (i) Determine a closed-form expression for X(eiw). Chap. 5 Problems 413 (ii) Using the result of part (i), determine an expression for the Fourier coeffi- cients ak. 5.42. In this problem, we derive the frequency-shift property of the discrete-time Fourier transform as a special case of the multiplication property. Let x[n] be any discrete- time signal with Fourier transform X(eiw), and let g[n] = eiwon x[n]. (a) Determine and sketch the Fourier transform of (b) The multiplication property of the Fourier transform tells us that, since g[n] = p[n]x[n], G(eiw) = _l_ f X(eifJ)P(ei<w-{}))dO. 27T <27T> Evaluate this integral to show that G(eiw) = X(ej(w-wo)). 5.43. Let x[n] be a signal with Fourier transform X(eiw), and let g[n] = x[2n] be a signal whose Fourier transform is G(eiw). In this problem, we derive the rela- tionship between G(eiw) and X(eiw). (a) Let (e- Jmz x[n]) + x[n] v[n] = . 2 Express the Fourier transform V(eiw) of v[n] in terms of X(eiw). (b) Noting that v[n] = 0 for n odd, show that the Fourier transform of v[2n] is equal to V(ei~). (c) Show that x[2n] = v[2n]. It follows that G(eiw) = V(ejw/2). Now use the result of part (a) to express G(eiw) in terms of X(eiw). 5.44. (a) Let x 1 [n] = cos (7Tn) + s.m (7TT n) 3 414 The Discrete-Time Fourier Transform Chap.5 be a signal, and let X1( ejw) denote the Fourier transform of x 1 [n]. Sketch x 1 [n], together with the signals with the following Fourier transforms: (i) X2(ejw) = X1(ejw)ejw, lwl < 1T (ii) X3(ejw) = XI (ejw)e- j3wl2, lwl < 1T (b) Let w(t) = cos (1TT t) + s.m (1TTt ) 3 2 be a continuous-time signal. Note that x 1 [n] can be regarded as a sequence of evenly spaced samples of w(t); that is, x 1 [n] = w(nT). Show that x2[n] = w(nT- a) and x3[n] = w(nT- /3) and specify the values of a and f3. From this result we can conclude that x 2 [n] and x3 [n] are also evenly spaced samples of w(t). 5.45. Consider a discrete-time signal x[n] with Fourier transform as illustrated in Figure P5.45. Provide dimensioned sketches of the following continuous-time signals: (a) XJ (t) = L~ = _ X[n]ej(27TIIO)nt 70 (b) X2(t) = L~=-xx[ -n]ej(27T!IO)nt CRe { X(eiw)} w -21T 1T 1T 21T w 2 Fig P5.45 Chap. 5 Problems 415 (c) X3(t) = 2.:= _0d{x[n]}e.i(21TI&)nt (d) x4(t) = 2.: 00 = _,/Re{x[n]}e.i(21TI6)nt 5.46. In Example 5.1, we showed that for Ia I < 1, ~ 1 a 11 u[n] ~ .. 1- ae- JW (a) Use properties of the Fourier transform to show that ~ 1 (n + 1)a 11 u[n] ~ ( 1 - ae _. ) . JW 2 (b) Show by induction that the inverse Fourier transform of X(e.iw) = 1 (1- ae-Jwy is (n + r- 1)! x[n] = 1 _ )' a 11 u[n]. n.(r 1 . 5.47. Determine whether each of the following statements is true or false. Justify your answers. In each statement, the Fourier transform of x[n] is denoted by X(e.iw). (a) If X(e.iw) = X(e.i(w-l)), then x[n] = 0 for lnl > 0. (b) If X(e.iw) = X(e.i(w-1T)), then x[n] = 0 for lnl > 0. (c) If X(eiw) = X(eiw12 ), then x[n] = 0 for lnl > 0. (d) If X(e.iw) = X(e.i2w), then x[n] = 0 for lnl > 0. 5.48. We are given a discrete-time, linear, time-invariant, causal system with input de- noted by x[n] and output denoted by y[n]. This system is specified by the following pair of difference equations, involving an intermediate signal w[n]: 1 y[n] + 4y[n- 1] + 1 2 w[n] + 2w[n- 1] = 3x[n], 5 5 y[n]- 4y[n- 1] + 2w[n]- 2w[n- 1] = - 3x[n]. (a) Find the frequency response and unit sample response of the system. (b) Find a single difference equation relating x[n] and y[n] for the system. 5.49. (a) A particular discrete-time system has input x[n] and output y[n]. The Fourier transforms of these signals are related by the equation Y(ejw) = 2X(ejw) + e- jw X(ejw)- dX~~w). (i) Is the system linear? Clearly justify your answer. (ii) Is the system time invariant? Clearly justify your answer. (iii) What is y[n] if x[n] = 8[n]? 416 The Discrete-Time Fourier Transform Chap.5 (b) Consider a discrete-time system for which the transform Y(eiw) of the output is related to the transform of the input through the relation Find an expression for y[n] in terms of x[n]. 5.50. (a) Suppose we want to design a discrete-time LTI system which has the property that if the input is x[n] = (l)n u[n] - 1 (l)n-1 2 4 2 u[n - 1], then the output is y[n] ~ (~ )"" u[n]. (i) Find the impulse response and frequency response of a discrete-time LTI system that has the foregoing property. (ii) Find a difference equation relating x[n] and y[n] that characterizes the system. (b) Suppose that a system has the response (114)nu[n] to the input (n + 2)(112)nu[n]. If the output of this system is 8[n] - ( -112)nu[n], what is the input? 5.51. (a) Consider a discrete-time system with unit sample response h[n] = 1 )n u[n] + 1 (1 )n (2 2 4 u[n]. Determine a linear constant-coefficient difference equation relating the input and output of the system. (b) Figure P5.51 depicts a block diagram implementation of a causal LTI system. (i) Find a difference equation relating x[n] and y[n] for this system. (ii) What is the frequency response of the system? (iii) Determine the system's impulse response. Fig PS.S1 Chap. 5 Problems 417 5.52. (a) Let h[n] be the impulse response of a real, causal, discrete-time LTI system. Show that the system is completely specified by the real part of its frequency response. (Hint: Show how h[n] can be recovered from Sv{h[n]}. What is the Fourier transform of Sv{ h[ n]} ?) This is the discrete-time counterpart of the real- part sufficiency property of causal LTI systems considered in Problem 4.47 for continuous-time systems. (b) Let h[n] be real and causal. If ffi-e{H(eiw)} = 1 +a cos 2w(a real), determine h[n] and H(eiw). (c) Show that h[n] can be completely recovered from knowledge of 9m{H(eiw)} and h[O]. (d) Find two real, causal LTI systems whose frequency responses have imaginary parts equal to sin w. EXTENSION PROBLEMS 5.53. One ofthe reasons for the tremendous growth in the use of discrete-time methods for the analysis and synthesis of signals and systems was the development of exceed- ingly efficient tools for performing Fourier analysis of discrete-time sequences. At the heart of these methods is a technique that is very closely allied with discrete-time Fourier analysis and that is ideally suited for use on a digital computer or for im- plementation in digital hardware. This technique is the discrete Fourier transform (DFT) for finite-duration signals. Let x[n] be a signal of finite duration; that is, there is an integer N1 so that x[n] = 0, outside the interval 0 :::::; n :::::; N1 - 1 Furthermore, let X(eiw) denote the Fourier transform of x[n]. We can construct a periodic signal i[n] that is equal to x[n] over one period. Specifically, let N 2:: N1 be a given integer, and let i[n] be periodic with period Nand such that i[n] = x[n], The Fourier series coefficients for i[n] are given by ak = _!_ L i[n]e- jk(27TIN)n N (N) Choosing the interval of summation to be that over which i[n] = x[n], we obtain 1 N-1 . ak = N L x[n]e- Jk(27TIN)n (P5.53-l) n=O The set of coefficients defined by eq. (P5.53-l) comprise the DFT of x[n]. Specifi- cally, the DFT of x[n] is usually denoted by X[k], and is defined as 418 The Discrete-Time Fourier Transform Chap.5 1 N-1 X[k] = ak = N L x[n]e- jk(27TIN)n, k = 0, 1, ... , N - 1 (P5.53-2) n=O The importance of the DFf stems from several facts. First note that the original finite duration signal can be recovered from its DFf. Specifically, we have N-1 x[n] = L X[k]ejk(27r!N)n, n = 0, 1, ... , N - 1 (P5.53-3) k=O Thus, the finite-duration signal can either be thought of as being specified by the finite set of nonzero values it assumes or by the finite set of values of X[ k] in its DFf. A second important feature of the DFf is that there is an extremely fast algorithm, called the fast Fourier transform (FFT), for its calculation (see Problem 5.54 for an introduction to this extremely important technique). Also, because of its close relationship to the discrete-time Fourier series and transform, the DFf inherits some of their important properties. (a) Assume that N 2: N1• Show that X[k] = ~x(ej<27TkJN)) where X[k] is the DFf of x[n]. That is, the DFf corresponds to samples of X(ejw) taken every 2TTIN. Equation (P5.53-3) leads us to conclude that x[n] can be uniquely represented by these samples of X(ejw). (b) Let us consider samples of X(ejw) taken every 27r/M, where M < N1• These samples correspond to more than one sequence of duration N1• To illustrate this, consider the two signals Xt [n] and x2 [n] depicted in Figure P5.53. Show that if we choose M = 4, we have Xt (ej(27Tkl4)) = X2 (ej(27Tk!4)) for all values of k. x1 [n] x2 [n] ...... :r .I: ..... . • ••••2 .Iri:~r ... 0 12 3 n _111112341 7 n -1 Fig P5.53 5.54. As indicated in Problem 5.53, there are many problems of practical importance in which one wishes to calculate the discrete Fourier transform (DFf) of discrete-time signals. Often, these signals are of quite long duration, and in such cases it is very Chap. 5 Problems 419 important to use computationally efficient procedures. One of the reasons for the significant increase in the use of computerized techniques for the analysis of signals was the development of a very efficient technique known as the fast Fourier trans- form (FFf) algorithm for the calculation of the OFf of finite-duration sequences. In this problem, we develop the principle on which the FFf is based. Let x[n] be a signal that is 0 outside the interval 0 ~ n ~ N1 - 1. For N ~ N1, theN-point OFf of x[n] is given by 1 N-1 X[k] = - ~ x[n]e- Jk(21TIN)n, k = 0, 1, ... , N- 1. (P5.54-1) N k=O It is convenient to write eq. (P5.54-1) as 1 N-1 X[k] = N ~ x[n]WNk, (P5.54-2) k=O where (a) One method for calculating X[k] is by direct evaluation of eq. (P5.54-2). A useful measure of the complexity of such a computation is the total number of complex multiplications required. Show that the number of complex multipli- cations required to evaluate eq. (P5.54-2) directly, fork = 0, 1, ... , N- 1, is N 2. Assume that x[ n] is complex and that the required values of WNk have been precomputed and stored in a table. For simplicity, do not exploit the fact that, for certain values of n and k, WNk is equal to ± 1 or ± j and hence does not, strictly speaking, require a full complex multiplication. (b) Suppose that N is even. Let f[n] = x[2n] represent the even-indexed samples of x[n], and let g[n] = x[2n + 1] represent the odd-indexed samples. (i) Show that f[n] and g[n] are zero outside the interval 0 ~ n ~ (N/2)- 1. (ii) Show that theN-point OFf X[k] of x[n] can be expressed as (N/2)-1 (N/2)-1 1 1 X[k] = N ~ f[n]WN72 + N w~ ~ g[n]WN72 n=O n=O 21 - 1 k - F [k] + 2W NG[k], k = 0, 1, ... , N - 1, (P5.54-3) where 2 (N/2)-1 F[k] = N ~ f[n]WN~2 , n=O 2 (N/2)-1 G[k] = N ~ g[n]WN72. n=O 420 The Discrete-Time Fourier Transform Chap.5 (iii) Show that, for all k, F- [k + 2N l = F-[k], G- [k + Nl = G-2 ~k]. Note that F[k], k = 0, 1, ... , (N/2) - 1, and G[k], k = 0, 1, ... , (N/2)- 1, are the (N/2)-point DFTs of f[n] and g[n], respectively. Thus, eq. (P5.54-3) indicates that the length-N DFT of x[n] can be calculated in terms of two DFTs of length N/2. (iv) Determine the number of complex multiplications required to compute X[k], k = 0, 1, 2, ... , N - 1, from eq. (P5.54-3) by first computing F[k] and G[k]. [Make the same assumptions about multiplications as in part (a), and ignore the multiplications by the quantity 1/2 in eq. (P5.54-3).] (c) If, likeN, N/2 is even, then f[n] and g[n] can each be decomposed into se- quences of even- and odd-indexed samples, and therefore, their DFTs can be computed using the same process as in eq. (P5.54-3). Furthermore, if N is an integer power of 2, we can continue to iterate the process, thus achieving sig- nificant savings in computation time. With this procedure, approximately how many complex multiplications are required for N = 32, 256, 1,024, and 4,096? Compare this to the direct method of calculation in part (a). 5.55. In this problem we introduce the concept of windowing, which is of great importance both in the design of LTI systems and in the spectral analysis of signals. Windowing is the operation of taking a signal x[ n] and multiplying it by a finite-duration window signal w[n]. That is, p[n] = x[n]w[n]. Note that p[n] is also of finite duration. The importance of windowing in spectral analysis stems from the fact that in numerous applications one wishes to compute the Fourier transform of a signal that has been measured. Since in practice we can measure a signal x[n] only over a finite time interval (the time window), the actual signal available for spectral analysis is p[n] = { x[n], -M:::; n:::; M 0, otherwise where - M :::; n :::; M is the time window. Thus, p[n] = x[n]w[n], where w[n] is the rectangular window; that is, -M:::; n ::SM w[n] = { 6: (P5.55-1) otherwise Windowing also plays a role in LTI system design. Specifically, for a variety of reasons (such as the potential utility of the FFT algorithm; see Problem P5.54), it is Chap. 5 Problems 421 often advantageous to design a system that has an impulse response of finite duration to achieve some desired signal-processing objective. That is, we often begin with a desired frequency response H(ejw) whose inverse transform h[n] is an impulse response of infinite (or at least excessively long) duration. What is required then is the construction of an impulse response g[ n] of finite duration whose transform G(ejw) adequately approximates H(ejw). One general approach to choosing g[n] is to find a window function w[n] such that the transform of h[n]w[n] meets the desired specifications for G(ejw). Clearly, the windowing of a signal has an effect on the resulting spectrum. In this problem, we illustrate that effect. (a) To gain some understanding of the effect of windowing, consider windowing the signal x[n] = L 8[n- k] k= -00 using the rectangular window signal given in eq. (P5.55-1). (i) What is X(ejw)? (ii) Sketch the transform of p[n] = x[n]w[n] when M = 1. (iii) Do the same forM = 10. (b) Next, consider a signal x[n] whose Fourier transform is specified by iwi < 7r/4 7r/4 < iwi ::::; 7T'. Let p[n] = x[n]w[n], where w[n] is the rectangular window of eq. (P5.55-1). Roughly sketch P(ejw) forM = 4, 8, and 16. (c) One of the problems with the use of a rectangular window is that it introduces ripples in the transform P(ejw). (This is in fact directly related to the Gibbs phenomenon.) For that reason, a variety of other window signals have been developed. These signals are tapered; that is, they go from 0 to 1 more gradually than the abrupt transition of the rectangular window. The result is a reduction in the amplitude of the ripples in P( ejw) at the expense of adding a bit of distortion in terms of further smoothing of X(ejw). To illustrate the points just made, consider the signal x[ n] described in part (b), and let p[n] = x[n]w[n], where w[n] is the triangular or Bartlett window; that is, w[n] = { 1 - J~1' -M::::; n::::; M 0, otherwise Roughly sketch the Fourier transform of p[n] = x[n]w[n] forM = 4, 8, and 16. [Hint: Note that the triangular signal can be obtained as a convolution of a rectangular signal with itself. This fact leads to a convenient expression for W(ejw).] 422 The Discrete-Time Fourier Transform Chap.5 (d) Let p[n] = x[n]w[n], where w[n] is a raised cosine signal known as the Han- ning window; i.e., w[n] = { 4[1 + COS(7Tn/M)], -M ~ n ~ M 0, otherwise Roughly sketch P( eiw) for M = 4, 8, and 16. 5.56. Let x[m, n] be a signal that is a function of the two independent, discrete variables m and n. In analogy with one dimension and with the continuous-time case treated in Problem 4.53, we can define the two-dimensional Fourier transform of x[m, n] as (P5.56-1) n= -'Xffl= -X (a) Show that eq. (P5.56-1) can be calculated as two successive one-dimensional Fourier transforms, first in m, with n regarded as fixed, and then inn. Use this result to determine an expression for x[m, n] in terms of X(eiw 1, eiw 2 ). (b) Suppose that x[m, n] = a[m]b[n], where a[m] and b[n] are each functions of only one independent variable. Let A(eiw) and B(eiw) denote the Fourier transforms of a[m] and b[n], respectively. Express X(eiw 1, eiw 2 ) in terms of A(eiw) and B(eiw). (c) Determine the two-dimensional Fourier transforms of the following signals: (i) x[m, n] = B[m- 1]B[n + 4] (ii) x[m, n] = <4Yz-mu[n- 2]u[ -m] (iii) x[m, n] = (4)n cos(27Tm/3)u[n] (. ) [ ] _ { 1, -2 < m < 2 and -4 < n < 4 IV x m, n - 0, otherwise 1 (v) x[m, n] = { o' -2 + n < m < 2 + nand -4 < n < 4 h . , ot erw1se (vi) x[m, n] = sin ( ~n + 2~m) (d) Determine the signal x[m, n] whose Fourier transform is 0 < lw1l ~ 7T/4 and 0 < lw2l ~ 7T/2 7T/4 < lw1l < 7T or 7T/2 < lw2l < 7T · (e) Let x[m, n] and h[m, n] be two signals whose two-dimensional Fourier trans- forms are denoted by X(eiw 1, eiw 2 ) and H(eiw 1, eiw 2 ), respectively. Deter- mine the transforms of the following signals in terms of X(eiw 1, eiw2 ) and H(eiw1, eiwz): (i) x[m, n]eiWJmeJWzn c·) [ ] { x[k, r], if m = 2k and n = 2r 11 Y m, n = 0, if m is not a multiple of 2 or n is not a multiple of 3 (iii) y[m, n] = x[m, n]h[m, n] 6 TIME AND FREQUENCY CHARACTERIZATION OFSIGNALSANDSYSTEMS 6.0 INTRODUCTION The frequency-domain characterization of an LTI system in terms of its frequency re- sponse represents an alternative to the time-domain characterization through convolution. In analyzing LTI systems, it is often particularly convenient to utilize the frequency do- main because differential and difference equations and convolution operations in the time domain become algebraic operations in the frequency domain. Moreover, concepts such as frequency-selective filtering are readily and simply visualized in the frequency domain. However, in system design, there are typically both time-domain and frequency-domain considerations. For example, as we briefly discussed in Examples 4.18 and 5 .12, and as we will illustrate in more detail in this chapter, significant oscillatory behavior in the impulse response of a frequency-selective filter may be undesirable, and consequently, we may wish to sacrifice the level of frequency selectivity in a filter in order to meet the required tolerances on impulse response behavior. Situations such as this are the rule rather than the exception in practice, as in most applications we would like to specify or constrain certain characteristics of a system in both the time domain and the frequency domain, frequently resulting in conflicting requirements. Hence, in system design and analysis, it is important to relate time-domain and frequency-domain characteristics and trade-offs. Introducing these issues and relationships is the primary focus of the chapter. 6. 1 THE MAGNITUDE-PHASE REPRESENTATION OF THE FOURIER TRANSFORM The Fourier transform is in general complex valued and, as we discussed, can be repre- sented in terms of its real and imaginary components or in terms of magnitude and phase. 423 424 Time and Frequency Characterization of Signals and Systems Chap.6 The magnitude-phase representation of the continuous-time Fourier transform X(jw) is X(jw) = IX(jw )lef1:X(Jw). (6.1) Similarly the magnitude-phase representation of the discrete-time Fourier transform X(efw) is (6.2) In the following discussion, we concentrate for the most part on the continuous-time case in describing and illustrating several points related to magnitude-phase representations. The essential points apply equally to the discrete-time case. From the Fourier transform synthesis equation (4.8), we can think of X(jw) as pro- viding us with a decomposition of the signal x(t) into a ""sum"" of complex exponentials at different frequencies. In fact, as discussed in Section 4.3.7, IX(jw)l2 may be interpreted as the energy-density spectrum of x(t). That is, IX(jw )l2dw/27T can be thought of as the amount of energy in the signal x(t) that lies in the infinitesimal frequency band between w and w + dw. Thus, the magnitude IX(jw )I describes the basic frequency content of a signal-i.e., IX(jw )I provides us with the information about the relative magnitudes of the complex exponentials that make up x(t). For example, if IX(jw )I = 0 outside of a small band of frequencies centered at zero, then x(t) will display only relatively low-frequency oscillations. The phase angle <r.X(jw ), on the other hand, does not affect the amplitudes of the individual frequency components, but instead provides us with information concerning the relative phases of these exponentials. The phase relationships captured by <r.X(jw) have a significant effect on the nature of the signal x(t) and thus typically contain a subs tan- tial amount of information about the signal. In particular, depending upon what this phase function is, we can obtain very different-looking signals, even if the magnitude function remains unchanged. For example, consider again the example illustrated in Figure 3.3. In this case, a ship encounters the superposition of three wave trains, each of which can be modeled as a sinusoidal signal. With fixed magnitudes for these sinusoids, the amplitude of their sum may be quite small or very large, depending on the relative phases. The im- plications of phase for the ship, therefore, are quite significant. As another illustration of the effect of phase, consider the signal + 1 2 x(t) = 1 2 cos(27Tt + </>1) + cos(47Tt + </>2) + 3 cos(67Tt + 4>3). (6.3) In Figure 3.4, we depicted x(t) for the case when </> 1 = </>2 = 4>3 = 0. In Figure 6.1, we illustrate x(t) for this case also and for several other choices for the phase of the individual components. As this figure demonstrates, the resulting signals can differ significantly for different relative phases. In general, changes in the phase function of X(jw) lead to changes in the time- domain characteristics of the signal x(t). In some instances phase distortion may be important, whereas in others it is not. For example, a well-known property of the auditory system is a relative insensitivity to phase. Specifically, if the Fourier transform of a spoken sound (e.g., a vowel) is subjected to a distortion such that the phase is changed but the magnitude is unchanged, the effect can be perceptually negligible, although the waveform in the time domain may look considerably different. While mild phase distortions such as those affecting individual sounds do not lead to a loss of intelligibility, more severe phase Sec. 6.1 The Magnitude-Phase Representation of The Fourier Transform 425 (a) (b) At\Af'J~~~ (c) Figure 6. 1 The signal x(t) given in eq. (6.3) for several different choices of the phase angles </>1, <1>2, and 4>3: (a) </>1 = <1>2 = 4>3 = 0; (b) </>1 = 4 rad, <1>2 = 8 rad, 4>3 = 12 rad; (c) </>1 = 6 rad, <1>2 = -2.7 rad, 4>3 = 0.93 rad; (d) 4>1 = 1.2 rad, <1>2 = 4.1 (d) rad, 4>3 = -7.02 rad. distortions of speech certainly do. As an extreme illustration, if x(t) is a tape recording of a sentence, then the signal x( -t) represents the sentence played backward. From Table 4.1, assuming x(t) is real valued, the corresponding effect in the frequency domain is to replace the Fourier transform phase by its negative: ~{x(-t)} =X(- jw) = IX(jw)le-J4:X(jw). That is, the spectrum of a sentence played in reverse has the same magnitude function as the spectrum of the original sentence and differs only in phase. Clearly, this phase change has a significant impact on the intelligibility of the recording. A second example illustrating the effect and importance of phase is found in examin- ing images. As we briefly discussed in Chapter 3, a black-and-white picture can be thought of as a signal x(t1, t2), with t 1 denoting the horizontal coordinate of a point on the picture, t2 the vertical coordinate, and x(t1, t2) the brightness of the image at the point (t1, t2). The Fourier transform X(jw 1, jw 2) of the image represents a decomposition of the image into complex exponential components of the form eJw,t, eiw 2t2 that capture the spatial varia- tions of x(t1, t2) at different frequencies in each of the two coordinate directions. Several elementary aspects of two-dimensional Fourier analysis are addressed in Problems 4.53 and 5.56. In viewing a picture, some of the most important visual information is contained in the edges and regions of high contrast. Intuitively, regions of maximum and minimum 426 Time and Frequency Characterization of Signals and Systems Chap.6 (a) (b) (c) (d) intensity in a picture are places at which complex exponentials at different frequencies are in phase. Therefore, it seems plausible to expect the phase of the Fourier transform of a picture to contain much of the information in the picture, and in pat1icular, the phase should capture the information about the edges. To substantiate this expectation, in Figure 6.2(a) we have repeated the picture shown in Figure 1.4. In Figure 6.2(b) we have depicted the magnitude of the two-dimensional Fourier transform of the image in Figure 6.2(a), where in this image the horizontal axis is w 1, the vertical is w 2, and the brightness of the image at the point (w 1, w 2 ) is proportional to the magnitude of the transform X(jw 1, jw 2 ) of the image in Figure 6.2(a). Similarly, the phase of this transform is depicted in Figure 6.2(c). Figure 6.2(d) is the result of setting the phase [Figure 6.2(c)] of X(jw 1, jw 2 ) to zero (with- out changing its magnitude) and inverse transforming. In Figure 6.2(e) the magnitude of X(jw 1, jw2) was set equal to 1, but the phase was kept unchanged from what it was in Figure 6.2(c). Finally, in Figure 6.2(f) we have depicted the image obtained by inverse transforming the function obtained by using the phase in Figure 6.2( c) and the magnitude of the transform of a completely different image-the picture shown in Figure 6.2(g)! These figures clearly illustrate the importance of phase in representing images. Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 427 (e) (f) Figure 6.2 (a) The image shown in Figure 1.4; (b) magnitude of the two-dimensional Fourier transform of (a); (c) phase of the Fourier trans- form of (a); (d) picture whose Fourier transform has magnitude as in (b) and phase equal to zero; (e) picture whose Fourier transform has magnitude equal to 1 and phase as in (c); (f) picture whose Fourier transform has phase as in (c) and magni- tude equal to that of the transform of the picture (g) shown in (g). 6.2 THE MAGNITUDE-PHASE REPRESENTATION OF THE FREQUENCY RESPONSE OF LTI SYSTEMS From the convolution property for continuous-time Fourier transforms, the transform Y(jw) of the output of an LTI system is related to the transform X(jw) of the input to the system by the equation Y(jw) = H(jw)X(jw), where H(jw) is the frequency response of the system-i.e., the Fourier transform of the system's impulse response. Similarly, in discrete time, the Fourier transforms of the input X(ei(<)) and ouput Y(ei(<)) of an LTI system with frequency response H(eiw) are related by (6.4) Thus, the effect that an LTI system has on the input is to change the complex ampli- tude of each of the frequency components of the signal. By looking at this effect in terms of the magnitude-phase representation, we can understand the nature of the effect in more 428 Time and Frequency Characterization of Signals and Systems Chap.6 detail. Specifically, in continuous time, \Y(jw )\ = \H(jw )\\X(jw )\ (6.5) and <J:Y(jw) = <J:H(jw) + <J:X(jw ), (6.6) and exactly analogous relationships hold in the discrete-time case. From eq. (6.5), we see that the effect an LTI system has on the magnitude of the Fourier transform of the sig- nal is to scale it by the magnitude of the frequency response. For this reason, \H(jw)i (or \H(ejw)\) is commonly referred to as the gain of the system. Also, from eq. (6.6), we see that the phase of the input <J:X(jw) is modified by the LTI system by adding the phase <J:H(jw) to it, and <J:H(jw) is typically referred to as the phase shift of the system. The phase shift of the system can change the relative phase relationships among the compo- nents of the input, possibly resulting in significant modifications to the time domain char- acteristics of the input even when the gain of the system is constant for all frequencies. The changes in the magnitude and phase that result from the application of an input to an LTI system may be either desirable, if the input signal is modified in a useful way, or undesirable, if the input is changed in an unwanted manner. In the latter case, the effects in eqs. (6.5) and (6.6) are commonly referred to as magnitude and phase distortions. In the following sections, we describe several concepts and tools that allow us to understand these effects a bit more thoroughly. 6.2.1 Linear and Nonlinear Phase When the phase shift at the frequency w is a linear function of w, there is a particularly straightforward interpretation of the effect in the time domain. Consider the continuous- time LTI system with frequency response H(jw) = e-jwto, (6.7) so that the system has unit gain and linear phase-i.e., \H(jw)\ = 1, <J:H(jw) = -wto. (6.8) As shown in Example 4.15, the system with this frequency response characteristic pro- duces an output that is simply a time shift of the input-i.e., y(t) = x(t - to). (6.9) In the discrete-time case, the effect oflinear phase is similar to that in the continuous- time case when the slope of the linear phase is an integer. Specifically, from Example 5.11, we know that the LTI system with frequency response e- jwno with linear phase function -wn0 produces an ouput that is a simple shift of the input-i.e., y[n] = x[n- n0 ]. Thus, a linear phase shift with an integer slope corresponds to a shift of x[n] by an integer number of samples. When the phase slope is not an integer, the effect in the time domain is some- what more complex and is discussed in Chapter 7, Section 7.5. Informally, the effect is a time shift of the envelope of the sequence values, but the values themselves may change. While linear phase shifts lead to very simple and easily understood and visualized changes in a signal, if an input signal is subjected to a phase shift that is a nonlin- ear function of w, then the complex exponential components of the input at different frequencies will be shifted in a manner that results in a change in their relative phases. When these exponentials are superimposed, we obtain a signal that may look considerably different from the input signal. This is illustrated in Figure 6.3 in the continuous-time case. 0 0 10 (a) (b) 0 0 (c) (d) Figure 6.3 (a) Continuous-time signal that is applied as the input to several systems for which the frequency response has unity magnitude; (b) response for a system with linear phase; (c) response for a system with nonlinear phase; and (d) response for a system with phase equal to the nonlinear phase of the system in part (c) plus a linear phase term. 430 Time and Frequency Characterization of Signals and Systems Chap.6 In Figure 6.3(a), we depict a signal that is applied as the input to three different systems. Figure 6.3(b) shows the output when the signal is applied as input to a system with fre- quency response H 1( jw) = e- jwto, resulting in an output that equals the input delayed by to seconds. In Figure 6.3( c), we display the output when the signal is applied to a system with unity gain and nonlinear phase function-i.e., (6.10) where 4-H2(jw) is a nonlinear function of w. Figure 6.3(d) shows the output from another system with nonlinear phase. In this case, the corresponding frequency response has a phase shift that is obtained by adding a linear phase term to 4-H2(jw )-i.e., (6.11) Thus, the output in Figure 6.3( d) can be thought of as the response to a cascade of the system H 2(jw) followed by a time shift, so that the waveforms in Figures 6.3(c) and (d) are related through a simple time shift. In Figure 6.4, we illustrate the effect of both linear and nonlinear phase in the discrete-time case. Once again, the signal in Figure 6.4(a) is applied as the input to three different LTI systems, all with unity gain (i.e., IH (e jw )j = 1) . The signals in the subse- quent parts of Figure 6.4 depict the corresponding outputs. In the case of Figure 6.4(b ), the system has linear phase characteristics with integer slope of -5, so that the output equals the input delayed by 5. The phase shifts for the systems associated with Figures 6.4( c) and (d) are nonlinear, but the difference between these two phase functions is linear with integer slope so that the signals in Figures 6.4(c) and (d) are related by a time shift. Note that all the systems considered in the examples illustrated in Figures 6.3 and 6.4 have unity gain, so that the magnitude of the Fourier transform of the input to any of these systems is passed through unchanged by the system. For this reason, such systems are commonly referred to as all-pass systems. The characteristics of an all-pass system are completely determined by its phase-shift characteristics. A more general LTI system H(jw) or H(ejw), of course, imparts both magnitude shaping through the gain jH(jw )j or jH(ejw)j and phase shift that may or may not be linear. 6.2.2 Group Delay As discussed in Section 6.2.1, systems with linear phase characteristics have the particu- larly simple interpretation as time shifts. In fact, from eqs. (6.8) and (6.9), the phase slope tells us the size of the time shift. That is, in continuous time, if 4-H(jw) = -wt0 , then the system imparts a time shift of -to or, equivalently, a delay of t0 . Similarly, in discrete time, 4-H(ejw) = -wn0 corresponds to a delay of n0 . The concept of delay can be very naturally and simply extended to include nonlin- ear phase characteristics. Suppose that we wish to examine the effects of the phase of a continuous-time LTI system on a narrowband input-i.e., an input x(t) whose Fourier transform is zero or negligibly small outside a small band of frequencies centered at w = w0 . By taking the band to be very small, we can accurately approximate the phase of this system in the band with the linear approximation 4-H(jw) = -4> - wa, (6.12) Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 431 0 n (a) 0 5 n (b) n (c) Figure 6.4 (a) Discrete-time signal that is applied as input to several sys- tems for which the frequency response has unity magnitude; (b) response for a system with linear phase with slope of -5; (c) response for a system with nonlinear phase; and (d) response for a system whose phase characteristic is that of part (c) plus a linear phase (d) term with integer slope. 432 Time and Frequency Characterization of Signals and Systems Chap.6 so that (6.13) Thus, the approximate effect of the system on the Fourier transform of this narrowband input consists of the magnitude shaping corresponding to IH(jw )I, multiplication by an overall constant complex factor e-N> and multiplication by a linear phase term e- jwa corresponding to a time delay of a seconds. This time delay is referred to as the group delay at w = w 0 , as it is the effective common delay experienced by the small band or group of frequencies centered at w = w 0 . The group delay at each frequency equals the negative of the slope of the phase at that frequency; i.e., the group delay is defined as r(w) = - d~ {1:-H(jw)}. (6.14) The concept of group delay applies directly to discrete-time systems as well. In the next example we illustrate the effect of nonconstant group delay on a signal. Example 6.1 Consider the impulse response of an all-pass system with a group delay that varies with frequency. The frequency response H (jw) for our example is the product of three factors; i.e., n3 H(jw) = H i(jw ), i=l where . 1 + (jwtwJ - 2j~i (wlwi) Hi(Jw) = (6.15) + 2 , 1 {jw!wi) + 2j~i (w!wi) w 1 = 315 rad/sec and ~1 = 0.066, w2 = 943 rad/sec and ~2 = 0.033, { w3 = 1888 radlsec and ~3 = 0.058. It is often useful to express the frequencies wi measured in radians per second in terms of frequencies fi measured in Hertz, where Wi = 27T fi. In this case, /1 =50Hz h =150Hz h =300Hz. Since the numerator of each of the factors Hi(jw) is the complex conjugate of the corresponding denominator, it follows that IHi(jw )I = 1. Consequently, we may also Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 433 conclude that IH(}w)l = 1. The phase for each H;(jw) can be determined from eq. (6.15): 1 1 <r.H;(jw) = -2 arctan 2{ (wlw·) l , [ 2 1 - (wlw;) and 3 <r.H(jw) = L <r.H;(jw). i= I If the values of <r.H (jw) are restricted to lie between -7r and 7T, we obtain the principal- phase function (i.e., the phase modulo 27T), as shown in Figure 6.5(a) where we have plotted the phase versus frequency measured in Hertz. Note that this function con- tains discontinuities of size 27T at various frequencies, making the phase function non- differentiable at those points. However, the addition or subtraction of any integer multiple of 27T to the value of the phase at any frequency leaves the original frequency response unchanged. Thus, by appropriately adding or subtracting such integer multiples of 27T from various portions of the principal phase, we obtain the unwrapped phase in Fig- ure 6.5(b ). The group delay as a function of frequency may now be computed as where <r.[H(jw )] represents the unwrapped-phase function corresponding to H(jw ). A plot of T(w) is shown in Figure 6.5(c). Observe that frequencies in the close vicinity of 50 Hz experience greater delay than frequencies in the vicinity of 150Hz or 300Hz. The effect of such nonconstant group delay can also be qualitatively observed in the impulse response (see Figure 6.5(d)) of the LTI system. Recall that ~{8(t)} = 1. The frequency components of the impulse are all aligned in time in such a way that they combine to form the impulse, which is, of course, highly localized in time. Since the all-pass system has nonconstant group delay, different frequencies in the input are delayed by different amounts. This phenomenon is referred to as dispersion. In the current example, the group delay is highest at 50 Hz. Consequently, we would expect the latter parts of the impulse response to oscillate at lower frequencies near 50 Hz. This clearly evident in Figure 6.5(d). Example 6.2 Nonconstant group delay is among the factors considered important for assessing the transmission performance of switched telecommunications networks. In a survey1 in- volving locations all across the continental United States, AT &T/Bell System reported group delay characteristics for various categories of toll calls. Figure 6.6 displays some of the results of this study for two such classes. In particular, what is plotted in each curve in Figure 6.6(a) is the nonconstant portion of the group delay for a specific cate- gory of toll calls. That is, for each category, a common constant delay corresponding to 1 ""Analog Transmission Performance on the Switched Telecommunications Network,"" by F. P. Duffy and T. W. Thatcher, Jr., in the Bell System Technical Journal, vol. 50, no. 4, April, 1971. 434 Time and Frequency Characterization of Signals and Systems Chap. 6 l Q) 0 gj .s::. 0.. -2 50 100 150 200 250 300 350 400 Frequency (Hz) (a) -5 ~ i--10 ra .s::. 0.. -15 -20L-----~-------L------~------L-----~-------L------~----~ 0 50 100 150 200 250 300 350 400 Frequency (Hz) (b) 0.08 ¥ ;:: 0.06 ra a; 0 g- 0.04 e :.!) 0.02 50 100 150 200 250 300 350 400 Frequency (Hz) (c) 400 200 0 -200 -400 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 Time (sec) (d) Figure 6.5 Phase, group delay, and impulse response for the all-pass sys- tem of Example 6.1: (a) principal phase; (b) unwrapped phase; (c) group delay; (d) impulse response. Each of these quantities is plotted versus frequency measured in Hertz. Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 435 7,000 0 Short Sho~ 'r ~ 6,000 -5 ~ I '0 c Medium \~ 8 5,000 Q) en 3 Medium~ -10 0 0 I Medium I4,ooo 1 0 dl ~ _Q a; 0 0 C\J -15 0.. Medium I e3 ,000 C) c ~ \\ I -20 en § 2,000 (.) \ c 0 z \\~ 1,000 I ~ -25 \ Short\ 0 ~ ~ Short ~ ~ -30 0 600 1 ,200 1 ,800 2,400 3,000 3,600 0 600 1,200 1,800 2,400 3,000 3,600 Frequency in (Hz) Frequency in (Hz) (a) (b) Figure 6.6 (a) Non-constant portion of the group delay; and (b) frequency re- sponse magnitude as functions of frequency for short- and medium-distance toll calls in switched telecommunications networks [after Duffy and Thatcher]. Each of these quantities is plotted versus frequency measured in Hertz. Also, as is commonly done in practice, the magnitudes of the frequency responses are plotted using a logarithmic scale in units of decibels. That is, what is plotted in (b) is 20 log10 IH(jw )I for the fre- quency responses corresponding to short- and medium-distance toll calls. The use of this logarithmic scale for the frequency-response magnitudes is discussed in detail in Section 6.2.3. the minimum of the group delay over all frequencies has been subtracted from the group delay, and the resulting difference is plotted in Figure 6.6(a). Consequently, each curve in Figure 6.6(a) represents the additional delay (beyond this common constant delay) experienced by the different frequency components of toll calls within each category. The curves labeled SHORT and MEDIUM respectively represent the results for short- distance (0-180 airline miles) and medium-distance (180-725 airline miles) toll calls. The group delay as a function of frequency is seen to be lowest at 1, 700Hz and increases monotonically as we move away from that figure in either direction. When the group delay characteristics illustrated in Figure 6.6(a) are combined with the characteristics of the magnitude of the frequency response reported in the same AT&T /Bell System survey and shown in Figure 6. 6(b ), we obtain impulse reponses of the type shown in Figure 6.7. The impulse response in Figure 6.7(a) corresponds to the short- distance category. The very low- and very high-frequency components of the response occur later than the components in the mid-frequency range. This is compatible with 436 Time and Frequency Characterization of Signals and Systems Chap.6 0.6 0.4 0.2 0 -0.2 -0.4 0 2 3 4 5 6 7 8 9 10 Time (msec) (a) 0.6 0.4 0.2 0 -0.2 -0.4 0 2 3 4 5 6 7 8 9 10 Time (msec) (b) Figure 6.7 Impulse responses associated with the group delay and magnitude char- acteristics in Figure 6.6: (a) impulse response corresponding to the short-distance cate- gory of toll calls; (b) impulse response for the medium-distance category. the corresponding group delay characteristics in Figure 6.6(a). Similarly, Figure 6.7(b) illustrates the same phenomenon for the impulse response corresponding to medium- distance toll calls. 6.2.3 Log-Magnitude and Bode Plots In graphically displaying continuous-time or discrete-time Fourier transforms and system frequency responses in polar form, it is often convenient to use a logarithmic scale for the magnitude of the Fourier transform. One of the principal reasons for doing this can be seen Sec. 6.2 The Magnitude-Phase Representation of the Frequency Response of LTI Systems 437 from eqs. (6.5) and (6.6), which relate the magnitude and phase of the output of an LTI system to those of the input and frequency response. Note that the phase relationship is additive, while the magnitude relationship involves the product of IH(Jw )I and IX(Jw )I. Thus, if the magnitudes of the Fourier transform are displayed on a logarithmic amplitude scale, eq. (6.5) takes the form of an additive relationship, namely, log IY(jw )I = log IH(Jw )I +log IX(Jw )I, (6.16) with an exactly analogous expression in discrete time. Consequently, if we have a graph of the log magnitude and phase of the Fourier transform of the input and the frequency response of an LTI system, the Fourier transform of the output is obtained by adding the log-magnitude plots and by adding the phase plots. In a similar fashion, since the frequency response of the cascade of LTI systems is the product ofthe individual frequency responses, we can obtain plots of the log magnitude and phase of the overall frequency response of cascaded systems by adding the corresponding plots for each of the component systems. In addition, plotting the magnitude of the Fourier transform on a logarithmic scale allows detail to be displayed over a wider dynamic range. For example, on a linear scale, the detailed magnitude characteristics in the stopband of a frequency-selective filter with high attenuation are typically not evident, whereas they are on a logarithmic scale. Typically, the specific logarithmic amplitude scale used is in units of 20 log 10 , re- ferred to as decibels2 (abbreviated dB). Thus, 0 dB corresponds to a frequency response with magnitude equal to 1, 20 dB is equivalent to a gain of 10, -20 dB corresponds to an attenuation of 0.1, and so on. Also, it is useful to note that 6 dB approximately corresponds to a gain of 2. For continuous-time systems, it is also common and useful to use a logarithmic frequency scale. Plots of 20 log 10 IH(Jw )I and 4-H(jw) versus log 10(w) are referred to as Bode plots. A typical Bode plot is illustrated in Figure 6.8. Note that, as discussed in Section 4.3.3, if h(t) is real, then IH(Jw )I is an even function of w and 4-H(jw) is an odd function of w. Because of this, the plots for negative w are superfluous and can be obtained immediately from the plots for positive w. This, of course, makes it pos- sible to plot frequency response characteristics versus log 10(w) for w > 0, as in the figure. The use of a logarithmic frequency scale offers a number of advantages in continu- ous time. For example, it often allows a much wider range of frequencies to be displayed than does a linear frequency scale. In addition, on a logarithmic frequency scale, the shape 2The origin of this particular choice of units and the term decibels can be traced to the definition of power ratios in systems. Specifically, since the square of the magnitude of the Fourier transform of a signal can be interpreted as the energy per unit frequency, or power, in a signal, the square of the magnitude, IH(}w )1 2 or IH(eiw)i 2 , of the frequency response of a system can be thought of as the power ratio between the input and the output of an LTI system. In honor of Alexander Graham Bell, the inventor of the telephone, the term bel was introduced to indicate a factor of 10 in a power ratio, and decibel was used to denote one-tenth of this factor on a logarithmic scale (so that the cascade of 10 systems with 1-dB power ratios each would result in 1 bel of power amplification). Thus, 10 log 10 iH(jw )1 2 is the number of decibels of power amplification for the frequency response H(jw ), and this in tum equals 20 log 10 iH(jw )I in magnitude amplification. 438 Time and Frequency Characterization of Signals and Systems Chap.6 20 - 10 S OdB r------- I ~ -10 i -20 ~ -30 -40 -50 0 r------ 3 7T I 2 v- -1T 10 100 1.000 Figure 6.8 A typical Bode plot. (Note that w is plotted using a logarithmic scale.) of a particular response curve doesn't change if the frequency is scaled. (See Problem 6.30.) Furthermore for continuous-time LTI systems described by differential equations, an approximate sketch of the log magnitude vs. log frequency can often be easily obtained through the use of asymptotes. In Section 6.5, we will illustrate this by developing sim- ple piecewise-linear approximate Bode plots for first- and second-order continuous-time systems. In discrete time, the magnitudes of Fourier transforms and frequency responses are often displayed in dB for the same reasons that they are in continuous time. However, in discrete time a logarithmic frequency scale is not typically used, since the range of frequencies to be considered is always limited and the advantage found for differential equations (i.e., linear asymptotes) does not apply to difference equations. Typical graphi- cal representations of the magnitude and phase of a discrete-time frequency response are shown in Figure 6.9. Here, we have plotted <r:.H(ejw) in radians and iH(ejw)i in decibels [i.e., 20 log10 IH(ejw)IJ as functions of w. Note that for h[n] real, we actually need plot H ( ejw) only for 0 :=::;; w :=::;; 7T, because in this case the symmetry property of the Fourier transform implies that we can then calculate H(ejw) for -7r :=::;; w :=::;; 0 using the relations iH(ejw)i = iH(e- jw)i and <r:.H(e- jw) = - <r:.H(ejw). Furthermore, we need not consider values of lwl greater than 7T, because of the periodicity of H(ejw). Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 439 Figure 6. 9 Typical graphical representations of the magnitude and phase of a discrete-time frequency response H(ei«>). As emphasized in this section, a logarithmic amplitude scale is often useful and important. However, there are many situations in which it is convenient to use a linear amplitude scale. For example, in discussing ideal filters for which the magnitude of the frequency response is a nonzero constant over some frequency bands and zero over others, a linear amplitude scale is more appropriate. Thus, we have introduced both linear and logarithmic graphical representations for the magnitude of the Fourier transform and will use each as appropriate. 6.3 TIME-DOMAIN PROPERTIES OF IDEAL FREQUENCY-SELECTIVE FILTERS In Chapter 3, we introduced the class of frequency-selective filters, i.e., LTI systems with frequency responses chosen so as to pass one or several bands of frequencies with little or no attenuation and to stop or significantly attenuate frequencies outside those bands. As we discussed in Chapters 3, 4, and 5, there are a number of issues of importance that arise 440 Time and Frequency Characterization of Signals and Systems Chap.6 in frequency-selective filtering applications and that relate directly to the characteristics of frequency-selective filters. In this section, we take another look at such filters and their properties. We focus our attention here on lowpass filters, although very similar concepts and results hold for other types of frequency-selective filters such as highpass or bandpass filters. (See Problems 6.5, 6.6, 6.26, and 6.38.) As introduced in Chapter 3, a continuous-time ideallowpass filter has a frequency response of the form H( ·w) = { 1 lwl ::; We . lwl (6.17) } 0 >We This is illustrated in Figure 6.1 O(a). Similarly, a discrete-time ideallowpass filter has a frequency response (6.18) H(jw) w (a) w (b) Figure 6. 1 o (a) The frequency response of a continuous-time ideal low- pass filter; (b) the frequency response of a discrete-time ideal lowpass filter. and is periodic in w, as depicted in Figure 6.10(b). As can be seen from eqs. (6.17) and (6.18) or from Figure 6.10, ideallowpass filters have perfect frequency selectivity. That is, they pass without attenuation all frequencies at or lower than the cutoff frequency We and completely stop all frequencies in the stopband (i.e., higher than we). Moreover, these filters have zero phase characteristics, so they introduce no phase distortion. As we have seen in Section 6.2, nonlinear phase characteristics can lead to signifi- cant changes in the time-domain characteristics of a signal even when the magnitude of its Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 441 spectrum is not changed by the system, and thus, a filter with a magnitude characteristic as in eq. (6.17) or eq. (6.18), but with nonlinear phase, might produce undesirable effects in some applications. On the other hand, an ideal filter with linear phase over the passband, as illustrated in Figure 6.11, introduces only a simple time shift relative to the response of the ideallowpass filter with zero phase characteristic. IH(jw)l 11 -we 0 we w <):: H(jw)= -aw w Figure 6. 11 Continuous-time ideal lowpass filter with linear phase charac- teristic. In Examples 4.18 and 5 .12, we computed the impulse responses of ideal lowpass filters. In particular, the impulse response corresponding to the filter in eq. (6.17) is h(t) = si:~et, (6.19) which is shown in Figure 6.12(a). Similarly, the impulse response of the discrete-time ideal filter in eq. ( 6.18) is h[n] = sin Wen, (6.20) 7Tn which is depicted in Figure 6.12(b) for w c = 7T/4. If either of the ideal frequency responses of eqs. (6.17) and (6.18) is augmented with a linear phase characteristic, the impulse response is simply delayed by an amount equal to the negative of the slope of this phase function, as is illustrated in Figure 6.13 for the continuous-time impulse response. Note that in both continuous and discrete time, the width of the filter passband is proportional to we, while the width of the main lobe of the impulse is proportional to 1/we. As the bandwidth of the filter increases, the impulse response becomes narrower, and vice versa, consistent with the inverse relationship between time and frequency discussed in Chapters 4 and 5. h(t) (a) h[n] n (b) Figure 6.12 (a) The impulse response of the continuous-time ideal lowpass filter of Figure 6.1 O(a); (b) the impulse response of the discrete-time ideal lowpass filter of Figure 6.10(b) with we = 7r/4. h(t-rr) Figure 6. 13 Impulse response of an ideal lowpass filter with magnitude and phase shown in Figure 6.11. 442 Sec. 6.3 Time-Domain Properties of Ideal Frequency-Selective Filters 443 The step responses s(t) and s[n] of the ideallowpass filters in continuous time and discrete time are displayed in Figure 6.14. In both cases, we note that the step responses exhibit several characteristics that may not be desirable. In particular, for these filters, the step responses overshoot their long-term final values and exhibit oscillatory behavior, frequently referred to as ringing. Also, recall that the step response is the running integral or sum of the impulse response-i.e., n s[n] = L h[m]. m= -oo s(t) (a) s[n] • 1 2 n (b) Figure 6. 14 (a) Step response of a continuous-time ideal lowpass filter; (b) step response of a discrete-time ideal lowpass filter. 444 Time and Frequency Characterization of Signals and Systems Chap.6 Since the impulse responses for the ideal filters have main lobes extending from -7Tlwc to +'TT!wc, the step responses undergo their most significant change in value over this time interval. That is, the so-called rise time of the step response, a rough measure of the response time of the filter, is also inversely related to the bandwidth of the filter. 6.4 TIME-DOMAIN AND FREQUENCY-DOMAIN ASPECTS OF NONIDEAL FILTERS The characteristics of ideal filters are not always desirable in practice. For example, in many filtering contexts, the signals to be separated do not always lie in totally disjoint frequency bands. A typical situation might be that depicted in Figure 6.15, where the spectra of two signals overlap slightly. In such a case, we may wish to trade off the fi- delity with which the filter preserves one of these signals-say, x 1( f)-against the level to which frequency components of the second signal x2(t) are attenuated. A filter with a gradual transition from passband to stopband is generally preferable when filtering the superposition of signals with overlapping spectra. X(jw) Figure 6. 1 5 Two spectra that are w slightly overlapping. Another consideration is suggested by examining the step responses of ideallowpass filters, shown in Figure 6.14. For both continuous time and discrete time, the step response asymptotically approaches a constant equal to the value of the step. In the vicinity of the discontinuity, however, it overshoots this value and exhibits ringing. In some situations, this time-domain behavior may be undesirable. Moreover, even in cases where the ideal frequency-selective characteristics are de- sirable, they may not be attainable. For example, from eqs. (6.18) and (6.19) and Fig- ure 6.12, it is evident that the ideal lowpass filter is noncausal. When filtering is to be carried out in real time, however, causality is a necessary constraint, and thus, a causal approximation to the ideal characteristics would be required. A further consideration that motivates providing some flexibility in the filter characteristics is ease of implementation. In general, the more precisely we try to approximate or implement an ideal frequency- selective filter, the more complicated or costly the implementation becomes, whether in terms of components such as resistors, capacitors, and operational amplifiers in continu- ous time or in terms of memory registers, multipliers, and aqders in discrete time. In many contexts, a precise filter characteristic may not be essential and a simple filter will suffice. For all of these reasons, nonideal filters are of of considerable practical importance, and the characteristics of such filters are frequently specified or quantified in terms of several parameters in both the frequency and time domain. First, because the magnitude characteristics of the ideal frequency-selective filter may be unachievable or undesirable, Sec. 6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters 445 it is preferable to allow some flexibility in the behavior of the filter in the passband and in the stopband, as well as to permit a more gradual transition between the passband and stopband, as opposed to the abrupt transition characteristic of ideal filters. For example, in the case of lowpass filters, the specifications may allow some deviation from unity gain in the passband and from zero gain in the stopband, as well as including both a passband edge and stopband edge with a transition band between them. Thus, specifications for a continuous-time lowpass filter are often stated to require the magnitude of the frequency response of the filter to be restricted to the nonshaded area indicated in Figure 6.16. In this figure, a deviation from unity of plus and minus o1 is allowed in the passband, and a deviation of 82 from zero is allowed in the stopband. The amount by which the frequency response differs from unity in the passband is referred to as the passband ripple, and the amount by which it deviates from zero in the stopband is referred to as the stopband ripple. The frequency w P is referred to as the passband edge and w.1. as the stopband edge. The frequency range from w P to w.1· is provided for the transition from passband to stopband and is referred to as the transition band. Similar definitions apply to discrete-time lowpass filters, as well as to other continuous- and discrete-time frequency-selective filters. IH(iw)l ' ' ' ' ' Figure 6. 16 Tolerances for the Passband Tran' ·sjtion Stopband magnitude characteristic of a lowpass ' ' filter. The allowable passband ripple ' ' is o1 and stopband ripple is o2. The ' ~---------------------- dashed curve illustrates one possible ' ...., .. ----- frequency response that stays within w the tolerable limits. In addition to the specification of magnitude characteristics in the frequency domain, in some cases the specification of phase characteristics is also important. In particular, a linear or nearly linear phase characteristic over the passband of the filter is frequently desirable. To control the time-domain behavior, specifications are frequently imposed on the step response of a filter. As illustrated in Figure 6.17, one quantity often of interest is the rise time tr of the step response-i.e., the interval over which the step response rises toward its final value. In addition, the presence or absence of oscillatory behavior, or ringing, in the step response is often of importance. If such ringing is present, then there are three other quantities that are often used to characterize the nature of these oscillations: the overshoot ~ of the final value of the step response, the ringing frequency w r, and the settling time ts-i.e., the time required for the step response to settle to within a specified tolerance of its final value. For nonideallowpass filters, a trade-off may be observed between the width of the transition band (a frequency-domain characteristic) and the settling time of the step re- sponse (a time-domain characteristic). The following example illustrates this trade-off. 446 Time and Frequency Characterization of Signals and Systems Chap.6 s(t) Figure 6. 17 Step response of a continuous-time lowpass filter, indicating the rise time tr, overshoot A, ringing frequency wr, and settling time t5-i.e., the time at which the step response settles to within ::': o of its final value. Example 6.3 Let us consider two specific lowpass filters designed to have a cutoff frequency of 500 Hz. Each filter has a fifth-order rational frequency response and a real-valued impulse response. The two filters are of specific types, one referred to as Butterworth filters and the other as elliptic filters. Both of these classes of filters are frequently used in practice. The magnitudes of the frequency responses of the two filters are plotted (versus frequency measured in Hertz) in Figure 6.18(a). We take the transition band of each filter as the region around the cutoff frequency (500Hz) where the frequency response magnitude is neither within .05 of unity magnitude (the passband ripple) nor within .05 of zero magnitude (the stopband ripple). From Figure 6.18(a), it can be seen that the transition band of the Butterworth filter is wider than the transition band of the elliptic filter. The price paid for the narrower transition band of the elliptic filter may be observed in Figure 6.18(b ), in which the step responses of both filters are displayed. We see that the ringing in the elliptic filter's step response is more prominent than for the Butterworth step response. In particular, the settling time for the step response is longer in the case of the elliptic filter. The consideration of the trade-offs between time-domain and frequency-domain characteristics and of other issues such as the complexity and cost of filters forms the core of the important field of filter design. In the next few sections, and in several of the problems at the end of the chapter, we provide additional examples of LTI systems and filters and their time- and frequency-domain characteristics. Sec. 6.4 Time-Domain and Frequency-Domain Aspects of Nonideal Filters 447 0.9 Q) en 0.8 c 0 0. en 0.7 ~ >- () c 0.6 Q) ::J ' 0"" ' ~ 0.5 '' ' 0 Q) 0.4 ' ""0 ' ' .a ' ·c: Ol 0.3 ' co '\ .....--- Butterworth filter ~ \ 0.2 \ \ \ 0.1 ' ' ' .... ... ___ _ ------~~~-~-----------------------------------~ 0 200 400 600 800 1 ,000 1 ,200 1 ,400 1 ,600 1 ,800 2,000 Frequency (Hz) 1.2 I I I I I I I I I 0.8 I I I I I I 0.6 I I I I I I 0.4 I I I I I I 0.2 I I I I I I 0 2 4 6 8 10 12 14 16 18 20 Time(msec) Figure 6. 18 Example of a fifth-order Butterworth filter and a fifth-order elliptic filter designed to have the same passband and stopband ripple and the same cutoff frequency: (a) magnitudes of the frequency responses plotted versus frequency measured in Hertz; (b) step responses. 448 Time and Frequency Characterization of Signals and Systems Chap.6 6.5 FIRST-ORDER AND SECOND-ORDER CONTINUOUS-TIME SYSTEMS LTI systems described by linear constant-coefficient differential equations are of great practical importance, because many physical systems can be modeled by such equations and because systems of this type can often be conveniently implemented. For a variety of practical reasons, high-order systems are frequently implemented or represented by combining first-order and second-order systems in cascade or parallel arrangements. Con- sequently, the properties of first- and second-order systems play an important role in an- alyzing, designing, and understanding the time-domain and frequency-domain behavior of higher order systems. In this section, we discuss these low-order systems in detail for continuous time. In Section 6.6, we examine their discrete-time counterparts. 6.5.1 First-Order Continuous-Time Systems The differential equation for a first-order system is often expressed in the form dy(t) T~ + y(t) = x(t), (6.21) where the coefficient T is a positive number whose significance will be made clear shortly. The corresponding frequency response for the first-order system is . ) 1 H(]W = . 1' (6.22) JWT + and the impulse response is (6.23) which is sketched in Figure 6.19(a). The step response of the system is s(t) = h(t) * u(t) = [1 - e- 11T]u(t). (6.24) This is sketched in Figure 6.19(b ). The parameter T is the time constant of the system, and it controls the rate at which the first-order system responds. For example, as illustrated in Figure 6.19, at t = T the impulse response has reached 11 e times its value at t = 0, and the step response is within 11 e of its final value. Therefore, as T is decreased, the impulse response decays more sharply, and the rise time of the step response becomes shorter- i.e., it rises more sharply toward its final value. Note also that the step response of a first-order system does not exhibit any ringing. Figure 6.20 depicts the Bode plot of the frequency response of eq. (6.22). In this figure we illustrate one of the advantages of using a logarithmic frequency scale: We can, without too much difficulty, obtain a useful approximate Bode plot for a continuous-time first-order system. To see this, let us first examine the plot of the log magnitude of the frequency response. Specifically, from eq. (6.22), we obtain 20log 10 IH(jw)l = -10log 2 10[(wT) + 1]. (6.25) From this, we see that for wT << 1, the log magnitude is approximately zero, while for wT >> 1, the log magnitude is approximately a linear function of log10(w ). That is, 20 log 10 IH(jw )I = 0 for w << 1/T, (6.26) h(t) (a) s(t) -------------------=--=-~-;;;.;----------- 'T Figure 6.19 Continuous-time first- order system: (a) impulse response; (b) (b) step response. 20 3 dB ___ j 1 0 dB ... Asymptotic I ... ~approximation 0 ci -20 .2 0 C\J -40 -60 0.1h 1h 10h 100fT w 1TI4 0 1 I \1 -1T/4 -1r12 -31T/4 0.1h 1h 10h 100fT Figure 6.20 Bode plot for a w continuous-time first-order system. 449 450 Time and Frequency Characterization of Signals and Systems Chap.6 and 20 log 10 IH(jw )I = -20 log 10(wT) (6.27) = -20log 10(w)- 20log 10(T) for w >> liT. In other words, for the first-order system, the low- and high-frequency asymptotes of the log magnitude are straight lines. The low-frequency asymptote [given by eq. (6.26)] is just the 0-dB line, while the high-frequency asymptote [specified by eq. (6.27)] corresponds to a decrease of 20 dB in IH (jw )I for every decade (i.e., factor of 10 ) in w. This is sometimes referred to as a ""20-dB-per-decade"" asymptote. Note that the two asymptotic approximations given in eqs. (6.26) and (6.27) are equal at the point log 10(w) = -log 10(T), or equivalently, w = liT. Interpreted graphically, this means that the two straight-line asymptotes meet at w = liT, which suggests a straight- line approximation to the magnitude plot. That is, our approximation to 20 log 10 IH(jw )I equals 0 for w :s liT and is given by eq. (6.27) for w 2:: liT. This approximation is also sketched (as a dashed line) in Figure 6.20. The point at which the slope of the approxima- tion changes is precisely w = liT, which, for this reason, is often referred to as the break frequency. Also, note that at w = liT the two terms [(wT)2 and 1] in the argument of the logarithm in eq. (6.25) are equal. Thus, at this point, the actual value of the magnitude is (6.28) Because of this, the point w = liT is sometimes called the 3-dB point. From the figure, we see that only near the break frequency is there any significant error in the straight-line approximate Bode plot. Thus, if we wish to obtain a more accurate sketch of the Bode plot, we need only modify the approximation near the break frequency. It is also possible to obtain a useful straight-line approximation to 1:-H(jw ): 1:-H(jw) = -tan -I (wT) 0, W :S O.liT (6.29) = -(7T/4)[log 10(wT) + 1], O.liT :S W :S 10/T. { -1T/2, W 2:: 10/T Note that this approximation decreases linearly (from 0 to -7T/2) as a function oflog 10(w) in the range 0.1 10 T T i.e., in the range from one decade below the break frequency to one decade above the break frequency. Also, zero is the correct asymptotic value of 1:-H(jw) for w << liT, and -1r12 is the correct asymptotic value of <H(jw) for w >> liT. Furthermore, the approximation agrees with the actual value of 1:-H(jw) at the break frequency w liT, at which point ·m(A) = -i· (6.30) This asymptotic approximation is also plotted in Figure 6.20, and from it we can see how, if desired, we can modify the straight-line approximation to obtain a more accurate sketch of 1:-H(jw ). Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 451 From this first-order system, we can again see the inverse relationship between time and frequency. As we make T smaller, we speed up the time response of the system [i.e., h(t) becomes more compressed toward the origin, and the rise time of the step response is reduced] and we simultaneously make the break frequency large [i.e., H(jw) becomes broader, since IH(jw )I = 1 for a larger range of frequencies]. This can also be seen by multiplying the impulse response by T and observing the relationship between rh(t) and H(jw): rh(t) = e-tiT u(t), H(jw) = jwr +I"" Thus, rh(t) is a function of tiT and H(jw) is a function of wr, and from this we see that changing Tis essentially equivalent to a scaling in time and frequency. 6.5.2 Second-Order Continuous-Time Systems The linear constant-coefficient differential equation for a second-order system is d 2y(t) dy(t) 2 2 ~ + 2~wndt + wny(t) = wnx(t). (6.31) Equations of this type arise in many physical systems, including RLC circuits and me- chanical systems, such as the one illustrated in Figure 6.21, composed of a spring, a mass, and a viscous damper or dashpot. In the figure, the input is the applied force x(t) and the output is the displacement of the mass y(t) from some equilibrium position at which the spring exerts no restoring force. The equation of motion for this system is d2y(t) = ( ) - k ( ) - bdy(t) m dt2 x t y t dt ' or 2 d y(t) + (!?_)dy(t) + (!__)y(t) = _!_x(t). dt2 m dt m m Comparing this to eq. (6.31), we see that if we identify Wn~A (6.32) and b ~=- 2Jb;z' ~ y(t) (displacement) x(t) (applied force) Figure 6.21 Second-order system consisting of a spring and dashpot attached to a moveable mass and a fixed support. 452 Time and Frequency Characterization of Signals and Systems Chap.6 then [except for a scale factor of k on x(t)] the equation of motion for the system of Figure 6.21 reduces to eq. (6.31). The frequency response for the second-order system of eq. (6.31) is 2 H(jw) = (jw)2 + 2(::(jw) + (6.33) wr The denominator of H(jw) can be factored to yield 2 H(jw) = (J' W- ~(- )' CJ JW- C2 where CJ = -(wn + WnJf2=1, (6.34) C2 = -(wn - Wn~· For ( =I= 1, c 1 and c2 are unequal, and we can perform a partial-fraction expansion of the form M M H(jw) = --- (6.35) jw- c 1 jw- c2' where (6.36) From eq. (6.35), the corresponding impulse response for the system is h(t) = M[ec 1t - ec2t]u(t). (6.37) If ( = 1, then CJ = c2 = -wn, and w2 H(jw) = (jw +nwn)2' (6.38) From Table 4.2, we find that in this case the impulse response is h(t) = w~te -wnt u(t). (6.39) Note from eqs. (6.37) and (6.39), that h(t)lwn is a function of wnt. Furthermore, eq. (6.33) can be rewritten as H(jw) = 2 1 , (jwlwn) + 2( (jwlwn) + 1 from which we see that the frequency response is a function of wlwn. Thus, changing wn is essentially identical to a time and frequency scaling. The parameter ( is referred to as the damping ratio and the parameter w n as the undamped natural frequency. The motivation for this terminology becomes clear when Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 453 we take a more detailed look at the impulse response and the step response of a second- order system. First, from eq. (6.35), we see that for 0 < ( < 1, c1 and c2 are complex, and we can rewrite the impulse response in eq. (6.37) in the form (6.40) Thus, for 0 < ( < 1, the second-order system has an impulse response that has damped oscillatory behavior, and in this case the system is referred to as being under- damped. If ( > 1, both c1 and c2 are real and negative, and the impulse response is the difference between two decaying exponentials. In this case, the system is overdamped. The case of ( = 1, when c1 = c2, is called the critically damped case. The impulse responses (multiplied by llwn) for second-order systems with different values of ( are plotted in Figure 6.22(a). (a) s(t) 2 Figure 6.22 Response of continu- ous-time second-order systems with different values of the damping ratio r (a) impulse response; (b) (b) step response. 454 Time and Frequency Characterization of Signals and Systems Chap.6 The step response of a second-order system can be calculated from eq. (6.37) for ( =I: 1. This yields the expression ecJt ec""t]J s(t) = h(t) * u(t) = { 1 + M [-c;- - ~ u(t). (6.41) For ( = 1, we can use eq. (6.39) to obtain s(t) = [1 - e-wnt - W te-w""t]u(t). 11 (6.42) The step response of a second-order system is plotted in Figure 6.22(b) for several values of (. From this figure, we see that in the underdamped case, the step response exhibits both overshoot (i.e., the step response exceeds its final value) and ringing (i.e., oscillatory behavior). For ( = 1, the step response has the fastest response (i.e., the shortest rise time) that is possible without overshoot and thus has the shortest settling time. As ( increases beyond 1, the response becomes slower. This can be seen from eqs. (6.34) and (6.41). As ( increases, c1 becomes smaller in magnitude, while c2 increases in magnitude. Therefore, although the time constant (l/jc2)) associated with ec2r decreases, the time constant (1/jc1)) associated with ec 1r increases. Consequently the term involving ec 1r in eq. (6.41) takes a longer time to decay to zero, and thus it is the time constant associated with this term that determines the settling time of the step response. As a result the step response takes longer to settle for large values of (. In terms of our spring -dashpot example, as we increase the magnitude of the damping coefficient b beyond the critical value at which (in eq. (6.33) equals 1, the motion of the mass becomes increasingly sluggish. Finally, note that, as we have said, the value of w 11 essentially controls the time scale of the responses h(t) and s(t). For example, in the underdamped case, the larger W 11 is, the more compressed is the impulse response as a function oft, and the higher is the frequency of the oscillations or ringing in both h(t) and s(t). In fact, from eq. (6.40), we see that the frequency of the oscillations in h(t) and s(t) is w 11 ~'which does increase with increasing w 11 • Note, however, that this frequency depends explicitly on the damping ratio and does not equal (and is in fact smaller than) w 11 , except in the undamped case, ( = 0. (It is for this reason that the parameter w n is traditionally referred to as the undamped natural frequency.) For the spring-dashpot example, we therefore conclude that the rate of oscillation of the mass equals w n when no dash pot is present, and the oscillation frequency decreases when we include the dashpot. In Figure 6.23, we have depicted the Bode plot of the frequency response given in eq. (6.33) for several values of(. As in the first-order case, the logarithmic frequency scale leads to linear high- and low-frequency asymptotes for the log magnitude. Specifically, from eq. (6.33), (6.43) From this expression, it follows that forw << Wn forw >> (6.44) Wn · Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 455 Asymptotic 8 approximation -20 I 0 Ol .Q -40 0 C\J -60 -80 0.1wn Wn 10wn w 0 -7T/4 3 Asymptotic ~ -7T/2 approximation v - 37T/4 w Figure 6.23 Bode plots for second-order systems with several different values of damping ratio r Therefore, the low-frequency asymptote of the log magnitude is the 0-dB line, while the high-frequency asymptote [given by eq. (6.44)] has a slope of -40 dB per decade; i.e., IH(jw )I decreases by 40 dB for every increase in w of a factor of 10. Also, note that the two straight-line asymptotes meet at the point w = wn. Thus, we obtain a straight-line approximation to the log magnitude by using the approximation given in eq. (6.44) for w ::; Wn. For this reason, Wn is referred to as the break frequency of the second-order system. This approximation is also plotted (as a dashed line) in Figure 6.23. We can, in addition, obtain a straight-line approximation to 4-H(jw ), whose exact expression can be obtained from eq. (6.33): . -I ( 2((wlwn) ) 4-H(jw) = -tan 1 - (wlwn)2 . (6.45) 456 Time and Frequency Characterization of Signals and Systems Chap. 6 The approximation is w ~ O.lwn O.lwn ~ w ~ !Own, (6.46) w 2::: !Own which is also plotted in Figure 6.23. Note that the approximation and the actual value again are equal at the break frequency w = Wn, where It is important to observe that the asymptotic approximations, eqs. (6.44) and (6.46), we have obtained for a second-order system do not depend on {, while the actual plots of !H(jw )I and <XH(jw) certainly do, and thus, to obtain an accurate sketch, especially near the break frequency w = Wn, we must take this into account by modifying the approxi- mations to conform more closely to the actual plots. The discrepancy is most pronounced for small values of{. In particular, note that in this case the actual log magnitude has a peak around w = Wn. In fact, straightforward calculations using eq. (6.43) show that, for { < J212 = 0.707, IH(jw)i has a maximum value at (6.47) and the value at this maximum point is (6.48) For { > 0. 707, however, H (jw) decreases monotonically as w increases from zero. The fact that H (jw) can have a peak is extremely important in the design of frequency-selective filters and amplifiers. In some applications, one may want to design such a circuit so that it has a sharp peak in the magnitude of its frequency response at some specified frequency, thereby providing large frequency-selective amplification for sinusoids at fre- quencies within a narrow band. The quality Q of such a circuit is defined to be a measure of the sharpness of the peak. For a second-order circuit described by an equation of the form of eq. (6.31), the quality is usually taken to be and from Figure 6.23 and eq. (6.48), we see that this definition has the proper behavior: The less damping there is in the system, the sharper is the peak in !H(jw )I. 6.5.3 Bode Plots for Rational Frequency Responses At the start of this section, we indicated that first- and second-order systems can be used as basic building blocks for more complex LTI systems with rational frequency responses. One consequence of this is that the Bode plots presented here essentially provide us with Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 457 all of the information we need to construct Bode plots for arbitrary rational frequ~ncy responses. Specifically, we have described the Bode plots for the frequency responses given by eqs. (6.22) and (6.33). In addition, we can readily obtain the Bode plots for frequency responses of the forms H(jw) = 1 + jwT (6.49) and . ) H(jw) = 1 + 2( (-.W ) + (-)'W )2 . (6.50) Wn Wn The Bode plots for eqs. (6.49) a11d (6.50) follow directly from Figures 6.20 and 6.23 and from the fact that 20 log10 IH(iw)l = -20 log10 IH(~w) I and <t(H(jw )) = - <t (H(~w)). Also, consider a system function that is a constant gain H(jw) = K. Since K = iKiej·O if K > 0 and K = iKiej7T if K < 0, we see that 20 log 10 iH(jw )I = 20 log 10 IKI -tH(1 · w) = { 0, ~f K > 0 1T, If K < 0 Since a rational frequency response can be factored into the product of a constant gain and first- and second-order terms, its Bode plot can be obtained by summing the plots for each of the terms. We illustrate further the construction of Bode plots in the next two examples. Example 6.4 Let us obtain the Bode plot for the frequency response 2 X 104 H(jw) = (jw )2 + lOOjw + 104 · First, we note that H(jw) = 2H(jw), where H(jw) has the same form as the standard second-order frequency response spec- ified by eq. (6.33). It follows that 20 log 10 iH(Jw )I = 20 log 10 2 + 20 log iH(Jw )1. 458 Time and Frequency Characterization of Signals and Systems Chap.6 By comparing H(jw) with the frequency response in eq. (6.33), we conclude that Wn = 100 and~ = 112 for H(jw). Using eq. (6.44), we may now specify the asymptotes for 20 log10 \H(}w )\: 20 log10 \H(jw )\ = 0 for w << 100, and 20 log10 \H(}w )\ = -40 log 10 w + 80 for w >> 100. It follows that 20 log10 \H(}w )\ will have the same asymptotes, except for a constant offset at all frequencies due to the addition of the 20 log10 2 term (which approxi- mately equals 6 dB). The dashed lines in Figure 6.24(a) represent these asymptotes. OdB -20 1 :I -40 0 c) .2 -60 0 C\J -80 -100~----------~----------~----------~----------~ 1 o0 1 o1 1 o2 1 o3 1 o4 w (a) ~ -TI/2 I v- w (b) Figure 6.24 Bode plot for system function in Example 6.4: (a) magnitude; (b) phase. Sec. 6.5 First-Order and Second-Order Continuous-Time Systems 459 The solid curve in the same figure represents the actual computer-generated Bode plot for 20 log 10 IH(jw )1. Since the value of~ for H(jw) is less than J212. the actual Bode plot has a slight peak near w = 100. To obtain a plot of <rH (jw ), we note that <rH (jw) = <rH (jw) and that <rH(jw) has its asymptotes specified in accordance with eq. (6.46); that is, 0, w ~ 10 <rH(jw) = =~/2)[1og 10(w/100) + 1], 10 ~ w ~ 1,000. { w ;:::: 1,000. The asymptotes and the actual values for <r.H(jw) are plotted with dashed and solid lines, respectively, in Figure 6.24(b). Example 6.5 Consider the frequency response H ·w _ 100(1 + jw) (j ) - (10 + jw)(lOO + jw) To obtain the Bode plot for H (jw ), we rewrite it in the following factored form: Here, the first factor is a constant, the next two factors have the standard form for a first- order frequency response as specified in eq. (6.22), and the fourth factor is the reciprocal of the same first-order standard form. The Bode plot for 20 log 10 IH(jw >I is therefore the sum of the Bode plots corresponding to each of the factors. Furthermore, the asymptotes corresponding to each factor may be summed to obtain the asymptotes for the overall Bode plot. These asymptotes and the actual values of 20 log 10 IH (jw )I are displayed in Figure 6.25(a). Note that the constant factor of 1/10 accounts for an offset of -20 dB at each frequency. The break frequency at w = 1 corresponds to the ( 1 + jw) factor, which produces the 20 dB/decade rise that starts at w = 1 and is canceled by the 20 dB/decade decay that starts at the break frequency at w = 10 and is due to the 11(1 + jwll 0) factor. Finally, the 11(1 + jw/1 00) factor contributes another break frequency at w = 100 and a subsequent decay at the rate of 20 dB/decade. Similarly we can construct the asymptotic approximation for H (jw) from the in- di vidual asymptotes for each factor, as illustrated, together with a plot of the exact value of the phase, in Figure 6.25(b). In particular, the constant factor 1110 contributes 0 to the phase, while the factor ( 1 + jw) contributes an asymptotic approximation that is 0 for w < 0.1, and rises linearly as a function of log 10(w) from a value of zero at w = 0.1 to a value of 7T/2 radians at w = 10. However, this rise is canceled at w = 1 by the asymptotic approximation for the angle of 11(1 + jwll 0) which contributes a linear de- crease in angle of 7T/2 radians over the range of frequencies from w = 1 to w = 100. Finally, the asymptotic approximation for the angle of 1/(1 + jw/100) contributes an- other linear decrease in angle of 7T/2 radians over the range of frequencies from w = 10 tow = 1000. 460 Time and Frequency Characterization of Signals and Systems Chap.6 -40 0.1 10 100 1000 w (a) -rr/2 -rr/4 0 --rr/4 --rr/2 0.1 10 100 1000 w (b) Figure 6.25 Bode plot for system function in Example 6.5: (a) magnitude; (b) phase. In our discussion of first-order systems in this section, we restricted our atten- tion to values of T > 0. In fact, it is not difficult to check that if T < 0, then the causal first-order system described by eq. (6.21) has an impulse response that is not absolutely integrable, and consequently, the system is unstable. Similarly, in analyzing the second-order causal system in eq. (6.31), we required that both ( and w~ be pos- itive numbers. If either of these is not positive, the resulting impulse response is not absolutely integrable. Thus, in this section we have restricted attention to those causal first- and second-order systems that are stable and for which we can define frequency responses. Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 461 6.6 FIRST-ORDER AND SECOND-ORDER DISCRETE-TIME SYSTEMS In this section, we examine the properties of first- and second-order discrete-time LTI systems, paralleling the development in the preceding section. As in continuous time, any system with a frequency response that is a ratio of polynomials in e- jw_i.e., any discrete- time LTI system described by a linear constant-coefficient difference equation-can be written as a product or sum of first- and second-order systems, implying that these ba- sic systems are of considerable value in both implementing and analyzing more complex systems. (See, for example, Problem 6.45.) 6.6.1 First-Order Discrete-Time Systems Consider the first-order causal LTI system described by the difference equation y[n] - ay[n - 1] = x[n], (6.51) with Ia I < 1. From Example 5 .18, the frequency response of this system is (6.52) and its impulse response is (6.53) which is illustrated in Figure 6.26 for several values of a. Also, the step response of the system is 1- an+l s[n] = h[n] * u[n] 1 -a u[n], (6.54) which is illustrated in Figure 6.27. The magnitude of the parameter a plays a role similar to that of the time constant Tin the continuous-time first-order system. Specifically, lal determines the rate at which the first-order system responds. For example, from eqs. (6.53) and (6.54) and Figures 6.26 and 6.27, we see that h[n] and s[n] converge to their final value at the rate at which lain converges to zero. Therefore, the impulse response decays sharply and the step response settles quickly for lal small. For lal nearer to 1, these responses are slower. Note that unlike its continuous-time counterpart, the first-order system described by eq. (6.51) can display oscillatory behavior. This occurs when a < 0, in which case the step response exhibits both overshoot of its final value and ringing. The magnitude and phase of the frequency response of the first-order system in eq. (6.51) are, respectively, IH (e jw )I = -----=-_1_ _- -:-:-::- (6.55) (1 + a2 - 2a cos w) 112 hI[n] 1 h[n] a=+1- 11 a=+1- •••••••••• t ••••••••••••••••••••: ••• •••••••••• Jr, •••••••••••••••••• : ••• 0 n 0 n h[n] h[n] 1 ......... ~1 a=-- 2 ol ................. :~~: .. . n n (a) (b) h[n] h[n] 11 a=+3- 11 7 0000000000 IItrr,, •.•........... : ... .......... IIIIIJttTTTtt•····:~:: ... 0 n 0 n h[n] h[n] a=-3- 7 4 a=-- 8 n 0 n (c) (d) Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 463 :2 r ·""+4! :r 8""+! ......•.. : JJIIIIIIJIJJJJJJJJJJJJJJJ 0 n ......... : tiiiiiiiiiiiiiiiiiiiiiiii 0 n :[tn] a=-.l :t[n] a=-.l 2 4 2 2 ••••••••• : tTTTTTTTTTTTTTTTTTTTTTTTT ••••••••• : tTTTTTTTTTTTTTTTTTTTTTTTT 0 n 0 n (a) (b) s[n] 8 a=+Z. 8 7 s[n] a=+3- 6 5 4 5 4 4 3 ~ ·ll ; ·ll 0 n 0 n st[n] a=-~ s[tn] a=-Z. 3 4 3 8 ••••••••• : tTtTttttttttttttttttttttt ••••••••• :_.t,T.Ttltlttttttttttttttt 0 n 0 n (c) (d) Figure 6.27 Step response s[n] of a first-order system: (a) a = ±1/4; (b) a = ±1/2; (c) a = ±3/4; (d) a = ±7/8. and <r..H (e jw) = - tan- I [ a sin w l· (6.56) 1- acosw In Figure 6.28( a), we have plotted the log magnitude and the phase of the frequency response in eq. (6.52) for several values of a > 0. The case of a < 0 is illustrated in 464 Time and Frequency Characterization of Signals and Systems Chap.6 Figure 6.28(b ). From these figures, we see that for a > 0, the system attenuates high frequencies [i.e., jH(ejw)i is smaller for w near ±1r than it is for w near 0], while when a < 0, the system amplifies high frequencies and attenuates low frequencies. Note also that for lal small, the maximum and minimum values, 1/(1 +a) and 1/(1- a), of jH(ejw)i are close together in value, and the graph ofjH(ejw)i is relatively flat. On the other hand, for lal near 1, these quantities differ significantly, and consequently jH(ejw)i is more sharply peaked, providing filtering and amplification that is more selective over a narrow band of frequencies. 20 log10 IH(ei""')l 20dB w -8 a= I 'IT 3 8 2 a=- 1 4 a=- 2 a=1 4 w (a) Figure 6.28 Magnitude and phase of the frequency response of eq. (6.52) for a first-order system: (a) plots for several values of a > 0; (b) plots for several values of a < 0. Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 465 20 log10 IH(eiw)l a=--7 8 a=-~ ~ 20dB 4 16 12 8 4 21T w -8 a=- I. 8 1T a=--3 4 2 a=--1 a=-12 4 w (b) Figure 6.28 Continued 6. 6. 2 Second-Order Discrete-Time Systems Consider next the second-order causal LTI system described by y[n] - 2r cos Oy[n - 1] + r 2y[n - 2] = x[n], (6.57) with 0 < r < 1 and 0 :::; 8 :::; 7r. The frequency response for this system is H(e jw) -- 1 1- . 2rcosoe-Jw + 2 .2 . (6.58) r e-J w The denominator of H ( ejw) can be factored to obtain jw _ 1 H(e ) - (6.59) [1 - (reJ· e )e-J.W ][1 - (re- ·e · · 1 )e-JW] 466 Time and Frequency Characterization of Signals and Systems Chap.6 For()¥= 0 or 7T, the two factors in the denominator of H(eiw) are different, and a partial- fraction expansion yields (6.60) where ej{J e-.ifJ A=--- B= (6.61) 2j sin()' 2j sin()· In this case, the impulse response of the system is h[n] = [A(rei8 yz + B(re-i 8)'1 ]u[n] _ 11 sin[(n + 1)()] [ ] (6.62) -r ·e un. Sill For() = 0 or 7T, the two factors in the denominator of eq. (6.58) are the same. When() = 0, (6.63) and h[n] = (n + l)r11 u[n]. (6.64) When() = 7T, (6.65) and h[n] = (n + 1)( -rYu[n]. (6.66) The impulse responses for second-order systems are plotted in Figure 6.29 for a range of values of rand(). From this figure and from eq. (6.62), we see that the rate of decay of h[n] is controlled by r-i.e., the closer r is to 1, the slower is the decay in h[n]. Similarly, the value of () determines the frequency of oscillation. For example, with () = 0 there is no oscillation in h[n], while for() = 7T the oscillations are rapid. The effect of different values of rand() can also be seen by examining the step response of eq. (6.57). For() ¥= 0 or 7T, (1_ ( rei8)n+l) (1 _( re-.i8)n+I )~ s[n] = h[n] * u[n] = [A 1 _ re.ifJ + B 1 _ re-JfJ ~ u[n]. (6.67) Also, using the result of Problem 2.52, we find that for () = 0, 1 r n r n] ] s[n] = [ (r- 1)2 - (r- 1)2 r + r- 1 (n + l)r u[n ' (6.68) while for() = 7T, s[n] = [(r: 1)2 + (r: !)2 (-r)"" + r ~ 1 (n + 1)(-r)""]u[n]. (6.69) The step response is plotted in Figure 6.30, again for a range of values of rand (). Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 467 r=~ r=~ 1 1 r=- r=- 4 2 0=0 1 !l=O 1~ !l=O 1l rl~ll~illllr~: :! 0 n 0 n 0 n 1 1 3 r=4 r=2 r=4 ~ !j--:4!! ~ !l=:!! 1 1 4 1 ~ e--:4!! I 0 n 0 n o!II '"" ... n 1 1 3 r=4 r=2 r=4 I !j-:!! !l=:!! 1 -2 l L !j--:2!! 1 2 I, I 0 n o l n o{ I I n 1 1 3 r= 4 r=2 r=4 e-- 3 1 1 41T I. !-j-4~ e--~4 e--~4 1 lr, of 11 r '· n o[' n of!l' n 1 1 3 r= 4 r=2 r= 4 t 1 1 !l=7r fl='IT !l=7r 1 lr r', i flor 1 TP TP"" or n n 0 n Figure 6.29 Impulse response of the second-order system of eq. (6.57) for a range of values of rand e. The second-order system given by eq. (6.57) is the counterpart of the underdamped second-order system in continuous time, while the special case of (} = 0 is the critically damped case. That is, for any value of(} other than zero, the impulse response has a damped 1 r=4 r=~ r--4~ s[n] s[n] s[n] 4 16 r=1- 4 4 r=-1 r=3- 14 2 4 3 fl=O 8=0 12 fl=O 3 10 1-l=O 2 2 8 6 4 2 0 n 0 n n s[n] s[n] s[n] 4 1 4 1 4 3 r= 4 r= 2 r=4 3 e- 3 3 fj-TI -T4I e--T4I -4 'IT 8=4 2 2 2 n 0 n 0 n s[n] s[n] s[n] 4 1 r=4 4 1 r=2 4 3 r=4 3 IJ--T2I 3 IJ--T2I 3 ll=:rr 'IT 2 8=2 2 2 2 0 n 0 n 0 n s[n] s[n] s[n] 4 1 4 1 3 r=4 4 r= 2 r=4 3 f-j-4~ 3 fl_-341T 3 e--~4 8-- 3'4TT 2 2 2 0 n 0 n n s[n] s[n] s[n] 4 1 4 1 4 3 r=4 r=2 r=4 3 3 3 fl=TI fl=TI fl=TI 8=7T 2 2 2 0 n n n *Note: The plot for r= ~ , 8=0 has a different scale from the others. Figure 6.30 Step response of the second-order system of eq. (6.57) for a range of values of rand o. 468 Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 469 oscillatory behavior, and the step response exhibits ringing and overshoot. The frequency response of this system is depicted in Figure 6.31 for a number of values of r and (}. From Figure 6.31, we see that a band of frequencies is amplified , and r determines how sharply peaked the frequency response is within this band. As we have just seen, the second-order system described in eq. (6.59) has factors with complex coefficients (unless(} = 0 or 7T). It is also possible to consider second-order systems having factors with real coefficients. Specifically, consider H(e jw ) -- . 1 . , (6 .70) (1 - d, e-.Jw)(l - d2e- .JW) where d 1 and d2 are both real numbers with ld1l, ld2 1 < 1. Equation (6.70) is the frequency response for the difference equation 8=0 (I) (a) Figure 6.31 Magnitude and phase of the frequency response of the second-order system of eq. (6.57): (a) e = 0; (b) e = TT/4; (c) e = TT/2; (d) e = 3TT!4; (e) e = TT. Each plot contains curves corresponding to r = 1/4, 1/2, and 3/4. 470 Time and Frequency Characterization of Signals and Systems Chap.6 w -12 w (b) Figure 6. 31 Continued (6.71) In this case, (6.72) where (6.73) Thus, h[n] = [Adf + Bd2Ju[n], (6.74) Sec. 6.6 First-Order and Second-Order Discrete-Time Systems 471 24 dB 20 16 r= ~ 4 w -8 -12 e=II 2 (c) Figure 6.31 Continued which is the sum of two decaying real exponentials. Also, 11 [ (1- d + I) (1 - dn+ I) ~ s[n] = A 1 _ ~ 1 + B 1 _ ~2 ~ u[n]. (6.75) The system with frequency response given by eq. (6.70) corresponds to the cascade of two first-order systems. Therefore, we can deduce most of its properties from our un- derstanding of the first-order case. For example, the log-magnitude and phase plots for eq. (6.70) can be obtained by adding together the plots for each of the two first-order terms. Also, as we saw for first-order systems, the response of the system is fast if ld11 and ld2l are small, but the system has a long settling time if either of these magnitudes is near 1. Furthermore, if d 1 and d2 are negative, the response is oscillatory. The case when both d 1 and d2 are positive is the counterpart of the overdamped case in continuous time, with the impulse and step responses settling without oscillation. 472 Time and Frequency Characterization of Signals and Systems Chap.6 20 log10 JH(eiw)J 24 dB 20 w -12 £l_3""1T u- 4 w (d) Figure 6.31 Continued In this section, we have restricted attention to those causal first- and second-order systems that are stable and for which the frequency response can be defined. In particular, the causal system described by eq. (6.51) is unstable for I a I ~ 1. Also, the causal system described by eq. (6.56) is unstable if r ~ 1, and that described by eq. (6.71) is unstable if either I d 1 I or I d2 I exceeds 1. 6. 7 EXAMPLES OF TIME- AND FREQUENCY-DOMAIN ANALYSIS OF SYSTEMS Throughout this chapter, we have illustrated the importance of viewing systems in both the time domain and the frequency domain and the importance of being aware of trade-offs in the behavior between the two domains. In this section, we illustrate some of these issues further. In Section 6.7.1, we discuss these trade-offs for continuous time in the context of an automobile suspension system. In Section 6. 7 .2, we discuss an important class of discrete-time filters referred to as moving-average or nonrecursive systems. Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 473 w 0='1T w (e) Figure 6.31 Continued 6. 7. 1 Analysis of an Automobile Suspension System A number of the points that we have made concerning the characteristics and trade-offs in continuous-time systems can be illustrated in the interpretation of an automobile sus- pension system as a lowpass filter. Figure 6.32 shows a diagrammatic representation of a simple suspension system comprised of a spring and dashpot (shock absorber). The road surface can be thought of as a superposition of rapid small-amplitude changes in elevation (high frequencies), representing the roughness of the road surface, and gradual changes in elevation (low frequencies) due to the general topography. The automobile suspension system is generally intended to filter out rapid variations in the ride caused by the road surface (i.e., the system acts as a lowpass filter). The basic purpose of the suspension system is to provide a smooth ride, and there is no sharp, natural division between the frequencies to be passed and those to be rejected. Thus, it is reasonable to accept and, in fact, prefer a lowpass filter that has a gradual 474 Time and Frequency Characterization of Signals and Systems Chap. 6 Reference elevation Figure 6.32 Diagrammatic representation of an automotive suspension system. Here, Yo represents the distance between the chassis and the road surface when the automobile is at rest, y(t) + Yo the position of the chassis above the reference elevation, and x(t) the elevation of the road above the reference elevation. transition from passband to stopband. Furthermore, the time-domain characteristics of the system are important. If the impulse response or step response of the suspension system exhibits ringing, then a large bump in the road (modeled as an impulse input) or a curb (modeled as a step input) will result in an uncomfortable oscillatory response. In fact, a common test for a suspension system is to introduce an excitation by depressing and then releasing the chassis. If the response exhibits ringing, it is an indication that the shock absorbers need to be replaced. Cost and ease of implementation also play an important role in the design of au- tomobile suspension systems. Many studies have been carried out to determine the most desirable frequency-response characteristics for suspension systems from the point of view of passenger comfort. In situations where the cost may be warranted, such as for passenger railway cars, intricate and costly suspension systems are used. For the automotive indus- try, cost is an important factor, and simple, less costly suspension systems are generally used. A typical automotive suspension system consists simply of the chassis connected to the wheels through a spring and a dashpot. In the diagrammatic representation in Figure 6.32, y0 represents the distance be- tween the chassis and the road surface when the automobile is at rest, y(t) + y0 the position of the chassis above the reference elevation, and x(t) the elevation of the road above the reference elevation. The differential equation governing the motion of the chassis is then 2 Md y(t) + bddy(tt) + k ()- k ) bdx(t) (6.76) ~ y t - x(t + df' where M is the mass of the chassis and k and b are the spring and shock absorber constants, respectively. The frequency response of the system is . k + bjw H(jw) = (jw)2M + b(jw) + k' or w~ + 2{wn(jw) (6.77) H(jw) = (jw )2 + 2{wn(jw) + w~' Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 475 where Wn = a and '4wn = ! As in Section 6.5 .2, the parameter w n is referred to as the undamped natural frequency and ~ as the damping ratio. A Bode plot of the log magnitude of the frequency response in eq. (6.77) can be constructed by using first-order and second-order Bode plots. The Bode plot for eq. (6.77) is sketched in Figure 6.33 for several different values of the damping ratio. Figure 6.34 illustrates the step response for several different values of the damping ratio. As we saw in Section 6.5 .2, the filter cutoff frequency is controlled primarily through w 11 , or equivalently for a chassis with a fixed mass, by an appropriate choice of spring constant k. For a given w 11 , the damping ratio is then adjusted through the damping factor b associated with the shock asorbers. As the natural frequency w 11 is decreased, the suspen- sion will tend to filter out slower road variations, thus providing a smoother ride. On the other hand, we see from Figure 6.34 that the rise time of the system increases, and thus the system will feel more sluggish. On the one hand, it would be desirable to keep w 11 small to improve the lowpass filtering; on the other hand, it would be desirable to have Wn large for a rapid time response. These, of course, are conflicting requirements and illustrate the need for a trade-off between time-domain and frequency-domain characteristics. Typically, a suspension system with a low value of w n, so that the rise time is long, is characterized as ""soft"" and one with a high value of wn, so that the rise time is short, is characterized as ""hard."" From Figures 6.33 and 6.34, we observe also that, as the damping ratio decreases, the frequency response of the system cuts off more sharply, but the overshoot and ring- ing in the step response tend to increase, another trade-off between the time and frequency 20 3 0 dB I 0 Oi .Q 0 C\J -20 -40 Frequency Figure 6.33 Bode plot for the magnitude of the frequency response of the automobile suspension system for several values of the damping ratio. 476 Time and Frequency Characterization of Signals and Systems Chap.6 s(t) Figure 6.34 Step response of the automotive suspension system for vari- ous values of the damping ratio (? = 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 1.5, 2.0, 5.0). domains. Generally, the shock absorber damping is chosen to have a rapid rise time and yet avoid overshoot and ringing. This choice corresponds to the critically damped case, with { = 1.0, considered in Section 6.5.2. 6. 7. 2 Examples of Discrete-Time Non recursive Filters In Section 3.11, we introduced the two basic classes of LTI filters described by difference equations, namely, recursive or infinite impulse response (IIR) filters and nonrecursive or finite impulse response (FIR) filters. Both of these classes of filters are of considerable importance in practice and have their own advantages and disadvantages. For example, recursive filters implemented as interconnections of the first- and second-order systems described in Section 6.6 provide a flexible class of filters that can be easily and efficiently implemented and whose characteristics can be adjusted by varying the number and the parameters of each of the component first- and second-order subsystems. On the other hand, as shown in Problem 6.64, it is not possible to design a causal, recursive filter with exactly linear phase, a property that we have seen is often desirable since, in that case, the effect of the phase on the output signal is a simple time delay. In contrast, as we show in this section, nonrecursive filters can have exactly linear phase. However, it is generally true that the same filter specifications require a higher order equation and hence more coefficients and delays when implemented with a nonrecursive equation, compared with a recursive difference equation. Consequently, for FIR filters, one of the principal trade-offs between the time and frequency domains is that increasing the flexibility in specifying the frequency domain characteristics of the filter, including, for example, achieving a higher degree of frequency selectivity, requires an FIR filter with an impulse response of longer duration. One of the most basic nonrecursive filters, introduced in Section 3.11.2, is the moving-average filter. For this class of filters, the output is the average of the values of the input over a finite window: M y[n] N M 1 L x[n - k]. (6.78) + + k= -N Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 477 The corresponding impulse response is a rectangular pulse, and the frequency response is H(e.iw) = 1 e.iw[(N-M)/2] sin[w(~ + N + 1)/2]. (6.79) N + M + 1 sm(w/2) In Figure 6.35, we show the log magnitude forM+ N + 1 = 33 and M + N + 1 = 65. The main, center lobe of each of these frequency responses corresponds to the effective pass- band of the corresponding filter. Note that, as the impulse response increases in length, the width of the main lobe of the magnitude of the frequency response decreases. This provides another example of the trade-off between the time and frequency domains. Specifically, in order to have a narrower passband, the filter in eqs. (6.78) and (6.79) must have a longer impulse response. Since the length of the impulse response of an FIR filter has a direct impact on the complexity of its implementation, this implies a trade-off between frequency selectivity and the complexity of the filter, a topic of central concern in filter design. Moving-average filters are commonly applied in economic analysis in order to at- tenuate the short-term fluctuations in a variety of economic indicators in relation to longer term trends. In Figure 6.36, we illustrate the use of a moving-average filter of the form of eq. (6.78) on the weekly Dow Jones stock market index for a 10-year period. The weekly Dow Jones index is shown in Figure 6.36(a). Figure 6.36(b) is a 51-day moving aver- age (i.e., N = M = 25) applied to that index, and Figure 6.36(c) is a 201-day moving average (i.e., N = M = 10 0) applied to the index. Both moving averages are considered useful, with the 51-day average tracking cyclical (i.e., periodic) trends that occur during the course of the year and the 20 1-day average primarily emphasizing trends over a longer time frame. The more general form of a discrete-time nonrecursive filter is M y[n] = L bkx[n - k], k=-N so that the output of this filter can be thought of as a weighted average of N t M + 1 neighboring points. The simple moving-average filter in eq. (6.78) then corresponds to setting all of these weights to the same value, namely, 1/(N + M + 1) . However, by choosing these coefficients in other ways, we have considerable flexibility in adjusting the filter's frequency response. There are, in fact, a variety of techniques available for choosing the coefficients in eq. (6.80) so as to meet certain specifications on the filter, such as sharpening the transition band as much as possible for a filter of a given length (i.e., for N+M+ 1 fixed). These pro- cedures are discussed in detail in a number of texts, 3 and although we do not discuss the procedures here, it is worth emphasizing that they rely heavily on the basic concepts and tools developed in this book. To illustrate how adjustment of the coefficients can influence 3See, for example, R. W. Hamming, Digital Filters, 3rd ed. (Englewood Cliffs, NJ: Prentice-Hall, Inc., 1989); A. V. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs, NJ: Prentice- Hall, Inc., 1989); and L. R. Rabiner and B. Gold, Theory and Application of Digital Signal Processing (Engle- wood Cliffs, NJ: Prentice-Hall, Inc., 1975). 478 Time and Frequency Characterization of Signals and Systems Chap.6 0 dB 3 -40 ~""~"""""""""""""""""""" """" """" """"""~""~""""""""""""~ ·~ I 0 (a) 0> _Q 0 -80 N -120 0 'IT/2 w 0 dB 3 -40 ~ I 0 (b) 0> _Q ~ -80 -120 0 'IT/2 w Figure 6.35 Log-magnitude plots for the moving-average filter of eqs. (6.78) and (6.79) for (a) M + N + 1 = 33 and (b) M + N + 1 = 65. the response ofthe filter, let us consider a filter of the form of eq. (6.80), with N = M = 16 and the filter coefficients chosen to be sin(27Tk/33) bk = { 7Tk ' lkl ~ 32 (6.81) 0, lkl > 32 Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 479 400 350 1\ 300 ~ 250 J lA 200 ,. J'lI f ""\ ,fV \1\ ,/ 150 \\ ... J'j/ - 100 v lA. .. '""\ ~ 50 I f'-.. ll 0 JM JM JM JM JM JM JM JM ~n ~n JM 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 (a) 400 350 I/\ ~ 300 250 I \~ .rV 200 1\ / '""\ _/ 150 v""' 100 \ - - 50 \ I'~ 0 ~n JM ~n JM JM ~n JM JM JM ~n JM 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 (b) 400 350 300 I/~ \ Figure 6.36 Effect of lowpass fil- I ~ tering on the Dow Jones weekly stock 250 ./v market index over a 10 -year period 200 1\ ,/ using moving-average filters: (a) weekly / index; (b) 51-day moving average ap- 150 / 100 """" ...... v"" plied to (a); (c) 201-day moving average applied to (a). The weekly '-- ../f"" stock market index and the two moving 50 averages are discrete-time sequences. For clarity in the graphical display, 0 the three sequences are shown here JM JM ~n JM JM ~n JM JM JM JM JM with their individual values connected 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 by straight lines to form a continuous (c) curve. 480 Time and Frequency Characterization of Signals and Systems Chap. 6 The impulse response of the filter is sin(21Tn/33) h[n] = 1rn ' lnl ::s 32 (6.82) { 0, lnl > 32 Comparing this impulse response with eq. (6.20), we see that eq. (6.82) corresponds to truncating, for lnl > 32, the impulse response for the ideal lowpass filter with cutoff fre- quency We = 211""133. In general, the coefficients bk can be adjusted so that the cutoff is at a desired fre- quency. For the example shown in Figure 6.37, the cutoff frequency was chosen to match approximately the cutoff frequency of Figure 6.35 for N = M = 16. Figure 6.37(a) shows the impulse response of the filter, and Figure 6.37(b) shows the log magnitude of the fre- quency response in dB. Comparing this frequency response to Figure 6.35, we observe that the passband of the filter has approximately the same width, but that the transition to h[n] 2 33 n (a) 20 0 dB 3 ·~ I -20 0 Ol .2 -40 0 N -60 -80 7T 37T 7T 7T 7T 7T 37T 7T -4 -16 0 8 T6 T6 8 T6 4 w (b) Figure 6.37 (a) Impulse response for the nonrecursive filter of eq. (6.82); (b) log magnitude of the frequency response of the filter. Sec. 6.7 Examples of Time- and Frequency-Domain Analysis of Systems 481 the stopband is sharper. In Figures 6.38(a) and (b), the magnitudes (on a linear amplitude scale) of the two filters are shown for comparison. It should be clear from the compari- son of the two examples that, by the intelligent choice of the weighting coefficients, the transition band can be sharpened. An example of a higher order lowpass filter (N = M = 125), with the coefficients determined through a numerical algorithm referred to as the Parks-McClellan algorithm,4 is shown in Figure 6.39. This again illustrates the trade-off between the time and frequency domains: If we increase the length N + M + 1 of a filter, then, by a judicious choice of the filter coefficients in eq. (6.80), we can achieve sharper transition band behavior and a greater degree of frequency selectivity. An important property of the examples we have given is that they all have zero or lin- ear phase characteristics. For example, the phase of the moving-average filter of eq. (6 . 79) is w[(N- M)/2]. Also, since the impulse response in eq. (6.82) is real and even, the im- pulse response of the filter described by that equation is real and even, and thus has zero phase. From the symmetry properties of the Fourier transform of real signals, we know that any nonrecursive filter with an impulse response that is real and even will have a frequency response H(ei(u) that is real and even and, consequently, has zero phase. Such a filter, of course, is noncausal, since its impulse response h[n] has nonzero values for n < 0. However, if a causal filter is required, then a simple change in the impulse response can achieve this, resulting in a system with linear phase. Specifically, since h[n] is the impulse response of an FIR filter, it is identically zero outside a range of values centered at the origin w Figure 6.38 Comparison, on a linear amplitude scale, of the frequency w responses of (a) Figure 6.37 and (b) (b) Figure 6.35. 4A. Y. Oppenheim and R. W. Schafer, Discrete-Time Signal Processing (Englewood Cliffs. NJ: Prentice- Hall, Inc., 1989), Chap. 7. 482 Time and Frequency Characterization of Signals and Systems Chap.6 0 ill -20 ~--~ 1J- -40 I-H~ 0.0615 1011 §, 0.020 3 -60 I- ~ 0.025 -~ ""0 0.070 c I -80 1- E o o.o5 o.1o 0 Cll dl Cll <1l ..2 -100 t-O.. 0 C\1 I IIIII 1111 II 1111 I I I II II II I I I -120 - -140 - -160 I I I I I I I I I I I I I I I I I I I (!) Figure 6.39 Lowpass nonrecursive filter with 251 coefficients designed to obtain the sharpest possible cutoff. (i.e., h[n] = 0 for all Jnl > N). If we now define the nonrecursive LTI system resulting from a simple N-step delay of h[n], i.e., h1 [n] = h[n - N], (6.83) then h 1 [n] = 0 for all n < 0, so that this LTI system is causal. Furthermore, from the time- shift property for discrete-time Fomier transforms, we see that the frequency response of the system is HI (ejw) = H(ejw)e- jwN_ (6.84) Since H(ejw) has zero phase, H 1( ejw) does indeed have linear phase. 6.8 SUMMARY In this chapter, we have built on the foundation of Fourier analysis of signals and systems developed in Chapters 3-5 in order to examine in more detail the characteristics of LTI systems and the effects they have on signals. In particular, we have taken a careful look at the magnitude and phase characteristics of signals and systems, and we have introduced log-magnitude and Bode plots for LTI systems. We have also discussed the impact of phase and phase distortion on signals and systems. This examination led us to understand the special role played by linear phase characteristics, which impart a constant delay at all frequencies and which, in tum, led to the concept of nonconstant group delay and disper- sion associated with systems having nonlinear phase characteristics. Using these tools and insights, we took another look at frequency-selective filters and the time-frequency trade- offs involved. We examined the properties of both ideal and non-ideal frequency-selective filters and saw that time-frequency considerations, causality constraints, and implemen- tation issues frequently make non -ideal filters, with transition bands and tolerance limits in the passbands and stopbands, the preferred choice. Chap. 6 Problems 483 We also examined in detail the time-frequency characteristics of first- and second- order systems in both continuous and discrete time. We noted in particular the trade-off between the response time of these systems and the frequency-domain bandwidth. Since first- and second-order systems are the building blocks for more complex, higher order LTI systems, the insights developed for those basic systems are of considerable use in practice. Finally, we presented several examples of LTI systems in order to illustrate many of the points developed in the chapter. In particular, we examined a simple model of an auto- mobile suspension system to provide a concrete example of the time-response-frequency- response concerns that drive system design in practice. We also considered several examples of discrete-time nonrecursive filters, ranging from simple moving-average filters to higher order FIR filters designed to have enhanced frequency selectivity. We saw, in addition, that FIR filters can be designed so as to have exactly linear phase. These examples, the development of the tools of Fourier analysis that preceded them, and the insights those tools provide illustrate the considerable value of the methods of Fourier analysis in analyzing and designing LTI systems. Chapter 6 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 6.1. Consider a continuous-time LTI system with frequency response H(jw) = IH(jw)l eJ<tHUw) and real impulse response h(t). Suppose that we apply an input x(t) = cos(w0 t +<Po) to this system. The resulting output can be shown to be of the form y(t) = Ax(t - to), where A is a nonnegative real number representing an amplitude-scaling factor and to is a time delay. (a) Express A in terms of iHUwo)i. (b) Express to in terms of <tH(jw0 ). 6.2. Consider a discrete-time LTI system with frequency response H(eiw) iH(eiw)jei<tH(elW) and real impulse response h[n]. Suppose that we apply the input x[n] = sin(w0 n + <Po) to this system. The resulting output can be shown to be of the form y[n] = iH(eiwo)ix[n - no], provided that <tH ( eiwo) and w 0 are related in a particular way. Determine this rela- tionship. 6.3. Consider the following frequency response for a causal and stable LTI system: . 1- jw H(jw) = --.- 1 + JW 484 Time and Frequency Characterization of Signals and Systems Chap. 6 (a) Show that IH(jw )I = A, and determine the value of A. (b) Determine which of the following statements is true about T(w ), the group delay of the system. (Note: T(w) = -d( -{.H(jw ))ldw, where -{.H(jw) is expressed in a form that does not contain any discontinuities.) 1. T( w) = 0 for w > 0 2. T( w) > 0 for w > 0 3. T( w) < 0 for w > 0 6.4. Consider a linear-phase discrete-time LTI system with frequency response H(eiw) and real impulse response h[n]. The group delay function for such a system is defined as d . T(W) = - dw -{.H(elw), where -{.H(eiw) has no discontinuities. Suppose that, for this system, IH(ej""12 )1 = 2, 4:H(ej0 ) = 0, and T (I) = 2 Determine the output of the syst*em) for each of the following inputs: (a) cos(¥n) (b) sinC; n + 6.5. Consider a continuous-time ideal bandpass filter whose frequency response is H(1 ' w) = { 1, We :::; lwl :::; 3we . 0, elsewhere (a) If h(t) is the impulse response of this filter, determine a function g(t) such that h(t) = ein1T:ct )g(t). (b) As we is increased, does the impulse response of the filter get more concentrated or less concentrated about the origin? 6.6. Consider a discrete-time ideal highpass filter whose frequency response is specified as 7T -We :::; lwl :::; 7T lwl < 7T- We (a) If h[n] is the impulse response of this filter, determine a function g[n] such that h[n] = ei:,n )g[n]. (b) As we is increased, does the impulse response of the filter get more concentrated or less concentrated about the origin? 6. 7. A continuous-time lowpass filter has been designed with a passband frequency of 1,000 Hz, a stopband frequency of 1,200 Hz, passband ripple of 0.1, and stopband ripple of0.05. Let the impulse response of this lowpass filter be denoted by h(t). We wish to convert the filter into a bandpass filter with impulse response g(t) = 2h(t) cos(4,0007Tt). Chap. 6 Problems 485 Assuming that IH(jw)l is negligible for lwl > 4,0001T, answer the following ques- tions: (a) If the passband ripple for the bandpass filter is constrained to be 0.1, what are the two passband frequencies associated with the bandpass filter? (b) If the stopband ripple for the bandpass filter is constrained to be 0.05, what are the two stopband frequencies associated with the bandpass filter? 6.8. A causal, nonideallowpass filter is designed with frequency response H(ejw). The difference equation relating the input x[n] and output y[n] for this filter is specified as N M y[n] = L aky[n - k] + L bkx[n- k]. k=I k=O The filter also satisfies the following specifications for the magnitude of its fre- quency response: passband frequency = w P• passband tolerance = BP • stopband frequency = w s. stopband tolerance = Bs. Now consider a causal LTI system whose input and output are related by the differ- ence equation N M y[n] = L( -l)kaky[n- k] + L( -l)kbkx[n- k]. k=I k=O Show that this filter has a passband with a tolerance of Bp, and specify the corre- sponding location of the passband. 6.9. Consider a continuous-time causal and stable LTI system whose input x(t) and out- put y(t) are related by the differential equation ddy(tt) + 5y(t) = 2x(t). What is the final value s(oo) of the step response s(t) of this filter? Also, determine the value of to for which s(t0 ) = s(oo) [I - :2 J. 6.10. For each first-order system whose frequency response is as follows, specify the straight-line approximation of the Bode magnitude plot: (a) 40(j~+O.l) (b) 0.04(!w+SO) jw+40 jw+0.2 6.11. For each second-order system whose frequency response is as follows, specify the straight- line approximation of the Bode magnitude plot: ( a ) 250 (b) 0 02 jw+50 (jw)2+50.5jw+25 ' (jw)2+0.2jw+I 486 Time and Frequency Characterization of Signals and Systems Chap. 6 6.12. A continuous-time LTI systemS with frequency response H(jw) is constructed by cascading two continuous-time LTI systems with frequency responses H 1 (jw) and H2(jw), respectively. Figures P6.12(a) and P6.12(b) show the straight-line approx- imations of the Bode magnitude plots of H1 (jw) and H(jw ), respectively. Specify H2(jw). 24dB -20 dB/decade 8 10 40 10 0 w (rad/sec) (a) 20 log1 o I H(jw)l -20dB~----------~ -40 dB/decade 8 10 10 0 w (rad/sec) (b) Figure P6.12 6.13. The straight-line approximation of the Bode magnitude plot of a second-order continuous-time LTI systemS is shown in Figure P6.13. S may be constructed by 20 log1o I H(jw)l dB 1-------.......... { -20 dB/decad~ 6 I I -26 dB - - - - - - - - - -, - - - - - - - - - { -40 dB/decade 2 10 80 10 0 w (rad/sec) Figure P6. 1 3 Chap. 6 Problems 487 either connecting two first-order systems sl and s2 in cascade or two first-order systems S3 and S4 in parallel. Determine which, if any, of the following statements are true or false. Justify your answers. (a) The frequency responses of S1 and S2 may be determined uniquely. (b) The frequency responses of S3 and S4 may be determined uniquely. 6.14. The straight-line approximation of the Bode magnitude plot of a causal and stable continuous-time LTI system S is shown in Figure P6.14. Specify the frequency re- sponse of a system that is the inverse of S. 20 log10 I H(jw)l 94 dB \_ 80 dB 0 dB/decade 12 dB 1------~ 0.1 0.2 10 50 100 w (rad/sec) Figure P6. 14 6.15. For each of the following second-order differential equations for causal and sta- ble LTI systems, determine whether the corresponding impulse response is under- damped, overdamped, or critically damped: (a) d:i~~r) + 4 d~;~n + 4y(t) = x(t) (b) sd~i~;f) +4d~~~t) +5y(t) = 7x(t) (c) d;i~~t) + 20d~;;n + y(t) = x(t) (d) 5 d""y~t) + 4 dy(f) + Sy(t) = 7 x(t) + _!_ dx(t) dt- dt 3 dt 6.16. A particular first-order causal and stable discrete-time LTI system has a step re- sponse whose maximum overshoot is 50% of its final value. If the final value is 1, determine a difference equation relating the input x[n] and output y[n] of this filter. 6.17. For each of the following second-order difference equations for causal and stable LTI systems, determine whether or not the step response of the system is oscillatory: (a) y[n] + y[n- 1] + ~y[n- 2] = x[n] (b) y[n] - y[n- 1] + ~y[n- 2] = x[n] 6.18. Consider the continuous-time LTI system implemented as the RC circuit shown in Figure P6.18. The voltage source x(t) is considered the input to this system. The voltage y(t) across the capacitor is considered the system output. Is it possible for the step response of the system to have an oscillatory behavior? 488 Time and Frequency Characterization of Signals and Systems Chap.6 R x(t) + y(t) c Figure P6. 18 6.19. Consider the continuous-time LTI system implemented as the RLC circuit shown in Figure P6.19. The voltage source x(t) is considered the input to this system. The voltage y(t) across the capacitor is considered the system output. How should R, L, and C be related so that there is no oscillation in the step response? R L 1 x(t) + y(t) c j Figure P6. 19 6.20. Consider a nonrecursive filter with the impulse response shown in Figure P6.20. What is the group delay as a function of frequency for this filter? 3 4 1 • 4 1 - - - - 0 2 3 4 n Figure P6.20 BASIC PROBLEMS 6.21. A causal LTI filter has the frequency response H(jw) shown in Figure P6.21. For each of the input signals given below, determine the filtered output signal y(t). (a) x(t) = ejt (b) x(t) = (sinwot)u(t) (c) X(jw) = (jw)(~+ 1 jw) (d) X(jw) = 2+ jw Chap. 6 Problems 489 H(jw} w Figure P6.21 6.22. Shown in Figure P6.22(a) is the frequency response H(jw) of a continuous-time filter referred to as a lowpass differentiator. For each of the input signals x(t) below, determine the filtered output signal y(t). (a) x(t) = cos(21Tt + 0) (b) x(t) = cos(41Tt+O) (c) x(t) is a half-wave rectified sine wave of period, as sketched in Figure P6.22(b). sin21Tt, m:::; t:::; (m+ ~) x(t) = { 0, (m + ~) :::; t :::; m for any integer m IH (jw}l <tH (jw} (a) x(t) 0 j_ 2 (b) Figure P6.22 6.23. Shown in Figure P6.23 is IH(jw)l for a lowpass filter. Determine and sketch the impulse response of the filter for each of the following phase characteristics: (a) 1:-H(jw) = 0 (b) 1:-H(jw) = wT, where Tis a constant 490 Time and Frequency Characterization of Signals and Systems Chap.6 w Figure P6.23 w >0 (c) -9-.H(jw) = { ~~ w <0 2 ' 6.24. Consider a continuous-time lowpass filter whose impulse response h(t) is known to be real and whose frequency response magnitude is given as: IH(jw )I = { 1, Jw I ::; 2001T 0, otherwise (a) Determine and sketch the real-valued impulse response h(t) for this filter when the corresponding group delay function is specified as: (i) r(w) = 5 (ii) r(w) = ~ (iii) r(w) = -~ (b) If the impulse response h(t) had not been specified to be real, would knowl- edge of IH(jw )J and r(w) be sufficient to determine h(t) uniquely? Justify your answer. 6.25. By computing the group delay at two selected frequencies, verify that each of the following frequency responses has nonlinear phase. (a) H(jw) = Jwl+l (b) H(jw) = (Jw~l)2 (c) H(jw) = (Jw+l)l(jw+ 2) 6.26. Consider an ideal highpass filter whose frequency response is specified as 1 Jwl >we H(jw) = { 0,• otherwise· (a) Determine the impulse response h(t) for this filter. (b) As We is increased, does h(t) get more or less concentrated about the origin? (c) Determine s(O) and s(oo), where s(t) is the step response of the filter. 6.27. The output y(t) of a causal LTI system is related to the input x(t) by the differential equation dy(t) ----;[[ + 2y(t) = x(t). (a) Determine the frequency response H(. ) = Y(jw) JW X(jw) of the system, and sketch its Bode plot. (b) Specify, as a function of frequency, the group delay associated with this system. (c) If x(t) = e-t u(t), determine Y(jw ), the Fourier transform of the output. Chap. 6 Problems 491 (d) Using the technique of partial-fraction expansion, determine the output y(t) for the input x(t) in part (c). (e) Repeat parts (c) and (d), first if the input has as its Fourier transform (i) X(jw) = l+~w 2+jw' then if (ii) X(jw) = 2+jw l+jw' and finally, if (iii) X(jw) = (2+jw)l(l+jw) . 6.28. (a) Sketch the Bode plots for the following frequency responses: (i) 1 + (jw/10) (ii) 1 - (jw/10) (iii) 16 (iv) 1-(jw/10) (Jw+2)4 l+jw (v) (jw/10)-1 (vi) I +(jw/10) I+Jw I+Jw (vii) 1-(jw/10) (viii) IO+Sjw+ IO(jw)2 (jw)2+(jw)+l I +(jw/10) (ix) 1 + jw + (jw)2 (x) 1- jw + (jw)2 ( "") (jw+ 10)(10jw+ I) XI [(jw/100+ I )][((jw )2 + jw +I)] (b) Determine and sketch the impulse response and the step response for the sys- tem with frequency response (iv). Do the same for the system with frequency response (vi). The system given in (iv) is often referred to as a non-minimum-phase system, while the system specified in (vi) is referred to as being a minimum phase. The corresponding impulse responses of (iv) and (vi) are referred to as a non -minimum-phase signal and a minimum-phase signal, respectively. By comparing the Bode plots of these two frequency responses, we can see that they have identical magnitudes; however, the magnitude of the phase of the system of (iv) is larger than for the system of (vi). We can also note differences in the time-domain behavior of the two sys- tems. For example, the impulse response of the minimum-phase system has more of its energy concentrated near t = 0 than does the impulse response of the non-minimum-phase system. In addition, the step response of (iv) initially has the opposite sign from its asymptotic value as t ~ oo, while this is not the case for the system of (vi). The important concept of minimum- and non -minimum-phase systems can be extended to more general LTI systems than the simple first-order systems we have treated here, and the distinguishing characteristics of these systems can be described far more thoroughly than we have done. 6.29. An LTI system is said to have phase lead at a particular frequency w = w0 if <r..H(jw0 ) > 0. The terminology stems from the fact that if e.iwot is the input to this system, then the phase of the output will exceed, or lead, the phase of the input. Similarly, if <r..H(jw0 ) < 0, the system is said to have phase lag at this frequency. Note that the system with frequency response 1 1 + jw'T 492 Time and Frequency Characterization of Signals and Systems Chap.6 has phase lag for all w > 0, while the system with frequency response 1 + jwT has phase lead for all w > 0. (a) Construct the Bode plots for the following two systems. Which has phase lead and which phase lag? Also, which one amplifies signals at certain frequencies? (i) 1 +(jw/10) (ii) 1 + IOjw I+ IOjw I +(jw/10) (b) Repeat part (a) for the following three frequency responses: +(jw/10))2 ("") (1 (ii) I+ jw/10 (iii) 1 + 10jw I (1 + 10jw)3 100(jw)2+ 10jw+ I O.Ol(jw)2+0.2jw+ I 6.30. Let h(t) have the Bode plot depicted in Figure P6.30. The dashed lines in the figure represent straight-line approximations. Sketch the Bode plots for 1O h(l Ot). OdBt----........ ::: ] -20 :f -40 0 Ci -60 .Q ~ -80 -100 0.1 10 100 1000 w 0.1 10 100 1000 w Figure P6.30 6.31. An integrator has as its frequency response H(jw) = _;_ + 7T 8(w ), )W where the impulse at w = 0 is a result of the fact that the integration of a constant input from t = -oo results in an infinite output. Thus, if we avoid inputs that are Chap. 6 Problems 493 constant, or equivalently, only examine H(jw) for w > 0, we see that 20logiH(Jw)l = -20log(w), -7T <f-H(jw) = 2 . In other words, the Bode plot for an integrator, as illustrated in Figure P6.31, consists of two straight-line plots. These plots reflect the principal characteristics of an inte- grator: a phase shift of -90° at all positive values of frequency and the amplification of low frequencies. (a) A useful, simple model of an electric motor is an LTI system with input equal to the applied voltage and output given by the motor shaft angle. This system can be visualized as the cascade of a stable LTI system (with the voltage as input and shaft angular velocity as output) and an integrator (representing the integra- tion of the angular velocity). Often, a model of first-order system is used for the first part of the cascade. Assuming, for example, that this first -order system has a time constant of 0.1 second, we obtain an overall motor frequency response of 40 20 3 ~ OdB 0 Oi _Q -20 0 N -40 -60 0.01 0.1 10 100 1000 w 1 0 r- :f v -1T 2 -1T f- I I I I I I 0.01 0.1 10 100 1000 w Figure P6.31 494 Time and Frequency Characterization of Signals and Systems Chap.6 the form 1 H(jw) = jw(1 + jw/10) + 7T o(w ). Sketch the Bode plot for the system for w > 0.001. (b) Sketch the Bode plot for a differentiator. (c) Do the same for systems with the following frequency responses: (i) H(jw) = I+J:::noo (ii) H(jw) = (1 +(jw)/16:(jw)211QO) 6.32. Consider the system depicted in Figure P6.32. This ""compensator"" box is a continu- ous-time LTI system. (a) Suppose that it is desired to choose the frequency response of the compensator so that the overall frequency response H(jw) of the cascade satisfies the following two conditions: 1. The log magnitude of H(jw) has a slope of -40 dB/decade beyond w = 1,000. 2. For 0 < w < 1,000, the log magnitude of H(jw) should be between -10 dB and 10 dB. Design a suitable compensator (that is, determine a frequency response for a compensator that meets the preceding requirements), and draw the Bode plot for the resulting H (j w). (b) Repeat (a) if the specifications on the log magnitude of H(jw) are as follows: 1. It should have a slope of +20 dB/decade for 0 < w < 10. 2. It should be between+ 10 and +30 dB for 10 < w < 100. 3. It should have a slope of -20 dB/decade for 100 < w < 1,000. 4. It should have a slope of -40 dB/decade for w > 1,000. 1 x(t)-J Compensator J •I- I • y(t) jW +50 Figure P6.32 6.33. Figure P6.33 shows a system commonly used to obtain a highpass filter from a lowpass filter and vice versa. (a) Show that, if H(jw) is a lowpass filter with cutoff frequency w 1P, the overall system corresponds to an ideal highpass filter. Determine the system's cutoff frequency and sketch its impulse response. (b) Show that, if H(jw) is an ideal highpass filter with cutoff frequency whP' the overall system corresponds to an ideallowpass filter, and determine the cutoff frequency of the system. (c) If the interconnection of Figure P6.33 is applied to an ideal discrete-time low- pass filter, will the resulting system be an ideal discrete-time highpass filter? x(t) -~lt---!•~81---......;;~J;.__ y(t) Figure P6.33 Chap. 6 Problems 495 6.34. In Problem 6.33, we considered a system commonly used to obtain a highpass filter from a lowpass filter and vice versa. In this problem, we explore the system further and, in particular, consider a potential difficulty if the phase of H(jw) is not properly chosen. (a) Referring to Figure P6.33, let us assume that H(jw) is real and as shown in Figure P6.34. Then 1 - o1 < H(jw) < 1 + o1, 0 ::; w ::; w 1, -02 < H(jw) < +02, w2 < w. Determine and sketch the resulting frequency response of the overall system of Figure P6.33. Does the resulting system correspond to an approximation to a highpass filter? (b) Now let H(jw) in Figure P6.33 be of the form (P6.34-1) where H1 (jw) is identical to Figure P6.34 and O(w) is an unspecified phase characteristic. With H(jw) in this more general form, does it still correspond to an approximation to a lowpass filter? (c) Without making any assumptions about O(w ), determine and sketch the toler- ance limits on the magnitude of the frequency response of the overall system of Figure P6.33. (d) If H(jw) in Figure P6.33 is an approximation to a lowpass filter with unspec- ified phase characteristics, will the overall system in that figure necessarily correspond to an approximation to a highpass filter? H(jw} 1 + 01 ~c----._---..._....- 1 -51 1--,..__---~ Figure P6.34 6.35. Shown in Figure P6.35 is the frequency response H(eiw) of a discrete-time differ- entiator. Determine the output signal y[n] as a function of w 0 if the input x[n] is x[n] = cos[won + 0]. 496 Time and Frequency Characterization of Signals and Systems Chap.6 -1T 1T w - -1T w ~----~-~ --- Figure P6.35 6.36. Consider a discrete-time lowpass filter whose impulse response h[n] is known to be real and whose frequency response magnitude in the region -7r :::s w :::s 7T is given as: iwi:::; * otherwise· Determine and sketch the real-valued impulse response h[n] for this filter when the corresponding group delay function is specified as: (a) T( W) = 5 (b) T( W) = ~ (C) T( W) = - ~ 6.37. Consider a causal LTI system whose frequency response is given as: . . 1 - lejw H(elw) = e- JW 2 .. 1 - le- JW 2 (a) Show that iH(ejw)i is unity at all frequencies. (b) Show that 1:-H ( ejw) = - w - 2 tan- 1 ( ! s!i n w )· 1 - cosw (c) Show that the group delay for this filter is given by 3 T(w) = 4 i- cosw. Sketch T(w ). (d) What is the output of this filter when the input is cos( ~n)? 6.38. Consider an ideal bandpass filter whose frequency response in the region -7T ::; w :::s 7T is specified as 1T H(elw. ) = { 1' 2f- We :::; iwi :::; 2 +we 0, otherwise Chap. 6 Problems 497 Determine and sketch the impulse response h[n] for this filter when (a) We = ~ (b) We = * (c) We = ~ As we is increased, does h[n] get more or less concentrated about the origin? 6.39. Sketch the log magnitude and phase of each of the following frequency responses. (a) 1 + ~e- Jw (b) 1 + 2e- Jw (c) 1 - 2e- Jw (d) 1 + 2e- i 2w l (O l+~e-jw (e) (l + ~ r jw )3 l - ~ e- jw (g) 1+2e-;w (h) 1-2e-jw l+~e-jw 1+~e-Jw (i) 1 (j) 1 (l - ~ r jw )(l - ~ e- jw) (l- ~ r jw )(l + ~ e- jw) (k) 1+2e-Zjw (1-~e-iw)2 6.40. Consider an ideal discrete-time lowpass filter with impulse response h[n] and for which the frequency response H(eiw) is that shown in Figure P6.40. Let us consider obtaining a new filter with impulse response h 1 [n] and frequency response H 1( eiw) as follows: ht [n] = { h[n/2], n even 0, n odd This corresponds to inserting a sequence value of zero between each sequence value of h[n]. Determine and sketch H 1( eiw) and state the class of ideal filters to which it belongs (e.g., lowpass, highpass, bandpass, multiband, etc.). I H(eiw) I <l::H(eiw)=O ~ I ~ -2TI -TI -we We 1T 2TI w Figure P6.40 6.41. A particular causal LTI system is described by the difference equation j2 1 y[n]- Ty[n- 1] + y[n- 2j = x[n]- x[n- 1]. 4 (a) Find the impulse response of this system. (b) Sketch the log magnitude and the phase of the frequency response of the system. 6.42. (a) Consider two LTI systems with the following frequency responses: 498 Time and Frequency Characterization of Signals and Systems Chap.6 Show that both of these frequency responses have the same magnitude function [i.e., \H 1 (e.iw)\ = \H2(e.iw)\], but the group delay of H 2(e.iw) is greater than the group delay of H 1 (e.i(u) for w > 0. (b) Determine and sketch the impulse and step responses of the two systems. (c) Show that H2(ejw) = G(e.iw)HI (eiw), where G(e.i(u) is an all-pass s_vstem [i.e., \G(e.iw)\ = 1 for all w ]. 6.43. When designing filters with highpass or bandpass characteristics, it is often conve- nient first to design a lowpass filter with the desired passband and stopband specifi- cations and then to transform this prototype filter to the desired highpass or bandpass filter. Such transformations are called lowpass-to-highpass or highpass-to-lowpass transformations. Designing filters in this manner is convenient because it requires us only to formulate our filter design algorithms for the class of filters with low- pass characteristics. As one example of such a procedure, consider a discrete-time lowpass filter with impulse response h1P[n] and frequency response H 1p(eiw), as sketched in Figure P6.43. Suppose the impulse response is modulated with these- quence ( -1)11 to obtain hhp[n] = ( -1)11 h1p[n]. (a) Determine and sketch Hhp( e.iw) in terms of H 1p( e.iw ). Show in particular that, for H 1p(e.iw) as shown in Figure P6.43, Hhp(e.iw) corresponds to a highpass filter. (b) Show that modulation of the impulse response of a discrete-time high pass filter by ( -1 )11 will transform it to a lowpass filter. HtP (eiw) 1 ___J_____~~--c..__;h__L~_~L___L._ w 1T Figure P6.43 6.44. A discrete-time system is implemented as shown in Figure P6.44. The system S shown in the figure is an LTI system with impulse response h1p[n]. (a) Show that the overall system is time invariant. (b) If h1p[n] is a lowpass filter, what type of filter does the system of the figure implement? x[n] ·I s •~y[n] ·~ h1p[n] t (-1t (-1t Figure P6.44 Chap. 6 Problems 499 6.45. Consider the following three frequency responses for causal and stable third-order LTI systems. By utilizing the properties of first- and second-order systems dis- cussed in Section 6.6, determine whether or not the impulse response of each of the third-order systems is oscillatory. (Note: You should be able to answer this ques- tion without taking the inverse Fourier transforms of the frequency responses of the third-order systems.) 1 H1 (e.iw) = (1 - .!.e- .iw)(l - .!.e- .iw)(l - .!.e- .iw)' 2 3 4 1 H2(e.iw) = ---:---------=------:---- (1 + ~e- .iw)(l - .!.e- .iw)(1 - .!.e- .iw)' - 3 4 H3(ejw) = -----------,--1- -----,------- (1 - 4e- .iw)(l - ~e- jw + ~e- j2w) · 6.46. Consider a causal, nonrecursive (FIR) filter whose real-valued impulse response h[n] is zero for n ~ N. (a) Assuming that N is odd, show that if h[n] is symmetric about (N- 1)/2 (i.e., if h[(N- 1)/2 + n] = h[(N- 1)/2- n]), then H(e.iw) = A(w)e-j[(N-I)!2Jw, where A(w) is a real-valued function of w. We conclude that the filter has linear phase. (b) Give an example of the impulse response h[n] of a causal, linear-phase FIR filter such that h[n] = 0 for n ~ 5 and h[n] =/= 0 for 0 ::; n ::; 4. (c) Assuming that N is even, show that if h[n] is symmetric about (N- 1)/2 (i.e., if h[(N/2) + n] = h[N/2- n- 1]), then H(e.iw) = A(w)e-j[(N-1)12lw, where A(w) is a real-valued function of w. (d) Give an example of the impulse response h[n] of a causal, linear-phase FIR filter such that h[n] = 0 for n ~ 4 and h[n] =/= 0 for 0 ::; n ::; 3. 6.47. A three-point symmetric moving average, referred to as a weighted moving average, is of the form y[n] = b{ax[n - 1] + x[n] + ax[n + 1]}. (P6.47-1) (a) Determine, as a function of a and b, the frequency response H(e.iw) of the three- point moving average in eq. (P6.47-1). (b) Determine the scaling factor b such that H ( e.iw) has unity gain at zero frequency. (c) In many time-series analysis problems, a common choice for the coefficient a in the weighted moving average in eq. (P6.47-1) is a = 112. Determine and sketch the frequency response of the resulting filter. 6.48. Consider a four-point, moving-average, discrete-time filter for which the difference equation is y[n] = box[n] + b1 x[n- 1] + b2x[n- 2] + b3x[n- 2]. 500 Time and Frequency Characterization of Signals and Systems Chap.6 Determine and sketch the magnitude of the frequency response for each of the fol- lowing cases: (a) bo = b3 = 0, b1 = b2 (b) b1 = b2 = 0, bo = b3 (c) bo = b 1 = b2 = b3 (d) bo = -hi = b2 = -b3 ADVANCED PROBLEMS 6.49. The time constant provides a measure of how fast a first-order system responds to inputs. The idea of measuring the speed of response of a system is also important for higher order systems, and in this problem we investigate the extension of the time constant to such systems. (a) Recall that the time constant of a first-order system with impulse response h(t) = ae -at u(t), a > 0, is 1/a, which is the amount of time from t = 0 that it takes the system step response s(t) to settle within lie of its final value [i.e., s(oo) = limr~""""s(t)]. Using this same quantitative definition, find the equation that must be solved in order to determine the time constant of the causal LTI system described by the differential equation d2y(t) + 11 dy(t) 10 () = 9 ( ) dt2 dt + y t X t . (P6.49-1) (b) As can be seen from part (a), if we use the precise definition of the time constant set forth there, we obtain a simple expression for the time constant of a first- order system, but the calculations are decidedly more complex for the system of eq. (P6.49-1). However, show that this system can be viewed as the parallel interconnection of two first-order systems. Thus, we usually think of the system of eq. (P6.49-1) as having two time constants, corresponding to the two first- order factors. What are the two time constants for this system? (c) The discussion given in part (b) can be directly generalized to all systems with impulse responses that are linear combinations of decaying exponentials. In any system of this type, one can identify the dominant time constants of the system, which are simply the largest of the time constants. These represent the slowest parts of the system response, and consequently, they have the dominant effect on how fast the system as a whole can respond. What is the dominant time constant of the system of eq. (P6.49-1)? Substitute this time constant into the equation determined in part (a). Although the number will not satisfy the equation exactly, you should see that it nearly does, which is an indication that it is very close to the time constant defined in part (a). Thus, the approach we have outlined in part (b) and here is of value in providing insight into the speed of response of LTI systems without requiring excessive calculation. (d) One important use of the concept of dominant time constants is in the reduction of the order of LTI systems. This is of great practical significance in problems Chap. 6 Problems 501 s(t) ! Figure P6.49 involving the analysis of complex systems having a few dominant time con- stants and other very small time constants. In order to reduce the complexity of the model of the system to be analyzed, one often can simplify the fast parts of the system. That is, suppose we regard a complex system as a parallel in- terconnection of first- and second-order systems. Suppose also that one of these subsystems, with impulse response h(t) and step response s(t), is fast-that is, that s(t) settles to its final values(o o) very quickly. Then we can approximate this subsystem by the subsystem that settles to the same final value instantaneously. That is, if s(t) is the step response to our approximation, then s(t) = s(oo)u(t). This is illustrated in Figure P6.49. Note that the impulse response of the ap- proximate system is then h(t) = s(oo)o(t), which indicates that the approximate system is memory less. Consider again the causal LTI system described by eq. (P6.49-l) and, in particular, the representation of it as a parallel interconnection of two first -order systems, as described in part (b). Use the method just outlined to replace the faster of the two subsystems by a memory less system. What is the differential equation that then describes the resulting overall system? What is the frequency response of this system? Sketch IH(jw )I (not log IH(jw )I) and <r:.H(jw) for both the original and approximate systems. Over what range of frequencies are these frequency responses nearly equal? Sketch the step responses for both systems. Over what range of time are the step responses nearly equal? From your plots, you will see some of the similarities and differences between the original sys- tem and its approximation. The utility of an approximation such as this de- pends upon the specific application. In particular, one must take into account both how widely separated the different time constants are and also the nature of the inputs to be considered. As you will see from your answers in this part of the problem, the frequency response of the approximate system is essentially the same as the frequency response of the original system at low frequencies. That is, when the fast parts of the system are sufficiently fast compared to the rate of fluctuation of the input, the approximation becomes useful. 502 Time and Frequency Characterization of Signals and Systems Chap.6 6.50. The concepts associated with frequency-selective filtering are often used to sepa- rate two signals that have been added together. If the spectra of the two signals do not overlap, ideal frequency-selective filters are desirable. However, if the spectra overlap, it is often preferable to design the filter to have a gradual transition between passband and stopband. In this problem, we explore one approach for determining the frequency response of a filter to be used for separating signals with overlapping spectra. Let x(t) denote a composite continuous-time signal consisting of the sum of two signals s(t) + w(t). As indicated in Figure P6.50(a), we would like to design an LTI filter to recover s(t) from x(t). The filter's frequency response H(jw) is to be chosen so that, in some sense, y(t) is a ""good"" approximation to s(t). Let us define a measure of the error between y(t) and s(t) at each frequency w as E(w) ~ IS(jw) - Y(jw )1 2, where S(jw) and Y(jw) are the Fourier transforms of s(t) and y(t), respectively. (a) Express E(w) in terms of S(jw), H(jw), and W(jw), where W(jw) is the Fourier transform of w(t). (b) Let us restrictH(jw) to be real, so thatH(jw) = H*(jw ). By setting the deriva- tive of E(w) with respect to H(jw) to be zero, determine the H(jw) required to minimize the error E( w ). (c) Show that if the spectra of S(jw) and W(jw) are non-overlapping, the result in part (b) reduces to an ideal frequency-selective filter. (d) From your result in part (b), determine and sketch H(jw) if S(jw) and W(jw) are as shown in Figure P6.50(b ). x(t) ~ s(t) + w(t)-8-- y(t) (a) S(jw) 11 -2 2 w W(jw) 1+ I -1 w (b) Figure P6.50 Chap. 6 Problems 503 6.51. An ideal bandpass filter is a bandpass filter that passes only a range of frequencies, without any change in amplitude or phase. As shown in Figure P6.5l(a), let the passband be w w wo - 2 :::; lwl :::; wo + 2· (a) What is the impulse response h(t) of this filter? (b) We can approximate an ideal bandpass filter by cascading a first-order lowpass and a first-order highpass filter, as shown in Figure P6.5l(b). Sketch the Bode diagrams for each of the two filters H 1( jw) and H 2(jw ). (c) Determine the Bode diagram for the overall bandpass filter in terms of your results from part (b). w (a) x(t)-[~~}----~y(t) H ("" ) 103 H ("" ) jw 1 jw = 103 +jw 2 jw = 100+jw (b) Figure P6.51 6.52. In Figure P6.52(a), we show the magnitude of the frequency response for an ideal continuous-time differentiator. A nonideal differentiator would have a frequency response that is some approximation to the frequency response in the figure. (a) Consider a nonideal differentiator with frequency response G(jw) for which IG(jw )I is constrained to be within ± 10% of the magnitude of the frequency response of the ideal differentiator at all frequencies; that is, -O.liH(jw)l :::; [IG(jw)I-IH(jw)IJ :::; O.liH(jw)l. Sketch the region in a plot of G(jw) vs. w where IG(jw )I must be confined to meet this specification. 504 Time and Frequency Characterization of Signals and Systems Chap.6 I H (jw) I I H (jw) I= lwl ~peo1 w (a) Multiplication by 1fT y(t) =tw(t) Ideal delay Tsec. ....__ __ __. x(t-T) (b) Figure P6.52 (b) The system in Figure P6.52(b), incorporating an ideal delay ofT seconds, is sometimes used to approximate a continuous-time differentiator. ForT = 10-2 second, determine the frequency range over which the magnitude of the fre- quency response of the system in the figure is within ± 10% of that for an ideal differentiator. 6.53. In many filtering applications, it is often undesirable for the step response of a filter to overshoot its final value. In processing pictures, for example, the overshoot in the step response of a linear filter may produce flare-that is, an increase in intensity- at sharp boundaries. It is possible, however, to eliminate overshoot by requiring that the impulse response of the filter be positive for all time. Show that if h(t), the impulse response of a continuous-time LTI filter, is al- ways greater than or equal to zero, the step response of the filter is a monotonically nondecreasing function and therefore will not have overshoot. 6.54. By means of a specific filter design procedure, a nonideal continuous-time lowpass filter with frequency response H0(jw ), impulse response h0(t), and step response s0(t) has been designed. The cutoff frequency of the filter is at w = 27T X 102 rad/sec, and the step response rise time, defined as the time required for the step response to go from 10% of its final value to 90% of its final value, is Tr = 10-2 second. From this design, we can obtain a new filter with an arbitrary cutoff fre- quency we by the use of frequency scaling. The frequency response of the resulting filter is then of the form Hip(jw) = Ho(jaw ), where a is an appropriate scale factor. (a) Determine the scale factor a such that H1p(jw) has a cutoff frequency of We. Chap. 6 Problems 505 (b) Determine the impulse response h1p(t) of the new filter in terms of We and h0 (t). (c) Determine the step response s1p(t) of the new filter in terms of we and s0(t). (d) Determine and sketch the rise time of the new filter as a function of its cutoff frequency we. This is one illustration of the trade-off between time-domain and frequency-domain characteristics. In particular, as the cutoff frequency de- creases, the rise time tends to increase. 6.55. The square of the magnitude of the frequency response of a class of continuous-time lowpass filters, known as Butterworth filters, is . 12 1 IB (jw) = 1 + (w/wc)2N. Let us define the passband edge frequency w P as the frequency below which IB(jw )1 2 is greater than one-half of its value at w = 0; that is, Now let us define the stopband edge frequency Ws as the frequency above which IB(jw )1 2 is less than 10-2 of its value at w = 0; that is, The transition band is then the frequency range between w P and w 5 • The ratio wsfw P is referred to as the transition ratio. For fixed w P' and making reasonable approximations, deteimine and sketch the transition ratio as a function of N for the class of Butterworth filters. 6.56. In this problem, we explore some of the filtering issues involved in the commercial version of a typical system that is used in most modem cassette tape decks to reduce noise. The primary source of noise is the high-frequency hiss in the tape playback process, which, in some part, is due to the friction between the tape and the playback head. Let us assume that the noise hiss that is added to the signal upon playback has the spectrum of Figure P6.56(a) when measured in decibels, with 0 dB equal to the signal level at 100Hz. The spectrum S(jw) of the signal has the shape shown in Figure P6.56(b ). The system that we analyze has a filter H 1 (jw) which conditions the signal s(t) before it is recorded. Upon playback, the hiss n(t) is added to the signal. The system is represented schematically in Figure P6.56(c). Suppose we would like our overall system to have a signal-to-noise ratio of 40 dB over the frequency range 50 Hz< w/27T <20kHz. (a) Determine the transfer characteristic of the filter H 1( jw ). Sketch the Bode plot of H 1(jw). (b) If we were to listen to the signal p(t), assuming that the playback process does nothing more than add hiss to the signal, how do you think it would sound? (c) What should the Bode plot and transfer characteristic of the filter H 2(jw) be in order for the signal s(t) to sound similar to s(t)? 506 Time and Frequency Characterization of Signals and Systems Chap.6 20 log1 0 I N(jw) I -28dBf / -40dB ~~---· ~I I 5kHz 10kHz 20kHz w f = 21T (a) 20 log10 I S(jw) I ~: :: fl--'--~- '----'------------'------'--'""""- 50 100 1kHz 10kHz 20kHz (b) n(t) p(t) s(t) (c) Figure P6.56 6.57. Show that if h[n], the impulse response of a discrete-time LTI filter, is always greater than or equal to zero, the step response of the filter is a monotonically nondecreasing function and therefore will not have overshoot. 6.58. In the design of either analog or digital filters, we often approximate a specified magnitude characteristic without particular regard to the phase. For example, stan- dard design techniques for lowpass and bandpass filters are typically derived from a consideration of the magnitude characteristics only. In many filtering problems, one would ideally like the phase characteristics to be zero or linear. For causal filters, it is impossible to have zero phase. However, for many digital filtering applications, it is not necessary that the unit sample response of the filter be zero for n < 0 if the processing is not to be carried out in real time. One technique commonly used in digital filtering when the data to be filtered are of finite duration and stored, for example, on a disc or magnetic tape is to process the data forward and then backward through the same filter. Let h[n] be the unit sample response of a causal filter with an arbitrary phase characteristic. Assume that h[n] is real, and denote its Fourier transform by H(ejw). Let x[n] be the data that we want to filter. The filtering operation is performed as follows: Chap. 6 Problems 507 x[n]----!~&- g[n] g[-n]----!~&- r[n] s[n] = r[-n] (a) x[n] &-g[n] x[-n] &-r[n] (b) 1I 3TI 'TT W 4 4 (c) Figure P6.58 (a) Method A: Process x[n] to get s[n], as indicated in Figure P6.58(a). 1. Determine the overall unit sample response h1 [n] that relates x[n] and s[n], and show that it has zero phase characteristic. 2. Determine IH1( eiw)l and express it in terms of IH(eiw)l and <r:H(eiw). (b) Method B: Process x[n] through the filter h[n] to get g[n] [Figure P6.58(b)]. Also, process x[n] backward through h[n] to get r[n]. The output y[n] is taken to be the sum of g[n] and r[ -n]. The composite set of operations can be repre- sented by a filter with input x[n], output y[n], and unit sample response h2 [n]. 1. Show that the composite filter h2 [n] has zero phase characteristic. 2. Determine IH2(eiw)1, and express it in terms of IH(eiw)l and <r:H(eiw). (c) Suppose that we are given a sequence of finite duration on which we would like to perform bandpass, zero-phase filtering. Furthermore, assume that we 508 Time and Frequency Characterization of Signals and Systems Chap.6 are given the bandpass filter h[n] with frequency response as specified in Fig- ure P6.58(c) and with magnitude cparacteristic that we desire, but with linear phase. To achieve zero phase, we could use either of the preceding methods, A or B. Determine and sketch IH1 (e.iw)l and IH2(e.iw)1. From these results, which method would you use to achieve the desired bandpass filtering operation? Ex- plain why. More generally, if h[n] has the desired magnitude, but a nonlinear phase characteristic, which method is preferable to achieve a zero phase char- acteristic? 6.59. Let hc~[n] denote the unit sample response of a desired ideal system with frequency response Hc~(e.iw), and let h[n] denote the unit sample response for an FIR system of length Nand with frequency response H(e.iw). In this problem, we show that a rectangular window of length N samples applied to he~ [ n] will produce a unit sample response h[n] such that the mean square error is minimized. (a) The error function E(e.iw) = Hc~(e.iw)- H(e.iw) can be expressed as the power series n=-x Find the coefficients e[n] in terms of hc~[n] and h[n]. (b) Using Parsevars relation, express the mean square error E 2 in terms of the co- efficients e[n]. (c) Show that for a unit sample response h[n] of length N samples, E 2 is minimized when h[n] = { hc~[n], O~n~N-1 0, otherwise That is, simple truncation gives the best mean square approximation to a desired frequency response for a fixed value of N. 6.60. In Problem 6.50, we considered one specific criterion for determining the frequency response of a continuous-time filter that would recover a signal from the sum of two signals when their spectra overlapped in frequency. For the discrete-time case, develop the result corresponding to that obtained in part (b) of Problem 6.50. 6.61. In many situations we have available an analog or digital filter module, such as a basic hardware element or computer subroutine. By using the module repetitively or by combining identical modules, it is possible to implement a new filter with improved passband or stopband characteristics. In this and the next problem, we consider two procedures for doing just that. Although the discussion is phrased in terms of discrete-time filters, much of it applies directly to continuous-time filters as well. Chap. 6 Problems 509 I H (ei""') I 1 + 01 1-------i 1 -01 ~----------- 1T w Figure P6.61 Consider a lowpass filter with frequency response H(eiw) for which IH(e.iw)l falls within the tolerance limits shown in Figure P6.61; that is, 1 - 81 :S IH(eiw)l :S 1 + 81, 0 :S W :S w1, 0 :S IH(ejw )I :S 82, w2 :S w :S 7T. A new filter with frequency response G(eiw) is formed by cascading two identical filters, both with frequency response H(eiw ). (a) Determine the tolerance limits on IG(eiw)l. (b) Assuming that H ( eiw) is a good approximation to a lowpass filter, so that o1 < < 1 and 82 << 1, determine whether the passband ripple for G(eiw) is larger or smaller than the passband ripple for H(e.iw). Also, determine whether the stop- band ripple for G(e.iw) is larger or smaller than the stopband ripple for H(ei(u). (c) If N identical filters with frequency response H(eiw) are cascaded to obtain a new frequency response G( eiw ), then, again assuming that o1 < < 1 and 82 < < I, determine the approximate tolerance limits on IG(e.iw)l. 6.62. In Problem 6.61, we considered one method for using a basic filter module repcti tively to implement a new filter with improved characteristics. Let us now con""idet an alternative approach, proposed by J. W. Tukey in the book, Explorator."" I >uru Analysis (Reading, MA: Addison-Wesley Publishing Co., Inc., 1976). The pwce- dure is shown in block diagram form in Figure P6.62(a). (a) Suppose that H ( eiw) is real and has a passband ripple of :±: o1 and d stopband ripple of :±:82 (i.e., H(e.iw) falls within the tolerance limits indicated in Fig- ure P6.62(b)). The frequency response G(eiw) of the overall system in Figure P6.62(a) falls within the tolerance limits indicated in Figure P6.62(c). Deter- mine A, B, C, and Din terms of 81 and 82. (b) If o1 < < I and 82 < < 1, what is the approximate passband ripple and stopband ripple associated with G(eiw)? Indicate in particular whether the passband rip- ple for G(e.i(u) is larger or smaller than the passband ripple for H(eiw). Also, indicate whether the stopband ripple for G(eiw) is larger or smaller than the stopband ripple for H(e.iw). (c) In parts (a) and (b), we assumed that H(eiw) is real. Now consider H(e.iw) to have the more general form H(ejw) = Hl(e.iw)eje(wl, where H 1( ei«J) is real and O(w) is an unspecified phase characteristic. If IH(e.iw)l 510 Time and Frequency Characterization of Signals and Systems Chap.6 g[n] x[n] _""""'!""""""_...,_...,.~I H(eiw) y[n] - I I_ -- - -- - --- - - --- - - - - - - --- - - - -- - _I (a) Wp I Ws 111' W ~-----.....,j (b) AI------ A ::::; G (eiw) :S B 0 :S w :S wp C :S G (eiw) :S D w5 :S w ::::; 11' 81-----...., c.....----.... I w5 o'-----• (c) Figure P6.62 is a reasonable approximation to an ideallowpass filter, williG(eiw)l necessarily be a reasonable approximation to an ideallowpass filter? (d) Now assume that H(efw) is an FIR linear-phase lowpass filter, so that H(eiw) = HI (efw)efMw, Chap. 6 Problems 511 where H 1( eiw) is real and M is an integer. Show how to modify the system in Figure P6.62(a) so that the overall system will approximate a lowpass filter. 6.63. In the design of digital filters, we often choose a filter with a specified magnitude characteristic that has the shortest duration. That is, the impulse response, which is the inverse Fourier transform of the complex frequency spectrum, should be as narrow as possible. Assuming that h[n] is real, we wish to show that if the phase (}(w) associated with the frequency response H(eiw) is zero, the duration of the impulse response is minimal. Let the frequency response be expressed as and let us consider the quantity n =-ex n= -ex to be a measure of the duration of the associated impulse response h[n]. (a) Using the derivative property of the Fourier transform and Parseval's relation, express Din terms of H(eiw). (b) By expressing H(eiw) in terms of its magnitude IH(eiw)l and phase (}(w ), use your result from part (a) to show that Dis minimized when (}(w) = 0. 6.64. For a discrete-time filter to be causal and to have exactly linear phase, its impulse response must be of finite length and consequently the difference equation must be nonrecursive. To focus on the insight behind this statement, we consider a particular case, that of a linear phase characteristic for which the slope of the phase is an integer. Thus, the frequency response is assumed to be of the form (P6.64-1) where Hr(eiw) is real and even. Let h[n] denote the impulse response of the filter with frequency response H(eiw) and let hr[n] denote the impulse response of the filter with frequency re- sponse Hr(eiw). (a) By using the appropriate properties in Table 5.1, show that: 1. hr[n] = hr[- n] (i.e., hr[n] is symmetric about n = 0). 2. h[n] = hr[n - M]. (b) Using your result in part (a), show that with H(eiw) of the form shown in eq. (P6.64-1), h[n] is symmetric about n = M, that is, h[M + n] = h[M - n]. (P6.64-2) (c) According to the result in part (b), the linear phase characteristic in eq. (P6.64-1) imposes a symmetry in the impulse response. Show that if h[n] is causal and has the symmetry in eq. (P6.64-2), then h[n] = 0, n < 0 and n >2M (i.e., it must be of finite length). 512 Time and Frequency Characterization of Signals and Systems Chap.6 6.65. For a class of discrete-time lowpass filters, known as Butterworth filters, the squared magnitude of the frequency response is given by jB(efw)l2 = 1 2N' 1 + ( tan(w/2) ) tan(wJ2) where We is the cutoff frequency (which we shall take to be 'TT'/2) and N is the order of the filter (which we shall consider to beN = 1). Thus, we have B(efw )j2 = 1 1 + tan2(w/2) · ' (a) Using trigonometric identities, show that IB(efw)l2 = cos2(w/2). (b) Let B(eiw) = acos(w/2). For what complex values of a is jB(efw)l2 the same as in part (a)? (c) Show that B(efw) from part (b) is the transfer function corresponding to a dif- ference equation of the form y[n] = ax[n] + f3x[n- y]. Determine a, f3, and 'Y. 6.66. In Figure P6.66(a) we show a discrete-time system consisting of a parallel combi- nation of N LTI filters with impulse response hk[n], k = 0, 1, · · ·, N- 1. For any k, hk[n] is related to h0 [n] by the expression hk[n] = ei(27Tnk!N) ho[n]. (a) If h0 [n] is an ideal discrete-time lowpass filter with frequency response Ho( efw) as shown in Figure P6.66(b), sketch the Fourier transforms of h1 [n] and hN-I [n] for win the range -'TT' < w :::; +'TT'. Yo[n] N-1 x[n] y[n] = !. Yk[n] k=O Figure P6.66a Chap. 6 Problems 513 1T w Figure P6.66b (b) Determine the value of the cutoff frequency We in Figure P6.66(b) in terms of N (0 < we ~ 7T) such that the system of Figure P6.66(a) is an identity system; that is, y[n] = x[n] for all nand any input x[n]. (c) Suppose that h[n] is no longer restricted to be an ideallowpass filter. If h[n] denotes the impulse response of the entire system in Figure P6.66(a) with input x[n] and output y[n], then h[n] can be expressed in the form h[n] = r[n]ho[n]. Determine and sketch r[ n]. (d) From your result of part (c), determine a necessary and sufficient condition on h0 [n] to ensure that the overall system will be an identity system (i.e., such that for any input x[n], the output y[n] will be identical to x[n]). Your answer should not contain any sums. 7 SAMPLING 7.0 INTRODUCTION Under certain conditions, a continuous-time signal can be completely represented by and recoverable from knowledge of its values, or samples, at points equally spaced in time. This somewhat surprising property follows from a basic result that is referred to as the sampling theorem. This theorem is extremely important and useful. It is exploited, for example, in moving pictures, which consist of a sequence of individual frames, each of which represents an instantaneous view (i.e., a sample in time) of a continuously changing scene. When these samples are viewed in sequence at a sufficiently fast rate, we perceive an accurate representation of the original continuously moving scene. As another example, printed pictures typically consist of a very fine grid of points, each corresponding to a sample of the spatially continuous scene represented in the picture. If the samples are sufficiently close together, the picture appears to be spatially continuous, although under a magnifying glass its representation in terms of samples becomes evident. Much of the importance of the sampling theorem also lies in its role as a bridge between continuous-time signals and discrete-time signals. As we will see in this chapter, the fact that under certain conditions a continuous-time signal can be completely recovered from a sequence of its samples provides a mechanism for representing a continuous-time signal by a discrete-time signal. In many contexts, processing discrete-time signals is more flexible and is often preferable to processing continuous-time signals. This is due in large part to the dramatic development of digital technology over the past few decades, result- ing in the availability of inexpensive, lightweight, programmable, and easily reproducible discrete-time systems. The concept of sampling, then, suggests an extremely attractive and widely employed method for using discrete-time system technology to implement continuous-time systems and process continuous-time signals: We exploit sampling to 514 Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 51 s convert a continuous-time signal to a discrete-time signal, process the discrete-time signal using a discrete-time system, and then convert back to continuous time. In the following discussion, we introduce and develop the concept of sampling and the process of reconstructing a continuous-time signal from its samples. In this discus- sion, we both identify the conditions under which a continuous-time signal can be exactly reconstructed from its samples and examine the consequences when these conditions are not satisfied. Following this, we explore the processing of continuous-time signals that have been converted to discrete-time signals through sampling. Finally, we examine the sampling of discrete-time signals and the related concepts of decimation and interpola- tion. 7.1 REPRESENTATION OF A CONTINUOUS-TIME SIGNAL BY ITS SAMPLES: THE SAMPLING THEOREM In general, in the absence of any additional conditions or information, we would not expect that a signal could be uniquely specified by a sequence of equally spaced samples. For example, in Figure 7.1 we illustrate three different continuous-time signals, all of which have identical values at integer multiples ofT; that is, Clearly, an infinite number of signals can generate a given set of samples. As we will see, however, if a signal is band limited-i.e., if its Fourier transform is zero outside a finite band of frequencies-and if the samples are taken sufficiently close together in relation to the highest frequency present in the signal, then the samples uniquely specify the signal, and we can reconstruct it perfectly. This result, known as the sampling theorem, is of profound importance in the practical application of the methods of signal and system analysis. Figure 7.1 Three continuous-time signals with identical values at integer multiples of T. 516 Sampling Chap. 7 7. 1 . 1 Impulse-Train Sampling In order to develop the sampling theorem, we need a convenient way in which to represent the sampling of a continuous-time signal at regular intervals. A useful way to do this is through the use of a periodic impulse train multiplied by the continuous-time signal x(t) that we wish to sample. This mechanism, known as impulse-train sampling, is depicted in Figure 7 .2. The periodic impulse train p(t) is referred to as the sampling function, the period T as the sampling period, and the fundamental frequency of p(t), Ws = 27r/T, as the sampling frequency. In the time domain, Xp(t) = x(t)p(t), (7.1) where +oc p(t) = 2, D(t - nT). (7.2) n= -oc Because of the sampling property of the unit impulse discussed in Section 1.4.2, we know that multiplying x(t) by a unit impulse samples the value of the signal at the point at which the impulse is located; i.e., x(t)D(t- to) = x(t0 )B(t- t0 ). Applying this to eq. (7.1), we see, as illustrated in Figure 7.2, that Xp(t) is an impulse train with the amplitudes of p(t) x(t) --.•.- --f~...,._--1•~ xp(t) 0 1-r-j p(t) 1 t t 1 t t t t 0 / / 0 Figure 7.2 Impulse-train sampling. Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 517 the impulses equal to the samples of x(t) at intervals spaced by T; that is, +oc Xp(t) = L x(nT)8(t - nT). (7.3) n= -oo From the multiplication property (Section 4.5), we know that X/}w) = -}J +x X(j8)P(j(w - 8))d8. (7.4) 27T -X and from Example 4.8, 2 L+oc P(jw) = ; 8(w - kws). (7.5) k= -00 Since convolution with an impulse simply shifts a signal [i.e., X(jw) * 8(w - w 0 ) = X(j(w - w0))], it follows that 1 +oc Xp(jw) = T L X(j(w - kws)). (7.6) k= -00 That is, Xp(jw) is a periodic function of w consisting of a superposition of shifted replicas of X(jw), scaled by 1/T, as illustrated in Figure 7.3. In Figure 7.3(c), WM < (ws- wM), or equivalently, Ws > 2wM, and thus there is no overlap between the shifted replicas of X(jw ), whereas in Figure 7.3(d), with Ws < 2wM, there is overlap. For the case illustrated in Figure 7.3(c), X(jw) is faithfully reproduced at integer multiples of the sampling fre- quency. Consequently, if Ws > 2wM, x(t) can be recovered exactly from xp(t) by means of X(jw) l -wM WM w (a) P(jw) 2;1 t t t t t ... Figure 7.3 Effect in the frequency -2w 0 2w domain of sampling in the time do- 5 -ws Ws 5 main: (a) spectrum of original signal; (b) (b) spectrum of sampling function; 518 Sampling Chap. 7 XP(jw) 1\1\lhl\1\1\ -wM 0 WM t W5 W (c) (ws- wM) Figure 7.3 Continued (c) spectrum of sampled signal with ws > 2wM; w (d) spectrum of sampled signal with Ws < 2wM. a lowpass filter with gain T and a cutoff frequency greater than w M and less than w.1. - w M, as indicated in Figure 7 .4. This basic result, referred to as the sampling theorem, can be stated as follows: 1 Sampling Theorem: Let x(t) be a band-limited signal with X(jw) = 0 for lwl > WM. Then x(t) is uniquely determined by its samples x(nT), n = 0, :±: 1, ±2, ... , if Ws > 2wM, where Ws Given these samples, we can reconstruct x(t) by generating a periodic impulse train in which successive impulses have amplitudes that are successive sample values. This impulse train is then processed through an ideal lowpass filter with gain T and cutoff frequency greater than w M and less than w .1· - w M. The resulting output signal will exactly equal x(t). 1 The important and elegant sampling theorem was available for many years in a variety of forms in the mathematics literature. See, for example, J. M. Whittaker, ""Interpolatory Function Theory,"" (New York: Stecher-Hafner Service Agency, 1964), chap. 4. It did not appear explicitly in the literature of communication theory until the publication in 1949 of the classic paper by Shannon entitled ""Communication in the Presence of Noise"" (Proceedings of the IRE, January 1949, pp. 10-21 ). However, H. Nyquist in 1928 and D. Gabor in 1946 had pointed out, based on the use of the Fourier Series, that 2TW numbers are sufficient to represent a function of duration T and highest frequency W. [H. Nyquist, ""Certain Topics in Telegraph Transmission Theory,"" AlEE Transactions, 1928, p. 617; D. Gabor, ""Theory of Communication,""Journa/ of lEE 93, no. 26 (1946), p. 429.] Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 519 p(t) = I S(t - nT) n = -x Xp(jw) x(t) --....,.•~~ Xp(l) • 81-----.....j•~xr(t) (a) X(jw) ~ w (b) XP(jw) ~AL(WM w (c) H(jw) T r----f-1--w-M..,<wc <(ws -wM) w (d) Figure 7.4 Exact recovery of a continuous-time signal from its sam- Xr(jw) ples using an ideal lowpass filter: A (a) system for sampling and recon- struction; (b) representative spectrum for x(t); (c) corresponding spectrum for xp(t); (d) ideal lowpass filter to re- cover X(jw) from Xp(jw ); (e) spectrum (e) Of Xr(t). The frequency 2w M, which, under the sampling theorem, must be exceeded by the sam- pling frequency, is commonly referred to as the Nyquist rate. 2 As discussed in Chapter 6, ideal filters are generally not used in practice for a va- riety of reasons. In any practical application, the ideallowpass filter in Figure 7.4 would be 2The frequency wM corresponding to one-half the Nyquist rate is often referred to as the Nyquist fre- quency. 520 Sampling Chap. 7 replaced by a nonideal filter H(jw) that approximated the desired frequency character- istic accurately enough for the problem of interest (i.e., H (jw) = 1 for lw I < w M, and H(jw) = 0 for jwj > ws- WM ). Obviously, any such approximation in the lowpass filter- ing stage will lead to some discrepancy between x(t) and x,.(t) in Figure 7.4 or, equiva- lently, between X(jw) and X,.(jw ). The particular choice of nonideal filter is then dictated by the acceptable level of distortion for the application under consideration. For conve- nience and to emphasize basic principles such as the sampling theorem, we will regularly assume the availability and use of ideal filters throughout this and the next chapter, with the understanding that in practice such a filter must be replaced by a nonideal filter designed to approximate the ideal characteristics accurately enough for the problem at hand. 7. 1 .2 Sampling with a Zero-Order Hold The sampling theorem, which is most easily explained in terms of impulse-train sampling, establishes the fact that a band-limited signal is uniquely represented by its samples. In practice, however, narrow, large-amplitude pulses, which approximate impulses, are also relatively difficult to generate and transmit, and it is often more convenient to generate the sampled signal in a form referred to as a zero-order hold. Such a system samples x(t) at a given instant and holds that value until the next instant at which a sample is taken, as illustrated in Figure 7.5. The reconstruction of x(t) from the output of a zero-order hold can again be carried out by lowpass filtering. However, in this case, the required filter no longer has constant gain in the passband. To develop the required filter characteristic, we first note that the output x0 (t) of the zero-order hold can in principle be generated by impulse-train sampling followed by an LTI system with a rectangular impulse response, as depicted in Figure 7 .6. To reconstruct x(t) from x0 (t), we consider processing x0 (t) with an LTI system with impulse response h,.(t) and frequency response H,.(jw ). The cascade of this system with the system of Figure 7.6 is shown in Figure 7.7, where we wish to specify H,.(jw) so that r(t) = x(t). Comparing the system in Figure 7.7 with that in Figure 7.4, we see that r(t) = x(t) if the cascade combination of h0(t) and h,.(t) is the ideallowpass filter H(jw) used in Figure 7.4. Since, from Example 4.4 and the time-shifting property in Section 4.3.2, Ho J.W ) -_ e -j·w T/'2 [2sin(wT/2)] ( , (7.7) w this requires that e.icvT/'2 H(jw) H,.(jw) = 2sin(wT/2) · (7.8) w x(t) __ Zerhoo-oldrd er Xo (t) ...,.~ Figure 7.5 Sampling utilizing a zero-order hold. Sec. 7.1 Representation of a Continuous-Time Signal by Its Samples: The Sampling Theorem 521 p(t) h0 (t) x(t)-.....,.~ 1b_ .,...._ _, .._ x (t) 0 0 T t x(t) /.-- ........... / ' / / ' --- ...... I I ' ' I "" x0 (t) Figure 7.6 Zero-order hold as \ impulse-train sampling followed by an LTI system with a rectangular impulse response. H(jw) r-------------------------------- p(t) 1 I I I Xp (t)l x0 (t) x(t) ---t~~ 1---+--~ r(t) I I I 0 Tt I IL ______________________________ _ Figure 7.7 Cascade of the representation of a zero-order hold (Figure 7.6) with a reconstruction filter. 522 Sampling Chap. 7 I Hr(jw)l (l) Figure 7.8 Magnitude and phase for the reconstruction filter for a zero- order hold. For example, with the cutoff frequency of H(jw) equal to w.J2, the ideal magnitude and phase for the reconstruction filter following a zero-order hold is that shown in Figure 7 .8. Once again, in practice the frequency response in eq. (7.8) cannot be exactly realized, and thus an adequate approximation to it must be designed. In fact, in many situations, the output of the zero-order hold is considered an adequate approximation to the original signal by itself, without any additionallowpass filtering, and in essence represents a possible, al- though admittedly very coarse, interpolation between the sample values. Alternatively, in some applications, we may wish to perform some smoother interpolation between sample values. In the next section, we explore in more detail the general concept of interpreting the reconstruction of a signal from its samples as a process of interpolation. 7.2 RECONSTRUCTION OF A SIGNAL FROM ITS SAMPLES USING INTERPOLATION Interpolation, that is, the fitting of a continuous signal to a set of sample values, is a commonly used procedure for reconstructing a function, either approximately or exactly, from samples. One simple interpolation procedure is the zero-order hold discussed in Section 7 .1. Another useful form of interpolation is linear interpolation, whereby adja- cent sample points are connected by a straight line, as illustrated in Figure 7. 9. In more Figure 7. 9 Linear interpolation be- tween sample points. The dashed curve represents the original signal and the solid curve the linear interpolation. Sec. 7.2 Reconstruction of a Signal from Its Samples Using Interpolation 523 complicated interpolation formulas, sample points may be connected by higher order poly- nomials or other mathematical functions. As we have seen in Section 7.1, for a band-limited signal, if the sampling instants are sufficiently close, then the signal can be reconstructed exactly; i.e., through the use of a lowpass filter, exact interpolation can be carried out between the sample points. The interpretation of the reconstruction of x(t) as a process of interpolation becomes evident when we consider the effect in the time domain of the lowpass filter in Figure 7 .4. In particular, the output is Xr(t) = Xp(t) * h(t) or, with Xp(t) given by eq. (7.3), +oo Xr(t) = L x(nT)h(t - nT). (7.9) n= -oc Equation (7 .9) describes how to fit a continuous curve between the sample points x(nT) and consequently represents an interpolation formula. For the ideallowpass filter H(jw) in Figure 7.4, WeT sin(wet) h(t) = ---- (7.10) 7TWet so that Xr(t) = f x(nT) WeT sin(wc(t - nT)). (7.11) n= 7T We(t- nT) -oo The reconstruction according to eq. (7.11) with We = w 5 /2 is illustrated in Figure 7.10. Figure 7.10(a) represents the original band-limited signal x(t), and Figure 7.10(b) rep- resents x p(t), the impulse train of samples. In Figure 7.1 0( c), the superposition of the individual terms in eq. (7.11) is illustrated. Interpolation using the impulse response of an ideallowpass filter as in eq. (7 .11) is commonly referred to as band-limited interpolation, since it implements exact re- construction if x(t) is band limited and the sampling frequency satisfies the condi- tions of the sampling theorem. As we have indicated, in many cases it is preferable to use a less accurate, but simpler, filter or, equivalently, a simpler interpolating func- tion than the function in eq. (7.10). For example, the zero-order hold can be viewed as a form of interpolation between sample values in which the interpolating function h(t) is the impulse response h0(t) depicted in Figure 7.6. In that sense, with x0(t) in the figure corresponding to the approximation to x(t), the system h0(t) represents an approximation to the ideal lowpass filter required for the exact interpolation. Fig- ure 7.11 shows the magnitude of the transfer function of the zero-order-hold interpo- lating filter, superimposed on the desired transfer function of the exact interpolating filter. · Both from Figure 7.11 and from Figure 7 .6, we see that the zero-order hold is a very rough approximation, although in some cases it is sufficient. For example, if additional 524 Sampling Chap. 7 x(t) (a) ,.-- .... I I ' I ' I (b) Figure 7. 1 0 Ideal band-limited in- terpolation using the sine function: (a) band-limited signal x(t); (b) im- pulse train of samples of x(t); (c) ideal band-limited interpolation in which the impulse train is replaced by a superpo- (c) sition of sine functions [eq. (7.11 )]. T ~Ideal interpolating filter Figure 7. 11 Transfer function for 0 the zero-order hold and for the ideal interpolating filter. lowpass filtering is naturally applied in a given application, it will tend to improve the overall interpolation. This is illustrated in the case of pictures in Figure 7 .12. Fig- ure 7.12(a) shows pictures with impulse sampling (i.e., sampling with spatially nar- row pulses). Figure 7.12(b) is the result of applying a two-dimensional zero-order hold to Figure 7.12(a), with a resulting mosaic effect. However, the human visual system inherently imposes lowpass filtering, and consequently, when viewed at a dis- tance, the discontinuities in the mosaic are smoothed. For example, in Figure 7.12(c) a (a) (b) (c) Figure 7.12 (a) The original pictures of Figures 6.2(a) and (g) with impulse sam- pling; (b) zero-order hold applied to the pictures in (a). The visual system naturally introduces lowpass filtering with a cutoff frequency that decreases with distance. Thus, when viewed at a distance, the discontinuities in the mosaic in Figure 7.12(b) are smoothed; (c) result of applying a zero-order hold after impulse sampling with one-fourth the horizontal and vertical spacing used in (a) and (b). 525 526 Sampling Chap. 7 zero-order hold is again used, but here the sample spacing in each direction is one-fourth that in Figure 7.12(a). With normal viewing, considerable lowpass filtering is naturally applied, although the mosaic effect is still evident. If the crude interpolation provided by the zero-order hold is insufficient, we can use a variety of smoother interpolation strategies, some of which are known collectively as higher order holds. In particular, the zero-order hold produces an output signal, as in Fig- ure 7.5, that is discontinuous. In contrast, linear interpolation, as illustrated in Figure 7.9, yields reconstructions that are continuous, although with discontinous derivatives due to the changes in slope at the sample points. Linear interpolation, which is sometimes referred to as a first-order hold, can also be viewed as interpolation in the form of Figure 7.4 and eq. (7 .9) with h(t) triangular, as illustrated in Figure 7 .13. The associated transfer function is also shown in the figure and is 2 H(. ) = 2_ [sin(wT/2)] JW (7.12) T w/2 The transfer function of the first -order hold is shown superimposed on the transfer function for the ideal interpolating filter. Figure 7.14 corresponds to the same pictures as those in Figure 7 .12(b ), but with a first-order hold applied to the sampled picture. In an analogous fashion, we can define second- and higher order holds that produce reconstructions with a higher degree of smoothness. For example, the output of a second-order hold provides an interpolation of the sample values that is continuous and has a continuous first derivative and discontinuous second derivative. p(t) ~ Xp(t) h(t) x(t) --.-1 X !--..__.....,.~ H(jw) 1---~ Xr(t) (a) T 2T (b) Figure 7. 1 3 Linear interpolation h(t) (first-order hold) as impulse-train sam- ;l pling followed by convolution with a triangular impulse response: (a) sys- tem for sampling and reconstruction; (b) impulse train of samples; (c) im- -T T pulse response representing a first- (c) order hold; Sec. 7.3 The Effect of Undersampling: Aliasing 527 (d) H(jw) T Ideal interpolating filter Figure 7.13 Continued (d) first- order hold applied to the sampled sig- 0 nal; (e) comparison of transfer function of ideal interpolating filter and first- (e) order hold. 7.3 THE EFFECT OF UNDERSAMPLING: ALIASING In previous sections in this chapter, it was assumed that the sampling frequency was sufficiently high that the conditions of the sampling theorem were met. As illustrated in Figure 7 .3, with Ws > 2w M, the spectrum of the sampled signal consists of scaled repli- cations of the spectrum of x(t), and this forms the basis for the sampling theorem. When (a) (b) Figure 7. 14 Result of applying a first-order hold rather than a zero-order hold af- ter impulse sampling with one-third the horizontal and vertical spacing used in Fig- ures 7.12(a) and (b). 528 Sampling Chap. 7 Ws < 2wM, X(jw), the spectrum of x(t), is no longer replicated in Xp(jw) and thus is no longer recoverable by lowpass filtering. This effect, in which the individual terms in eq. (7 .6) overlap, is referred to as aliasing, and in this section we explore its effect and consequences. Clearly, if the system of Figure 7.4 is applied to a signal with Ws < 2wM, the reconstructed signal Xr(t) will no longer be equal to x(t). However, as explored in Problem 7.25, the original signal and the signal Xr(t) that is reconstructed using band- limited interpolation will always be equal at the sampling instants; that is, for any choice ofws, Xr(nT) = x(nT), n = 0, ±1, ±2, .... (7.13) Some insight into the relationship between x(t) and Xr(t) when Ws < 2wM is pro- vided by considering in more detail the comparatively simple case of a sinusoidal signal. Thus, let x(t) = cos wot, (7.14) with Fourier transform X(jw) as indicated in Figure 7.15(a). In this figure, we have graphically distinguished the impulse at w0 from that at -w0 for convenience. Let us consider X p(jw ), the spectrum of the sampled signal, and focus in particular on the effect of a change in the frequency w 0 with the sampling frequency w s fixed. In Fig- ures 7.15(b)-(e), we illustrate Xp(jw) for several values of w0. Also indicated by a dashed line is the passband of the lowpass filter of Figure 7.4 with We = ws/2. Note that no aliasing occurs in (b) and (c), since w0 < w sf2, whereas aliasing does occur in (d) and (e). For each of the four cases, the lowpass filtered output Xr(t) is given as follows: Ws (a) wo = 6' Xr(t) = cos wot = x(t) 2ws (b) wo = Xr(t) = cos wot = x(t) 6 ' 4ws (c) wo = Xr(t) = cos(ws - wo)t ~ x(t) 6 ' 5ws (d) wo = Xr(t) = cos(ws - wo)t ~ x(t). 6 ' When aliasing occurs, the original frequency w 0 takes on the identity of a lower fre- quency, Ws- wo. For ws/2 < wo < Ws, as wo increases relative toWs, the output frequency w s - w0 decreases. When w s = w0 , for example, the reconstructed signal is a constant. This is consistent with the fact that, when sampling once per cycle, the samples are all equal and would be identical to those obtained by sampling a constant signal (w0 = 0). In Figure 7.16, we have depicted, for each of the four cases in Figure 7.15, the signal x(t), its samples, and the reconstructed signal Xr(t). From the figure, we can see how the lowpass Aliasing Figure 7. 1 5 Effect in the frequency domain of oversampling and under- sampling: (a) spectrum of original si- nusoidal signal; (b), (c) spectrum of sampled signal with ws > 2wo; (d), (e) spectrum of sampled signal with ws < 2wo. As we increase wo in mov- ing from (b) through (d), the impulses drawn with solid lines move to the right, while the impulses drawn with dashed lines move to the left. In (d) and (e), these impulses have moved sufficiently that there is a change in the ones falling within the passband of the ideal lowpass filter. filter interpolates between the samples, in particular always fitting a sinusoid of frequency less than wsf2 to the samples of x(t). As a variation on the preceding examples, consider the signal x(t) = cos(wot + </>). (7.15) V1 / Original signal IN 0 / Reconstructed signal wo=~ 6 ' ', / ' / ' ' ' / ' ' / ' ' ' (a) ' 2w wo=65 ' / / \ ' \ I /f ' \ I ' \ I ' \ \ I \ I \ I \ \ I \ I \ I \ I \ \ I I \ \ \ I I ' / ' / ' / ' / (b) Figure 7. 16 Effect of aliasing on a sinusoidal signal. For each of four values of wo. the original sinusoidal signal (solid curve), its samples, and the reconstructed sig- nal (dashed curve) are illustrated: (a) w0 = w5/6; (b) w0 = 2w5/6; (c) w0 = 4wsf6; (d) w0 = 5w5/6. In (a) and (b) no aliasing occurs, whereas in (c) and (d) there is aliasing. ii<O ii<O II II 0 0 3 3 I / ' I / / / / / I I I I ' I / / ' ' ' \ / \ I I I '1c:u:J :::::s :§ c::: 0 <:.,:) ' I ..0 ~ / ~ ,..: / I / ~ I :I at Li: ' I ' ' ' / ' \ I I / / ' \ I / / / / / I I I 531 532 Sampling Chap. 7 In this case, the Fourier transform of x(t) is essentially the same as Figure 7.15(a), ex- cept that the impulse indicated with a solid line now has amplitude 7TeN>, while the impulse indicated with a dashed line has amplitude with the opposite phase, namely, 7Te- N>. If we now consider the same set of choices for w 0 as in Figure 7.15, the re- sulting spectra for the sampled versions of cos(w0t + cp) are exactly as in the figure, with all solid impulses having amplitude 7TeN> and all dashed ones having amplitude 7Te- N>. Again, in cases (b) and (c) the condition of the sampling theorem is met, so that x,(t) = cos(w0t + cp) = x(t), while in (d) and (e) we again have aliasing. How- ever, we now see that there has been a reversal in the solid and dashed impulses ap- pearing in the passband of the lowpass filter. As a result, we find that in these cases, xr(t) = cos[(ws -w0 )t-cp], where we have a change in the sign of the phase cp, i.e., a phase reversal. It is important to note that the sampling theorem explicity requires that the sampling frequency be greater than twice the highest frequency in the signal, rather than greater than or equal to twice the highest frequency. The next example illustrates that sampling a sinusoidal signal at exactly twice its frequency (i.e., exactly two samples per cycle) is not sufficient. Example 7.1 Consider the sinusoidal signal and suppose that this signal is sampled, using impulse sampling, at exactly twice the frequency of the sinusoid, i.e., at sampling frequency Ws. As shown in Problem 7.39, if this impulse-sampled signal is applied as the input to an ideallowpass filter with cutoff frequency w sf2, the resulting output is x,(l) ~ (cos c/>) cos ( ~' t). As a consequence, we see that perfect reconstruction of x(t) occurs only in the case in which the phase l/J is zero (or an integer multiple of 27T). Otherwise, the signal Xr(t) does not equal x(t). As an extreme example, consider the case in which lfJ = -7T/2, so that This signal is sketched in Figure 7.17. We observe that the values of the signal at integer multiples of the sampling period 21Tiws are zero. Consequently, sampling at this rate produces a signal that is identically zero, and when this zero input is applied to the ideal lowpass filter, the resulting output Xr(t) is also identically zero. Sec. 7.3 The Effect of Undersampling: Aliasing 533 ~v(\v( \-T(v\v,(T\3\(J \v(\ v( Figure 7. 1 7 Sinusoidal signal for Example 7.1. The effect of undersampling, whereby higher frequencies are reflected into lower frequencies, is the principle on which the stroboscopic effect is based. Consider, for exam- ple, the situation depicted in Figure 7 .18, in which we have a disc rotating at a constant rate with a single radial line marked on the disc. The flashing strobe acts as a sampling system, since it illuminates the disc for extremely brief time intervals at a periodic rate. When the strobe frequency is much higher than the rotational speed of the disc, the speed of rotation of the disc is perceived correctly. When the strobe frequency becomes less than twice the rotational frequency of the disc, the rotation appears to be at a lower frequency than is actu- ally the case. Furthermore, because of phase reversal, the disc will appear to be rotating in the wrong direction! Roughly speaking, if we track the position of a fixed line on the disc at successive samples, then when w0 < Ws < 2w0 , so that we sample somewhat more fre- quently than once per revolution, samples of the disc will show the fixed line in positions that are successively displaced in a counterclockwise direction, opposite to the clockwise rotation of the disc itself. At one flash per revolution, corresponding tows = w 0 , the radial line appears stationary (i.e., the rotational frequency of the disc and its harmonics have been aliased to zero frequency). A similar effect is commonly observed in Western movies, Strobe Figure 7. 18 Strobe effect. 534 Sampling Chap. 7 where the wheels of a stagecoach appear to be rotating more slowly than would be consis- tent with the coach's forward motion, and sometimes in the wrong direction. In this case, the sampling process corresponds to the fact that moving pictures are a sequence of indi- vidual frames with a rate (usually between 18 and 24 frames per second) corresponding to the sampling frequency. The preceding discussion suggests interpreting the stroboscopic effect as an exam- ple of a useful application of aliasing due to undersampling. Another practical application of aliasing arises in a measuring instrument referred to as a sampling oscilloscope. This instrument is intended for observing very high-frequency waveforms and exploits the prin- ciples of sampling to alias these frequencies into ones that are more easily displayed. The sampling oscilloscope is explored in more detail in Problem 7 .38. 7.4 DISCRETE-TIME PROCESSING OF CONTINUOUS-TIME SIGNALS In many applications, there is a significant advantage offered in processing a continuous- time signal by first converting it to a discrete-time signal and, after discrete-time process- ing, converting back to a continuous-time signal. The discrete-time signal processing can be implemented with a general- or special-purpose computer, with microprocessors, or with any of the variety of devices that are specifically oriented toward discrete-time signal processing. In broad terms, this approach to continuous-time signal processing can be viewed as the cascade of three operations, as indicated in Figure 7 .19, where xc(t) and Yc(t) are continuous-time signals and xd[n] and Yc~[n] are the discrete-time signals corresponding to Xc(t) and Yc(t). The overall system is, of course, a continuous-time system in the sense that its input and output are both continuous-time signals. The theoretical basis for converting a continuous-time signal to a discrete-time signal and reconstructing a continuous-time signal from its discrete-time representation lies in the sampling theorem, as discussed in Section 7 .1. Through the process of periodic sampling with the sampling frequency consistent with the conditions of the sampling theorem, the continuous-time signal Xc(t) is exactly represented by a sequence of instantaneous sample values Xc(nT); that is, the discrete-time sequence xd[n] is related to Xc(t) by Xd[n] = Xc(nT). (7.16) ------------------------------------------- 1 xd[n] Conversion to Discrete-time Conversion to discrete time system continous time Figure 7. 19 Discrete-time processing of continuous-time signals. Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 535 The transformation of Xc(t) to xd[n] corresponding to the first system in Figure 7.19 will be referred to as continuous-to-discrete-time conversion and will be abbreviated C/D. The reverse operation corresponding to the third system in Figure 7.19 will be abbreviated D/C, representing discrete-time to continuous-time conversion. The D/C operation performs an interpolation between the sample values provided to it as input. That is, the D/C operation produces a continuous-time signal YcU) which is related to the discrete-time signal Yd[n] by Yd[n] = Yc(nT). This notation is made explicit in Figure 7 .20. In systems such as digital computers and digital systems for which the discrete-time signal is represented in digital form, the device commonly used to implement the C/D conversion is referred to as an analog-to-digital (A- to-D) converter, and the device used to implement the D/C conversion is referred to as a digital-to-analog (D-to-A) converter. r-------, xd [n] = Xc (nT) Yd [n] = Yc (nT) Xc (t) 1-------t~ Yc (t) Discrete -Time t-----+1 System T T Figure 7.20 Notation for continuous-to-discrete-time conversion and discrete-to-continuous-time conversion. T represents the sampling period. To understand further the relationship between the continuous-time signal xc(t) and its discrete-time representation xd [ n], it is helpful to represent C/D as a process of periodic sampling followed by a mapping of the impulse train to a sequence. These two steps are illustrated in Figure 7 .21. In the first step, representing the sampling process, the impulse train Xp(t) corresponds to a sequence of impulses with amplitudes corresponding to the samples of Xc(t) and with a time spacing equal to the sampling period T. In the conver- sion from the impulse train to the discrete-time sequence, we obtain xd[n], corresponding to the same sequence of samples of Xc(t), but with unity spacing in terms of the new in- dependent variable n. Thus, in effect, the conversion from the impulse train sequence of samples to the discrete-time sequence of samples can be thought of as a normalization in time. This normalization in converting xp(t) to xd[n] is evident in Figures 7.2l(b) and (c), in which Xp(t) and xd[n] are respectively illustrated for sampling rates ofT = T 1 and T = 2Tt. It is also instructive to examine the processing stages in Figure 7.19 in the frequency domain. Since we will be dealing with Fourier transforms in both continuous and dis- crete time, in this section only we distinguish the continuous-time and discrete-time fre- quency variables by using w in continuous time and 0 in discrete time. For example, the continuous-time Fourier transforms of Xc(t) and Yc(t) are Xc(Jw) and Yc(jw ), respectively, while the discrete-time Fourier transforms of xd[n] and Yd[n] are Xd(ej0 ) and Yd(ej0 ), respectively. 536 Sampling Chap. 7 C/O conversion ----------------, I I p(t) : ~ Conversion of x (t) ~ x Xp (t) impulse tr~in xd[n] c 1 to d1screte-t1me sequence 1 I I_-------------- _I (a) Xp (t) = - - - / / / / / --r-- / / / ' / / / / / ~ 0 T 2T 0 T 2T t (b) x[n] -4 -3 -2-1 0 1 2 3 4 n -4 -3 -2 -1 0 1 2 3 4 n (c) Figure 7.21 Sampling with a periodic impulse train followed by conversion to a discrete-time sequence: (a) overall system; (b) xp(t) for two sampling rates. The dashed envelope represents Xc(t); (c) the output sequence for the two different sampling rates. To begin let us express Xp(jw ), the continuous-time Fourier transform of Xp(t), in terms of the sample values of Xc(t) by applying the Fourier transform to eq. (7 .3). Since +:xo Xp(t) = ~ Xc(nT)8(t - nT), (7.17) n=-:xo and since the transform of 8(t- nT) is e- jwnT, it follows that +oo Xp(jw) = ~ Xc(nT)e- jwnT. (7.18) n= -oo Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 537 Now consider the discrete-time Fourier transform of xd[n], that is, +oo Xd(eifl) = L xd[n]e- Jfln, (7.19) n= -oo or, using eq. (7.16), +oo Xd(eifl) = L Xc(nT)e- jfln. (7.20) n= -oo Comparing eqs. (7.18) and (7.20), we see that Xd(eifl) and Xp(}w) are related through Xd(eifl) = Xp(}fl/T). (7.21) Also, recall that, as developed in eq. (7.6) and illustrated in Figure 7.3, 1 +oo Xp(}w) = T L Xc(j(w - kws)). (7.22) k= -00 Consequent! y, Xd(eifl) = ~ f Xc(j(fl - 27rk)IT). (7.23) k= -00 The relationship among Xc(}w ), Xp(}w ), and Xd(eifl) is illustrated in Figure 7.22 for two different sampling rates. Here, Xd(eifl) is a frequency-scaled version of Xp(}w) Xc (jw) Xc (jw) i i w w Xp(jw) i T= T1 1\ 1\ _2'1T 0 2'1T w T1 T1 xd (ejn) ~i~ -2'1T 2'1T n Figure 7.22 Relationship between Xc(jw),Xp(jw), and Xd(ejn) for two dif- ferent sampling rates. 538 Sampling Chap. 7 and, in particular, is periodic in !1 with period 27T. This periodicity is, of course, charac- teristic of any discrete-time Fourier transform. The spectrum of xd[n] is related to that of xc(t) through periodic replication, represented by eq. (7.22), followed by linear frequency scaling, represented by eq. (7.21). The periodic replication is a consequence of the first step in the conversion process in Figure 7.21, namely, the impulse-train sampling. The linear frequency scaling in eq. (7.21) can be thought of informally as a consequence of the normalization in time introduced by converting from the impulse train xp(t) to the discrete-time sequence xd[n]. From the time-scaling property of the Fourier transform in Section 4.3.5, scaling of the time axis by liT will introduce a scaling of the frequency axis by T. Thus, the relationship n = w T is consistent with the notion that, in converting from Xp(t) to xd[n], the time axis is scaled by liT. In the overall system of Figure 7.19, after processing with a discrete-time system, the resulting sequence is converted back to a continuous-time signal. This process is the reverse of the steps in Figure 7.21. Specifically, from the sequence Yd[n], a continuous- time impulse train Yp(t) can be generated. Recovery of the continuous-time signal YcU) from this impulse train is then accomplished by means of lowpass filtering, as illustrated in Figure 7.23. 0/C conversion ~------------------------------ 1 I I I Conversion of I Yp (t) discrete-time _, IT Yd[n]~ ... sequence to I I T~ Yc (t) I w w I impulse train - Ws s 2 2 Figure 7.23 Conversion of a I I discrete-time sequence to a continuous- I time signal. Now let us consider the overall system of Figure 7.19, represented as shown in Fig- ure 7.24. Clearly, if the discrete-time system is an identity system (i.e., xd[n] = Yd[n]), then, assuming that the conditions of the sampling theorem are met, the overall system will be an identity system. The characteristics of the overall system with a more general frequency response Hd(ejfl) are perhaps best understood by examining the representative example depicted in Figure 7.25. On the left-hand side of the figure are the representative p(t) l Conversion of xd [n] Yd [n] Conversion of Yp (t) impulse train ....,._......,~ ....,._......,~ sequence to Yc (t) to sequence impulse train 2 2 Figure 7.24 Overall system for filtering a continuous-time signal using a discrete- time filter. (!) (a) (d) XP (jw) Hp (jw), XP (jw) 1\Lh/\ AihA (!) - WM - De 0 De WM (!) T T (b) (e) xd (ein) He (jw), Xc (jw) ~JhL 0 (!) (c) (f) Figure 7.25 Frequency-domain illustration of the system of Figure 7.24: (a) continuous- time spectrum XcUw ); (b) spectrum after impulse-train sampling; (c) spectrum of discrete-time sequence xd[n]; (d) Hd(ei0 ) and Xd(ei0 ) that are multiplied to form Yd(ei0 ); (e) spectra that are multiplied to form Yp{jw}; (f) spectra that are multiplied to form YcUw ). 540 Sampling Chap. 7 spectra Xc(jw ), Xp(jw ), and Xd(ei0 ), where we assume that WM < w.J2, so that there is no aliasing. The spectrum Yd ( ei0 ) corresponding to the output of the discrete-time filter is the product of Xd(ei0 ) and Hd(ei0 ), and this is depicted in Figure 7.25(d) by overlaying Hd(ei0 ) and Xd(e.i0 ). The transformation to Yc(}w) then corresponds to applying a fre- quency scaling and lowpass filtering, resulting in the spectra indicated in Figure 7 .25( e) and (f). Since Yd(e.i0 ) is the product of the two overlaid spectra in Figure 7.25(d), the frequency scaling and lowpass filtering are applied to both. In comparing Figures 7.25(a) and (f), we see that (7.24) Consequently, for inputs that are sufficiently band limited, so that the sampling theorem is satisfied, the overall system of Figure 7.24 is, in fact, equivalent to a continuous-time LTI system wit~ frequency response Hc(jw) which is related to the discrete-time frequency response Hd(e.i0 ) through lwl < Ws/2 lwl (7.25) > w.J2 · The equivalent frequency response for this continuous-time filter is one period of the frequency response of the discrete-time filter with a linear scale change applied to the frequency axis. This relationship between the discrete-time frequency response and the equivalent continuous-time frequency response is illustrated in Figure 7 .26. The equivalence of the overall system of Figure 7.24 to an LTI system is somewhat surprising in view of the fact that multiplication by an impulse train is not a time-invariant operation. In fact, the overall system of Figure 7.24 is not time invariant for arbitrary in- puts. For example, if Xc(t) was a narrow rectangular pulse of duration less than T, then a time shift of Xc(t) could generate a sequence x[n] that either had all zero values or had one nonzero value, depending on the alignment of the rectangular pulse relative to the Figure 7.26 Discrete-time fre- quency response and the equivalent w continuous-time frequency response for the system of Figure 7.24. Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 541 sampling impulse train. However, as suggested by the spectra of Figure 7.25, for band- limited input signals with a sampling rate sufficiently high so as to avoid aliasing, the system of Figure 7.24 is equivalent to a continuous-time LTI system. For such inputs, Figure 7.24 and eq. (7.25) provide the conceptual basis for continuous-time processing using discrete-time filters. This is now explored further in the context of some examples. 7 .4.1 Digital Differentiator Consider the discrete-time implementation of a continuous-time band-limited differenti- ating filter. As discussed in Section 3.9.1, the frequency response of a continuous-time differentiating filter is Hc(jw) = jw, (7.26) and that of a band-limited differentiator with cutoff frequency we is Hc(jw) = { jw, iwi <we (7.27) 0, iwi >we' as sketched in Figure 7.27. Using eq. (7.25) with a sampling frequency Ws = 2wc, we see that the corresponding discrete-time transfer function is (7.28) as sketched in Figure 7.28. With this discrete-time transfer function, Yc(t) in Figure 7.24 will be the derivative of Xc(t), assuming that there is no aliasing in sampling xc(t). I He (jw) I w ~ .......- -.., 2 w 1T Figure 7.27 Frequency response 2 of a continuous-time ideal band-limited differentiator HcUw) = jw, lwl < we. 542 Sampling Chap. 7 1T 2 Figure 7.28 Frequency response n of discrete-time filter used to imple- 1T 2 ment a continuous-time band-limited differentiator. Example 7.2 By considering the output of the digital differentiator for a continuous-time sine input, we may conveniently determine the impulse response hd [n] of the discrete-time filter in the implementation of the digital differentiator. With reference to Figure 7 .24, let sin( 7rt/T) Xc(t) = --- (7.29) 'TTl where T is the sampling period. Then lwl < 7riT otherwise ' which is sufficiently band limited to ensure that sampling Xc(t) at frequency Ws = 27r/T does not give rise to any aliasing. It follows that the output of the digital differentiator is (t) = !!__ x (t) = cos( 7rt/T) _ sin( 7rt/T) y( dt Tt 7rt2 (7.30) c For Xc(t) as given by eq. (7.29), the corresponding signal xd[n] in Figure 7.24 may be expressed as (7.31) That is, for n =F 0, Xc(nT) = 0, while 1 Xd [0] = Xc(O) = T which can be verified by l'H6pital's rule. We can similarly evaluate Yd[n] in Figure 7.24 corresponding to Yc(t) in eq. (7.30). Specifically n=FO (7.32) n = 0 Sec. 7.4 Discrete-Time Processing of Continuous-Time Signals 543 which can be verified for n =P 0 by direct substitution into eq. (7.30) and for n = 0 by application of l'H6pital's rule. Thus when the input to the discrete-time filter given by eq. (7.28) is the scaled unit impulse in eq. (7.31), the resulting output is given by eq. (7.32). We then conclude that the impulse response of this filter is given by ( -1)11 hd [n] = ~' n =P 0 { 0, n = 0 7 .4.2 Half-Sample Delay In this section, we consider the implementation of a time shift (delay) of a continuous-time signal through the use of a system in the form of Figure 7.19. Thus, we require that the input and output of the overall system be related by Yc(t) = Xc(t- d) (7.33) when the input Xc(t) is band limited and the sampling rate is high enough to avoid alias- ing and where d represents the delay time. From the time-shifting property derived in Section 4.3.2 Y c(jw) = e- jwb. Xc(jW ). From eq. (7.25), the equivalent continuous-time system to be implemented must be band limited. Therefore, we take - jwb. Hc(jw) = e ' lwl <we (7.34) { 0, otherwise' where We is the cutoff frequency of the continuous-time filter. That is, Hc(jw) corresponds to a time shift as in eq. (7.33) for band-limited signals and rejects all frequencies greater than We. The magnitude and phase of the frequency response are shown in Figure 7.29(a). With the sampling frequency w.1· taken as w.1· = 2wc, the corresponding discrete-time I He(jw) I I Hd(eifl) I 11 1 I I I -we We w -1T 1T n < Hd(ein) __...-o1 ~T ar ~ ~~~~ n (a) (b) Figure 7.29 (a) Magnitude and phase of the frequency response for a continuous-time delay; (b) magnitude and phase of the frequency response for the corresponding discrete-time delay. 544 Sampling Chap. 7 frequency response is (7.35) and is shown in Figure 7.29(b). For appropriately band-limited inputs, the output of the system of Figure 7.24 with Hd(ejf!) as in eq. (7.35) is a delayed replica of the input. For il.IT an integer, the sequence y d [ n] is a delayed replica of xd [ n]; that is, y d [ n] = xd [ n - ~ l· (7 .36) For il.!T not an integer, eq. (7.36), as written, has no meaning, since sequences are defined only at integer values of the index. However, we can interpret the relationship between xd[n] and Yd[n] in these cases in terms of band-limited interpolation. The signals Xc(t) and xd[n] are related through sampling and band-limited interpolation, as are Yc(t) and Yd[n]. With Hd(ejf!) in eq. (7.35), Yd[n] is equal to samples of a shifted version of the band-limited interpolation of the sequence xd[n]. This is illustrated in Figure 7.30 with il./T = 1/2, which is sometimes referred to as a half-sample delay. 0 T 2T (a) ,f'I--r,'f--I,,r-I,,id~""~ = Yc(nT) = '""''""-~)T] Figure 7.30 (a) Sequence of sam- 0 T 2T ples of a continuous-time signal Xc(t); (b) sequence in (a) with a half-sample (b) delay. Example 7.3 The approach in Example 7.2 is also applicable to determining the impulse response hd[n] of the discrete-time filter in the half-sample delay system. With reference to Fig- ure 7.24, let sin('TT't/T) Xc(t) = --- (7.37) 'TT't It follows from Example 7.2 that 1 xd[n] = Xc(nT) = T8[n]. Sec. 7.5 Sampling of Discrete-Time Signals 545 Also, since there is no aliasing for the band-limited input in eq. (7 .37), the output of the half-sample delay system is Yc(t) = Xc(t- T/2) = sin( 7r(t- T/2)/T) 1r(t- T/2) and the sequence Yd[n] in Figure 7.24 is sin(1r(n- ~)) Yc~[n] = Yc(nT) = T1r(n- 1 • 2) We conclude that sin( 1r(n - ~)) h[n] = 7T(n- 21 • ) 7.5 SAMPLING OF DISCRETE-TIME SIGNALS Thus far in this chapter, we have considered the sampling of continuous-time signals, and in addition to developing the analysis necessary to understand continuous-time sampling, we have introduced a number of its applications. As we will see in this section, a very sim- ilar set of properties and results with a number of important applications can be developed for sampling of discrete-time signals. 7.5.1 Impulse-Train Sampling In analogy with continuous-time sampling as carried out using the system of Figure 7.2, sampling of a discrete-time signal can be represented as shown in Figure 7 .31. Here, the new sequence x P [ n] resulting from the sampling process is equal to the original sequence x[n] at integer multiples of the sampling period Nand is zero at the intermediate samples; that is, [ ] = { x[n], if n = an integer multiple of N Xp n O, (7.38) otherwise As with continuous-time sampling in Section 7.1, the effect in the frequency do- main of discrete-time sampling is seen by using the multiplication property developed in Section 5.5. Thus, with +oc Xp[n] = x[n]p[n] L x[kN]8[n- kN], (7.39) k= -oc we have, in the frequency domain, (7.40) 546 Sampling Chap. 7 x[n] --+-~ X J---~ xp[n] +oo p[n] = ~ o [n - kN] k=-00 x[n] tiiiiiiiiiiiiiiiiii n p[n: . .I .. I • • I • • I.. I • • I.. I • • n I xp[n] • • I • • • • I • • I • • I.. I• • I. Figure 7.31 Discrete-time n sampling. As in Example 5.6, the Fourier transform of the sampling sequence p[n] is 2 +oo P(el.w ) = N7 TL"" ""' 8(w- kws), (7.41) k= -00 where Ws, the sampling frequency, equals 27T/N. Combining eqs. (7.40) and (7.41), we have (7.42) Equation (7 .42) is the counterpart for discrete-time sampling of eq. (7 .6) for continuous-time sampling and is illustrated in Figure 7 .32. In Figure 7 .32( c), with Ws- WM > WM, or equivalently, Ws > 2wM, there is no aliasing [i.e., the nonzero portions of the replicas of X(ejw) do not overlap], whereas with Ws < 2wM, as in Figure 7.32(d), frequency-domain aliasing results. In the absence of aliasing, X(ejw) is faithfully repro- duced around w = 0 and integer multiples of 27T. Consequently, x[n] can be recovered from xp[n] by means of a lowpass filter with gain Nand a cutoff frequency greater than Sec. 7.5 Sampling of Discrete-Time Signals 547 X(eiw) zL -wM 0 WM (a) P(eiw) t t t 2~ 1 t t t ws (b) Xp(eiw) ~~~~~6~ - WM WM """" Ws (c) (w5 -wM) (d) Figure 7.32 Effect in the frequency domain of impulse-train sampling of a discrete-time signal: (a) spectrum of original signal; (b) spectrum of sampling sequence; (c) spectrum of sampled signal with w 5 > 2wM; (d) spectrum of sampled signal with w 5 < 2wM. Note that aliasing occurs. WM and less than w 5 - WM, as illustrated in Figure 7.33, where we have specified the cutoff frequency of the lowpass filter as wsf2. If the overall system of Figure 7.33(a) is ap- plied to a sequence for which Ws < 2wM, so that aliasing results, Xr[n] will no longer be equal to x[n]. However, as with continuous-time sampling, the two sequences will be equal at multiples of the sampling period; that is, corresponding to eq. (7 .13 ), we have Xr[kN] = x[kN], k = 0, :±: 1, :±:2, ... , (7.43) independently of whether aliasing occurs. (See Problem 7.46.) 548 Sampling Chap. 7 p[n] x[n] .~ xP[n ] • EH(jw)J --Xr[n] (a) X(jw) ~ ~ ~ -21T -wM WM 21T w XP(jw) (0 ,A.. /\w?hM(> ,A..~ 21T (b) Figure 7.33 Exact recovery of a discrete-time signal from its samples us- ing an ideal lowpass filter: (a) block diagram for sampling and reconstruction of a band-limited signal from its samples; (b) spectrum of the signal x[n]; (c) spectrum of Xp[n]; (d) frequency response of an ideal lowpass filter with cutoff frequency ws/2; (e) spectrum of the reconstructed signal x,[n]. For the example depicted here w5 > 2wM so that no aliasing occurs and consequently x,[n] = x[n]. Example 7.4 Consider a sequence x[n] whose Fourier transform X(eiw) has the property that X(eiw) = 0 for 27T/9 ~ iwi ~ 7T. Sec. 7.5 Sampling of Discrete-Time Signals 549 To determine the lowest rate at which x[n] may be sampled without the possibility of aliasing, we must find the largest N such that N21T (27T) 2 2 9 ==? N =:::; 9/2. We conclude that Nmax = 4, and the corresponding sampling frequency is 2n/4 = 11""12. The reconstruction of x[n] through the use of a lowpass filter applied to xp[n] can be interpreted in the time domain as an interpolation formula similar to eq. (7.11). With h[n] denoting the impulse response of the lowpass filter, we have (7.44) The reconstructed sequence is then Xr[n] = Xp[n] * h[n], (7.45) or equivalently, Xr[n] = f x[kN]Nwc sinwc(n- kN)_ (7.46) k= -oc 7T Wc(n- kN) Equation (7.46) represents ideal band-limited interpolation and requires the implemen- tation of an ideal lowpass filter. In typical applications a suitable approximation for the lowpass filter in Figure 7.33 is used, in which case the equivalent interpolation formula is of the form +co Xr[n] = L x[kN]hr[n- kN], (7.47) k= -oc where hr [ n] is the impulse response of the interpolating filter. Some specific examples, in- eluding the discrete-time counterparts of the zero-order hold and first-order hold discussed in Section 7.2 for continuous-time interpolation, are considered in Problem 7.50. 7.5.2 Discrete-Time Decimation and Interpolation There are a variety of important applications of the principles of discrete-time sampling, such as in filter design and implementation or in communication applications. In many of these applications it is inefficient to represent, transmit, or store the sampled sequence xp[n] directly in the form depicted in Figure 7.31, since, in between the sampling instants, xp[n] is known to be zero. Thus, the sampled sequence is typically replaced by a new sequence Xb[n], which is simply every Nth value of xp[n]; that is, (7.48) Also, equivalently, Xb[n] = x[nN], (7 .49) 550 Sampling Chap. 7 since x P [ n] and x[ n] are equal at integer multiples of N. The operation of extracting every Nth sample is commonly referred to as decimation. 3 The relationship between x[n], xp[n], and xb[n] is illustrated in Figure 7.34. To determine the effect in the frequency domain of decimation, we wish to determine the relationship between Xb(eiw)-the Fourier transform of xb[n]-and X(eiw). To this end, we note that +oc Xb(eiw) = L Xb[k]e- jwk, (7.50) k= -00 or, using eq. (7.48), +oo Xb(eiw) = L Xp[kN]e- }wk. (7.51) k= -00 If we let n = kN, or equivalently k = n/N, we can write n=integer multiple of N and since Xp[n] = 0 when n is not an integer multiple of N, we can also write +oo Xb(eiw) = L Xp[n]e- jwn!N. (7.52) n= -oo x[n] tl IIIIJJIII Jlllll It 0 n ~~ . I. .~1 • • r• • I ..1.. l~ .. 1• • 0 n tlnllr Figure 7.34 Relationship between Xp[n] corresponding to sampling and 0 n xb[n] corresponding to decimation. 3Technically, decimation would correspond to extracting every tenth sample. However, it has become common terminology to refer to the operation as decimation even when N is not equal to 10. Sec. 7.5 Sampling of Discrete-Time Signals 551 Furthermore, we recognize the right-hand side of eq. (7.52) as the Fourier transform of Xp[n]; that is, L+x Xp[n]e- jwn/N = Xp(eiw!N). (7.53) n= -oo Thus, from eqs. (7.52) and (7.53), we conclude that Xb(eiw) = Xp(ejw/N). (7.54) This relationship is illustrated in Figure 7 .35, and from it, we observe that the spectra for the sampled sequence and the decimated sequence differ only in a frequency scaling or normalization. If the original spectrum X(eiw) is appropriately band limited, so that there is no aliasing present in Xp(eiw), then, as shown in the figure, the effect of decimation is to spread the spectrum of the original sequence over a larger portion of the frequency band. 1 7T 27T w 7T 27T w Xb(eiw) ~~~ Figure 7.35 Frequency-domain illustration of the relationship between sampling and decimation. If the original sequence x[n] is obtained by sampling a continuous-time signal, the process of decimation can be viewed as reducing the sampling rate on the signal by a factor of N. To avoid aliasing, X(eiw) cannot occupy the full frequency band. In other words, if the signal can be decimated without introducing aliasing, then the original continuous- time signal was oversampled, and thus, the sampling rate can be reduced without aliasing. With the interpretation of the sequence x[n] as samples of a continuous-time signal, the process of decimation is often referred to as downsampling. 552 Sampling Chap. 7 C/D xd[n] Discrete time conversion lowpass filter Hd(eiw) Xe(jw) Lh -wM WM w Xd(eiw) ~ -21T w 1T 21T w Hd(eiw) Ib Figure 7.36 Continuous-time sig- D D nal that was originally sampled at the Nyquist rate. After discrete-time fil- tering, the resulting sequence can be -21T2 -we We 21T w further downsampled. Here Xc(jw) Yd(eiw) is the continuous-time Fourier trans- form of Xc(t), Xd(eiw) and Yd(eiw) are ch the discrete-time Fourier transforms Q Q of xd[n] and Yd[n] respectively, and Hd ( eiw) is the frequency response of the discrete-time lowpass filter de- -21T -we We 21T w picted in the block diagram. In some applications in which a sequence is obtained by sampling a continuous- time signal, the original sampling rate may be as low as possible without introducing aliasing, but after additional processing and filtering, the bandwidth of the sequence may be reduced. An example of such a situation is shown in Figure 7.36. Since the output of the discrete-time filter is band limited, downsampling or decimation can be applied. Just as in some applications it is useful to downsample, there are situations in which it is useful to convert a sequence to a higher equivalent sampling rate, a process referred to as upsampling or interpolation. Upsampling is basically the reverse of decimation or downsampling. As illustrated in Figures 7.34 and 7 .35, in decimation we first sample and then retain only the sequence values at the sampling instants. To upsample, we reverse the process. For example, referring to Figure 7 .34, we consider upsampling the sequence xb[n] to obtain x[n]. From xb[n], we form the sequence xp[n] by inserting N - 1 points with zero amplitude between each of the values in xb[n]. The interpolated sequence x[n] is then obtained from xp[n] by lowpass filtering. The overall procedure is summarized in Figure 7.37. Conversion of Ideal lowpass decimated sequence xb[n] filter -----+- x[n] to sampled H(ejw) sequence (a) xb[n] Xb(ejw) ~ & / n -2'1T 2'1T w xp[n] n -2TI -'IT 'IT 'IT '1T 2'1T w 2 2 x[n] X(ejw) \ I i- t\ I L n -2TI '1T 2'1T w Figure 7.37 Upsampling: (a) overall system; (b) associated sequences and spectra for upsampling by a factor of 2. U1 U1 1.\1 554 Sampling Chap. 7 Example 7.5 In this example, we illustrate how a combination of interpolation and decimation may be used to further downsample a sequence without incurring aliasing. It should be noted that maximum possible downsampling is achieved once the non -zero portion of one period of the discrete-time spectrum has expanded to fill the entire band from -7r to 1T. Consider the sequence x[n] whose Fourier transform X(eiw) is illustrated in Figure 7.38(a). As discussed in Example 7.4, the lowest rate at which impulse-train sampling may be used on this sequence without incurring aliasing is 27T/4. This corresponds to 27r 0 27r 'IT w -9 9 (a) ,/'... -271"" 1071"" -'IT 87r 0 87r -9 -9 9 (b) w ub(eJw) .. I I I I .. -271"" -'IT 0 'IT 27r w (d) Figure 7.38 Spectra associated with Example 7.5. (a) Spectrum of x[n]; (b) spectrum after downsampling by 4; (c) spectrum after upsampling x[n] by a factor of 2; (d) spectrum after upsampling x[n] by 2 and then downsampling by 9. Sec. 7.6 Summary 555 sampling every 4th value of x[ n]. If the result of such sampling is decimated by a factor of 4, we obtain a sequence xb[n] whose spectrum is shown in Figure 7.38(b). Clearly, there is still no aliasing of the original spectrum. However, this spectrum is zero for 87r/9 :s: Jw J :s: Tr, which suggests there is room for further downsampling. ' Specifically, examining Figure 7 .38(a) we see that if we could scale frequency by a factor of 9/2, the resulting spectrum would have nonzero values over the entire frequency interval from -7r to Tr. However, since 9/2 is not an integer, we can't achieve this purely by downsampling. Rather we must first upsample x[n] by a factor of 2 and then downsample by a factor of 9. In particular, the spectrum of the signal xu [ n] obtained when x[n] is upsampled by a factor of 2, is displayed in Figure 7.38(c). When xu[n] is then downsampled by a factor of 9, the spectrum of the resulting sequence Xub[n] is as shown in Figure 7.38(d). This combined result effectively corresponds to downsampling x[n] by a noninteger amount, 9/2. Assuming that x[n] represents unaliased samples of a continuous-time signal Xc(t), our interpolated and decimated sequence represents the maximum possible (aliasing-free) downsampling of Xc(t). 7.6 SUMMARY In this chapter we have developed the concept of sampling, whereby a continuous-time or discrete-time signal is represented by a sequence of equally spaced samples. The con- ditions under which the signal is exactly recoverable from the samples is embodied in the sampling theorem. For exact reconstruction, this theorem requires that the signal to be sampled be band limited and that the sampling frequency be greater than twice the high- est frequency in the signal to be sampled. Under these conditions, exact reconstruction of the original signal is carried out by means of ideal lowpass filtering. The time-domain interpretation of this ideal reconstruction procedure is often referred to as ideal band- limited interpolation. In practical implementations, the lowpass filter is approximated and the interpolation in the time domain is no longer exact. In some instances, simple inter- polation procedures such as a zero-order hold or linear interpolation (a first-order hold) suffice. If a signal is undersampled (i.e., if the sampling frequency is less than that required by the sampling theorem), then the signal reconstructed by ideal band-limited interpolation will be related to the original signal through a form of distortion referred to as aliasing. In many instances, it is important to choose the sampling rate so as to avoid aliasing. However, there are a variety of important examples, such as the stroboscope, in which aliasing is exploited. Sampling has a number of important applications. One particularly significant set of applications relates to using sampling to process continuous-time signals with discrete- time systems, by means of minicomputers, microprocessors, or any of a variety of devices specifically oriented toward discrete-time signal processing. The basic theory of sampling is similar for both continuous-time and discrete- time signals. In the discrete-time case there is the closely related concept of decimation, whereby the decimated sequence is obtained by extracting values of the original sequence at equally spaced intervals. The difference between sampling and decimation lies in the fact that, for the sampled sequence, values of zero lie in between the sample values, whereas in the decimated sequence these zero values are discarded, thereby compressing the sequence in time. The inverse of decimation is interpolation. The ideas of decima- 556 Sampling Chap. 7 tion and interpolation arise in a variety of important practical applications of signals and systems, including communication systems, digital audio, high-definition television, and many other applications. Chapter 1 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 7.1. A real-valued signal x(t) is known to be uniquely determined by its samples when the sampling frequency is Ws = 10,000'77'. For what values of w is X(jw) guaranteed to be zero? 7.2. A continuous-time signal x(t) is obtained at the output of an ideal lowpass filter with cutoff frequency We = 1,000'77'. If impulse-train sampling is performed on x(t), which of the following sampling periods would guarantee that x(t) can be recovered from its sampled version using an appropriate lowpass filter? (a) T = 0.5 x 10-3 (b) T = 2 X 1o -3 (c) T = 10-4 7 .3. The frequency which, under the sampling theorem, must be exceeded by the sam- pling frequency is called the Nyquist rate. Determine the Nyquist rate corresponding to each of the following signals: (a) x(t) = 1 + cos(2,000'7Tt) + sin(4,000'7Tt) (b) x(t) = sin(4,0007Tt) 7Tt (c) x(t) = ( sin(4,~~0m) t 7.4. Let x(t) be a signal with Nyquist rate w 0 . Determine the Nyquist rate for each of the following signals: (a) x(t) + x(t - 1) (b) d~;t) (c) x2(t) (d) x(t) cos wot 7.5. Let x(t) be a signal with Nyquist rate w 0 . Also, let y(t) = x(t)p(t- 1), Chap. 7 Problems 557 where ~ 2 p(t) = L o(t- nT), and T < ____!!__ n=-~ WQ Specify the constraints on the magnitude and phase of the frequency response of a filter that gives x(t) as its output when y(t) is the input. 7.6. In the system shown in Figure P7.6, two functions of time, XI (t) and x2(t), are mul- tiplied together, and the product w(t) is sampled by a periodic impulse train. XI (t) is band limited tow 1, and x2(t) is band limited to w2 ; that is, XI(jw) = 0, lwl ~WI, X2(jw) = 0, lwl ~ w2. Determine the maximum sampling interval T such that w(t) is recoverable from wp(t) through the use of an ideallowpass filter. p(t) = ~ o(t -nT) x1(t)---p-.X_ _w_ (t)~-~~o~-x • Wp(l) x2(t) - X1(jw) ch Figure P7.6 7.7. A signal x(t) undergoes a zero-order hold operation with an effective sampling pe- riod T to produce a signal x0(t). Let XI (t) denote the result of a first-order hold operation on the samples of x(t); i.e., XI (t) = L x(nT)hi (t- nT), n= -oo where hi (t) is the function shown in Figure P7.7. Specify the frequency response of a filter that produces x 1 (t) as its output when x0(t) is the input. 558 Sampling Chap. 7 -T 0 T Figure P7.7 7.8. Consider a real, odd, and periodic signal x(t) whose Fourier series representation may be expressed as x(t) = ~5 (1 )k 2 sin(k1rt). Let x(t) represent the signal obtained by performing impulse-train sampling on x(t) using a sampling period of T = 0. 2. (a) Does aliasing occur when this impulse-train sampling is performed on x(t)? (b) If x(t) is passed through an ideallowpass filter with cutoff frequency 1riT and passband gain T, determine the Fourier series representation of the output signal g(t). 7.9. Consider the signal which we wish to sample with a sampling frequency of Ws = 1507T to obtain a signal g(t) with Fourier transform G(jw ). Determine the maximum value of w0 for which it is guaranteed that G(jw) = 75X(jw) for lwl ::s wo, where X(jw) is the Fourier transform of x(t). 7.10. Determine whether each of the following statements is true or false: (a) The signal x(t) = u(t + T0 ) - u(t- T0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 2T0 . (b) The signal x(t) with Fourier transform X(jw) = u(w + w0)- u(w- w0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 7Tiwo. (c) The signal x(t) with Fourier transform X(jw) = u(w)- u(w- w0 ) can undergo impulse-train sampling without aliasing, provided that the sampling period T < 27Tiwo. 7.11. Let Xc(t) be a continuous-time signal whose Fourier transform has the property that Xc(jw) = 0 for lwl ~ 2,0001T. A discrete-time signal xd[n] = Xc(n(0.5 X 10-3)) Chap. 7 Problems 559 is obtained. For each of the following constraints on the Fourier transform Xd(ejw) of xd[n], determine the corresponding constraint on Xc(jw ): (a) Xd(ejw) is real. (b) The maximum value of Xd(ejw) over all w is 1. (c) Xd(ejw) = 0 for 3 ; ::; lw I ::; 1T. (d) Xd(ejw) = Xd(ej(w-1T)). 7.12. A discrete-time signal xd[n] has a Fourier transform Xd(ejw) with the property that Xd(ejw) = 0 for 37T/4 ::; lwl ::; 1T. The signal is converted into a continuous-time signal Loo sin ¥U - nT) Xc(t) = T xd[n] 1T(t _ nT) , n= -oo where T = 1o -3 . Determine the values of w for which the Fourier transform Xc(jw) of xc(t) is guaranteed to be zero. 7.13. With reference to the filtering approach illustrated in Figure 7.24, assume that the sampling period used is T and the input Xc(t) is band limited, so that Xc(jw) = 0 for lwl 2:: 1TIT. If the overall system has the property that Yc(t) = xc(t-2T), determine the impulse response h[n] of the discrete-time filter in Figure 7.24. 7.14. Repeat the previous problem, except this time assume that 7.15. Impulse-train sampling of x[n] is used to obtain g[n] = L x[n]S[n- kN]. k= -00 If X(ejw) = 0 for 37T/ 7 ::; lwl ::; 1T, determine the largest value for the sampling interval N which ensures that no aliasing takes place while sampling x[n]. 7.16. The following facts are given about the signal x[n] and its Fourier transform: 1. x[n] is real. 2. X(ejw) -:t= 0 for 0 < w < 1T. 3. x[n]L~=-oo8[n- 2k] = S[n]. Determine x[n]. You may find it useful to note that the signal (sin ~n)/( 1Tn) satisfies two of these conditions. 560 Sampling Chap. 7 7.17. Consider an ideal discrete-time bandstop filter with impulse response h[ n] for which the frequency response in the interval -7r ::; w ::; 1T is 3 lwl ::; *and lwl ~ ; . elsewhere Determine the frequency response of the filter whose impulse response is h[2n]. 7 .18. Suppose the impulse response of an ideal discrete-time lowpass filter with cutoff fre- quency 1r12 is interpolated (in accordance with Figure 7 .37) to obtain an upsampling by a factor of 2. What is the frequency response corresponding to this upsampled impulse response? 7.19. Consider the system shown in Figure P7.19, with input x[n] and the correspond- ing output y[n]. The zero-insertion system inserts two points with zero amplitude between each of the sequence values in x[n]. The decimation is defined by y[n] = w[5n], where w[n] is the input sequence for the decimation system. If the input is of the form sinw1n x[n] = --- 1Tn determine the output y[n] for the following values of w1: (a) 3 WI ::; ; (b) WI > 3 ; H(eiw) - - 1 w[n] Zero insertion Decimation x[n] y[n] -7T -7T/5 7T/5 7T Figure P7. 1 9 7 .20. Two discrete-time systems S I and S2 are proposed for implementing an ideal low- pass filter with cutoff frequency 7T/4. System S1 is depicted in Figure P7.20(a). System S2 is depicted in Figure P7 .20(b ). In these figures, SA corresponds to a zero- insertion system that inserts one zero after every input sample, while S 8 corresponds to a decimation system that extracts every second sample of its input. (a) Does the proposed system S1 correspond to the desired ideallowpass filter? (b) Does the proposed system S2 correspond to the desired ideallowpass filter? Chap. 7 Problems 561 -·~l._ __ ill sA_ _ ill Ss -x[n-] I y[n] -'IT/8 0 'IT/8 (a) -x[n-]-•~1._ __ss_ _ • ]1 SA I ]1 y[n) -'IT/2 0 'IT/2 -'IT/2 0 'IT/2 (b) Figure P7.20 BASIC PROBLEMS 7.21. A signal x(t) with Fourier transform X(jw) undergoes impulse-train sampling to generate <X) X p(t) = L x(nT) o(t - nT) n= -oo where T = 10-4 • For each of the following sets of constraints on x(t) and/or X(jw ), does the sampling theorem (see Section 7.1.1) guarantee that x(t) can be recovered exactly from x p(t)? (a) X(jw) = 0 for lwl > 50007T (b) X(jw) = 0 for lwl > 150007T (c) (Jl.e{X(jw)} = 0 for lwl > 50007T (d) x(t) real and X(jw) = 0 for w > 50007T (e) x(t) real and X(jw) = 0 for w < -150007T (f) X(jw) * X(jw) = 0 for lwl > 150007T (g) IX(jw )I = 0 for w > 50007T 7.22. The signal y(t) is generated by convolving a band-limited signal XI (t) with another band-limited signal x2(t), that is, y(t) = XI (t) * X2(t) where X1(jw) = 0 for lw I > 10 007T X2(jw) = 0 for lw I > 20007T. Impulse-train sampling is performed on y(t) to obtain 562 Sampling Chap. 7 +oo Yp(t) = L y(nT)o(t - nT). n= -oo Specify the range of values for the sampling period T which ensures that y(t) is recoverable from Yp(t). 7.23. Shown in Figure P7.23 is a system in which the sampling signal is an impulse train with alternating sign. The Fourier transform of the input signal is as indicated in the figure. (a) Ford< 7rl(2wM), sketch the Fourier transform of Xp(t) and y(t). (b) Ford< 7rl(2wM), determine a system that will recover x(t) from Xp(t). (c) Ford< 7rl(2wM ), determine a system that will recover x(t) from y(t). (d) What is the maximum value of din relation to WM for which x(t) can be recov- ered from either xp(t) or y(t)? p(t) x(t)--,.._1 p(t) ... t 1 i Ll t l _J 2Ll l t X(jw) ~ H(jw) 1 D t D 3'TT (I) T Figure P7.23 7 .24. Shown in Figure P7 .24 is a system in which the input signal is multiplied by a periodic square wave. The period of s(t) is T. The input signal is band limited with IX(jw)l = 0 for lwl ;:::: WM. Chap. 7 Problems 563 (a) For~ = T/3, determine, in terms of WM, the maximum value ofT for which there is no aliasing among the replicas of X(jw) in W(jw ). (b) For ~ = T 14, determine, in terms of w M, the maximum value of T for which there is no aliasing among the replicas of X(jw) in W(jw ). x(t)---~~~ w(t) t s(t) Figure P7 .24 7 .25. In Figure P7 .25 is a sampler, followed by an ideallowpass filter, for reconstruction of x(t) from its samples x p(t). From the sampling theorem, we know that if w s = 27TIT is greater than twice the highest frequency present in x(t) and We = wsf2, then the reconstructed signal Xr(t) will exactly equal x(t). If this condition on the bandwidth of x(t) is violated, then Xr(t) will not equal x(t). We seek to show in this problem that if We = wsf2, then for any choice ofT, Xr(t) and x(t) will always be equal at the sampling instants; that is, Xr(kT) = x(kT), k = 0, ± 1, ±2, .... +oo p(t) = l o(t -nT) n = -oo H(jw) ill 1----~ Xr (t) Figure P7.25 To obtain this result, consider eq. (7.11), which expresses Xr(t) in terms of the samples of x(t): Xr (t ) = ~ ( T)TWe sin[we(t- nT)] L X n ( T) . n=-oo 7T Wet-n With We = wsf2, this becomes oo sin [ f (I - nT)] Xr(t) = L x(nT) 7T • (P7.25-l) n= -oo T(t- nT) 564 Sampling Chap. 7 By considering the values of a for which [sin(a)]/a = 0, show from eq. (P7.25-l) that, without any restrictions on x(t), Xr(kT) = x(kT) for any integer value of k. 7.26. The sampling theorem, as we have derived it, states that a signal x(t) must be sam- pled at a rate greater than its bandwidth (or equivalently, a rate greater than twice its highest frequency). This implies that if x(t) has a spectrum as indicated in Figure P7.26(a) then x(t) must be sampled at a rate greater than 2w2• However, since the signal has most of its energy concentrated in a narrow band, it would seem reason- able to expect that a sampling rate lower than twice the highest frequency could be used. A signal whose energy is concentrated in a frequency band is often referred to as a bandpass signal. There are a variety of techniques for sampling such signals, generally referred to as bandpass-sampling techniques. X(jw) 1t w (a) +co x(t) ----:~I~~:~~T) ·1,.....---H-(J-.W-) ___,~ x,(t) 1 1 1 1 T H(jw) At (b) Figure P7 .26 Chap. 7 Problems 565 To examine the possibility of sampling a bandpass signal as a rate less than the total bandwidth, consider the system shown in Figure P7.26(b). Assuming that w 1 > w2 - w 1, find the maximum value ofT and the values of the constants A, wa, and wb such that Xr(t) = x(t). 7.27. In Problem 7 .26, we considered one procedure for bandpass sampling and recon- struction. Another procedure, used when x(t) is real, consists of multiplying x(t) by a complex -exponential and then sampling the product. The sampling system is shown in Figure P7.27(a). With x(t) real and with X(jw) nonzero only for w 1 < lwl < w2, the frequency is chosen to be w0 = (112)(w 1 + w2), and the lowpass filter H 1(jw) has cutoff frequency (112)(w2 - w 1) . (a) For X(jw) as shown in Figure P7.27(b), sketch Xp(jw ). (b) Determine the maximum sampling period T such that x(t) is recoverable from Xp(t). (c) Determine a system to recover x(t) from xp(t). x(t) -----+-@ ·I H(jw) t e-iwot +70 p(t) = ~ 8(t-nT) n = -x (a) X(jw) L1 1t -w2 -w1 (b) Figure P7.27 7.28. Figure P7.28(a) shows a system that converts a continuous-time signal to a discrete- time signal. The input x(t) is periodic with a period of 0.1 second. The Fourier series coefficients of x(t) are 1 ak = ( ""2 Jkl , -oo < k < +oo. The lowpass filter H(jw) has the frequency response shown in Figure P7.28(b). The sampling period T = 5 x 10-3 second. (a) Show that x[n] is a periodic sequence, and determine its period. (b) Determine the Fourier series coefficients of x[n]. 566 Sampling Chap. 7 Conversion Lowpass Xc(t) of an x(t) filter X impulse train x[n] = xc (nT) H(jw) to a sequence p(t) = I 8(t -nT) n = -oo (a) H(jw) 11 I -205'7T 205'7T w (b) Figure P7.28 7 .29. Figure P7 .29( a) shows the overall system for filtering a continuous-time signal using a discrete-time filter. If Xc(jw) and H(eiw) are as shown in Figure P7.29(b), with liT = 20kHz, sketch Xp(jw ), X(eiw), Y(eiw), Yp(jw ), and Yc(jw ). H(jw) Xc (t) Xp (t) Conversion x[n] = Xc (nT) y[n] = Yc (nT) h [n] Conversion Yp (t) Yc (t) X to a H(eiw) to an ~ ~ sequence impulse train m -'lTIT 1T/T p(t) =I 8(t-nT) n = -oo (a) Xc(jw) H(eiw) A 1 I I -'lT X104 '7T X104 w -41T 1T 4 w (b) Figure P7 .29 Chap. 7 Problems 567 7.30. Figure P7.30 shows a system consisting of a continuous-time LTI system followed by a sampler, conversion to a sequence, and an LTI discrete-time system. The continuous-time LTI system is causal and satisfies the linear, constant-coefficient differential equation dyc(t) d[ + Yc(t) = Xc(t). The input Xc(t) is a unit impulse o(t). (a) Determine Yc(t). (b) Determine the frequency response H(ejw) and the impulse response h[n] such that w[n] = o[n]. t LTI y, ( ) , r- Conversion of impu;~~ y[n] train I '--------J- l w[n] sequence p(t) = !,o(t-nT) y[n] = Yc (nT) n = -x Figure P7.30 7 .31. Shown in Figure P7 .31 is a system that processes continuous-time signals using a digital filter h[n] that is linear and causal with difference equation 1 y[n] = 2y [n - 1] + x[n]. For input signals that are band limited such that Xc(jw) = 0 for lw I > niT, the system in the figure is equivalent to a continuous-time LTI system. Determine the frequency response Hc(jw) of the equivalent overall system with input xc(t) and output Yc(t). (t) Conversion of x[n] y[n] Conversion of y(t) Ideal lowpass x (t) ~~xP impulse train h[n] sequence filter cutoff c to a to a frenquency sequence impulse train 1r/T p(t) =!, o(t-nT) n = -x. Hgute ~1.3' 7.32. A signal x[n] has a Fourier transform X(ejw) that is zero for ( 7T/4) ~ lwl ~ 7T. Another signal 568 Sampling Chap. 7 g[n] = x[n] L, o[n- 1 - 4k] k=-% is generated. Specify the frequency response H(e.iw) of a lowpass filter that produces x[n] as output when g[n] is the input. 7.33. A signal x[n] with Fourier transform X(e.iw) has the property that G[n] '~% ll[n- 3k] )• (si*!n) = x[n]. For what values of w is it guaranteed that X ( e.iw) = 0? 7.34. A real-valued discrete-time signal x[n] has a Fourier transform X(e.iw) that is zero for 31TI14 ~ lwl ~ 1T. The nonzero portion of the Fourier transform of one period of X(e.iw) can be made to occupy the region lwl < 1T by first performing upsampling by a factor of L and then performing downsampling by a factor of M. Specify the values of L and M. 7.35. Consider a discrete-time sequence x[n] from which we form two new sequences, xp[n] and xd[n], where Xp[n] corresponds to sampling x[n] with a sampling period of 2 and xd[n] corresponds to decimating x[n] by a factor of 2, so that Xp[n] = { x,[n], n = 0, ±2, ±4, .. . 0 n = ±1, ±3, .. . and xd [n] = x[2n]. (a) If x[n] is as illustrated in Figure P7.35(a), sketch the sequences Xp[n] and xd[n]. (b) If X(e.iw) is as shown in Figure P7.35(b), sketch Xp(e.iw) and Xd(e.iw). • • ' l I I II I I I I II I t~l . 0 n (a) / 0 37T 57T 117T w 4 4 4 (b) Figure P7. 3 5 ADVANCED PROBLEMS 7.36 Letx(t)beaband-limitedsignalsuchthatX(jw) = Oforlwl2 ¥· (a) If x(t) is sampled using a sampling period T, determine an interpolating function Chap. 7 Problems 569 g(t) such that dx(t) L x(nT)g(t - nT). dt n=-x (b) Is the function g(t) unique? 7.37. A signal limited in bandwidth to lw I < W can be recovered from nonuniformly spaced samples as long as the average sample density is 2(W/27T) samples per sec- ond. This problem illustrates a particular example of nonuniform sampling. Assume that in Figure P7.37(a): 1. x(t) is band limited; X(jw) = 0, lwl > W. 2. p(t) is a nonuniformly spaced periodic pulse train, as shown in Figure P7.37(b). 3. f(t) is a periodic waveform with period T = 27TIW. Since f(t) multiplies an impulse train, only its values f(O) = a and f(~) = b at t = 0 and t = ~' re- spectively, are significant. 4. H 1( jw) is a 90° phase shifter; that is, H1(jw) = { j, . w >0 - ], w <0' f(t) l ~~ y, (t) L___j ~ y, (t) Sampled x(t) x(t) --~1 x 1----.. 0-----1 H2(jw) 1---...-1•~ z(t) t Y3 (t) p(t) (a) t t 1 (b) Figure P7. 3 7 570 Sampling Chap. 7 5. H 2(jw) is an ideallowpass filter; that is, K, O<w < W H2(jw) = K*, -W<w<O, { 0, lwi>W where K is a (possibly complex) constant. (a) Find the Fourier transforms of p(t), Y1 (t), Y2(t), and y3(t). (b) Specify the values of a, b, and K as functions of d such that z(t) = x(t) for any band-limited x(t) and any d such that 0 < d < 1riW. 7 .38. It is frequently necessary to display on an oscilloscope screen waveforms having very short time structures-for example, on the scale of thousandths of a nanosec- ond. Since the rise time of the fastest oscilloscope is longer than this, such displays cannot be achieved directly. If however, the waveform is periodic, the desired result can be obtained indirectly by using an instrument called a sampling oscilloscope. The idea, as shown in Figure P7.38(a), is to sample the fast waveform x(t) once each period, but at successively later points in successive periods. The increment d should be an appropriately chosen sampling interval in relation to the bandwidth of x(t). If the resulting impulse train is then passed through an appropriate interpolat- (a) x(t) ---~~ x )-------!~ H(jw) t------!)1~ y(t) Periodic with period T; I X Gw) I= o for I w I >Wx 00 p(t) = ~ 3[t-n(T + Ll)] 1,lwl< -1- H(jw) = 2(T + Ll) { 0, elsewhere (b) Figure P7.38 Chap. 7 Problems 571 ing lowpass filter, the output y(t) will be proportional to the original fast waveform slowed down or stretched out in time [i.e., y(t) is proportional to x(at), where a < 1]. For x(t) = A+ B cos[(27TIT)t + 8], find a range of values of~ such that y(t) in Figure P7.38(b) is proportional to x(at) with a < 1. Also, determine the value of a in terms ofT and~. 7.39 A signal xp(t) is obtained through impule-train sampling of a sinusoidal signal x(t) whose frequency is equal to half the sampling frequency Ws. x(t) = cos ( ~' t + </.>) and +cc Xp(t) = L x(nT)8(t- nT) n=-cc where T = 27TIWs. (a) Find g(t) such that x(t) = cos( cf>) cos 2WI' t ) + g(t). ( (b) Show that g(nT) = 0 for n = 0, ± 1, ±2, · · · (c) Using the results of the previous two parts, show that if xp(t) is applied as the input to an ideallowpass filter with cutoff frequency wsf2, the resulting output lS y(t) = cos(cf>) cos Ws ) ( 2 t . 7 .40. Consider a disc on which four cycles of a sinusoid are painted. The disc is rotated at approximately 15 revolutions per second, so that the sinusoid, when viewed through a narrow slit, has a frequency of 60 Hz. The arrangement is indicated in Figure P7 .40. Let v(t) denote the position of the line seen through the slit. Then v(t) = A cos(w0 t + cf> ), w 0 = 1207T. Position of line varies sinusoidally at 60 cycles per second + ---- ......... / / / 1- ....... """"-. Disk rotating at / I \ f \ ""\ 15 rps I \ ""I I /~-lo. -- \ ....... I \ \ f L- \ \ \ \ ----- , __ f I ....... / \ I \ I '\ \ f I \ ""-._ - -- I / ....... / / ' - / Figure P7.40 572 Sampling Chap. 7 For notational convenience, we will normalize v(t) so that A = 1. At 60Hz, the eye is not able to follow v(t), and we will assume that this effect can be explained by modeling the eye as an ideallowpass filter with cutoff frequency 20 Hz. Sampling of the sinusoid can be accomplished by illuminating the disc with a strobe light. Thus, the illumination can be represented by an impulse train; that is, +oo i(t) = L 8(t - kT), k= -oc where liT is the strobe frequency in hertz. The resulting sampled signal is the prod- uct r(t) = v(t)i(t). Let R(jw ), V(jw ), and l(jw) denote the Fourier transforms of r(t), v(t), and i(t), respectively. (a) Sketch V(jw ), indicating clearly the effect of the parameters cp and w 0 . (b) Sketch /(jw ), indicating the effect ofT. (c) According to the sampling theorem, there is a maximum value for T in terms of w 0 such that v(t) can be recovered from r(t) using a lowpass filter. Determine this value ofT and the cutoff frequency of the lowpass filter. Sketch R(jw) when T is slightly less than the maximum value. If the sampling period T is made greater than the value determined in part (c), aliasing of the spectrum occurs. As a result of this aliasing, we perceive a lower frequency sinusoid. (d) Suppose that 27T/T = w 0 + 207T. Sketch R(jw) for lwl < 407T. Denote by va(t) ~he apparent position of the line as we perceive it. Assuming that the eye be- haves as an ideallowpass filter with 20-Hz cutoff and unity gain, express va(t) in the form Va(t) = Aa cos(wa + cf>a), where Aa is the apparent amplitude, Wa the apparent frequency, and cf>a the apparent phase of Va(t). (e) Repeat part ~d) for 27T/T = w 0 - 207T. 7.41. In many practical situations a signal is recorded in the presence of an echo, which we would like to remove by appropriate processing. For example, in Figure P7.41(a), we illustrate a system in which a receiver simultaneously receives a signal x(t) and an echo represented by an attenuated delayed replication of x(t). Thus, the receiver output is s(t) = x(t) + ax(t- T0 ), where Ia I < 1. This output is to be processed to recover x(t) by first converting to a sequence and then using an appropriate digital filter h[n], as indicated in Figure P7.4l(b). Assume that x(t) is band limited [i.e., X(jw) = 0 for lwl > WM] and that lal < 1. (a) IfT0 < 7T/wM,andthesamplingperiodistakentobeequaltoT0 (i.e.,T =To), determine the difference equation for the digital filter h[n] such that Yc(t) is proportional to x(t). (b) With the assumptions of part (a), specify the gain A of the ideallowpass filter such that Yc(t) = x(t). (c) Now suppose that 7TIWM <To< 27T/wM. Determine a choice for the sampling period T, the lowpass filter gain A, and the frequency response for the digital filter h[n] such that Yc(t) is proportional to x(t). Chap. 7 Problems 573 ~//////////& ~~t-T0) ~ Receiver output s(t) = x(t) +a x(t-T0) (a) ldeallowpass filter sc(t) = x(t) +ax(t-T ) ---L 0 Conversion of Conversion of sp(t) impulse train s[n] y[n] sequence to a h[n] to sequence impulse train -~ 'IT T T p(t) = ~ 8(t-kT) k =-X (b) Figure P7.41 7.42. Consider a band-limited signal xc(t) that is sampled at a rate higher than the Nyquist rate. The samples, spaced T seconds apart, are then converted to a sequence x[n], as indicated in Figure P7.42. p(t) = ~ 8(t-nT) n = -x 1 xp(t) Conversion of Xc(t) ----~1 x 1----~~ impulse train 1-----i~ x[n] = Xc (nT) to sequence Figure P7.42 Determine the relation between the energy Ed of the sequence, the energy Ec of the original signal, and the sampling interval T. The energy of a sequence x[n] is defined as n= -cxc and the energy in a continuous-time function Xc(t) is defined as +oc Ec = I-c 2 xc lxc(t)j dt. 574 Sampling Chap. 7 7.43. Figure P7.43(a) depicts a system for which the input and output are discrete-time signals. The discrete-time input x[n] is converted to a continuous-time impulse train Xp(t). The continuous-time signal Xp(t) is then filtered by an LTI system to produce the output Yc(t), which is then converted to the discrete-time signal y[n]. The LTI system with input Xc(t) and output Yc(t) is causal and is characterized by the linear constant -coefficient differential equation 2 d yc(t) + 4 dyc(t) () _ () ~ ~ + 3 Yc t - Xc t . The overall system is equivalent to a causal discrete-time LTI system, as indicated in Figure P7 .43(b ). Determine the frequency response H(eiw) and the unit sample response h[n] of the equivalent LTI system. +oo ~ 8(t-nT) n = -x HOw) x[n] Conversion Ih Conversion ~ toan h(t) to a y[n] impulse train sequence -'ITIT 'ITIT +-x xp(t) = ~ x[n] 8(t-nT) n =-x +x Yp(t) = Yc(t) ~ 8(t -nT) n =-x y[n] = Yc(nT) (a) h[n]; H(eiw) x[n] ---~ equivalent t----~ y[n] LTI system (b) Figure P7.43 7 .44. Suppose we wish to design a continuous-time generator that is capable of producing sinusoidal signals at any frequency satisfying where w 1 and w2 are given positive numbers. Our design is to take the following form: We have stored a discrete-time cosine wave of period N; that is, we have stored x[O], ... , x[N - 1], where Chap. 7 Problems 575 x[k] =cos N21 Tk) ( . Every T seconds we output an impulse weighted by a value of x[k], where we pro- ceed through the values of k = 0, l, ... , N - 1 in a cyclic fashion. That is, Yp(kT) = x(k modulo N), or equivalently, Yp(kT) = cos 21Tk) (N , and +o Yp(t) = k~oo o (2 cos ~ k) o(t - kT). (a) Show that by adjusting T, we can adjust the frequency of the cosine signal being sampled. That is, show that +oo Yp(t) = (cos wot) ~ o(t - kT), k= -oo where w 0 = 21TI NT. Determine a range of values for T such that yp Ct) can rep- resent samples of a cosine signal with a frequency that is variable over the full range (b) Sketch Y p(jw ). The overall system for generating a continuous-time sinusoid is depicted in Figure P7.44(a). H(jw) is an idea11owpass filter with unity gain in its pass- band; that is, 1 H(jw) = { ' lwl <We 0, otherwise· x[O] ~._I_H_(j_w_)- ---~~-----·... ... y(t) x[N-1] y(t)--.... G(jw) t----•.,._ cos wt (a) (b) Figure P7 .44 576 Sampling Chap. 7 The parameter We is to be determined so that y(t) is a continuous-time cosine signal in the desired frequency band. (c) Consider any value of T in the range determined in part (a). Determine the minimum value of Nand some value for we such that y(t) is a cosine signal in the range w 1 :5 w :5 w2. (d) The amplitude of y(t) will vary, depending upon the value of w chosen between w 1 and w 2 . Thus, we must design a system G(jw) that normalizes the signal as shown in Figure P7.44(b). Find such a G(jw ). 7.45. In the system shown in Figure P7.45, the input Xc(t) is band limited with Xc(jw) = 0, lwl > 27T X 104 . The digital filter h[n] is described by the input-output relation HUw) Conversion x[n] = Xc(nT) y[n] Conversion Yp(t) Yc(t) ·~ to a h[n] to an _ill_ ~ sequence impulse train l -~ 1T T T +oo p(t) = L o(t-nT) n = -x Figure P7 .45 II y[n] = T L x[k]. (P7.45-l) k= -'X (a) What is the maximum value of T allowed if aliasing is to be avoided in the transformation from Xc(t) to Xp(t). (b) With the discrete-time LTI system specified through eq. (P7.45-l), determine its impulse response h[n]. (c) Determine whether there is any value ofT for which ,!~ y[ n] = )~II! Lx ,.( T) d T. (P7 .45-2) If so, determine the maximum value. If not, explain and specify how T would be chosen so that the equality in eq. (P7.45-2) is best approximated. (Think carefully about this part; it is easy to jump to the wrong conclusion!) 7.46 A signal x[n] is sampled in discrete time as shown in Figure P7.46. hr[n] is an ideal lowpass filter with frequency response lwl < ~ ~ < lwl < 1T From eqs. (7.46) and (7.47), the filter output is expressible as x,[n] = k""!t;~ x[kN]h,[n- kN] = k'!toc x[kN]N;, si:~~(~ ~~~) Chap. 7 Problems 577 x[n] +oo p[n] = l o(n -kN) k =-'X Figure P7.46 where We = 2n""/N. Show that independent of whether the sequence x[n] is sam- pled above or below the Nyquist rate, Xr[mN] = x[mN], where m is any positive or negative integer. 7.47. Suppose x[n] has a Fourier transform that is zero for 7T/3 :::; lwl :::; 'TT. Show that oo (sin(-~(n - 3k))) x[n] = k~oo x[3k] ~(n - 3k) . 7.48. Ifx[n] = cos(~n+<f>0)with0:::; <Po< 27Tandg[n] = x[n]L~=-ooo[n-4k],what additional constraints must be imposed on <Po to ensure that sin !!_n) g[n] * ( ~~ = x[n]? 7.49. As discussed in Section 7.5 and illustrated in Figure 7.37, the procedure for interpo- lation or upsampling by an integer factor N can be thought of as the cascade of two operations. The first operation, involving system A, corresponds to inserting N - 1 zero-sequence values between each sequence value of x[ n], so that n = 0, ±N, ±2N, ... otherwise For exact band-limited interpolation, H(eiw) is an ideallowpass filter. (a) Determine whether or not system A is linear. (b) Determine whether or not system A is time invariant. (c) For Xd(eiw) as sketched in Figure P7.49 and with N = 3, sketch Xp(eiw). (d) For N = 3, Xd(eiw) as in Figure P7.49, and H(eiw) appropriately chosen for exact band-limited interpolation, sketch X(eiw). -'IT 'IT w Figure P7 .49 578 Sampling Chap. 7 7 .50. In this problem, we consider the discrete-time counterparts of the zero-order hold and first-order hold, which were discussed for continuous time in Sections 7 .1.2 and 7 .2. Let x[n] be a sequence to which discrete-time sampling, as illustrated in Fig- ure 7.31, has been applied. Suppose the conditions of the discrete-time sampling theorem are satisfied; that is, w s > 2w M, where Ws is the sampling frequency and X(ejw) = 0, WM < lwl ::=; 1T. The original signal x[n] is then exactly recoverable from Xp[n] by ideal lowpass filtering, which, as discussed in Section 7.5, corre- sponds to band-limited interpolation. The zero-order hold represents an approximate interpolation whereby every sample value is repeated (or held) N - 1 successive times, as illustrated in Figure P7.50(a) for the case of N = 3. The first-order hold represents a linear interpolation between samples, as illustrated in the same figure. x[n] I II I J I I r I I IJ JI I I J I t n xp[n] J • • I . . I . . I • • L. I• • I n rn-[JJlrJ1JTilllirt-:""[""~OH n x1 [n] FOH rfllflll1II1111111l (a) n ZOH x0[n]~x0[n] x0[n]-Et--- x[n] (b) (c) FOH ><p[n]~x1 [n] (d) Figure P7.50 Chap. 7 Problems 579 (a) The zero-order hold can be represented as an interpolation in the form of eq. (7.47) or, equivalently, the system in Figure P7.50(b). Determine and sketch h0 [n] for the general case of a sampling period N. (b) x[n] can be exactly recovered from the zero-order-hold sequence x0 [n] using an appropriate LTI filter H(efw), as indicated in Figure P7.50(c). Determine and sketch H(efw). (c) The first-order-hold (linear interpolation) can be represented as an interpolation in the form of eq. (7.47) or, equivalently, the system in Figure P7.50(d). Deter- mine and sketch h 1 [ n] for the general case of a sampling period N. (d) x[n] can be exactly recovered from the first-order-hold sequence x 1 [n] using an appropriate LTI filter with frequency response H(efw). Determine and sketch H(efw). 7.51. As shown in Figure 7.37 and discussed in Section 7.5.2, the procedure for inter- polation or upsampling by an integer factor N can be thought of as a cascade of two operations. For exact band-limited interpolation, the filter H(efw) in Figure 7.37 is an ideal lowpass filter. In any specific application, it would be necessary to implement an approximate lowpass filter. In this problem, we explore some use- ful constraints that are often imposed on the design of these approximate lowpass filters. (a) Suppose that H(efw) is approximated by a zero-phase FIR filter. The filter is to be designed with the constraint that the original sequence values xd [ n] get reproduced exactly; that is, x[n] ~ xd [I]• n ~ 0, ±L, ±2L, .... (P7 .51-1) This guarantees that, even though the interpolation between the original se- quence values may not be exact, the original values are reproduced exactly in the interpolation. Determine the constraint on the impulse response h[n] of the lowpass filter which guarantees that eq. (P7 .51-1) will hold exactly for any se- quence xd[ n]. (b) Now suppose that the interpolation is to be carried out with a linear-phase, causal, symmetric FIR filter of length N; that is h[n] = 0, n < 0, n > N - l, (P7.51-2) (P7.51-3) where HR(e.iw) is real. The filter is to be designed with the constraint that the original sequence values xd [ n] get reproduced exactly, but with an integer delay a, where a is the negative of the slope of the phase of H(efw); that is, x[n] = xd [-nL- a-] , n - a = 0, ±L, ±2L, ... (P7.51-4) Determine whether this imposes any constraint on whether the filter length N is odd or even. 580 Sampling Chap. 7 (c) Again, suppose that the interpolation is to be carried out with a linear-phase, causal, symmetric FIR filter, so that H(ejw) = HR(ejw)e- jf3w, where H R( ejw) is real. The filter is to be designed with the constraint that the original sequence values xd[n] get reproduced exactly, but with a delay M that is not necessarily equal to the slope of the pha&e; that is, x[n] = xd [-nL- a-] , n- M = 0, ±L, ±2L, .... Determine whether this imposes any constraint on whether the filter length N is odd or even. 7.52 In this problem we develop the dual to the time-domain sampling theorem, whereby a time-limited signal can be reconstructed fromfrequency-domain samples. To de- velop this result, consider the frequency-domain sampling operation in Figure P7 .52. - +oo X(jw)--~ X 1-----i~X(jw) = X(jw)P(jw) = ~ X0kw0) 3(w-kw0) k = -00 +oo P(jw) = ~ 3(w- kw0) k = -00 (J) P(jw) t t t t l 1 1 1 t (J) Figure P7.52 Chap. 7 Problems 581 (a) Show that x(t) = x(t) * p(t) where x(t), x(t), and p(t) are the inverse Fourier transforms of X(jw ), X(jw ), and P(jw ), respectively. (b) Assuming that x(t) is time-limited so that x(t) = 0 for ltl 2: _!!_,show that x(t) Wo can be obtained from x(t) through a ""low-time windowing"" operation. That is, x(t) = x(t)w(t) where wo, w(t) = { O, (c) Show that x(t) is not recoverable from x(t) if x(t) is not constrained to be zero for It I 2: .!!_. wo 8 CoMMUNICATION SYSTEMS 8.0 INTRODUCTION Communication systems play a key role in our modem world in transmitting information between people, systems, and computers. In general terms, in all communication systems the information at the source is first processed by a transmitter or modulator to change it into a form suitable for transmission over the communication channel. At the receiver, the signal is then recovered through appropriate processing. This processing is required for a variety of reasons. In particular, quite typically, any specific communication channel has associated with it a frequency range over which it is best suited for transmitting a signal and outside of which communication is severely degraded or impossible. For example, the atmosphere will rapidly attenuate signals in the audible frequency range ( 10 Hz to 20 kHz), whereas it will propagate signals at a higher frequency range over longer distances. Thus, in transmitting audio signals such as speech or music over a communication channel that relies on propagation through the atmosphere, the transmitter first embeds the signal through an appropriate process into another, higher frequency signal. Many of the concepts and techniques we have developed in the earlier chapters of this text play a central role in the analysis and design of communication systems. As with any concept that is closely tied to a wide variety of important applications, there are a large number of detailed issues to be considered, and, as indicated in the bibliography, there are many excellent texts on the subject. While a full and detailed analysis of communication systems is well beyond the scope of our discussions here, with the background of the previous chapters we are now in a position to introduce some of the basic principles and issues encountered in the design and analysis of these systems. The general process of embedding an information -bearing signal into a second signal is typically referred to as modulation. Extracting the information -bearing signal 582 Sec. 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 583 is known as demodulation. As we will see, modulation techniques not only allow us to embed information into signals that can be transmitted effectively, but also make possible the simultaneous transmission of more than one signal with overlapping spectra over the same channel, through a concept referred to as multiplexing. There are a wide variety of modulation methods used in practice, and in this chapter we examine several of the most important of these. One large class of modulation meth- ods relies on the concept of amplitude modulation or AM in which the signal we wish to transmit is used to modulate the amplitude of another signal. A very common form of am- plitude modulation is sinusoidal amplitude modulation, which we explore in some detail in Sections 8.1-8.4 together with the related concepts of frequency-division multiplexing. Another important class of AM systems involves the modulation of the amplitude of a pulsed signal, and in Sections 8.5 and 8.6 we examine this form of modulation as well as the con~ept of time-division multiplexing. In Section 8.7 we then examine a different form of modulation, namely sinusoidal frequency modulation in which the information -bearing signal is used to vary the frequency of a sinusoidal signal. All of the discussion up through Section 8.7 focuses attention on continuous-time signals, since most transmission media, such as the atmosphere, are best thought of as continuous-time phenomena. Nevertheless, not only is it possible to develop analogous techniques for discrete-time signals, but it is of considerable practical importance to con- sider modulation concepts involving such signals, and in Section 8.8 we examine some of the basic ideas behind the communication of discrete-time signals. 8. 1 COMPLEX EXPONENTIAL AND SINUSOIDAL AMPLITUDE MODULATION Many communication systems rely on the concept of sinusoidal amplitude modulation, in which a complex exponential or sinusoidal signal c(t) has its amplitude multiplied (mod- ulated) by the information -bearing signal x(t). The signal x(t) is typically referred to as the modulating signal and the signal c(t) as the carrier signal. The modulated signal y(t) is then the product of these two signals: y(t) = x(t)c(t) As we discussed in Section 8.0, an important objective in modulation is to produce a signal whose frequency range is suitable for transmission over the communication channel to be used. In telephone transmission systems, for example, long-distance transmission is often accomplished over microwave or satellite links. The individual voice signals are in the frequency range 200 Hz to 4 kHz, whereas a microwave link requires signals in the range 300 megahertz (MHz) to 300 gigahertz (GHz), and communication satellite links operate in the frequency range from a few hundred MHz to over 40 GHz. Thus, for transmission over these channels, the information in a voice signal must be shifted into these higher ranges of frequency. As we will see in this section, sinusoidal amplitude modulation achieves such a shift in frequency in a very simple manner. 8. 1. 1 Amplitude Modulation with a Complex Exponential Carrier There are two common forms of sinusoidal amplitude modulation, one in which the carrier signal is a complex exponential of the form c(t) = ei(w,t+8,) (8.1) 584 Communication Systems Chap. 8 and the second in which the carrier signal is sinusoidal and of the form c(t) = cos( wet + (J c). (8.2) In both cases, the frequency We is referred to as the carrier frequency. Let us consider first the case of a complex exponential carrier, and for convenience, let us choose (J e = 0, so that the modulated signal is (8.3) From the multiplication property (Section 4.5), and with X(jw ), Y(jw ), and C(jw) denoting the Fourier transforms of x(t), y(t), and c(t), respectively, } f+x Y(jw) = 7T _ x X(j(})C(j(w - (}))d(}. (8.4) 2 For c(t) a complex exponential as given in eq. (8.1), C(jw) = 27T8(w -We), (8.5) and hence, Y(jw) = X(jw - jwc). (8.6) Thus, the spectrum of the modulated output y(t) is simply that of the input, shifted in frequency by an amount equal to the carrier frequency We. For example, with X(jw) band limited with highest frequency WM (and bandwidth 2wM), as depicted in Figure 8.1(a), the output spectrum Y(jw) is that shown in Figure 8.l(c). X(jw) ~ -wM WM w (a) C(jw) 27T I t We w (b) Figure 8.1 Effect in the frequency domain of amplitude modulation with a complex exponential carrier: (a) spec- trum of modulating signal x(t); (b) spectrum of carrier c(t) = ejwct; (c) w spectrum of amplitude-modulated sig- (c) nal y(t) = x(t)ejwct. Sec. 8.1 Complex Exponential and Sinusoidal Amplitude Modulation 585 From eq. (8.3), it is clear that x(t) can be recovered from the modulated signal y(t) by multiplying by the complex exponential e- jwct; that is, x(t) = y(t)e-jwct. (8.7) In the frequency domain, this has the effect of shifting the spectrum of the modulated signal back to its original position on the frequency axis. The process of recovering the original signal from the modulated signal is referred to as demodulation, a topic we discuss at more length in Section 8.2. Since ejwct is a complex signal, eq. (8.3) can be rewritten as y(t) = x(t)coswct + jx(t)sinwct. (8.8) Implementation of eq. (8.7) or (8.8) with x(t) real utilizes two separate multipliers and two sinusoidal carrier signals that have a phase difference of 7T/2, as depicted in Figure 8.2 for c(t) given by eq. (8.1). In Section 8.4 we give an example of one of the applications in which there are particular advantages to using a system, such as in Figure 8.2, employing two sinusoidal carriers with a phase difference of 7T/2. x(t) Figure 8.2 Implementation of am- plitude modulation with a complex ex- ponential carrier c(t) = ei(wct+ec). 8. 1 .2 Amplitude Modulation with a Sinusoidal Carrier In many situations, using a sinusoidal carrier of the form of eq. (8.2) is often simpler than and equally as effective as using a complex exponential carrier. In effect, using a sinusoidal carrier corresponds to retaining only the real or imaginary part of the output of Figure 8.2. A system that uses a sinusoidal carrier is depicted in Figure 8.3. x(t) --co.:. ~ ;rl--ec-) ----l·~ y(t) Figure 8.3 Amplitude modulation with a sinusoidal carrier. The effect of amplitude modulation with a sinusoidal carrier in the form of eq. (8.2) can be analyzed in a manner identical to that in the preceding subsection. Again, for 586 Communication Systems Chap.8 convenience we choose () c = 0. In this case, the spectrum of the carrier signal is C(jw) = 7T[8(w - we) + 8(w + We)], (8.9) and thus, from eq. (8.4 ), 1 Y(jw) = 2 [X(jw - jwc) + X(jw + jwe)]. (8.10) With X(jw) as depicted in Figure 8.4(a), the spectrum of y(t) is that shown in Figure 8.4(c). Note that there is now a replication of the spectrum of the original signal, centered around both +we and -we. As a consequence, x(t) is recoverable from y(t) only if we>ww since otherwise the two replications will overlap in frequency. This is in contrast to the case of a complex exponential carrier, for which a replication of the spectrum of the original signal is centered only around We. Specifically, as we saw in Section 8.1.1, in the case of amplitude modulation with a complex exponential carrier, x(t) can always be recovered from y(t) for any choice of We by shifting the spectrum back to its original location by multiplying by e~ jw, t, as in eq. (8.7). With a sinusoidal carrier, on the other hand, as we see from Figure 8.4, if We < WM, then there will be an overlap between the two replications of X(jw ). For example, Figure 8.5 depicts Y(jw) for we = wM/2. Clearly, the spectrum of x(t) is no longer replicated in Y(jw ), and thus, it may no longer be possible to recover x(t) from y(t). X(jw) ch -wM WM w (a) C(jw) f I f -we We w (b) Figure 8.4 Effect in the frequency domain of amplitude modulation with a sinusoidal carrier: (a) spectrum of modulating signal x(t); (b) spectrum w of carrier c(t) = cos wet; (c) spectrum (c) of amplitude-modulated signal. Sec. 8.2 Demodulation for Sinusoidal AM 587 (a) Figure 8.5 Sinusoidal amplitude modulation with carrier cos wet for which we = wM/2: (a) spectrum of (J) modulating signal; (b) spectrum of (b) modulated signal. 8.2 DEMODULATION FOR SINUSOIDAL AM At the receiver in a communication system, the information-bearing signal x(t) is recov- ered through demodulation. In this section, we examine the process of demodulation for sinusoidal amplitude modulation, as introduced in the previous section. There are two com- monly used methods for demodulation, each with its own advantages and disadvantages. In Section 8.2.1 we discuss the first of these, a process referred to as synchronous demod- ulation, in which the transmitter and receiver are synchronized in phase. In Section 8.2.2, we describe an alternative method referred to as asynchronous demodulation. 8.2.1 Synchronous Demodulation Assuming that we > w M, demodulation of a signal that was modulated with a sinusoidal carrier is relatively straightforward. Specifically, consider the signal y(t) = x(t) cos Wet. (8.11) As was suggested in Example 4.21, the original signal can be recovered by modulating y(t) with the same sinusoidal carrier and applying a lowpass filter to the result. To see this, consider w(t) = y(t) cos Wet. (8.12) Figure 8.6 shows the spectra of y(t) and w(t), and we observe that x(t) can be recovered from w(t) by applying an ideallowpass filter with a gain of 2 and a cutoff frequency that is greater than w M and less than 2w e - w M. The frequency response of the lowpass filter is indicated by the dashed line in Figure 8.6(c). The basis for using eq. (8.12) and a lowpass filter to demodulate y(t) can also be seen algebraically. From eqs. (8.11) and (8.12), it follows that w(t) = x(t) cos2 Wet, 588 Communication Systems Chap. 8 Y(jw) A ~ -we (we-wM) We (we+wM) w (a) C(jw) 1T 1T 1 I 1 -we We w (b) - W(jw) I -.1 I 2&. -, I I I I I I I I w (c) Figure 8.6 Demodulation of an amplitude-modulated signal with a sinu- soidal carrier: (a) spectrum of modulated signal; (b) spectrum of carrier signal; (c) spectrum of modulated signal multiplied by the carrier. The dashed line indicates the frequency response of a lowpass filter used to extract the de- modulated signal. or, using the trigonometric identity 1 1 cos2 Wet = 2 + 2 cos 2wet, we can rewrite w(t) as 1 1 w(t) = 2x (t) + 2x (t) cos 2wet. (8.13) Thus, w(t) consists of the sum of two terms, namely one-half the original signal and one- half the original sighal modulated with a sinusoidal carrier at twice the original carrier frequency We. Both of these terms are apparent in the spectrum shown in Figure 8.6(c). Applying the lowpass filter to w(t) corresponds to retaining the first term on the right-hand side of eq. (8.13) and eliminating the second t~rm. The overall system for amplitude modulation and demodulation using a complex exponential carrier is depicted in Figure 8.7, and the overall system for modulation and demodulation using a sinusoidal carrier is depicted in Figure 8.8. In these figures, we have indicated the more general case in which, for both the complex exponential and the sinusoidal carrier, a carrier phase () c is included. The modification of the preceding analysis so as to include ()cis straightforward and is considered in Problem 8.21. Sec. 8.2 Demodulation for Sinusoidal AM 589 x(t) ------~~ X l---~ y(t) x(t)--~0~--~ y(t) l ei(wct + flc) (a) (a) y(t) ~w(t) H(jw) w(t) l y(t) --~ 0 X ~--~-~~ 21 1-----'!~ x(t) e-j(Wcl + flc) l Lowpass filter (b) cos (wet + ec) Figure 8. 7 System for ampli- (b) tude modulation and demodulation using a complex exponential car- Figure 8.8 Amplitude modulation and demodulation with a sinusoidal car- rier: (a) modulation; (b) demodula- rier: (a) modulation system; (b) demodulation system. The lowpass filter cut- tion. off frequency Wco is greater than wM and less than 2wc- wM. In the systems of Figures 8.7 and 8.8, the demodulating signal is assumed to be synchronized in phase with the modulating signal, and consequently the process is referred to as synchronous demodulation. Suppose, however, that the modulator and demodulator are not synchronized in phase. For the case of the complex exponential carrier, with 8 c denoting the phase of the modulating carrier and cf>c the phase of the demodulating carrier, y(t) = ej(wct+8c) x(t), (8.14) w(t) = e- j(wct+4>c) y(t), (8.15) and consequently, w(t) = ej(Oc-4>c) x(t). (8.16) Thus, if 8 c -# cf>c, w(t) will have a complex amplitude factor. For the particular case in which x(t) is positive, x(t) = lw(t)l, and thus x(t) can be recovered by taking the magni- tude of the demodulated signal. For the sinusoidal carrier, again let 8 c and cf>c denote the phases of the modulating and demodulating carriers, respectively, as indicated in Figure 8.9. The input to the lowpass filter is now w(t) = x(t) cos( wet + 8 c) cos(w et+ cf>c), (8.17) or, using the trigonometric identity (8.18) 590 Communication Systems Chap.8 x(t) __. .....,.~ X 1--~ y(t) (a) H(jw) w(t) y(t) --ooo~•~0 X 1------11•~1 21 l (b) Figure 8. 9 Sinusoidal amplitude modulation and demodulation system for which the carrier signals and the modulator and demodulator are not synchro- nized: (a) modulator; (b) demodulator. we have (8.19) and the output of the lowpass filter is then x(t) multiplied by the amplitude factor cos(8c- cf>c). If the oscillators in the modulator and demodulator are in phase, 8 c = cf>c, and the output of the lowpass filter is x(t). On the other hand, if these oscillators have a phase difference of 7r/2, the output will be zero. In general, for a maximum output signal, the os- cillators should be in phase. Of even more importance, the phase relation between the two oscillators must be maintained over time, so that the amplitude factor cos( 8 c - cf>c) does not vary. This requires careful synchronization between the modulator and the demodu- lator, which is often difficult, particularly when they are geographically separated, as is typical in a communication system. The corresponding effects of, and the need for, syn- chronization not only between the phase of the modulator and demodulator, but between the frequencies of the carrier signals used in both, are explored in detail in Problem 8.23. 8.2.2 Asynchronous Demodulation In many systems that employ sinusoidal amplitude modulation, an alternative demod- ulation procedure referred to as asynchronous demodulation is commonly used. Asyn- chronous demodulation avoids the need for synchronization between the modulator and demodulator. In particular, suppose that x(t) is always positive and that the carrier fre- quency w c is much higher than w M, the highest frequency in the modulating signal. The modulated signal y(t) will then have the general form illustrated in Figure 8.10. Sec. 8.2 Demodulation for Sinusoidal AM 591 In particular, the envelope of y(t)-that is, a smooth curve connecting the peaks in y(t)- would appear to be a reasonable approximation to x(t). Thus, x(t) could be approximately recovered through the use of a system that tracks these peaks to extract the envelope. Such a system is referred to as an envelope detector. One example of a simple circuit that acts as an envelope detector is shown in Figure 8.ll(a). This circuit is generally followed by a lowpass filter to reduce the variations at the carrier frequency, which are evident in Figure 8.ll(b) and which will generally be present in the output of an envelope detector of the type indicated in Figure 8.11(a). The two basic assumptions required for asynchronous demodulation are that x(t) be positive and that x(t) vary slowly compared to We, so that the envelope is easily tracked. The second condition is satisfied, for example, in audio transmission over a radio- frequency (RF) channel, where the highest frequency present in x(t) is typically 15 to 20 kHz and wei2TT is in the range 500kHz to 2 MHz. The first condition, that x(t) be positive, can be satisfied by simply adding an appropriate constant value to x(t) or, equivalently, by a simple change in the modulator, as shown in Figure 8.12. The output of the envelope detector then approximates x(t) +A, from which x(t) is easily obtained. To use the envelope detector for demodulation, we require that A be sufficiently large so that x(t) + A is positive. Let K denote the maximum amplitude of x(t); that is, lx(t)l ::::; K. For x(t) +A to be positive, we require that A> K. The ratio KIA is commonly referred to as the modulation index m. Expressed in percent, it is referred to as the percent modulation. An illustration of the output of the modulator of Figure 8.12 for x(t) sinu- soidal and form = 0.5 (50% modulation) and m = 1.0 (100% modulation), is shown in Figure 8.13. In Figure 8.14, we show a comparison of the spectra associated with the modulated signal when synchronous demodulation and when asynchronous demodulation are used. We note in particular that the output of the modulator for the asynchronous system in Figure 8.12 has an additional component A cos wet that is neither present nor necessary in the synchronous system. This is represented in the spectrum of Figure 8.14(c) by the pres- ence of impulses at +we and -we. For a fixed maximum amplitude K of the modulating signal, as A is decreased the relative amount of carrier present in the modulated output decreases. Since the carrier component in the output contains no information, its presence y(t) J( Envelope ______, Figure 8. 1 0 Amplitude-modulated signal for which the modulating signal Envelope is positive. The dashed curve repre- sents the envelope of the modulated signal. 592 Communication Systems Chap.8 + y(t) c R w(t) (a) (b) Figure 8. 11 Demodulation by envelope detection: (a) circuit for envelope detection using half-wave rectification; (b) waveforms associated with the en- velope detector in (a): r(t) is the half-wave rectified signal, x(t) is the true envelope, and w(t) is the envelope obtained from the circuit in (a). The rela- tionship between x(t) and w(t) has been exaggerated in (b) for purposes of illustration. In a practical asynchronous demodulation system, w(t) would typi- cally be a much closer approximation to x(t) than depicted here. x(t)--~ 1---~ y(t)=(A+x(t)) coswct A Figure 8. 12 Modulator for an asynchronous modulation-demodulation system. represents an inefficiency-for example, in the amount of power required to transmit the modulated signal-and thus, in one sense it is desirable to make the ratio Kl A-i.e., the modulation index m-as large as possible. On the other hand, the ability of a simple envelope detector such as that in Figure 8.11 to follow the envelope and thus extract x(t) improves as the modulation index decreases. Hence, there is a trade-off between the effi- Sec. 8.2 Demodulation for Sinusoidal AM 593 (a) Figure 8. 1 3 Output of the am- plitude modulation system of Figure 8.12: (a) modulation index m = 0.5; (b) (b) modulation index m = 1.0. X(jw) & -wM WM w (a) 6 it 6 w Figure 8. 14 Comparison of spec- -we We (b) tra for synchronous and asynchronous sinusoidal amplitude modulation sys- tems: (a) spectrum of modulating signal; (b) spectrum of x(t) cos wet representing modulated signal in a synchronous system; (c) spectrum of [x(t) + A] cos wet representing modu- w lated signal in an asynchronous (c) system. 594 Communication Systems Chap.8 ciency of the system in terms of the power in the output of the modulator and the quality of the demodulated signal. There are a number of advantages and disadvantages to the asynchronous modulation- demodulation system of Figures 8.11 and 8.12, compared with the synchronous system of Figure 8.8. The synchronous system requires a more sophisticated demodulator because the oscillator in the demodulator must be synchronized with the oscillator in the modu- lator, both in phase and in frequency. On the other hand, the asynchronous modulator in general requires transmitting more power than the synchronous modulator, since, for the envelope detector to operate properly, the envelope must be positive, or equivalently, there must be a carrier component present in the transmitted signal. This is often preferable in cases such as that associated with public radio broadcasting, in which it is desirable to mass-produce large numbers of receivers (demodulators) at moderate cost. The additional cost in transmitted power is then offset by the savings in cost for the receiver. On the other hand, in situations in which transmitter power requirements are at a premium, as in satellite communication, the cost of implementing a more sophisticated synchronous receiver is warranted. 8.3 FREQUENCY-DIVISION MULTIPLEXING Many systems used for transmitting signals provide more bandwidth than is required for any one signal. For example, a typical microwave link has a total bandwidth of several gigahertz, which is considerably greater than the bandwidth required for one voice chan- nel. If the individual voice signals, which are overlapping in frequency, have their fre- quency content shifted by means of sinusoidal amplitude modulation so that the spectra of the modulated signals no longer overlap, they can be transmitted simultaneously over a single wide band channel. The resulting concept is referred to as frequency-division multi- plexing (FDM). Frequency-division multiplexing using a sinusoidal carrier is illustrated in Figure 8.15. The_ individual signals to be transmitted are assumed to be band limited and COSWat ~ Ya(t) Xa(t)~ coswbt ~ Yb(t) xb(t) X w(t) Figure 8. 1 5 Frequency-division multiplexing using sinusoidal amplitude , modulation. Sec. 8.3 Frequency-Division Multiplexing 595 are modulated with different carrier frequencies. The modulated signals are then summed and transmitted simultaneously over the same communication channel. The spectra of the individual subchannels and the composite multiplexed signal are illustrated in Figure 8.16. Through this multiplexing process, the individual input signals are allocated distinct seg- ments of the frequency band. To recover the individual channels in the demultiplexing process requires two basic steps: bandpass filtering to extract the modulated signal corre- sponding to a specific channel, followed by demodulation to recover the original signal. This is illustrated in Figure 8.17 to recover channel a, where, for purposes of illustration, synchronous demodulation is assumed. Xa(jw) Xb(jw) Xe(jw) __ill_ __rb_ ~ -wM WM w -wM WM w -wM WM w Ya(jw) ~ I ~ -wa -wa w Yb(jw) (I (I I -wb -wb w Ye(jw) 1\ I 1\ -we We W W(jw) /\(I~ ~(II\ Figure 8.16 Spectra associated I with the frequency-division multiplexing -we -wb -wa Wa wb We w system of Figure 8.15. 596 Communication Systems Chap.8 ~+-----Demultiplexing-----f-+--------Demodulation-----~ Bandpass Lowpass filter coswat filter H1(jw) ~ H2 (jw) w(t) Ya(t) X 1,1111,1 I 12 I -wa Wa w -wM wM w Figure 8. 1 7 Demultiplexing and demodulation for a frequency-division multiplexed signal. Telephone communication is one important application of frequency-division multi- plexing. Another is the transmission of signals through the atmosphere in the RF band. In the United States, the use of radio frequencies for transmitting signals over the range 10 kHz to 275 GHz is controlled by the Federal Communications Commission, and different portions of the range are allocated for different purposes. The current allo- cation of frequencies is shown in Figure 8.18. As indicated, the frequency range in the neighborhood of 1 MHz is assigned to the AM broadcast band, where AM refers specifically to the use of sinusoidal amplitude modulation. Individual AM radio sta- tions are assigned specific frequencies within the AM band, and thus, many stations can broadcast simultaneously through this use of frequency-division multiplexing. In principle, at the receiver, an individual radio station can be selected by demultiplex- ing and demodulating, as illustrated in Figure 8.17. The tuning dial on the receiver would then control both the center frequency of the bandpass filter and the frequency of the demodulating oscillator. In fact, for public broadcasting, asynchronous modulation and demodulation are used to simplify the receiver and reduce its cost. Furthermore, the demultiplexing in Figure 8.17 requires a sharp cutoff bandpass filter with variable center frequency. Variable frequency-selective filters are difficult to implement, and consequently, a fixed filter is implemented instead, and an intermediate stage of mod- ulation and filtering [referred to in a radio receiver as the intermediate-frequency (IF) stage] is used. The use of modulation to slide the spectrum of the signal past a fixed bandpass filter replaces the use of a variable bandpass filter in a manner similar to the procedure discussed in Section 4.5 .1. This basic procedure is incorporated into typical home AM radio receivers. Some of the more detailed issues involved are considered in Problem 8.36. As illustrated in Figure 8.16, in the frequency-division multiplexing system of Fig- ure 8.15 the spectrum of each individual signal is replicated at both positive and negative frequencies, and thus the modulated signal occupies twice the bandwidth of the original. This represents an inefficient use of bandwidth. In the next section we consider an al- ternative form of sinusoidal amplitude modulation, which leads to more efficient use of bandwidth at the cost of a more complicated modulation system. Sec. 8.4 Single-Sideband Sinusoidal Amplitude Modulation 597 Frequency Propagation Channel range Designation Typical uses method features 30-300 Hz ELF Macrowave, submarine com- Megametric waves Penetration of conducting (extremely munication earth and seawater low frequency) 0.3-3 kHz VF Data terminals, telephony Copper wire (voice frequency) 3-30kHz VLF Navigation, telephone, tele- Surface ducting Low attenuation, little fading, (very low fre- graph, frequency and timing (ground wave) extremely stable phase and quency) standards frequency, large antennas 30-300 kHz LF Industrial (power line) com- Mostly surface ducting Slight fading, high atmo- (low frequency) munication, aeronautical spheric pulse and maritime long-range navigation, radio beacons 0.3-3 MHz MF Mobile, AM broadcasting, Ducting and ionospheric Increased fading, but reliable (medium frequency) amateur, public safety reflection (sky wave) 3-30MHz HF Military communication, aero- Ionospheric reflecting sky Intermittent and frequency- (high frequency) nautical mobile, interna- wave, 50-400 km layer selective fading, multipath tiona) fixed, amateur and altitudes citizen's band, industrial 30-300 MHz VHF FM and TV broadcast, land Sky wave (ionospheric and Fading, scattering, and multi- (very high transportation (taxis, buses, tropospheric scatter) path frequency) railroad) 0.3-3 GHz UHF UHF TV, space telemetry, Transhorizon tropospheric (ultra high radar, military scatter and line-of-sight frequency) relaying 3-30 GHz SHF Satellite and space commu- Line-of-sight ionosphere Ionospheric penetration, (super high nication. common carrier penetration extraterrestrial noise, frequency) (CC), microwave high directly 30-300 GHz EHF Experimental, government, Line of sight Water vapor and oxygen (extremely high radio astronomy absorption frequency) 103-107 GHz Infrared, visible light, Optical communications Line of sight ultraviolet Figure 8. 18 Allocation of frequencies in the RF spectrum. 8.4 SINGLE-SIDEBAND SINUSOIDAL AMPLITUDE MODULATION For the sinusoidal amplitude modulation systems discussed in Section 8.1, the total bandwidth of the original signal x(t) is 2w M, including both positive and negative frequencies, where WM is the highest frequency present in x(t). With the use of a complex exponential carrier, the spectrum is translated to w c, and the total width of the frequency band over which there is energy from the signal is still 2w M, although the modulated signal is now complex. With a sinusoidal carrier, on the other hand, the spec- trum of the signal is shifted to +we and -we, and thus, twice the bandwidth is required. This suggests that there is a basic redundancy in the modulated signal with a sinusoidal carrier. Using a technique referred to as single-sideband modulation, we can remove the redundancy. The spectrum of x(t) is illustrated in Figure 8.19(a), in which we have shaded the positive and negative frequency components differently to distinguish them. The spectrum 598 Communication Systems Chap. a in Figure 8.19(b) results from modulation with a sinusoidal carrier, where we identify an upper and lower sideband for the portion of the spectrum centered at +we and that centered at -we. Comparing Figures 8.19(a) and (b), we see that X(jw) can be recovered if only the upper sidebands at positive and negative frequencies are retained, or alternatively, if only the lower sidebands at positive and negative frequencies are retained. The resulting spectrum if only the upper sidebands are retained is shown in Figure 8.19(c), and the resulting spectrum if only the lower sidebands are retained is shown in Figure 8.19(d). The conversion of x(t) to the form corresponding to Figure 8.19(c) or (d) is referred to as single-sideband modulation (SSB), in contrast to the double-sideband modulation (DSB) of Figure 8.19(b), in which both sidebands are retained. There are several methods by which the single-sideband signal can be obtained. One is to apply a sharp cutoff bandpass or high pass filter to the double-sideband signal of Figure 8.19(b ), as illustrated in Figure 8.20, to remove the unwanted sideband. Another is to use a procedure that utilizes phase shifting. Figure 8.21 depicts a system designed X(jw) ~ -wM WM w (a) Y(jw) A t A w sideband sideband (b) sideband sideband Yu(jw) ~ ~l ~ -we We w (c) Yj(jw) t Figure 8.19 Double- and single- sideband modulation: (a) spectrum of ~ ~ modulating signal; (b) spectrum af- ter modulation with a sinusoidal car- rier; (c) spectrum with only the upper -we We w sidebands; (d) spectrum with only the (d) lower sidebands. Sec. 8.4 Single-Sideband Sinusoidal Amplitude Modulation 599 Y(t) H(jw) • Yu(t) ·I Y(jw) A ~t A -we We w H(jw) 1t -we We w Yu(jw) ~ ~1 ~ Figure 8.20 System for retaining the upper sidebands using ideal high- -we We w pass filtering. to retain the lower sidebands. The system H(jw) in the figure is referred to as a ""90° phase-shift network,"" for which the frequency response is of the form H(jw) = { ~ j, w >0 (8.20) ], w <0"" The spectra of x(t), Y1 (t) = x(t) cos Wet, Y2(t) = Xp(t) sin wet, and y(t) are illustrated in Figure 8.22. As is examined in Problem 8.28, to retain the upper sidebands instead of the lower sidebands, the phase characteristic of H(jw) is reversed so that H(jw) = { j, . w >0 (8.21) - ], w <0"" As is explored in Problem 8.29, synchronous demodulation of single-sideband systems can be accomplished in a manner identical to synchronous demodulation of double-sideband systems. The price paid for the increased efficiency of single-sideband systems is added complexity in the modulator. 600 Communication Systems Chap. a Y1 (t) x(t)--.. y(t) t H(jw) <):: H(jw) ± 1T ---~2 w w -¥1---- Figure 8.21 System for single-sideband amplitude modulation, using a goo phase-shift network, in which only the lower sidebands are retained. In summary, in Sections 8.1 through 8.4 we have seen a number of variations of complex exponential and sinusoidal amplitude modulation. With asynchronous demod- ulation, discussed in Section 8.2.2, a constant must be added to the modulating signal so that it is positive. This results in the presence of the carrier signal as a component in the modulated output, requiring more power for transmission, but resulting in a simpler demodulator than is required in a synchronous system. Alternatively, only the upper or lower sidebands in the modulated output may be retained, which makes more efficient use of bandwidth and transmitter power, but requires a more sophisticated modulator. Sinu- soidal amplitude modulation with both sidebands and the presence of a carrier is typically abbreviated as AM-DSB/WC (amplitude modulation, double sideband/with carrier) and, when the carrier is suppressed or absent, as AM-DSB/SC (amplitude modulation, double- sideband/suppressed carrier). The corresponding single-sideband systems are abbreviated AM-SSB/WC and AM-SSB/SC. Sections 8.1 through 8.4 are intended to provide an introduction to many of the basic concepts associated with sinusoidal amplitude modulation. There are many variations in details and implementation, and the reader is referred to the bibliography for an indication of the numerous excellent books that explore this topic further. Sec. 8.5 Amplitude Modulation with a Pulse-Train Carrier 601 X(jw) w Y1(jw) L\, ~1 A -we We w Y2 (jw) 1 2 w Y(jw) 1t Figure 8.22 Spectra associated with the single-sideband system of w Figure 8.21. 8.5 AMPLITUDE MODULATION WITH A PULSE-TRAIN CARRIER 8.5.1 Modulation of a Pulse-Train Carrier In previous sections, we examined amplitude modulation with a sinusoidal carrier. Another important class of amplitude modulation techniques corresponds to the use of a carrier signal that is a pulse train, as illustrated in Figure 8.23; amplitude modulation of this type effectively corresponds to transmitting equally spaced time slices of x(t). In general, we would not expect that an arbitrary signal could be recovered from such a set of time slices. However, our examination of the concept of sampling in Chapter 7 suggests that this should be possible if x(t) is band limited and the pulse repetition frequency is high enough. From Figure 8.23, y(t) = x(t)c(t); (8.22) i.e., the modulated signal y(t) is the product of x(t) and the carrier c(t). With Y(jw ), X(jw ), and C(jw) representing the Fourier transforms of each of these signals, it follows from 602 Communication Systems Chap.8 c(t) ! x(t) ~ y(t) x(t) 0 c(t) 1:~· 1 r-l dJ D D D 0 y(t) ~ b D d Figure 8.23 Amplitude modulation 0 of a pulse train. the multiplication property that Y(jw) }J +x = - X(jO)C(j(w - O))dO. (8.23) 27T -x Since c(t) is periodic with period T, C(jw) consists of impulses in frequency spaced by 27TIT; that is, C(jw) = 27T L akB(w- kwc). (8.24) k=-X where We = 27T/T and the coefficients ak are the Fourier series coefficients of c(t), which, from Example 3.5, are sin(kwc~/2) (8.25) 7Tk Sec. 8.5 Amplitude Modulation With a Pulse Train-Carrier 603 The spectrum of c(t) is shown in Figure 8.24(b). With the spectrum of x(t) as illustrated in Figure 8.24(a), the resulting spectrum of the modulated signal y(t) is shown in Fig- ure 8.24(c). From eqs. (8.23) and (8.24), Y(jw) is a sum of scaled and shifted replicas of X(jw): +QG Y(jw) = .2:= akX(j(w - kwc)). (8.26) k= -'X X(jw) (a) C(jw) / / / / ' ' ' ' w (b) Y(jw) w (c) Figure 8.24 Spectra associated with amplitude modulation of a pulse train: (a) spectrum of a bandlimited-signal x(t); (b) spectrum of the pulse carrier signal c(t) in Figure 8.23; (c) spectrum of the modulated pulse train y(t). 604 Communication Systems Chap.B Comparing eq. (8.26) with eq. (7.6) and Figure 8.24 with Figure 7.3(c), we see that the spectrum of y(t) is very similar in form to the spectrum resulting from sampling with a periodic impulse train, the only difference being the values of the Fourier coefficients of the pulse train. For the periodic impulse train used in Chapter 7, all of the Fourier coefficients are equal to liT in value, while for the pulse train c(t) in Figure 8.23, the Fourier coefficients are given by eq. (8.25). Consequently, the replicas of X(jw) do not overlap as long as We> 2wM, which corresponds to the condition of the Nyquist sampling theorem. If this constraint is satisfied, then, as with impulse-train sampling, x(t) can be recovered from y(t) through the use of a lowpass filter with cutoff frequency greater than WM and less than We- WM· Note that the same conclusion holds for a wide variety of other pulselike carrier waveforms: If c(t) is any periodic signal with Fourier transform as in eq. (8.24) for some set of Fourier coefficients ak. then Y(jw) is given by eq. (8.26). Then, as long as We = 2n'/T > 2w M, the replicas of X(jw) do not overlap, allowing us to recover x(t) by lowpass filtering, provided that the DC Fourier coefficient a0 is nonzero. As shown in Problem 8.11, if a0 is zero or unacceptably small, then, by using a bandpass filter to select one of the shifted replicas of X(jw) with a larger value of ak. we obtain a sinusoidal AM signal with a scaled version of x(t) as the modulating signal. Using the demodulation methods described in Section 8.2, we can then recover x(t). 8.5.2 Time-Division Multiplexing Amplitude modulation with a pulse-train carrier is often used to transmit several signals over a single channel. As indicated in Figure 8.23, the modulated output signal y(t) is nonzero only when the carrier signal c(t) is on (i.e., is nonzero). During the intervals in which c(t) is off, other similarly modulated signals can be transmitted. Two equivalent representations of this process are shown in Figure 8.25. In this technique for transmitting several signals over a single channel, each signal is in effect assigned a set of time slots of duration A that repeat every T seconds and that do not overlap with the slots assigned to other signals. The smaller the ratio AfT, the larger the number of signals that can be transmitted over the channel. This procedure is referred to as time-division multiplexing (TDM). Whereas frequency-division mul~iplexing, as discussed in Section 8.3, assigns different frequency intervals to individual signals, time-division multiplexing assigns dif- ferent time intervals to individual signals. Demultiplexing the individual signals from the composite signal in Figure 8.25 is accomplished by time gating, to select the particular time slots associated with each individual signal. 8.6 PULSE-AMPLITUDE MODUlATION 8.6. 1 Pulse-Amplitude Modulated Signals In Section 8.5 we described a modulation system in which a continuous-time signal x(t) modulates a periodic pulse train, corresponding to transmitting time slices of x(t) of dura- tion A seconds every T seconds. As we saw both in that discussion and in our investigation of sampling in Chapter 7, our ability to recover x(t) f~om these time slices depends not on their duration A, but rather on their frequency 27T/T, which must exceed the Nyquist rate Sec. 8.6 Pulse-Amplitude Modulation 605 / ' ·k.._ __, _\1r--------+-• y(t) I '( (a) ···D n n x1( t) Y1(t) ···D n n D··· x2(t) ···D n n D··· y(t) x3(t) ···D n D··· x4(t) Figure 8.25 Time-division (b) multiplexing. in order to ensure an alias-free reconstruction of x(t). That is, in principle, we need only transmit the samples x(nT) of the signal x(t). In fact, in modern communication systems, sampled values of the information- bearing signal x(t), rather than time slices are more typically transmitted. For practical reasons, there are limitations on the maximum amplitude that can be transmitted over a communication channel, so that transmitting impulse-sampled versions of x(t) is not practical. Instead, the samples x(nT) are used to modulate the amplitude of a sequence of pulses, resulting in what is referred to as a pulse-amplitude modulation (PAM) system. 606 Communication Systems Chap. 8 The use of rectangular pulses corresponds to a sample-and-hold strategy in which pulses of duration ~ and amplitude proportional to the instantaneous sample values of x(t) are transmitted. The resulting waveform for a single PAM channel of this type is illus- trated in Figure 8.26. In the figure, the dotted curve represents the signal x(t). As with the modulation scheme in Section 8.5, PAM signals can be time multiplexed. This is illus- trated in Figure 8.27, which depicts the transmitted waveform with three time-multiplexed channels. The pulses associated with each channel are distinguished by shading, as well as by the channel number above each pulse. For a given pulse-repetition period T, as the pulse width decreases, more time-multiplexed channels can be transmitted over the same communication channel or medium. However, as the pulse width decreases, it is typically necessary to increase the amplitude of the transmitted pulses so that a reasonable amount of energy is transmitted in each pulse. In addition to energy considerations, a number of other issues must be addressed in designing a PAM signal. In particular, as long as the sampling frequency exceeds the Nyquist rate, we know that x(t) can be reconstructed exactly from its samples, and con- - - -""'!! y(t) / / / / /- / Figure 8.26 Transmitted waveform for a single PAM channel. The dotted curve represents the signal x{t). y(t) Figure 8.27 Transmitted waveform with three time-multiplexed PAM channels. The pulses associated with each channel are distinguished by shading, as well as by the channel number above each pulse. Here, the intersymbol spacing is 7; = T/3. Sec. 8.6 Pulse-Amplitude Modulation 607 sequently we can use these samples to modulate the amplitude of a sequence of pulses of any shape. The choice of pulse shape is dictated by considerations such as the frequency selectivity of the communication medium being used and the problem of intersymbol in- terference, which we discuss next. 8.6.2 lntersymbol Interference in PAM Systems In the TDM pulse-amplitude modulation system just described, the receiver can, in prin- ciple, separate the channels by sampling the time-multiplexed waveform at appropriate times. For example, consider the time-multiplexed signal in Figure 8.27, which consists of pulse-amplitude-modulated versions of three signals x1 (t), x2(t), and x3(t). If we sam- ple y(t) at appropriate times, corresponding, for example, to the midpoints of each pulse, we can separate the samples of the three signals. That is, y(t) = Ax 1 (t), t = 0, ±3T1, ±6T1, ••• , y(t) = Ax2(t), t = T1, T1 :±: 3T1, T1 :±: 6T1, ••• , (8.27) y(t) = Ax3(t), t = 2T1, 2T1 :±: 3T1, T1 :±: 6T1, ••• , where T1 is the intersymbol spacing, here equal to T 13, and where A is the appropriate proportionality constant. In other words, samples of x1 (t), x2(t), and x3(t) can be obtained by appropriate sampling of the received time-multiplexed PAM signal. The strategy indicated in the preceding paragraph assumes that the transmitted pulses remain distinct as they propagate over the communication channel. In transmis- sion through any realistic channel, however, the pulses can be expected to be distorted through effects such as additive noise and filtering. Additive noise in the channel will, of course, introduce amplitude errors at the sampling times. Filtering due to the nonideal frequency response of a channel causes a smearing of the individual pulses that can cause the received pulses to overlap in time. This interference is illustrated in Figure 8.28 and is referred to as intersymbol inteiference. intersymbol interference I t Sampling time for channel 2 ~ Sampling time Sampling time for channel 1 for channel 3 Figure 8.28 lntersymbol interference. 608 Communication Systems Chap. a The smearing over time of the idealized pulses in Figure 8.27 can result from the bandwidth constraints of the channel or from phase dispersion caused by nonconstant group delay, as was discussed in Section 6.2.2. (See in particular, Example 6.1.) If the intersymbol interference is due only to the limited bandwidth of the channel, an approach is to use a pulse shape p(t) that is itself band limited and therefore not affected (or only minimally affected) by the restricted bandwidth of the channel. In particular, if the chan- nel has a frequency response H(jw) that has no distortion over a specified frequency band (e.g., if H(jw) = 1 for lwl < W), then if the pulse that is used is band limited (i.e., if P(jw) = 0 for iw I 2: W), each PAM signal will be received without distortion. On the other hand, by using such a pulse, we no longer have pulses without overlap as in Fig- ure 8.27. Nevertheless, intersymbol interference can be avoided in the time domain, even with a band-limited pulse, if the pulse shape is constrained to have zero-crossings at the other sampling times [so that eq. (8.27) continues to hold]. For example, consider the sine pulse = -Tt -sin-( 7T-t1T-t) p(t ) 7Tt and its corresponding spectrum displayed in Figure 8.29. Since the pulse is zero at integer multiples of the symbol spacing T 1, as indicated in Figure 8.30, there will be no intersym- bol interference at these instants. That is, if we sample the received signal at t = kT1, then the contributions to this sampled value from all of the other pulses, i.e., from p(t- mTt) form =I= k, will be identically zero. Of course, avoiding interference from adjacent symbols p(t) P(jw) ------~----~----~-----w Figure 8.29 A sine pulse and its corresponding spectrum. Sec. 8.6 Pulse-Amplitude Modulation 609 Pulse used to transmit sample of channel 2 Pulse used to transmit sample of channel 1 \ Pulse used to transmit sample of channel 3 Sampling Sampling Sampling time for time for time for channel1 channel2 channel3 Figure 8.30 Absence of intersymbol interference when sine pulses with correctly chosen zero-crossings are used. requires high accuracy in the sampling times, so that sampling occurs at the zero-crossings of the adjacent symbols. The sine pulse is only one of many band-limited pulses with time-domain zero- crossings at ±T1, ±2T1, etc. More generally, consider a pulse p(t) with spectrum of the form l+PI(jw), P(jw) = lP 1( jw ), (8.28) 0, otherwise and with P1 (jw) having odd symmetry around TT!T1, so that P, (- jw + j ~ ) = - P 1 (jw + j ~ ) 0 ,; w ,; (8.29) as illustrated in Figure 8.31. If P 1( jw) = 0, p(t) is the sine pulse itself. More generally, as explored in Problem 8.42, for any P(jw) satisfying the conditions in eqs. (8.28) and (8.29), p(t) will have zero-crossing at ±T1, ±2T1, •••• While signals satisfying eqs. (8.28) and (8.29) allow us to overcome the problem of limited channel bandwidth, other channel distortions may occur that require a differ- ent choice of pulse waveform or some additional processing of the received signal prior to the separation of the different TOM signals. In particular, if jH(jw )j is not constant over the passband, there may be a need to perform channel equalization-i.e., filtering of 610 Communication Systems Chap.8 P(jw) 1T -T1 Figure 8.31 Odd symmetry around 1r!T1 as defined in eq. (8.29). the received signal to correct for the nonconstant channel gain. Also, if the channel has nonlinear phase, distortion can result that leads to intersymbol interference, unless com- pensating signal processing is performed. Problems 8.43 and 8.44 provide illustrations of these effects. 8.6.3 Digital Pulse-Amplitude and Pulse-Code Modulation The PAM system described in the preceding subsections involves the use of a discrete set of samples to modulate a sequence of pulses. This set of samples can be thought of as a discrete-time signal x[n], and in many applications x[n] is in fact stored in or generated by a digital system. In such cases, the limited word length of a digital system implies that x[n] can take on only a finite, quantized set of values, resulting in only a finite set of possible amplitudes for the modulated pulses. In fact, in many cases this quantized form of digital PAM is reduced to a system using only a few-typically, only two-amplitude values. In particular, if each sample of x[n] is represented as a binary number (i.e., a finite string of O's and 1' s), then a pulse with one of two possible values (one value corresponding to a 0 and one value to a 1) can be set for each binary digit, or bit, in the string. More generally, in order to protect against transmission errors or provide secure communication, the sequence of binary digits rep- resenting x[n] might first be transformed or encoded into another sequence of O's and 1' s before transmission. For example, a very simple error detection mechanism is to transmit one additional modulated pulse for each sample of x[n], representing a parity check. That is, this additional bit would be set to 1 if the binary representation of x[n] has an odd num- ber of 1' sin it and to 0 if there is an even number of 1' s. The receiver can then check the received parity bit against the other received bits in order to detect inconsistencies. More complex coding and error correction schemes can certainly be employed, and the design of codes with particular desirable properties is an important component of communication system design. For obvious reasons, a PAM system modulated by an encoded sequence of O's and 1 'sis referred to as a pulse-code modulation (PCM) system. Sec. 8.7 Sinusoidal Frequency Modulation 611 8.7 SINUSOIDAL FREQUENCY MODULATION In the preceding sections, we discussed a number of specific amplitude modulation sys- tems in which the modulating signal was used to vary the amplitude of a sinusoidal or a pulse carrier. As we have seen, such systems are amenable to detailed analysis using the frequency-domain techniques we developed in preceding chapters. In another very important class of modulation techniques referred to as frequency modulation ( FM), the modulating signal is used to control the frequency of a sinusoidal carrier. Modulation sys- tems of this type have a number of advantages over amplitude modulation systems. As suggested by Figure 8.1 0, with sinusoidal amplitude modulation the peak amplitude of the envelope of the carrier is directly dependent on the amplitude of the modulating signal x(t), which can have a large dynamic range-i.e., can vary significantly. With frequency modulation, the envelope of the carrier is constant. Consequently, an FM transmitter can always operate at peak power. In addition, in FM systems, amplitude variations introduced over a transmission channel due to additive disturbances or fading can, to a large extent, be eliminated at the receiver. For this reason, in public broadcasting and a variety of other contexts, FM reception is typically better than AM reception. On the other hand, as we will see, frequency modulation generally requires greater bandwidth than does sinusoidal amplitude modulation. Frequency modulation systems are highly nonlinear and, consequently, are not as straightforward to analyze as are the amplitude modulation systems discussed in the pre- ceding sections. However, the methods we have developed in earlier chapters do allow us to gain some understanding of the nature and operation of these systems. We begin by introducing the general notion of angle modulation. Consider a sinu- soidal carrier expressed in the form c(t) = Acos(wct + Oc) = AcosO(t), (8.30) where O(t) = w ct + (} c and where w cis the frequency and(} c the phase of the carrier. Angle modulation, in general, corresponds to using the modulating signal to change or vary the angle O(t). One form that this sometimes takes is to use the modulating signal x(t) to vary the phase (} c so that the modulated signal takes the form y(t) = A cos[wct + Oc(t)], (8.31) where (} c is now a function of time, specifically of the form (} c(t) = Oo + kpx(t). (8.32) If x(t) is, for example, constant, the phase of y(t) will be constant and proportional to the amplitude of x(t). Angle modulation of the form of eq. (8.31) is referred to as phase modulation. Another form of angle modulation corresponds to varying the derivative of the angle proportionally with the modulating signal; that is, y(t) = A cos O(t), (8.33) where (8.34) 612 Communication Systems Chap.B For x(t) constant, y(t) is sinusoidal with a frequency that is offset from the carrier fre- quency we by an amount proportional to the amplitude of x(t). For that reason, angle modulation of the form of eqs. (8.33) and (8.34) is commonly referred to as frequency modulation. Although phase modulation and frequency modulation are different forms of angle modulation, they can be easily related. From eqs. (8.31) and (8.32), for phase modulation, dO(t) _ k dx(t) -----;[( - w (' + p -----;[(' (8.35) x(t) x(t) k::/ k::/ y(t) y(t) (a) (b) x(t) y(t) (c) Figure 8.32 Phase modulation, frequency modulation, and their relationship: (a) phase modulation with a ramp as the modulating signal; (b) frequency modulation with a ramp as the modulating signal; (c) frequency modulation with a step (the derivative of a ramp) as the modulating signal. Sec. 8.7 Sinusoidal Frequency Modulation 613 and thus, comparing eqs. (8.34) and (8.35), we see that phase modulating with x(t) is iden- tical to frequency modulating with the derivative of x(t). Likewise, frequency modulating with x(t) is identical to phase modulating with the integral of x(t). An illustration of phase modulation and frequency modulation is shown in Figures 8.32(a) and (b). In both cases, the modulating signal is x(t) = tu(t) (i.e., a ramp signal increasing linearly with time for t > 0). In Figure 8.32(c ), an example of frequency modulation is shown with a step (the derivative of a ramp) as the modulating signal [i.e., x(t) = u(t)]. The correspondence between Figures 8.32(a) and (c) should be evident. Frequency modulation with a step corresponds to the frequency of the sinusoidal carrier changing instantaneously from one value to another when x(t) changes value at t = 0, much as the frequency of a sinusoidal oscillator changes when the frequency setting is switched instantaneously. When the frequency modulation is a ramp, as in Figure 8.32(b) , the frequency changes linearly with time. This notion of a time-varying frequency is often best expressed in terms of the concept of instantaneous frequency. For y(t) = A cos O(t), (8.36) the instantaneous frequency of the sinusoid is defined as ·( ) _ WIt - ddO(ft)' (8.37) Thus, for y(t) truly sinusoidal [i.e., O(t) = (wet + 00)], the instantaneous frequency is we, as we would expect. For phase modulation as expressed in eqs. (8.31) and (8.32), the instantaneous frequency is We+ kp(dx(t)ldt), and for frequency modulation as expressed in eqs. (8.33) and (8.34), the instantaneous frequency is We + k1 x(t). Since frequency modulation and phase modulation are easily related, we will phrase the remaining discussion in terms of frequency modulation alone. To gain some insight into how the spectrum of the frequency-modulated signal is affected by the modulating signal x(t), it is useful to consider two cases in which the modulating signal is sufficiently simple so that some of the essential properties of frequency modulation become evident. 8. 7. 1 Narrowband Frequency Modulation Consider the case of frequency modulation with x(t) = Acoswmt. (8.38) From eqs. (8.34) and (8.37), the instantaneous frequency is Wi(t) = We + kt A COS Wmt, (8.39) which varies sinusoidally between We+ ktA and We- k1A. With we have 614 Communication Systems Chap.8 and y(t) = cos[wet + f x(t)dt] (8.40) = cos (wet+ ~w sinwntf +eo), Wm where 00 is a constant of integration. For convenience we will choose 00 = 0, so that y(t) = COS Wet + -~Sw in. [ Wmt l. (8.41) Wm The factor ~wlw 111 , which we denote by m, is defined as the modulation index for frequency modulation. The properties of FM systems tend to be different, depending on whether the modulation index m is small or large. The case in which m is small is referred to as narrowband FM. In general, we can rewrite eq. (8.41) as y(t) = cos( wet + m sin W111 t) (8.42) or y(t) = coswctcos(msinw 111t)- sinwctsin(msinwmt). (8.43) When m is sufficiently small ( << Tr/2), we can make the approximations cos(m sinwmt) = 1, (8.44) sin(msinwmt) = msinw 111 t, (8.45) so that eq. (8.42) becomes y(t) = cos Wet - m(sin W 111 t)(sin Wet). (8.46) The spectrum of y(t) based on this approximation is shown in Figure 8.33. We note that it has a similarity to AM-DSB/WC in that the carrier frequency is present in the spectrum and there are sidebands representing the spectrum of the modulating signal in eq. (8.38). However, in AM-DSB/WC the additional carrier injected is in phase with the modulated carrier, whereas, as we see in eq. (8.46) for the case of the narrowband FM, the carrier signal has a phase difference of Trl2 in relation to the amplitude-modulated carrier. The waveforms corresponding to AM-DSB/WC and FM are also very different. Figure 8.34(a) illustrates the narrowband FM waveform corresponding to eq. (8.46). For comparison, Figure 8.34(b) shows the AM-DSB/WC signal (8.47) For the narrowband FM signal of eq. (8.46), the bandwidth of the sidebands is equal to the bandwidth of the modulating signal, and in particular, although the approximation in the equation is based on assuming that m << Trl2, the bandwidth of the sidebands is otherwise independent of the modulation index m (i.e., it depends only on the bandwidth of the modulating signal, not on its amplitude). A similar statement applies for narrowband FM with a more general modulating signal. Sec. 8.7 Sinusoidal Frequency Modulation 615 Y(jw) 7T 7T m7T m7T 2 2 (!) Figure 8.33 Approximate spectrum for narrowband FM. (a) Figure 8.34 Comparison of narrowband FM and AM-DSB/WC: (b) (a) narrowband FM; (b) AM-DSB/WC. 8. 7.2 Wideband Frequency Modulation When m is large, the approximation leading to eq. (8.46) no longer applies, and the spec- trum of y(t) depends on both the amplitude and the spectrum of the modulating signal x(t). With y(t) expressed in the form of eq. (8.43), we note that the terms cos[m sinw 111 t] and sin[m sinw 111 t] are periodic signals with fundamental frequency w 111 • Thus, the Fourier transform of each of these signals is an impulse train with impulses at integer multiples of w 111 and amplitudes proportional to the Fourier series coefficients. The coefficients for these two periodic signals involve a class of functions referred to as Bessel functions of the first kind. The first term in eq. (8.43) corresponds to a sinusoidal carrier of the form cos wet amplitude modulated by the periodic signal cos[m sin w 111 t] and the second term to a sinusoidal carrier sin wet amplitude modulated by the periodic signal sin[m sinw 111 t]. Multiplication by the carrier signals has the effect in the frequency domain of translating the spectrum of eq. (8.43) to the carrier frequency, so that it is centered at plus and minus 616 Communication Systems Chap.8 We. In Figures 8.35(a) and (b) we illustrate, for w > 0, the the magnitude of the spectra of the two individual terms in eq. (8.43), and in Figure 8.35(c) the magnitude of the combined spectrum representing the modulated signal y(t). The spectrum of y(t) consists of impulses at frequencies ±we + nwm, n = 0, ± 1, ±2, ... , and is not, strictly speaking, band limited around ±we. However, the behavior of the Fourier series coefficients of cos[m sinwmt] and sin[m sin wmt] are such that the amplitude of the nth harmonic for In I > m can be considered negligible, and thus, the total bandwidth B of each sideband centered around +we and -we is effectively limited to 2mwm. That is, B = 2mwnz, (8.48) or, since m = k1Aiwm = dwlwm, B = 2k;-A = 2dw. (8.49) Comparing eqs. (8.39) and (8.49), we note that the effective bandwidth of each sideband is equal to the total excursion of the instantaneous frequency around the carrier (a) (b) Figure 8.35 Magnitude of spec- trum of wideband frequency modula- tion with m = 12: (a) magnitude of spectrum of cos wcfcos[msin wmt]; (b) magnitude of spectrum of sin wcfsin[msin wmt]; (c) combined spectral magnitude of (c) cos[ wet+ msin wmt]. Sec. 8.7 Sinusoidal Frequency Modulation 617 frequency. Therefore, for wideband FM, since we assume that m is large, the bandwidth of the modulated signal is much larger than the bandwidth of the modulating signal, and in contrast to the narrowband case, the bandwidth of the transmitted signal in wideband FM is directly proportional to amplitude A of the modulating signal and the gain factor k f. 8.7.3 Periodic Square-Wave Modulating Signal Another example that lends insight into the properties of frequency modulation is that of a modulating signal which is a periodic square wave. Referring to eq. (8.39), let k f = 1 so that ~w = A, and let x(t) be given by Figure 8.36. The modulated signal y(t) is illustrated in Figure 8.37. The instantaneous frequency is We+ ~w when x(t) is positive and We- ~w when x(t) is negative. Thus, y(t) can also be written as y(t) = r(t) cos[(w, + Aw )t] + r ~ ~ ~ )cos[(w, ~ Aw )t], (8.50) x(t) A T T T T t -2 -4 T 4 2 -A-r- Figure 8.36 Symmetric periodic square wave. y(t) J Figure 8.37 Frequency modulation u with a periodic square-wave modulat- u ing signal. 618 Communication Systems Chap.8 where r(t) is the symmetric square wave shown in Figure 8.38. Thus, for this particular modulating signal, we are able to recast the problem of determining the spectrum of the FM signal y(t) as the determination of the spectrum of the sum of the two AM signals in eq. (8.50). Specifically, 1 Y(jw) = [R(jw +)we+ jb,.w) + R(jw- )we- jb,.w)] 2 1 + 2[RT(jw +)we - jb,.w) + RT(jW -)we+ jb,.w )], (8.51) where R(jw) is the Fourier transform of the periodic square wave r(t) in Figure 8.38 and RT()w) is the Fourier transform of r(t- T/2). From Example 4.6, with T = 4T1, (8.52) and RT()w) = R(jw )e- JwT/2. (8.53) The magnitude of the spectrum of Y(jw) is illustrated in Figure 8.39. As with wideband FM, the spectrum has the general appearance of two sidebands, centered around We± dw, that decay for w <We - b,.w and w >We+ b,.w. r(t) ...] 11 I I I 3T T T 0 T T 3T -4 -2 -4 T 4 2 4 Figure 8.38 Symmetric square wave r(t) in eq. (8.50). Y(jw) 1 ~~~~1~1~11~11~1~11~1~~~~11~1~11~1~11~11~1~11~1~11~11~1~11~11~1~11~1~11~11~1~11~1~11~11~1~11~1~11~11~1~11~11~'~11 ___ _ (we~ Llw) We (we+ Llw) w Figure 8.39 Magnitude of the spectrum for w > 0 corresponding to fre- quency modulation with a periodic square-wave modulating signal. Each of the vertical lines in the figure represents an impulse of area proportional to the height of the line. Sec. 8.8 Discrete-Time Modulation 619 Systems for the demodulation of FM signals typically are of two types. One type of demodulation system corresponds to converting the FM signal to an AM signal through differentiation, while demodulation systems of the second type directly track the phase or frequency of the modulated signal. The foregoing discussion provides only a brief intro- duction to the characteristics of frequency modulation, and we have again seen how the basic techniques developed in the earlier chapters can be exploited to analyze and gain an insight into an important class of systems. 8.8 DISCRETE-TIME MODUlATION 8.8. 1 Discrete-Time Sinusoidal Amplitude Modulation A discrete-time amplitude modulation system is depicted in Figure 8.40, in which c[n] is the carrier and x[n] the modulating signal. The basis for our analysis of continuous- time amplitude modulation was the multiplication property for Fourier transforms- specifically, the fact that multiplication in the time domain corresponds to convolution in the frequency domain. As we discussed in Section 5.5, there is a corresponding property for discrete-time signals which we can use to analyze discrete-time amplitude modulation. Specifically, consider y[n] = x[n]c[n]. With X(eiw), Y(eiw), and C(eiw) denoting the Fourier transforms of x[n], y[n], and c[n], respectively, Y(eiw) is proportional to the periodic convolution of X(eiw) and C(eiw); that is, Y(eiw) = - 1 I X(eifJ)C(ei(w-fJ))d(). (8.54) 27T 27T Since X(eiw) and C(eiw) are periodic with a period of27T, the integration can be performed over any frequency interval of length 27T. Let us first consider sinusoidal amplitude modulation with a complex exponential carrier, so that (8.55) As we saw in Section 5.2, the Fourier transform of c[n] is a periodic impulse train; that is, +<X) C(eiw) = L 27TO(w -We + k27T), (8.56) k= -00 c[n] l x[n] --~0~--~ y[n] Figure 8.40 Discrete-time ampli- tude modulation. 620 Communication Systems Chap. a which is sketched in Figure 8.4l(b). With X(eiw) as illustrated in Figure 8.41(a), the spec- trum of the modulated signal is that shown in Figure 8.41(c). In particular, we note that Y(e.iw) = X(ei<w-wc)). This is the discrete-time counterpart to Figure 8.1, and here again, with x[n] real, the modulated signal will be complex. Demodulation is accomplished by multiplying by e- Jw,n to translate the spectrum back to its original location on the fre- quency axis, so that x[n] = y[n]e- Jw,n. (8.57) As explored in Problem 6.43, if We = 7T so that c[n] = ( -l)n, the result of modula- tion in the time domain is to change the algebraic sign of x[ n] for odd values of n, while in the frequency domain the consequence is the interchanging of high and low frequencies. Problem 6.44 explores the use of this type of modulation in utilizing a lowpass filter to achieve highpass filtering and vice versa. As an alternative to a complex exponential carrier, we can use a sinusoidal carrier, in which case, with x[n] real, the modulated signal y[n] will also be real. With c[n] = cos wen, the spectrum of the carrier consists of periodically repeated pairs of impulses at -21T -wM w 0 WM 21T (a) C(eiw) rn 2n I 2n I I -21T -21T+We 0 We 21T 21T+We w (b) Y(eiw) (we-wM) We (we+wM) w (c) Figure 8.41 (a) Spectrum of x[n]; (b) spectrum of c[n] = eiwcn; (c) spec- trum of y[n] = x[n]c[n]. Sec. 8.8 Discrete-Time Modulation 621 w = ±we+ k27T, as illustrated in Figure 8.42(b). With X(ejw) as shown in Figure 8.42(a), the resulting spectrum for the modulated signal is shown in Figure 8.42(c) and corresponds to replicating X(ejw) at the frequencies w = ±we + k27T. In order that the individual replications of X(ejw) do not overlap, we require that (8.58) and or, equivalently, (8.59) The first condition is identical to that in Section 8.2 for continous-time sinusoidal ampli- tude modulation, while the second results from the inherent periodicity of discrete-time X(eiw) 1\ ~ 1\ -2'TT -wM 0 WM 2'TT w (a) C(eiw) I r r I r r I r -2'TT -2'TT+We -we 0 We 2'TT-We 2'TT 2'TT+We w (b) Y(eiw) (2'TT-we-wM) I 1\ 1\ t 1\ \1\ I /\··· -2'TT -2'TT+We -we 0 ( We \ 2'TT-We 2'TT 2'TT+We w we-wM we+WM (c) Figure 8.42 Spectra associated with discrete-time modulation using a sinusoidal carrier: (a) spectrum of a bandlimited-signal x[n]; (b) spectrum of a sinusoidal carrier signal c[n] = cos wen; (c) spectrum of the modulated signal y[n] = x[n]c[n]. 622 Communication Systems Chap. a spectra. Combining eqs. (8.58) and (8.59), we see that for amplitude modulation with a sinusoidal carrier, we must restrict w c so that (8.60) Demodulation can be accomplished in a manner similar to that employed in con- tinuous time. As illustrated in Figure 8.43, multiplication of y[n] with the same carrier used in the modulator results in several replications of the spectrum of the original signal, one of which is centered about w = 0. By lowpass filtering to eliminate the unwanted replications of X(ejw), the demodulated signal is obtained. As should be evident from the foregoing discussion, analysis of discrete-time am- plitude modulation proceeds in a manner similar to that of continuous-time amplitude modulation, with only slight differences. For example, as explored in Problem 8.47, in the synchronous modulation and demodulation system, the effect of a phase difference or a frequency difference between the sinusoidal carriers in the modulator and demodulator is identical in both discrete and continuous time. In addition, just as in continuous time, we can use discrete-time sinusoidal AM as the basis for frequency-division multiplexing in cos wen Lowpass filter ~ H(eiw) y[n] w[n] X Ih x[n] -Wco Wco w Y(eiw) -21·~ !\1iL I L.. I T 1T -w 0 w 1T t 21T w -21T -1T -wco Wco 1T 21T w X(eiw) ~ ! L Figure 8.43 System and associated spectra for discrete-time synchronous -21T 21T w demodulation. Sec. 8.9 Summary 623 discrete time. Furthermore, as explored in Problem 8.48, we can also consider using a discrete-time signal to modulate a pulse train, leading to time-division multiplexing of discrete-time signals. The implementation of discrete-time multiplexing systems provides an excellent ex- ample of the flexibility of discrete-time processing in general and the importance of the operation of upsampling (see Section 7.5.2) in particular. Consider a discrete-time FDM system with M sequences that we wish to frequency-division multiplex. With M channels, it is required that the spectral energy for each input channel xi[n] be band limited; that is, 7T - < lwl < 7T. (8.61) M If the sequences originally occupied the entire frequency band corresponding, for example, to having sampled a set of continuous-time signals at the Nyquist rate, then they would first have to be converted to a higher sampling rate (i.e., upsampled) before frequency-division multiplexing. This idea is explored further in Problem 8.33. 8.8.2 Discrete-Time Transmodulation One context in which discrete-time modulation is widely used, together with the oper- ations of decimation, upsampling, and interpolation introduced in Chapter 7, is digital communication systems. Typically, in such systems continuous-time signals are trans- mitted over communication channels in the form of discrete-time signals, obtained by sampling. The continous-time signals are often in the form of time-division-multiplexed (TDM) or frequency-division-multiplexed (FDM) signals. The signals are then converted to discrete-time sequences whose values are represented digitally, for storage or long- distance transmission. In some systems, because of different constraints or requirements at the transmitting end and the receiving end, or because sets of signals that have been in- dividually multiplexed by different methods are then multiplexed together, there is often the requirement for converting from sequences representing TDM signals to sequences representing FDM signals or vice versa. This conversion from one modulation or multi- plexing scheme to another is referred to as transmodulation or transmultiplexing. In the context of digital communication systems, one obvious way of implementing transmulti- plexing is to convert back to continuous-time signals, demultiplex and demodulate, and then modulate and multiplex as required. However, if the new signal is then to be con- verted back to a discrete-time signal, it is clearly more efficient for the entire process to be carried out directly in the discrete-time domain. Figure 8.44 shows, in block diagram form, the steps involved in converting a discrete-time TDM signal to a discrete-time FDM signal. Note that, after demultiplexing the TDM signal, each channel must be upsampled in preparation for frequency-division multiplexing. 8.9 SUMMARY In this chapter, we have examined a number of the basic concepts associated with com- munication systems. In particular, we have examined the concept of modulation, in which a signal we wish to communicate is used to modulate a second signal referred to as the r ll TDM signal r I I I I Demultiplex 21T r 1 i 1 o (N channels) cos (N )n l x4[n] diid! Figure 8.44 Block diagram for TDM-to-FDM transmultiplexing. Sec. 8.9 Summary 625 carrier, and we have looked in detail at the concept of amplitude modulation. The proper- ties of amplitude modulation are most easily interpreted in the frequency domain through the multiplication property of the Fourier transform. Amplitude modulation with a com- plex exponential or sinusoidal carrier is typically used to shift the spectrum of a sig- nal in frequency and is applied, for example, in communication systems to place the spectrum in a frequency range suitable for transmission and to permit frequency-division multiplexing. Variations of sinusoidal amplitude modulation, such as the insertion of a carrier signal for asynchronous systems and single- and double-sideband systems, were discussed. We also examined several other forms of modulation -based communication. In this regard, we briefly introduced the concepts of frequency and phase modula- tion. Although these forms of modulation are more difficult to analyze in detail, it is possible to gain significant insight into their characteristics through the frequency domain. We further examined in some detail amplitude modulation of a pulsed signal, which led us to the concepts of time-division multiplexing and pulse-amplitude modulation, in which successive samples of a discrete-time signal are used to mod- ulate the amplitude of a sequence of pulses. This led in tum to an examination of discrete-time modulation and digital communication, in which the flexibility of discrete- time processing facilitates the design and implementation of more sophisticated communication systems involving concepts such as pulse-code modulation and transmodulation. Chapter 8 Probleml The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining two sections contain problems belonging to the basic and advanced categories, respectively. BASIC PROBLEMS WITH ANSWERS 8.1. Let x(t) be a signal for which X(jw) = 0 when lwl > WM. Another signal y(t) is specified as having the Fourier transform Y(jw) = 2X(j(w - we)). Determine a signal m(t) such that x(t) = y(t)m(t). 8.2. Let x(t) be a real-valued signal for which X(jw) = 0 when lwl > 1,0007T. Suppos- ing that y(t) = ejwct x(t), answer the following questions: (a) What constraint should be placed on We to ensure that x(t) is recoverable from y(t)? (b) What constraint should be placed on We to ensure that x(t) is recoverable from (Re{y(t)}? 626 Communication Systems Chap.B 8.3. Let x(t) be a real-valued signal for which X(jw) = 0 when lwl > 2,0007T. Ampli- tude modulation is performed to produce the signal g(t) = x(t) sin(2,0007Tt). A proposed demodulation technique is illustrated in Figure P8.3 where g(t) is the input, y(t) is the output, and the ideallowpass filter has cutoff frequency 2,0007T and passband gain of 2. Determine y(t). Ideal g(t) ..,__ _. ..,.~I lowpass 1----...._ y(t) filter cos(2000'1Tt) Figure P8.3 8.4. Suppose x(t) = sin 2007Tt + 2 sin 4007Tt and g(t) = x(t) sin 4007Tt. If the product g(t)(sin 4007Tt) is passed through an ideallowpass filter with cutoff frequency 4007T and passband gain of 2, determine the signal obtained at the output of the lowpass filter. 8.5. Suppose we wish to transmit the signal sin 1,0007Tt x(t ) = 7Tt using a modulator that creates the signal w(t) = (x(t) + A) cos(l0,0007Tt). Determine the largest permissible value of the modulation index m that would allow asynchronous demodulation to be used to recover x(t) from w(t). For this problem, you should assume that the maximum magnitude taken on by a side lobe of a sine function occurs at the instant of time that is exactly halfway between the two zero- crossings enclosing the side lobe. 8.6. Assume that x(t) is a signal whose Fourier transform X(jw) is zero for lwl > WM. The signal g(t) may be expressed in terms of x(t) as where* denotes convolution and We> WM. Determine the value of the constant A such that Chap. 8 Problems 627 AsinwMt x(t) = (g(t) cos Wet)* . 1Tt 8.7. An AM-SSB/SC system is applied to a signal x(t) whose Fourier transform X(jw) is zero for lw I > w M. The carrier frequency w c used in the system is greater than w M. Let g(t) denote the output of the system, assuming that only the upper sidebands are retained. Let q(t) denote the output of the system, assuming that only the lower sidebands are retained. The system in Figure P8.7 is proposed for converting g(t) into q(t). How should the parameter w0 in the figure be related to we? What should be the value of passband gain A? g(t) 1------i~ q (t) coswot Figure P8. 7 8.8. Consider the modulation system shown in Figure P8.8. The input signal x(t) has a Fourier transform X(jw) that is zero for lw I > w M. Assuming that We > w M, answer the following questions: (a) Is y(t) guaranteed to be real if x(t) is real? (b) Can x(t) be recovered from y(t)? -J· W>O H(jw)= { . ' J ,w<O x(t) y(t) Figure P8.8 8.9. Two signals x1 (t) and x2(t), each with a Fourier transform that is zero for lwl > we, are to be combined using frequency-division multiplexing. The AM-SSB/SC technique of Figure 8.21 is applied to each signal in a manner that retains the lower sidebands. The carrier frequencies used for x 1( t) and x2(t) are We and 2we, respec- tively. The two modulated signals are then summed together to obtain the FDM signal y(t). 628 Communication Systems Chap. a (a) For what values of w is Y(jw) guaranteed to be zero? (b) Specify the values of A and w 0 so that XJ (t) = [{ y(t) * sin wot} l A sin Wet ~ COS Wot * Trt , where* denotes convolution. 8.10. A signal x(t) is multiplied by the rectangular pulse train c(t) shown in Figure P8.10. (a) What constraint should be placed on X(jw) to ensure that x(t) can be recovered from the product x(t)c(t) by using an ideallowpass filter? (b) Specify the cutoff frequency w c and the passband gain A of the ideal lowpass filter needed to recover x(t) from x(t)c(t). [Assume that X(jw) satisfies the constraint determined in part (a).] c(t) r 0 25X10-3sec ~ - 1_. ...... - :---- ... 0 t(sec) Figure P8. 1 0 8.11. Let X c(t) = L akejkw,r, k=-X where a0 = 0 and a 1 =rf 0, be a real-valued periodic signal. Also, let x(t) be a signal with X(jw) = 0 for lwl 2: wJ2. The signal x(t) is used to modulate the carrier c(t) to obtain y(t) = x(t)c(t). (a) Specify the passband and the passband gain of an ideal bandpass filter so that, with input y(t), the output of the filter is g(t) = (a I ejwct +a~ e- jw,.t)x(t). (b) If a1 = latlej<ta1, show that g(t) = Acos(wct + cp)x(t), and express A and cp in terms of la1l and <ta1. 8.12. Consider a set of 10 signals xi(t), i = 1, 2, 3, ... , 10. Assume that each Xi(t) has Fourier transform such that Xi(jw) = 0 for lwl 2: 2,00071'. All10 signals are to be time-division multiplexed after each is multiplied by a carrier c(t) shown in Figure P8.12. If the period T of c(t) is chosen to have the maximum allowable value, what is the largest value of~ such that all 10 signals can be time-division multiplexed? Chap. 8 Problems 629 - - - -T 0 T 2T Figure P8. 12 8.13. A class of popularly used pulses in PAM are those which have a raised cosine fre- quency response. The frequency response of one of the members of this class is P(jw) = { i(1 +coswJ'), 0::::; lwl::::; ~~, 0, elsewhere where T1 is the intersymbol spacing. (a) Determine p(O). (b) Determine p(kTI), where k = ± 1, ±2, .... 8.14. Consider the frequency-modulated signal y(t) = cos( wet + m cos Wmt), where we>> Wm and m << 7T/2. Specify an approximation to Y(jw) for w > 0. 8.15. For what values of w 0 in the range -7r < w 0 ::::; 7T is amplitude modulation with carrier ejwon equivalent to amplitude modulation with carrier cos w 0 n? 8.16. Suppose x[n] is a real-valued discrete-time signal whose Fourier transform X(ejw) has the property that . 7T X(elw) = 0 for S ::::; w ::::; 'TT. We use x[n] to modulate a sinusoidal carrier c[n] = sin(57T/2)n to produce y[n] = x[n]c[n]. Determine the values of win the range 0 ::::; w ::::; 7T for which Y(ejw) is guaranteed to be zero. 8.17. Consider an arbitrary finite-duration signal x[n] with Fourier transform X(ejw). We generate a signal g[n] through insertion of zero-valued samples: x[n/4], n = 0, ±4, ±8, ± 12, ... g [ n ] = [ ] X(4) n = { 0, otherwise The signal g[n] is passed through an ideallowpass filter with cutoff frequency 7T/4 and passband gain of unity to produce a signale q:[n ]. Finally, we obtain y[n] = q[n] cos n). For what values of w is Y(ejw) guaranteed to be zero? 630 Communication Systems Chap. 8 8.18. Let x[n] be a real-valued discrete-time signal whose Fourier transform X(ejw) is zero for w 2: 7T/4. We wish to obtain a signal y[n] whose Fourier transform has the property that, in the interval -7r < w :::; 1T, X(ej(w-~)), '!!_ < w < 37T 2 - 4 Y(ejw) = X(ej(w+~)), - 37T < w :::; 7T { 4 2 0, otherwise The system in Figure P8.18 is proposed for obtaining y[n] from x[n]. Determine constraints that the frequency response H ( ejw) of the filter in the figure must satisfy for the proposed system to work. x[n] y[n] Figure PS. 18 8.19. Consider 10 arbitrary real-valued signals Xi[n], i = 1, 2, ... , 10. Suppose each Xi[n] is upsampled by a factor of N, and then sinusoidal amplitude modulation is applied to it with carrier frequency wi = i1TI10. Determine the value of N which would guarantee that all 10 modulated signals can be summed together to yield an FDM signal y[n] from which each Xi[n] can be recovered. 8.20. Let v1 [n] and v2 [n] be two discrete-time signals obtained through the sampling (without aliasing) of continuous-time signals. Let be a TDM signal, where, fori = 1, 2, n = 0, ±2, ±4, ±6, ... otherwise The signal y[n] is processed by the systemS depicted in Figure P8.20 to obtain a signal g[n]. For the two filters used inS, lwl:::; ~ ~<w:::;1T· Chap. 8 Problems 631 Determine the signal p[n] used in S such that g[n] represents frequency-division multiplexing of v1 [n] and v2 [n]. p[n] + y[n] g[n] + p[n-1] Figure P8.20 BASIC PROBLEMS 8.21. In Sections 8.1 and 8.2, we analyzed the sinusoidal amplitude modulation and de- modulation system of Figure 8.8, assuming that the phase 8 c of the carrier signal was zero. (a) For the more general case of arbitrary phase 8 c in the figure, show that the signal in the demodulation system can be expressed as 1 1 w(t) = 2x(t) + 2x(t)cos(2wct + 28c). (b) If x(t) has a spectrum that is zero for lw I 2:: w M, determine the relationships required among w co [the cutoff frequency of the ideallowpass filter in Figure 8.8(b)], We (the carrier frequency), and WM so that the output of the lowpass filter is proportional to x(t). Does your answer depend on the carrier phase 8 c? 8.22. In Figure P8.22(a), a system is shown with input signal x(t) and output signal y(t). The input signal has the Fourier transform X(jw) shown in Figure P8.22(b). Deter- mine and sketch Y(jw ), the spectrum of y(t). 1 1 n 1 n ~ y(t) I I I -5w -3w 3w 5w w -3w 3w w cos(5wt) cos(3wt) (a) Figure P8.22 632 Communication Systems Chap.8 X(jw) ~ (1) -2w 2w (b) Figure P8.22 Continued 8.23. In Section 8.2, we discussed the effect of a loss of synchronization in phase between the carrier signals in the modulator and demodulator in sinusoidal amplitude modu- lation. We showed that the output of the demodulation is attenuated by the cosine of the phase difference, and in particular, when the modulator and demodulator have a phase difference of Tr/2, the demodulator output is zero. As we demonstrate in this problem, it is also important to have frequency synchronization between the modulator and demodulator. Consider the amplitude modulation and demodulation systems in Figure 8.8 with () c = 0 and with a change in the frequency of the demodulator carrier so that w(t) = y(t) cos wc~t, where y(t) = x(t) cos Wet. Let us denote the difference in frequency between the modulator and demodulator as ~w (i.e., wd- We = ~w ). Also, assume that x(t) is band limited with X(jw) = 0 for lw I 2 w M, and assume that the cutoff frequency w co of the lowpass filter in the demodulator satisfies the inequality (a) Show that the output of the lowpass filter in the demodulator is proportional to x(t) cos(~wt). (b) If the spectrum of x(t) is that shown in Figure P8.23, sketch the spectrum of the output of the demodulator. X(jw) ~ wM w Figure P8.23 8.24. Figure P8.24 shows a system to be used for sinusoidal amplitude modulation, where x(t) is band limited with maximum frequency WM, so that X(jw) = 0, lwl > WM. Chap. 8 Problems 633 As indicated, the signal s(t) is a periodic impulse train with period T and with an offset from t = 0 of d. The system H(jw) is a bandpass filter. (a) With Li = 0, WM = 7ri2T, w 1 = 7r!T, and wh = 37r/T, show that y(t) is pro- portional to x(t) cos wet, where We = 27r!T. (b) If WM, w 1, and wh are the same as given in part (a), but dis not necessarily zero, show that y(t) is proportional to x(t) cos(wct + Oc), and determine We and 0 c as a function of T and d. (c) Determine the maximum allowable value of WM relative toT such that y(t) is proportional to x(t) cos( wet + 0 c). x(t) H(jw) y(t) s(t) s(t) 1-T-1 t t t1 I t t 0 Ll (T +Ll) (2T +Ll) H{jw} AI -wh -w.R w.R wh w Figure P8.24 8.25. A commonly used system to maintain privacy in voice communication is a speech scrambler. As illustrated in Figure P8.25(a), the input to the system is a normal speech signal x(t) and the output is the scrambled version y(t). The signal y(t) is transmitted and then unscrambled at the receiver. We assume that all inputs to the scrambler are real and band limited to the frequency w M; that is, X (j w) = 0 for lw I > w M. Given any such input, our pro- posed scrambler permutes different bands of the spectrum of the input signal. In addition, the output signal is real and band limited to the same frequency band; that is, Y(jw) = 0 for lw I > w M. The specific algorithm for the scrambler is Y(jw) = X(j(w- WM)), w > 0, Y(jw) = X(j(w + WM)), w < 0. (a) If X(jw) is given by the spectrum shown in Figure P8.25(b), sketch the spec- trum of the scrambled signal y(t). (b) Using amplifiers, multipliers, adders, oscillators, and whatever ideal filters you find necessary, draw the block diagram for such an ideal scrambler. (c) Again using amplifiers, multipliers, adders, oscillators, and ideal filters, draw a block diagram for the associated unscrambler. 634 Communication Systems Chap.B x(t)~ x(t) (Normal speech) (a) X(jw) & (b) Figure P8.25 8.26. In Section 8.2.2, we discussed the use of an envelope detector for asynchronous demodulation of an AM signal of the form y(t) = [x(t) + A] cos(wct + Oc). An alternative demodulation system, which also does not require phase synchroniza- tion, but does require frequency synchronization, is shown in block diagram form in Figure P8.26. The lowpass filters both have a cutoff frequency of We. The signal y(t) = [x(t) +A] cos(wct + Oc), with Oc constant but unknown. The signal x(t) is band limited with X(jw) = 0, lwl > WM, and with WM <We. As we required for the use of the envelope detector, x(t) +A > 0 for all t. Show that the system in Figure P8.26 can be used to recover x(t) from y(t) without knowledge of the modulator phase ec · cos wet Low pass filter Square y(t) r(t) root Low pass filter sin wet Figure P8.26 8.27. As discussed in Section 8.2.2, asynchronous modulation-demodulation requires the injection of the carrier signal so that the modulated signal is of the form Chap. 8 Problems 635 y(t) = [A + x(t)] cos(wct + Oc), (P8.27-1) where A+ x(t) > 0 for all t. The presence of the carrier means that more transmitter power is required, representing an inefficiency. (a) Let x(t) = cos WMt with WM <We and A+ x(t) > 0. For a periodic signal y(t) with period T, the average power over time is defined asPy = (1/T) JT y2(t) dt. Determine and sketch Py for y(t) in eq. (P8.27-1). Express your answer as a function of the modulation index m, defined as the maximum absolute value of x(t) divided by A. (b) The efficiency of transmission of an amplitude-modulated signal is defined to be the ratio of the power in the sidebands of the signal to the total power in the signal. With x(t) = coswMt, and with WM <We and A+ x(t) > 0, determine and sketch the efficiency of the modulated signal as a function of the modulation index m. 8.28. In Section 8.4 we discussed the implementation of single-sideband modulation using 90° phase-shift networks, and in Figures 8.21 and 8.22 we specifically illustrated the system and associated spectra required to retain the lower sidebands. Figure P8.28(a) shows the corresponding system required to retain the upper sidebands. ,....--~~y,(t) x(t) __{_ +_j _.w_>_O ___. Xp(t) ~~ y(t) H(jw)= . -J ,W< 0 '-- (a) X(jw) w (b) Figure P8.28 636 Communication Systems Chap. 8 (a) With the same X(jw) illustrated in Figure 8.22, sketch Y1( jw ), Y2(jw ), and Y(jw) for the system in Figure P8.28(a), and demonstrate that only the upper sidebands are retained. (b) For X(jw) imaginary, as illustrated in Figure P8.28(b), sketch Y 1(jw), Y2(jw), and Y(jw) for the system in Figure P8.28(a), and demonstrate that, for this case also, only the upper sidebands are retained. 8.29. Single-sideband modulation is commonly used in point-to-point voice communica- tion. It offers many advantages, including effective use of available power, con- servation of bandwidth, and insensitivity to some forms of random fading in the channel. In double-sideband suppressed carrier (DSB/SC) systems the spectrum of the modulating signal appears in its entirety in two places in the transmitted spectrum. Single-sideband modulation eliminates this redundancy, thus conserving bandwidth and increasing the signal-to-noise ratio within the remaining portion of the spectrum that is transmitted. In Figure P8.29(a), two systems for generating an amplitude-modulated single-sideband signal are shown. The system on the top can be used to generate a single-sideband signal for which the lower sideband is retained, and the system on the bottom can produce a single-sideband signal for which the upper sideband is retained. (a) For X(jw) as shown in Figure P8.29(b), determine and sketch S(jw), the Fourier transform of the lower sideband modulated signal, and R(jw ), the Fourier transform of the upper sideband modulated signal. Assume that We> W3. The upper sideband modulation scheme is particularly useful with voice communication, as any real filter has a finite transition region for the cutoff (i.e., near we). This region can be accommodated with negligible distortion, since the voice signal does not have any significant energy near w = 0 (i.e., for jwj < Wt = 27T X 40Hz). (b) Another procedure for generating a single-sideband signal is termed the phase- shift method and is illustrated in Figure P8.29(c). Show that the single- sideband signal generated is proportional to that generated by the lower sideband modulation scheme of Figure P8.29(a) [i.e., p(t) is proportional to s(t)]. (c) All three AM-SSB signals can be demodulated using the scheme shown on the right-hand side of Figure P8.29(a). Show that, whether the received sig- nal is s(t), r(t), or p(t), as long as the oscillator at the receiver is in phase with oscillators at the transmitter, and w = We, the output of the demodulator is x(t). The distortion that results when the oscillator is not in phase with the trans- mitter, called quadrature distortion, can be particularly troublesome in data communication. 8.30. Amplitude modulation with a pulse-train carrier may be modeled as ip Figure P8.30(a). The output of the system is q(t). (a) Let x(t) be a band-limited signal [i.e., X(jw) = 0, jwj 2: 7TIT], as shown in Figure P8.30(b ). Determine and sketch R(jw) and Q(jw ). ~~ 141 I x(t) s(t) - w w (J) _______. x(t) r(t) (a) X(jw) (b) (c) Figure P8.29 637 638 Communication Systems Chap. 8 (b) Find the maximum value of~ such that w(t) = x(t) with an appropriate filter M(jw). (c) Determine and sketch the compensating filter M(jw) such that w(t) = x(t). PAM system h(t) x(t) r(t) _ffi_ ,_....,.._q(_t). .... ~ -ll/2 ll/2 p(t)= L o(t-nT) -----~=~~---------------------1 (a) XOw) cb w .:rr T (b) Figure P8.30 8.31. Let x[n] be a discrete-time signal with spectrumX(ejw), and let p(t) be a continuous- time pulse function with spectrum P(jw ). We form the signal +oc y(t) = L x[n]p(t - n). n= -oc (a) Determine the spectrum Y(jw) in terms of X(ejw) and P(jw ). (b) If p(t) = { cos 81Tt, O:st:sl 0, elsewhere ' determine P(jw ) and Y (j w). 8.32. Consider a discrete-time signal x[n] with Fourier transform shown in Figure P8.32(a). The signal is amplitude modulated by a sinusoidal sequence, as indi- cated in Figure P8.32(b). Chap. 8 Problems 639 (a) Determine and sketch Y(e.iw), the Fourier transform of y[n]. (b) A proposed demodulation system is shown in Figure P8.32(c). For what value of fJ c. w1p, and G will x[n] = x[n]? Are any restrictions on we and w1P necessary to guarantee that x[n] is recoverable from y[n]? 1T (a) x[n]-...,.Ty[n] (b) Figure P8.32 8.33. Let us consider the frequency-division multiplexing of discrete-time signals xi[n], i = 0, 1, 2, 3. Furthermore, each Xi[n] potentially occupies the entire frequency band ( -7r < w < 1r). The sinusoidal modulation of upsampled versions of each of these signals may be carried out by using either double-sideband techniques or single-sideband techniques. (a) Suppose each signal Xi[n] is appropriately upsampled and then modulated with cos[i(1TI4)n]. What is the minimum amount ofupsampling that must be carried out on each xi[n] in order to ensure that the spectrum of the FDM signal does not have any aliasing? (b) If the upsampling of each xi[n] is restricted to be by a factor of 4, how would you use single-sideband techniques to ensure that the FDM signal does not have any aliasing? Hint: See Problem 8.17. 640 Communication Systems Chap.8 ADVANCED PROBLEMS 8.34. In discussing amplitude modulation systems, modulation and demodulation were carried out through the use of a multiplier. Since multipliers are often difficult to implement, many practical systems use a nonlinear element. In this problem, we illustrate the basic concept. In Figure P8.34, we show one such nonlinear system for amplitude modu- lation. The system consists of squaring the sum of the modulating signal and the carrier and then bandpass filtering to obtain the amplitude-modulated signal. Assume that x(t) is band limited, so that X(jw) = 0, lwl > WM. Deter- mine the bandpass filter parameters A, w 1, and wh such that y(t) is an amplitude- modulated version of x(t) [i.e., such that y(t) = x(t) cos wet]. Specify the necessary constraints, if any, on we and w M. y(t) Figure P8.34 8.35. The modulation -demodulation scheme proposed in this problem is similar to sinu- soidal amplitude modulation, except that the demodulation is done with a square wave with the same zero-crossings as cos wet. The system is shown in Figure P8.35(a); the relation between cos wet and p(t) is shown in Figure P8.35(b). Let the input signal x(t) be a band-limited signal with maximum frequency WM <We, as shown in Figure P8.35(c). (a) Sketch and dimension the real and imaginary parts of Z(jw ), P(jw ), and Y(jw ), the Fourier transforms of z(t), p(t), and y(t), respectively. (b) Sketch and dimension a filter H(jw) so that v(t) = x(t). Modulation Demodulation 1--------, ---------------------, I I x(t) ~@-..__z(_t) __- t•~@~o---Y(_t)---!•~~~~~ v(t) l t t L::.f: cos wet p(t) I ---------------------1 (a) Figure P8.35 Chap. 8 Problems 641 (b) Gte {X(jw)} ~m{X(jw)} ,!, (c) Figure P8.35 Continued 8.36. The accurate demultiplexing-demodulation of radio and television signals is gener- ally performed using a system called the superheterodyne receiver, which is equiv- alent to a tunable filter. The basic system is shown in Figure P8.36(a). (a) The input signal y(t) consists of the superposition of many amplitude-modulated signals that have been multiplexed using frequency-division multiplexing, so that each signal occupies a different frequency channel. Let us consider one such channel that contains the amplitude-modulated signal y 1 (t) = x1 (t) cos wet, with spectrum Y1( jw) as depicted at the top of Figure P8.36(b). We want to de- multiplex and demodulate y 1 (t) to recover the modulating signal x1( t), using the system of Figure P8.36(a). The coarse tunable filter has the spectrum H 1( jw) shown at the bottom ofFigureP8.36(b). Determine the spectrumZ(jw) of the in- put signal to the fixed selective filter H2(jw ). Sketch and label Z(jw) for w >0. (b) The fixed frequency-selective filter is a bandpass type centered around the fixed frequency w 1 , as shown in Figure P8.36(c). We would l~ke the output of the filter with spectrum H2(jw) to be r(t) = XJ(t)cosw 1t. In terms of We and WM, what constraint must WT satisfy to guarantee that an undistorted spectrum of x1( t) is centered around w = w f? (c) What must G, a, and {3 be in Figure P8.36(c) so that r(t) = x1 (t) cos w 1t? 8.37. The following scheme has been proposed to perform amplitude modulation: The in- put signal x(t) is added to the carrier signal cos wet and then put through a nonlinear 642 Communication Systems Chap. 8 We Oscilator We cos(we + wt)t It Coarse Fixed tunable z(t) y(t) -----. selective r(t) ~ To X ""? filter filter demodulator H1(jw) H2(jw) (a) Y(jw) (we-wM) We(we+wM) w Input signal K Coarse tunable filter H2(jw) (b) Gl.__________.__ ~I- a Wt J3 w (c) Figure P8.36 Chap. 8 Problems 643 device, so that the output z(t) is related to the input by z(t) = eY<n - 1, y(t) = x(t) + cos Wet. This is illustrated in Figure P8.37(a). Such a nonlinear relation can be implemented through the current-voltage characteristics of a diode, where, with i(t) and v(t) the diode and current and voltage, respectively, i(t) = Ioeav(t) - 1 (a real). To study the effects of nonlinearity, we can examine the spectrum of z(t) and how it relates to X(jw) and we. To accomplish this, we use the power series for eY, which is 1 1 eY = 1 + y + 2 y2 + 6 y3 + .... (a) If the spectrum of x(t) is given by Figure P8.37(b), and if We = 100w 1, sketch and label Z(jw ), the spectrum of z(t), using the first four terms in the power series for eY. (b) The bandpass filter has parameters as shown in Figure P8.37(c). Determine the range of a and the range of f3 such that r(t) is an amplitude-modulated version of x(t). x(t) -TI z=e'-1 1 z(t). ~ r(t) coswct (a) X(jw) 11 -w1 (1)1 (J) (b) H(jw) n 11 n -J3 -a a J3 (J) (c) Figure P8.37 8.38. In Figure P8.38(a), a communication system is shown that transmits a band-limited signal x(t) as periodic bursts of high-frequency energy. Assume that X(jw) = 0 for lwl > WM. Two possible choices, m 1 (t) and m2(t), are considered for the modulating signal m(t). m1( t) is a periodic train of sinusoidal pulses, each of duration D, as shown in Figure P8.38(b). That is, 644 Communication Systems Chap. 8 oc m1 (t) = L, p(t - kT), k= -oc where ( ) _ { COS Wet, ltl < (D/2) p t - 0, ltl > (D/2) . m2(t) is cos Wet periodically blanked or gated; that is, m2(t) = g(t) cos wet, where g(t) is as shown in Figure P8.38(b ). m(t) coswct x(t)-~ ~_d;_ r(t) (a) p(t) :-n-1-""-, -012 ~- _v_ ~ v:=\JD/2 m1(t) _0 12 :-_n_v -~ _ 1_ v-:""JJ-D/2 g(t) I I I ' -D/2 D/2 T m2(t) :-_n~ _v -_ 1_ v-:""=-\J, (b) Figure P8.38 Chap. 8 Problems 645 The following relationships between the parameters T, D, We, and WM are assumed: D<T, 27T We>> D' 27T T >2wM. Also, assume that [sin(x)]/x is negligible for x >> 1. Determine whether, for some choice of w1p, either m1 (t) or m2(t) will result in a demodulated signal x(t). For each case in which your answer is yes, determine an acceptable range for WJp· 8.39. Suppose we wish to communicate one of two possible messages: message m0 or message m1• To do so, we will send a burst of one of two frequencies over a time interval of length T. Note that Tis independent of which message is being trans- mitted. For message m0 we will send cos w 0t, and for message m1 we will send cosw 1t. Thus, a burst b(t) will look as shown in Figure P8.39(a). Such a communi- m 0(t) lr\/\J'J\ m 1(t) ~1VV\fV\f'T (a) cosw 0 t Line ""m0 "" Choose maximum b(t) of the ""m 0 ""or absolute ""m1"" values Line ""m 1"" (b) Figure P8.39 646 Communication Systems Chap. a cation system is called frequency shift keying (FSK). When the burst of frequency b(t) is received, we wish to determine whether it represents message m0 or message m1• To accomplish this, we do as illustrated in Figure P8.39(b). (a) Show that the maximum difference between the absolute values of the two lines in Figure P8.39(b) occurs when cos w 0 t and cos w 1t have the relationship i,T cos wotcos w 1t dt = 0. (b) Is it possible to choose w 0 and w 1 such that there is no interval of length T for which LT cosw0tcosw,tdt = 0? 8.40. In Section 8.3, we discussed the use of sinusoidal modulation for frequency- division multiplexing whereby several signals are shifted into different frequency bands and then summed for simultaneous transmission. In the current problem, we explore another multiplexing concept referred to as quadrature multiplexing. In this multiplexing procedure, two signals can be transmitted simultaneously in the same frequency band if the two carrier signals are 90° out of phase. The multiplexing sys- tem is shown in Figure P8.40(a) and the demultiplexing system in Figure P8.40(b). x 1(t) and x2(t) are both assumed to be band limited with maximum frequency WM, so that X1( jw) = X2(jw) = 0 for lwl > WM. The carrier frequency We is assumed to be greater than WM. Show that Y1 (t) = x 1 (t) and Y2(t) = x2(t). r(t) =multiplexed signal (a) Figure P8.40 8.41. In Problem 8.40, we introduced the concept of quadrature multiplexing, whereby two signals are summed after each has been modulated with carrier signals of iden- tical frequency, but with a phase difference of 90°. The corresponding discrete-time multiplexer and demultiplexer are shown in Figure P8.41. The signals x 1 [n] and x2 [n] are both assumed to be band limited with maximum frequency WM, so that Chap. 8 Problems 647 r(t) } Demultiplexed outputs H(jw) 12 (I) (b) Figure P8.40 Continued r[n] =multiplexed signal (a) r[n] ra----y2[n] Demultiplexed outputs sin wen (b) Figure P8.41 648 Communication Systems Chap. a (a) Determine the range of values for We so that x 1 [n] and x2[n] can be recovered from r[n]. (b) With We satisfying the conditions in part (a), determine H(efw) so that YI [n] = XI [n] and y2[n] = x2[n]. 8.42. In order to avoid intersymbol interference, pulses used in PAM systems are designed to be zero at integer multiples of the symbol spacing TI. In this problem, we develop a class of pulses which are zero at t = kTI, k = ± 1, ±2, ±3, .... Consider a pulse PI (t) that is real and even and that has a Fourier transform PI (jw ). Also, assume that P1 (- jw + j ~ ) = - P1 ~w + j ~ } 0 <; w -; (a) Define a periodic sequence PI (t) with Fourier transform P1(jw) = m~oo P1 (jw- jm ~7} and show that P- I (}.W ) = - p-I (]·W - } .2T7;T ) · (b) Use the result of the previous part to show that for some T PI (t) = 0, t = kT, k = 0, ±2, ±4, .... (c) Use the result of the previous part to show that Pt(kTt) = 0, k = ± 1, ±2, ±3, .... (d) Show that a pulse p(t) with Fourier transform 1 + PI(jw), lwl ~ ¥; P(jw) = Pt(jw), f; ~ lwl ~ ~7 { 0, otherwise also has the property that p(kTI) = 0, k = ±1, ±2, ±3, .... 8.43. The impulse response of a channel used for PAM communication is specified by h(t) = 10,000e-I,OOOtu(t). It is assumed that the phase response of the channel is approximately linear in the bandwidth of the channel. A pulse that is received after passing through the channel is processed using an LTI systemS with impulse response g(t) in order to compen- sate for the nonuniform gain over the channel bandwidth. Chap. 8 Problems 649 (a) Verify that if g(t) has the Fourier transform G(jw) = A+ }Bw, where A and Bare real constants, then g(t) can compensate for the nonuniform gain over the channel bandwidth. Determine the values of A and B. (b) It is proposed that S be implemented with the system shown in Figure P8.43. Determine the values of the gain factors a and f3 in this system. x(t) x(t) y(t) (Received signal (Received signal before compensation) after compensation) Figure P8.43 8.44. In this problem, we explore an equalization method used to avoid intersymbol in- terference caused in PAM ~ystems by the channel having nonlinear phase over its bandwidth. When a PAM pulse with zero-crossings at integer multiples of the symbol spacing T 1 is passed through a channel with nonlinear phase, the received pulse may no longer have zero-crossings at times that are integer multiples ofT 1 • Therefore, in order to avoid intersymbol interference, the received pulse is passed through a zero- forcing equalizer, which forces the pulse to have zero-crossings at integer multiples of T1• This equalizer generates a new pulse y(t) by summing up weighted and shifted versions of the received pulse x(t). The pulse y(t) is given by N y(t) = L GtX(t- lTJ), (P8.44-1) 1=-N where the a1 are all real and are chosen such that ~ 6: k = 0 y(kT1) { k = ±1, ±2, ±3, ... , ±N. (a) Show that the equalizer is a filter and determine its impulse response. (b) To illustrate the selection of the weights a1, let us consider an example. If x(OTJ) = 0.0, x(- TJ) = 0.2, x(TJ) = -0.2, and x(kT1) = 0 for lkl > 1, de- termine the values of a0 , a 1, and a_ 1 such that y(±TJ) = 0. 650 Communication Systems Chap.8 8.45. A band-limited signal x(t) is to be transmitted using narrowband FM techniques. That is, the modulation index m, as defined in Section 8.7, is much less than Tr/2. Before x(t) is transmitted to the modulator, it is processed so that X(jw )iw =O = 0 and lx(t)l < 1. This normalized x(t) is now used to angle-modulate a carrier to form the FM signal (a) Determine the instantaneous frequency w;. (b) Using eqs. (8.44) and (8.45), the narrowband assumption (m << Tr/2), and the preceding normalization conditions, show that (c) What is the relationship among the bandwidth of y(t), the bandwidth of x(t), and the carrier frequency We? 8.46. Consider the complex exponential function of time, s(t) = ei8(t)' (P8.46-1) where O(t) = w t2 0 /2. Since the instantaneous frequency w; = d8/dt is also a function of time, the signal s(t) may be regarded as an FM signal. In particular, since the signal sweeps linearly through the frequency spectrum with time, it is often called a frequency ""chirp"" or ""chirp signal."" (a) Determine the instantaneous frequency. (b) Determine and sketch the magnitude and phase of the Fourier transform of the ""chirp signal."" To evaluate the Fourier transform integral, you may find it help- ful to complete the square in the exponent in the integrand and to use the relation +oc . 2 1T e12 J dz = H-(1 + j). -oc 2 LTI x(t) ------~~@-------+-1 h (t) = s (t) 11----l•~@-------+- y(t) t t s*(t) s*(t) Figure P8.46 Chap. 8 Problems 651 (c) Consider the system in Figure P8.46, in which s(t) is the ""chirp signal"" in eq. (P8.46-1). Show that y(t) = X(jw 0 t), where X(jw) is the Fourier trans- form of x(t). (Note: The system in Figure P8.46 is referred to as the ""chirp"" transform algo- rithm and is often used in practice to obtain the Fourier transform of a signal.) 8.47. In Section 8.8 we considered synchronous discrete-time modulation and demod- ulation with a sinusoidal carrier. In this problem we want to consider the effect of a loss in synchronization in phase and/or frequency. The modulation and demod- ulation systems are shown in Figure P8.47(a), where both a phase and frequency difference between the modulator and demodulator carriers is indicated. Let the frequency difference w d - w c be denoted as ~w and the phase difference () d - () c as ~e. (a) If the spectrum of x[n] is that shown in Figure P8.47(b), sketch the spectrum of w[n], assuming ~w = 0. (b) If ~w = 0, show that w can be chosen so that the output r[ n] is r[ n] = x[n] cos~(). In particular, what is r[n] if~() = n/2? (c) For~() = 0, and w = WM + ~w, show that the output r[n] = x[n] cos[~wn] (assume that ~w is small). ~ x[n]~~y[n] 1---~ r[n] -w w w (a) (b) Figure P8.47 8.48. In this problem, we consider the analysis of discrete-time amplitude modulation of a pulse-train carrier. The system to be considered is shown in Figure P8.48(a). 652 Communication Systems Chap. a (a) Determine and sketch the discrete-time Fourier transform of the periodic square-wave signal p[n] in Figure P8.48(a). (b) Assume that x[ n] has the spectrum shown in Figure P8.48(b ). With wM = 7ri2N and with M = 1 in Figure P8.48(a), sketch Y(ejw), the Fourier transform of y[n]. (c) Now assume that X(ejw) is known to be band limited with X(ejw) = 0, WM < w < 27r- WM, but is otherwise unspecified. For the system of Figure P8.48(a), determine, as a function of N, the maximum allowable value of w M that will permit x[n] to be recovered from y[n]. Indicate whether your result depends onM. (d) With w M and N satisfying the condition determined in part (c), state or show in block diagram form how to recover x[n] from y[n]. x[n] ---.~ y[n] t p[n] p[n] IIIIII ••••••••••• IIIIIt ••••••••••• IIIIII ••••••••••• I 01• • •M N (N+M) n (a) 1T w (b) Figure P8.48 8.49. In practice it is often very difficult to build an amplifier at very low frequencies. Consequently~ low-frequency amplifiers typically exploit the principles of ampli- tude modulation to shift the signal into a higher-frequency band. Such an amplifier is referred to as a chopper amplifier and is illustrated in the block-diagram form in Figure P8.49. x(t)--y--8 y(t) s(t) s(t) Figure P8.49 Chap. 8 Problems 653 s(t) D 1c6 D -T 0 T T T 42 H1(jw) At -=1rT 1T 31T w T T H2(jw) 11 I -=1rT 1T w T Figure P8.49 Continued (a) Determine in terms ofT the highest allowable frequency present in x(t), if y(t) is to be proportional to x(t) (i.e., if the overall system is to be equivalent to an amplifier). (b) With x(t) bandlimited as specified in part (a), determine the gain of the overall system in Figure P8.49 in terms of A and T. 9 THE LAPLACE TRANSFORM 9.0 INTRODUCTION In the preceding chapters, we have seen that the tools of Fourier analysis are extremely useful in the study of many problems of practical importance involving signals and LTI systems. This is due in large part to the fact that broad classes of signals can be represented as linear combinations of periodic complex exponentials and that complex exponentials are eigenfunctions of LTI systems. The continuous-time Fourier transform provides us with a representation for signals as linear combinations of complex exponentials of the form est with s = jw. However the eigenfunction property introduced in Section 3.2 and many of its consequences apply as well for arbitrary values of s and not only those values that are purely imaginary. This observation leads to a generalization of the continuous-time Fourier transform, known as the Laplace transform, which we develop in this chapter. In the next chapter we develop the corresponding discrete-time generalization known as the z-transform. As we will see, the Laplace and z-transforms have many of the properties that make Fourier analysis so useful. Moreover, not only do these transforms provide additional tools and insights for signals and systems that can be analyzed using the Fourier transform, but they also can be applied in some very important contexts in which Fourier transforms cannot. For example Laplace and z-transforms can be applied to the analysis of many un- stable systems and consequently play an important role in the investigation of the stability or instability of systems. This fact, combined with the algebraic properties that Laplace and z-transforms share with Fourier transforms, leads to a very important set of tools for system analysis and in particular for the analysis of feedback systems, which we develop in Chapter 11. 654 Sec. 9.1 The Laplace Transform 655 9.1 THE lAPlACE TRANSFORM In Chapter 3, we saw that the response of a linear time-invariant system with impulse response h(t) to a complex exponential input of the form est is y(t) = H(s)est, (9.1) where H(s) = I:oo h(t)e-st dt. (9.2) For s imaginary (i.e., s = jw ), the integral in eq. (9.2) corresponds to the Fourier trans- form of h(t). For general values of the complex variables, it is referred to as the Laplace transform of the impulse response h(t). The Laplace transform of a general signal x(t) is defined as 1 +oo X(s) ~ I-o o x(t)e-st dt, (9.3) and we note in particular that it is a function of the independent variable s corresponding to the complex variable in the exponent of e-st. The complex variables can be written as s = (j' + jw, with (j' and w the real and imaginary parts, respectively. For convenience, we will sometimes denote the Laplace transform in operator form as cC{x(t)} and denote the transform relationship between x(t) and X(s) as oC x(t) ~ X(s). (9.4) Whens jw, eq. (9.3) becomes +oo X(jw) = I-o o x(t)e- jwt dt, (9.5) which corresponds to the Fourier transform of x(t); that is, X(s)ls= jw = 5={x(t)}. (9.6) The Laplace transform also bears a straightforward relationship to the Fourier trans- form when the complex variables is not purely imaginary. To see this relationship, consider X(s) as specified in eq. (9.3) with s expressed ass = (j' + jw, so that +oo X((J + jw) = I-o o x(t)e-(O""+ jw)t dt, (9.7) 1 The transform defined by eq. (9.3) is often called the bilateral Laplace transform, to distinguish it from the unilateral Laplace transform, which we discuss in Section 9.9. The bilateral transform in eq. (9.3) involves an integration from -oo to +oc, while the unilateral transform has a form similar to that in eq. (9.3), but with limits of integration from 0 to +oo. As we are primarily concerned with the bilateral transform, we will omit the word ""bilateral,"" except where it is needed in Section 9.9 to avoid ambiguity. 656 The Laplace Transform Chap.9 or +x X(u + jw) = f-oo [x(t)e-(]'1]e- jwt dt. (9.8) We recognize the right-hand side of eq. (9.8) as the Fourier transform of x(t)e-(]'1; that is, the Laplace transform of x(t) can be interpreted as the Fourier transform of x(t) after multiplication by a real exponential signal. The real exponential e-m may be decaying or growing in time, depending on whether u is positive or negative. To illustrate the Laplace transform and its relationship to the Fourier transform, let us consider the following example: Example 9.1 Let the signal x(t) = e-aru(t). From Example 4.1, the Fourier transform X(jw) con- verges for a > 0 and is given by a> 0. (9.9) From eq. (9.3), the Laplace transform is X(s) = J"""" e-aru(t)e-s1dt = ("""" e-<s + a)tdt, (9.10) -x Jo or, withs = u + jw, X(u + jw) = L""' e-(a+a)te-jwt dt. (9.11) By comparison with eq. (9.9) we recognize eq. (9.11) as the Fourier transform of e -(a+a)t u(t), and thus, X(u + jw) = u +a> 0, (9.12) (u +a)+ jw' or equivalently, since s = u + jw and u = (Jl.e{s}, 1 X(s) = --, (Jl.e{s} > -a. (9.13) s+a That is, .c 1 e-aru(t) ~ --, CRe{s} >-a. (9.14) s+a For example, for a 0, x(t) is the unit step with Laplace transform X(s) = lis, (Jl.e{s} > 0. We note, in particular, that just as the Fourier transform does not converge for all signals, the Laplace transform may converge for some values of CRe{s} and not for others. In eq. (9.13), the Laplace transform converges only for u = ffi-e{s} > -a. If a is positive, Sec. 9.1 The Laplace Transform 657 then X(s) can be evaluated at u = 0 to obtain 1 X(O+jw)= (9.15) jw +a' As indicated in eq. (9.6), for u = 0 the Laplace transform is equal to the Fourier transform, as is evident in the preceding example by comparing eqs. (9.9) and (9.15). If a is negative or zero, the Laplace transform still exists, but the Fourier transform does not. Example 9.2 For comparison with Example 9.1, let us consider as a second example the signal x(t) = -e-at u( -t). (9.16) Then X(s) = - J:oo e-ate-stu(-t)dt (9.17) = - J:oo e-(s+a)t dt, or 1 X(s) = --. (9.18) s+a For convergence in this example, we require that ffi-e{s +a} < 0, or ffi-e{s} < -a; that is, J3 1 -e-atu(-t) ~ --, ffi-e{s} < -a. (9.19) s+a Comparing eqs. (9.14) and (9.19), we see that the algebraic expression for the Laplace transform is identical for both of the signals considered in Examples 9.1 and 9 .2. However, from the same equations, we also see that the set of values of s for which the expression is valid is very different in the two examples. This serves to illustrate the fact that, in specifying the Laplace transform of a signal, both the algebraic expression and the range of values of s for which this expression is valid are required. In general, the range of values of s for which the integral in eq.(9.3) converges is referred to as the region of convergence (which we abbreviate as ROC) of the Laplace transform. That is, the ROC consists of those values of s = u + jw for which the Fourier transform of x(t)e-ut converges. We will have more to say about the ROC as we develop some insight into the properties of the Laplace transform. A convenient way to display the ROC is shown in Figure 9 .1. The variable s is a complex number, and in the figure we display the complex plane, generally referred to as the s-plane, associated with this complex variable. The coordinate axes are ffi-e{s} along the horizontal axis and dm{s} along the vertical axis. The horizontal and vertical axes are sometimes referred to as the u-axis and the jw-axis, respectively. The shaded region in Figure 9.1 (a) represents the set of points in the s-plane corresponding to the region of convergence for Example 9 .1. The shaded region in Figure 9.1 (b) indicates the region of convergence for Example 9. 2. 658 The Laplace Transform Chap.9 9m 9m s-plane s-plane -a 1 ffi-e ,-a ffi-e I I I I I I I I I I I I I I (a) (b) Figure 9.1 (a) ROC for Example 9.1; (b) ROC for Example 9.2. Example 9.3 In this example, we consider a signal that is the sum of two real exponentials: (9.20) The algebraic expression for the Laplace transform is then X(s) = I~x [3e- 21 u(t)- 2e- 1u(t)]e-I1 dt (9.21) = 3 J:x e-'2te- 11 U(t)dt- 2 J:""' e- 1e-11 u(t)dt. Each of the integrals in eq. (9.21) is of the same form as the integral in eq. (9.10), and consequently, we can use the result in Example 9.1 to obtain 3 X(s) = -- -- -2- . (9.22) s+2 s+l To determine the ROC we note that x(t) is a sum of two real exponentials, and from eq. (9.21) we see that X(s) is the sum of the Laplace transforms of each of the individual terms. The first term is the Laplace transform of 3e- 21 u(t) and the second term the Laplace transform of -2e- 1u(t). From Example 9.1, we know that £ 1 e-r u(t) ~ --1' ffi-e{s} > -1, s+ 1 1 £ 1 e-- u(t) ~ -- ffi-e{s} > -2. s + 2' The set of values of ffi-e{s} for which the Laplace transforms of both terms converge is ffi-e{s} > -1, and thus, combining the two terms on the right-hand side of eq. (9.22), we obtain 21 1 £ s- 1 3e- u(t)-2e- u(t)~ . ffi£{s}>-l. (9.23) s-1 + 3_s + 2 Sec. 9.1 The Laplace Transform 659 Example 9.4 In this example, we consider a signal that is the sum of a real and a complex exponential: x(t) = e-21 u(t) + e-1(cos 3t)u(t). (9.24) Using Euler's relation, we can write x(t) = [e-2t + ~e-(l-3j)l + ~e-(1+3j)t] u(t), and the Laplace transform of x(t) then can be expressed as + -1 Joo e-0 - 31.) 1 u(t)e-st dt (9.25) 2 -00 +-1 Joo e-(1+31.l 1u(t)e-s1 dt. 2 -00 Each of the integrals in eq. (9.25) represents a Laplace transform of the type en- countered in Example 9.1. It follows that e-21 J3 1 u(t) ~ s + , CRe{s} > -2, (9.26) 2 1 e-0-3jltu(t) ~ CRe{s} > -1, + (9.27) s (1- 3j)' 1 e-(1+3jltu(t) ~ CRe{s} > -1. (9.28) s + (1 + 3j)' For all three Laplace transforms to converge simultaneously, we must have CRe{s} > -1. Consequently, the Laplace transform of x(t) is 1 1( 1 ) 1( 1 ) CRe{s} > -1, s + (9.29) 2 + 2 s + ( 1 - 3 j) + 2 s + ( 1 + 3 j) ' or, with terms combined over a common denominator, 2 -21 -t £ 2s + 5s + 12 ro e u(t) + e (cos 3t)u(t) ~ (s2 + 25 + 10)(s + 2)' IJ\-e{s} > -1. (9.30) In each of the four preceding examples, the Laplace transform is rational, i.e., it is a ratio of polynomials in the complex variables, so that N(s) X(s) = D(s)' (9.31) where N(s) and D(s) are the numerator polynomial and denominator polynomial, respec- tively. As suggested by Examples 9.3 and 9.4, X(s) will be rational whenever x(t) is a linear combination of real or complex exponentials. As we will see in Section 9.7, rational 660 The Laplace Transform Chap.9 transforms also arise when we consider LTI systems specified in terms of linear constant- coefficient differential equations. Except for a scale factor, the numerator and denominator polynomials in a rational Laplace transform can be specified by their roots; thus, mark- ing the locations of the roots of N(s) and D(s) in the s-plane and indicating the ROC provides a convenient pictorial way of describing the Laplace transform. For example, in Figure 9.2(a) we show the s-plane representation of the Laplace transform of Example 9.3, with the location of each root of the denominator polynomial in eq. (9.23) indicated with ""X"" and the location of the root of the numerator polynomial in eq. (9.23) indicated with ""o."" The corresponding plot of the roots of the numerator and denominator polynomials for the Laplace transform in Example 9.4 is given in Figure 9.2(b). The region of convergence for each of these examples is shaded in the corresponding plot. !Jm I I I s-plane I I I I )( ~ -2 -11 I I I I I I (a) !Jm o*l s-plane I I I I I Figure 9.2 s-plane representation -2 -11 ffi-e of the Laplace transforms for (a) Ex- I I ample 9.3 and (b) Example 9.4. Each I x in these figures marks the location 01 of a pole of the corresponding Laplace I )I( transform-i.e., a root of the denomi- I nator. Similarly, each o marks a zero- i.e., a root of the the numerator. The (b) shaded regions indicate the ROCs. For rational Laplace transforms, the roots of the numerator polynomial are com- monly referred to as the zeros of X(s), since, for those values of s, X(s) = 0. The roots of the denominator polynomial are referred to as the poles of X(s), and for those values of s, X(s) is infinite. The poles and zeros of X(s) in the finite s-plane completely char- acterize the algebraic expression for X(s) to within a scale factor. The representation of X(s) through its poles and zeros in the s-plane is referred to as the pole-zero plot of X(s). Sec. 9.1 The Laplace Transform 661 However, as we saw in Examples 9.1 and 9 .2, know ledge of the algebraic form of X (s) does not by itself identify the ROC for the Laplace transform. That is, a complete specification, to within a scale factor, of a rational Laplace transform consists of the pole-zero plot of the transform, together with its ROC (which is commonly shown as a shaded region in the s-plane, as in Figures 9.1 and 9.2). Also, while they are not needed to specify the algebraic form of a rational transform X(s), it is sometimes convenient to refer to poles or zeros of X(s) at infinity. Specifically, if the order of the denominator polynomial is greater than the order of the numerator poly- nomial, then X(s) will become zero ass approaches infinity. Conversely, if the order of the numerator polynomial is greater than the order of the denominator, then X(s) will become unbounded as s approaches infinity. This behavior can be interpreted as zeros or poles at infinity. For example, the Laplace transform in eq. (9.23) has a denominator of order 2 and a numerator of order only 1, so in this case X(s) has one zero at infinity. The same is true for the transform in eq. (9.30), in which the numerator is of order 2 and the denominator is of order 3. In general, if the order of the denominator exceeds the order of the numerator by k, X(s) will have k zeros at infinity. Similarly, if the order of the numerator exceeds the order of the denominator by k, X(s) will have k poles at infinity. Example 9.5 Let (9.32) The Laplace transform of the second and third terms on the right-hand side of eq. (9.32) can be evaluated from Example 9.1. The Laplace transform of the unit impulse can be evaluated directly as (9.33) which is valid for any value of s. That is, the ROC of £{8(t)} is the entire s-plane. Using this result, together with the Laplace transforms of the other two terms in eq. (9.32), we obtain 4 1 1 1 X ( s) = 1 - 3 s + 1 + 3 s - 2, ffi-e{ s} > 2, (9.34) or X (s- 1)2 (s) (s + l)(s- 2)' ffi-e{s} > 2, (9.35) = where the ROC is the set of values of s for which the Laplace transforms of all three terms in x(t) converge. The pole-zero plot for this example is shown in Figure 9.3, together with the ROC. Also, since the degrees of the numerator and denominator of X(s) are equal, X(s) has neither poles nor zeros at infinity. 662 The Laplace Transform Chap.9 9m s-plane I I I I X X I -1 +1 +2 (Jl.e I I I I I I I I I Figure 9.3 Pole-zero plot and ROC for Example 9.5. Recall from eq. (9.6) that, for s = jw, the Laplace transform corresponds to the Fourier transform. However, if the ROC of the Laplace transform does not include the jw-axis, (i.e., if CRe{s} = 0), then the Fourier transform does not converge. As we see from Figure 9.3, this, in fact, is the case for Example 9.5, which is consistent with the fact that the term (1/3)e2 t u(t) in x(t) does not have a Fourier transform. Note also in this example that the two zeros in eq. (9.35) occur at the same value of s. In general, we will refer to the order of a pole or zero as the number of times it is repeated at a given location. In Example 9.5 there is a second-order zero at s = 1 and two first-order poles, one at s = - 1, the other at s = 2. In this example the ROC lies to the right of the rightmost pole. In general, for rational Laplace transforms, there is a close relationship between the locations of the poles and the possible ROCs that can be associated with a given pole-zero plot. Specific constraints on the ROC are closely associated with time-domain properties of x(t). In the next section, we explore some of these constraints and properties. 9.2 THE REGION OF CONVERGENCE FOR lAPlACE TRANSFORMS In the preceding section, we saw that a complete specification of the Laplace transform re- quires not only the algebraic expression for X(s), but also the associated region of conver- gence. As evidenced by Examples 9.1 and 9 .2, two very different signals can have identical algebraic expressions for X(s), so that their Laplace transforms are distinguishable only by the region of convergence. In this section, we explore some specific constraints on the ROC for various classes of signals. As we will see, an understanding of these constraints often permits us to specify implicitly or to reconstruct the ROC from knowledge of only the al- gebraic expression for X(s) and certain general characteristics of x(t) in the time domain. Property 1: The ROC of X(s) consists of strips parallel to the jw-axis in the s-plane. The validity of this property stems from the fact that the ROC of X(s) consists of those values of s = (J' + jw for which the Fourier transform of x(t)e-at converges. That Sec. 9.2 The Region of Convergence for Laplace Transforms 663 is, the ROC of the Laplace transform of x(t) consists ofthose values of s for which x(t)e-crt is absolutely integrable:2 r: lx(t)le -m dt < 00. (9.36) Property 1 then follows, since this condition depends only on a, the real part of s. Property 2: For rational Laplace transforms, the ROC does not contain any poles. Property 2 is easily observed in all the examples studied thus far. Since X (s) is infinite at a pole, the integral in eq. (9.3) clearly does not converge at a pole, and thus the ROC cannot contain values of s that are poles. Property 3: If x(t) is of finite duration and is absolutely integrable, then the ROC is the entire s-plane. The intuition behind this result is suggested in Figures 9.4 and 9.5. Specifically, a finite-duration signal has the property that it is zero outside an interval of finite duration, as illustrated in Figure 9.4. In Figure 9.5(a), we have shown x(t) of Figure 9.4 multiplied by a decaying exponential, and in Figure 9.5(b) the same signal multiplied by a growing Figure 9.4 Finite-duration signal. """"./Decaying exponential '- Growing exponential '...... --~-- =-=- (a) (b) Figure 9.5 (a) Finite-duration signal of Figure 9.4 multiplied by a decaying exponen- tial; (b) finite-duration signal of Figure 9.4 multiplied by a growing exponential. 2For a more thorough and formal treatment of Laplace transforms and their mathematical properties, including convergence, see E. D. Rainville, The Laplace Transform: An Introduction (New York: Macmil- lan, 1963), and R. V. Churchill and J. W. Brown, Complex Variables and Applications (5th ed.) (New York: McGraw-Hill, 1990). Note that the condition of absolute integrability is one of the Dirichlet conditions intro- duced in Section 4.1 in the context of our discussion of the convergence of Fourier transforms. 664 The Laplace Transform Chap.9 exponential. Since the interval over which x(t) is nonzero is finite, the exponential weight- ing is never unbounded, and consequently, it is reasonable that the integrability of x(t) not be destroyed by this exponential weighting. A more formal verification of Property 3 is as follows: Suppose that x(t) is absolutely integrable, so that To I /x(t)/ dt < oo. (9.37) Tl For s = if + jw to be in the ROC, we require that x(t)e-crt be absolutely integrable, i.e., To I /x(t)/e -crt dt < oo. (9.38) Tl Eq. (9.37) verifies that sis in the ROC when ffi-e{s} = if = 0. For if > 0, the maximum value of e-(rt over the interval on which x(t) is nonzero is e-aT1 , and thus we can write (9.39) Since the right-hand side of eq.(9.39) is bounded, so is the left-hand side; therefore, the s-plane for ffi-e{s} > 0 must also be in the ROC. By a similar argument, if if < 0, then To < JTo -/x(t)/e-m dt e-aT2 -/x(t)/ dt, (9.40) JTl Tl and again, x(t)e-crt is absolutely integrable. Thus, the ROC includes the entire s-plane. Example 9.6 Let x(t) = { e-at, 0 < t < T (9.41) 0, otherwise Then X(s) = (Te-at e-st dt = _I_ [1 - e-(s+a)T]. Jo s (9.42) +a Since in this example x(t) is of finite length, it follows from Property 3 that the ROC is the entire s-plane. In the form of eq. (9.42), X(s) would appear to have a pole at s = -a, which, from Property 2, would be inconsistent with an ROC that consists of the entire s-plane. In fact, however, in the algebraic expression in eq. (9.42), both numerator and denominator are zero at s = -a, and thus, to determine X(s) at s = -a, we can use L'hopital's rule to obtain !L(l - e-(s+a)T) l lim X(s) = lim ds = lim Te-aT e-sT, \'->-a s->-a [ !L(s +a) s->-a ds so that X(-a) = T. (9.43) Sec. 9.2 The Region of Convergence for Laplace Transforms 665 It is important to recognize that, to ensure that the exponential weighting is bounded over the interval in which x(t) is nonzero, the preceding discussion relies heavily on the fact that x(t) is of finite duration. In the next two properties, we consider modifications of the result in Property 3 when x(t) is of finite extent in only the positive-time or negative- time direction. Property 4: If x(t) is right sided, and if the line CR-€{s} = cr0 is in the ROC, then all values of s for which CR-€{s} > cr0 will also be in the ROC. A right-sided signal is a signal for which x(t) = 0 prior to some finite time T1, as illustrated in Figure 9.6. It is possible that, for such a signal, there is no value of s for which 2 the Laplace transform will converge. One example is the signal x(t) = et u(t). However, suppose that the Laplace transform converges for some value of cr, which we denote by cro. Then (9.44) or equivalently, since x(t) is right sided, (9.45) x(t) Figure 9.6 Right-sided signal. Then if cr 1 > cr0 , it must also be true that x(t)e-a1t is absolutely integrable, since e-a1t decays faster than e-aot as t ~ +oo, as illustrated in Figure 9.7. Formally, we can say that with c:TJ > cro, 'YJ lx(t)le-att dt = f'YJ lx(t)le-aote-(at-ao)t dt f Tt Tt (9.46) :::::; e-(at-ao)Tt f'YJ lx(t)le-aot dt. Tt Since T1 is finite, it follows from eq. (9.45) that the right side of the inequality in eq. (9.46) is finite, and hence, x(t)e-a 1t is absolutely integrable. Note that in the preceding argument we explicitly rely on the fact that x(t) is right sided, so that, although with cr1 > cr0 , e-a 1t diverges faster than e-aot as t ~ -oo, x(t)e-a 1t cannot grow without bound in the negative-time direction, since x(t) = 0 for 666 The Laplace Transform Chap.9 Figure 9.7 If x(t) is right sided and x(t)e-'rot is absolutely integrable, then x(t)e-'r1t, u1 > u0, will also be absolutely integrable. t < T1• Also, in this case, if a points is in the ROC, then all the points to the right of s, i.e., all points with larger real parts, are in the ROC. For this reason, the ROC in this case is commonly referred to as a right-half plane. Property 5: If x(t) is left sided, and if the line CRe{s} = u 0 is in the ROC, then all values of s for which CRe{s} < u 0 will also be in the ROC. A left-sided signal is a signal for which x(t) = 0 after some finite time T2, as illus- trated in Figure 9.8. The argument and intuition behind this property are exactly analogous to the argument and intuition behind Property 4. Also, for a left-sided signal, the ROC is commonly referred to as a left-half plane, as if a point s is in the ROC, then all points to the left of s are in the ROC. x(t) T2 Figure 9.8 Left-sided signal. Property 6: If x(t) is two sided, and if the line ffi-£{s} = u 0 is in the ROC, then the ROC will consist of a strip in the s-plane that includes the line ffi-£{s} = u 0 . A two-sided signal is a signal that is of infinite extent for both t > 0 and t < 0, as illustrated in Figure 9.9(a). For such a signal, the ROC can be examined by choosing an arbitrary time To and dividing x(t) into the sum of a right-sided signal xR(t) and a left- sided signal XL(t), as indicated in Figures 9.9(b) and 9.9(c). The Laplace transform of x(t) converges for values of s for which the transforms of both XR(t) and XL(t) converge. From Property 4, the ROC of £{xR(t)} consists of a half-plane ffi-£{s} > uR for some value uR, and from Property 5, the ROC of £{xL(t)} consists of a half-plane ffi-£{s} < uL for some value uL. The ROC of £{x(t)} is then the overlap of these two half-planes, as indicated in Figure 9.1 0. This assumes, of course, that u R < u L, so that there is some overlap. If this is not the case, then even if the Laplace transforms of XR(t) and xL(t) individually exist, the Laplace transform of x(t) does not. Sec. 9.2 The Region of Convergence for Laplace Transforms 667 x(t) ""---- (a) """"'----- J (b) (c) Figure 9. 9 Two-sided signal divided into the sum of a right-sided and left-sided sig- nal: (a) two-sided signal x{t); (b) the right-sided signal equal to x{t) for t > To and equal to 0 for t < T0; (c) the left-sided signal equal to x{t) for t < To and equal to 0 for t >To. 9m <TRI ffi..e I I I I I I I (a) 9m 9m I al ffi..e aRI <TL ffi..e I I I I I I I I I I I I I I (b) (c) Figure 9.10 (a) ROC for xR(t) in Figure 9.9; (b) ROC for xL(t) in Figure 9.9; (c) the ROC for x(t) = xR(t) + xL(t), assuming that the ROCs in (a) and (b) overlap. 668 The Laplace Transform Chap.9 Example 9.7 Let (9.47) as illustrated in Figure 9.11 for both b > 0 and b < 0. Since this is a two-sided signal, let us divide it into the sum of a right-sided and left-sided signal; that is, (9.48) Figure 9.11 Signal x(t) = e-bltl for both b > 0 and b < 0. From Example 9.1, .c 1 e-br u(t) ~ s + b' CRe{s} > -b, (9.49) and from Example 9.2, .c -1 e+b1u(-t) ~ --, CRe{s} <+b. (9.50) s-b Although the Laplace transforms of each of the individual terms in eq. (9.48) have a region of convergence, there is no common region of convergence if b s; 0, and thus, for those values of b, x(t) has no Laplace transform. If b > 0, the Laplace transform of x(t) is -bltl .c 1 1 -2b e ~ s + b - s- b = s2- b2' -b < (Jl.e{s} < +b. (9.51) The corresponding pole-zero plot is shown in Figure 9.12, with the shading indicating the ROC. Sec. 9.2 The Region of Convergence for Laplace Transforms 669 9m I I : : s-plane I I I I I I -----bX I : ----+----I¥ I b- ----CR-e I I I I I I I I I I Figure 9.12 Pole-zero plot and ROC for Example 9.7. A signal either does not have a Laplace transform or falls into one of the four cate- gories covered by Properties 3 through 6. Thus, for any signal with a Laplace transform, the ROC must be the entire s-plane (for finite-length signals), a left-half plane (for left- sided signals), a right-half plane (for right-sided signals), or a single strip (for two-sided signals). In all the examples that we have considered, the ROC has the additional property that in each direction (i.e., CR-e{s} increasing and CR-e{s} decreasing) it is bounded by poles or extends to infinity. In fact, this is always true for rational Laplace transforms: Property 7: If the Laplace transform X(s) of x(t) is rational, then its ROC is bounded by poles or extends to infinity. In addition, no poles of X(s) are contained in the ROC. A formal argument establishing this property is somewhat involved, but its validity is essentially a consequence of the facts that a signal with a rational Laplace transform consists of a linear combination of exponentials and, from Examples 9.1 and 9.2, that the ROC for the transform of individual terms in this linear combination must have the property. As a consequence of Property 7, together with Properties 4 and 5, we have Property 8: If the Laplace transform X(s) of x(t) is rational, then if x(t) is right sided, the ROC is the region in the s-plane to the right of the rightmost pole. If x(t) is left sided, the ROC is the region in the s-plane to the left of the leftmost pole. To illustrate how different ROCs can be associated with the same pole-zero pattern, let us consider the following example: Example 9.8 Let 1 X(s) = (s + l)(s + 2)' (9.52) with the associated pole-zero pattern in Figure 9.13(a). As indicated in Figures 9.13(b)- ( d), there are three possible ROCs that can be associated with this algebraic expression, corresponding to three distinct signals. The signal associated with the pole-zero pat- tern in Figure 9.13(b) is right sided. Since the ROC includes the jw-axis, the Fourier 670 The Laplace Transform Chap.9 9m 9m I s-plane I s-plane I I I ---x-x~--ffi-£ ---x-x~--------ffi--£ (a) (b) 9m 9m I s-plane I s-plane I I I I I --~-X-+---ffi-£ --x-x~------ 1 I ffi£ I I I I I I I I I I I I (c) (d) Figure 9.13 (a) Pole-zero pattern for Example 9.8; (b) ROC corresponding to a right-sided sequence; (c) ROC corresponding to a left-sided sequence; (d) ROC corresponding to a two-sided sequence. transform of this signal converges. Figure 9 .13( c) corresponds to a left -sided signal and Figure 9.13(d) to a two-sided signal. Neither of these two signals have Fourier trans- forms, since their ROCs do not include the jw-axis. 9.3 THE INVERSE LAPLACE TRANSFORM In Section 9.1 we discussed the interpretation of the Laplace transform of a signal as the Fourier transform of an exponentially weighted version of the signal; that is, with s ex- pressed ass = u + jw, the Laplace transform of a signal x(t) is X(<T + jw) = :J{x(l)e -m) = [,, x(t)e -me- Jwt dt (9.53) for values of s = u + jw in the ROC. We can invert this relationship using the inverse Fourier transform as given in eq. (4.9). We have x(t)e-m = ~- 1 {X(u + jw)} = - 1 f +x X(u + jw)e1. w 1 dw, (9.54) 27T -X or, multiplying both sides by em, we obtain x(t) = - 1 f +oc X(u + jw )e(if+ . ;w)t dw. (9.55) 27T -X Sec. 9.3 The Inverse Laplace Transform 671 That is, we can recover x(t) from its Laplace transform evaluated for a set of values of s = u + jw in the ROC, with u fixed and w varying from -oo to +oo. We can highlight this and gain additional insight into recovering x(t) from X(s) by changing the variable of integration in eq. (9.55) from w to sand using the fact that u is constant, so that ds = j dw. The result is the basic inverse Laplace transform equation: 1 fer+ jx x(t) = - . X(s)est ds. (9.56) 21 1') cr-j':fj This equation states that x(t) can be represented as a weighted integral of complex exponentials. The contour of integration in eq. (9.56) is the straight line in the s-plane corresponding to all points s satisfying CRc{s} = u. This line is parallel to the jw-axis. Furthermore, we can choose any such line in the ROC-i.e., we can choose any value of u such that X(u + jw) converges. The formal evaluation of the integral for a general X(s) requires the use of contour integration in the complex plane, a topic that we will not consider here. However, for the class of rational transforms, the inverse Laplace transform can be determined without directly evaluating eq. (9.56) by using the technique of partial- fraction expansion in a manner similar to that used in Chapter 4 to determine the inverse Fourier transform. Basically, the procedure consists of expanding the rational algebraic expression into a linear combination of lower order terms. For example, assuming no multiple-order poles, and assuming that the order of the denominator polynomial is greater than the order of the numerator polynomial, we can expand X(s) in the form m X(s) = L,A-· 1 -. (9.57) i=l s + ai From the ROC of X(s), the ROC of each of the individual terms in eq. (9.57) can be inferred, and then, from Examples 9.1 and 9 .2, the inverse Laplace transform of each of these terms can be determined. There are two possible choices for the inverse transform of each term AJ(s + ai) in the equation. If the ROC is to the right of the pole at s = - ai, then the inverse transform of this term is Aie-aJ u(t), a right-sided signal. If the ROC is to the left of the pole at s = -ai, then the inverse transform of the term is -Aie-aJu(-t), a left-sided signal. Adding the inverse transforms of the individual terms in eq. (9.57) then yields the inverse transform of X(s). The details of this procedure are best presented through a number of examples. Example 9.9 Let 1 X(s) (s + 1)(s + 2)' CRe{s} > -1. (9.58) = To obtain the inverse Laplace transform, we first perform a partial-fraction expansion to obtain 1 A B X(s) = (s + l)(s + 2) --+--. (9.59) s+l s+2 672 The Laplace Transform Chap.9 As discussed in the appendix, we can evaluate the coefficients A and B by multiplying both sides of eq. (9.59) by (s + 1)(s + 2) and then equating coefficients of equal powers of son both sides. Alternatively, we can use the relation A = [(s + l)X(s)lls= -I = 1, (9.60) B = [(s + 2)X(s)]l.l·= -2 = -1. (9.61) Thus, the partial-fraction expansion for X(s) is 1 1 X(s) = --1 - --2. (9.62) s + s + From Examples 9.1 and 9.2, we know that there are two possible inverse trans- forms for a transform of the form ll(s + a), depending on whether the ROC is to the left or the right of the pole. Consequently, we need to determine which ROC to associate with each of the individual first-order terms in eq. (9.62). This is done by reference to the properties of the ROC developed in Section 9.2. Since the ROC for X(s) is CRe{s} > -1, the ROC for the individual terms in the partial-fraction expansion of eq. (9.62) includes (Jl.e{s} > -1. The ROC for each term can then be extended to the left or right (or both) to be bounded by a pole or infinity. This is illustrated in Figure 9.14. Figure 9.14(a) shows the pole-zero plot and ROC for X(s), as specified in eq. (9.58). Figure 9.14(b) and 9.14(c) represent the individual terms in the partial-fraction expansion in eq. (9.62). The ROC for the sum is indicated with lighter shading. For the term represented by Figure 9.14(c), the ROC for the sum can be extended to the left as shown, so that it is bounded by a pole. I I s-plane I I I I -X-X--+----- -2 -1 I I I I I (a) 9m I I I s-plane I s-plane I I I I I I I ---x--~------ ->k---+---- -1 -2 I I I I I (b) (c) Figure 9.14 Construction of the ROCs for the individual terms in the partial-fraction expansion of X(s) in Example 9.8: (a) pole-zero plot and ROC for X(s); (b) pole at s = -1 and its ROC; (c) pole at s = -2 and its ROC. Sec. 9.3 The Inverse Laplace Transform 673 Since the ROC is to the right of both poles, the same is true for each of the indi- vidual terms, as can be seen in Figures 9.14(b) and (c). Consequently, from Property 8 in the preceding section, we know that each of these terms corresponds to a right-sided signal. The inverse transform of the individual terms in eq. (9.62) can then be obtained by reference to Example 9.1: £ 1 e-tu(t) ~ --1, CRe{s} > -1, (9.63) s+ £ 1 e- 21 u(t) ~ -- CR.e{s} > -2. (9.64) s + 2' We thus obtain -t -21 £ 1 [e - e ]u(t) ~ (s + 1)(s + 2)' (R.e{s} > -1. (9.65) Example 9.10 Let us now suppose that the algebraic expression for X(s) is again given by eq. (9.58), but that the ROC is now the left-half plane ffi-e{s} < -2. The partial-fraction expansion for X(s) relates only to the algebraic expression, so eq. (9.62) is still valid. With this new ROC, however, the ROC is to the left of both poles and thus, the same must be true for each of the two terms in the equation. That is, the ROC for the term corresponding to the pole at s = -1 is ffi-e{s} < -1, while the ROC for the term with pole at s = -2 is ffi-e{s} < -2. Then, from Example 9.2, I £ 1 -e- u( -t) ~ s + , ffi-e{s} < -1, (9.66) 1 21 £ 1 -e- u( -t) ~ s + , ffi-e{s} < -2, (9.67) 2 so that £ 1 x(t) = [ -e-1 + e- 21 ]u( -t) ~ (s + )(s + ), ffi-e{s} < -2. (9.68) 1 2 Example 9. 1 1 Finally, suppose that the ROC of X(s) in eq. (9.58) is -2 < ffi-e{s} < -1. In this case, the ROC is to the left of the pole at s = -1 so that this term corresponds to the left-sided signal in eq. (9.66), while the ROC is to the right of the pole at s = -2 so that this term corresponds to the right-sided signal in eq. (9.64). Combining these, we find that £ 1 x(t) = -e-1 u( -t)- e- 21 u(t) ~ (s + )(s + ), -2 < ffi-e{s} < -1. (9.69) 1 2 As discussed in the appendix, when X(s) has multiple-order poles, or when the de- nominator is not of higher degree than the numerator, the partial-fraction expansion of X(s) will include other terms in addition to the first-order terms considered in Examples 9.9- 9 .11. In Section 9.5, after discussing properties of the Laplace transform, we develop some other Laplace transform pairs that, in conjunction with the properties, allow us to extend the inverse transform method outlined in Example 9.9 to arbitrary rational transforms. 674 The Laplace Transform Chap.9 9.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT As we saw in Section 9.1, the Fourier transform of a signal is the Laplace transform evalu- ated on the jw-axis. In this section we discuss a procedure for geometrically evaluating the Fourier transform and, more generally, the Laplace transform at any set of values from the pole-zero pattern associated with a rational Laplace transform. To develop the procedure, let us first consider a Laplace transform with a single zero [i.e., X(s) = s - a], which we evaluate at a specific value of s, say, s = s1• The algebraic expression s 1 - a is the sum of two complex numbers, s1 and -a, each of which can be represented as a vector in the complex plane, as illustrated in Figure 9.15. The vector representing the complex number s1 -a is then the vector sum of s1 and -a, which we see in the figure to be a vector from the zero at s = a to the point s1• The value of X(s1) then has a magnitude that is the length of this vector and an angle that is the angle of the vector relative to the real axis. If X(s) instead has a single pole at s = a [i.e., X(s) = 1/(s- a)], then the denominator would be represented by the same vector sum of s1 and -a, and the value of X(st) would have a magnitude that is the reciprocal of the length of the vector from the pole to s = s 1 and an angle that is the negative of the angle of the vector with the real axis. s-plane a ffi-£ Figure 9.15 Complex plane rep- resentation of the vectors s1, a, and s1 - a representing the complex num- bers St, a, and St - a, respectively. A more general rational Laplace transform consists of a product of pole and zero terms of the form discussed in the preceding paragraph; that is, it can be factored into the form (9.70) To evaluate X(s) at s = s1, each term in the product is represented by a vector from the zero or pole to the point s1• The magnitude of X(st) is then the magnitude of the scale factor M, times the product of the lengths of the zero vectors (i.e., the vectors from the zeros to s1) divided by the product of the lengths of the pole vectors (i.e., the vectors from the poles to s1) . The angle of the complex number X (s1) is the sum of the angles of the zero vectors minus the sum of the angles of the pole vectors. If the scale factor Min eq. (9.70) is negative, an additional angle of 1T would be included. If X(s) has a multiple pole or zero Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 675 (or both), corresponding to some of the a /s being equal to each other or some of the {3/s being equal to each other (or both), the lengths and angles of the vectors from each of these poles or zeros must be included a number of times equal to the order of the pole or zero. Example 9.12 Let 1 1 X(s) = --, ffi-e{s} > - s+! 2. (9.71) 2 The Fourier transform is X(s)ls= jw. For this example, then, the Fourier transform is X(jw) = jw ~ 112 (9.72) The pole-zero plot for X(s) is shown in Figure 9.16. To determine the Fourier transform graphically, we construct the pole vector as indicated. The magnitude of the Fourier transform at frequency w is the reciprocal of the length of the vector from the pole to the point jw on the imaginary axis. The phase of the Fourier transform is the negative of the angle of the vector. Geometrically, from Figure 9.16, we can write (9.73) and <f:X(jw) = -tan -I 2w. (9.74) s-plane w jw +j_ 2 Figure 9.16 Pole-zero plot for Example 9.12. IX(jw)l is the reciprocal of the length of the vector shown, and <r:X(jw) is the negative of the angle of the vector. Often, part of the value of the geometric determination of the Fourier transform lies in its usefulness in obtaining an approximate view of the overall characteristics of the transform. For example, in Figure 9 .16, it is readily evident that the length of the pole vector monotonically increases with increasing w, and thus, the magnitude of the Fourier 676 The Laplace Transform Chap.9 transform will monotonically decrease with increasing w. The ability to draw general con- elusions about the behavior of the Fourier transform from the pole-zero plot is further il- lustrated by a consideration of general first- and second-order systems. 9 .4. 1 First-Order Systems As a generalization of Example 9.12, let us consider the class of first-order systems that was discussed in some detail in Section 6.5.1. The impulse response for such a system is h(t) = -1e - tiT u(t), (9.75) T and its Laplace transform is = -- > --1 H(s) ffi-e{s} . (9.76) ST + 1' T The pole-zero plot is shown in Figure 9.17. Note from the figure that the length of the pole vector is minimal for w = 0 and increases monotonically as w increases. Also, the angle of the pole in~reases monotonically from 0 to 7r/2 as w increases from 0 to oo. s-plane w _1 T Figure 9.17 Pole-zero plot for first- order system of eq. (9.76). From the behavior of the pole vector as w varies, it is clear that the magnitude of the frequency response H(jw) monotonically decreases as w increases, while <t.H(jw) monotonically decreases from 0 to -'TT'/2, as shown in the Bode plots for this system in Figure 9.18. Note also that when w = liT, the real and imaginary parts of the pole vector are equal, yielding a value of the magnitude of the frequency response that is reduced by a factor of J2, or approximately 3 dB, from its maximum at w = 0 and a value of 7r/4 for the angle of the frequency response. This is consistent with our examination of first-order systems in Section 6.5.1, where we noted that w = liT is often referred to as the 3-dB point or the break frequency-i.e., the frequency at which the straight-line approximation of the Bode plot of IH (jw )I has a break in its slope. As we also saw in Section 6.5 .1, the time constant T controls the speed of response of first-order systems, and we now see that Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 677 20 3 dB _i 0 dB .... Asymptotic ]: ~approximation I 0 -20 ci) .2 0 C\J -40 -60 0.1/T 1/T 1 0/T 100/T w 7T/4 0 3 :C -7T/4 >.f -7T/2 -37T/4 0.1/T 1/T 10/T 100/T Figure 9.18 Frequency response w for a first-order system. the pole of such a system at s = -liT is on the negative real axis, at a distance to the origin that is the reciprocal of the time constant. From our graphical interpretation, we can also see how changing the time constant or, equivalently, the position of the pole of H(s) changes the characteristics of a first-order system. In particular, as the pole moves farther into the left-halfplane, the break frequency and, hence, the effective cutoff frequency of the system increases. Also, from eq. (9.75) and from Figure 6.19, we see that this same movement of the pole to the left corresponds to a decrease in the time constant T, resulting in a faster decay of the impulse response and a correspondingly faster rise time in the step response. This relationship between the real part of the pole locations and the speed of the system response holds more generally; that is, poles farther away from the jw-axis are associated with faster response terms in the impulse response. 9.4.2 Second-Order Systems Let us next consider the class of second-order systems, which was discussed in some detail in Section 6.5.2. The impulse response and frequency response for the system, originally 678 The Laplace Transform Chap.9 given in eqs. (6.37) and (6.33), respectively, are h(t) = M[ec 1t - ec2']u(t), (9.77) where Ct = -{wn + Wn~, c2 = -{wn-Wn~' M = __w _n_ _ 2~' and w~ (9.78) H(jw) = (jw)2 + 2{wn(jw) + wr The Laplace transform of the impulse response is (9.79) For { > 1, c1 and c2 are real and thus both poles lie on the real axis, as indicated in Figure 9.19(a). The case of { > 1 is essentially a product of two first-order terms, as in Section 9.4.1. Consequently, in this case iH(jw)i decreases monotonically as lwl in- creases, while <r:H(jw) varies from 0 at w = 0 to -7r as w ~ oo. This can be verified from Figure 9.19(a) by observing that the length of the vector from each of the two poles to the points = jw increases monotonically as w increases from 0, and the angle of each of these vectors increases from 0 to 7T/2 as w increases from 0 to oo. Note also that as { increases, one pole moves closer to the jw-axis, indicative of a term in the impulse re- sponse that decays more slowly, and the other pole moves farther into the left-half plane, indicative of a term in the impulse response that decays more rapidly. Thus, for large val- ues of{, it is the pole close to the jw-axis that dominates the system response for large time. Similarly, from a consideration of the pole vectors for { >> 1, as indicated in Fig- ure 9.19(b), for low frequencies the length and angle of the vector for the pole close to the jw-axis are much more sensitive to changes in w than the length and angle of the vector for the pole far from the jw-axis. Hence, we see that for low frequencies, the char- acteristics of the frequency response are influenced principally by the pole close to the jw-axis. For 0 < { < 1, c1 and c2 are complex, so that the pole-zero plot is that shown in Figure 9.19(c). Correspondingly, the impulse response and step response have oscilla- tory parts. We note that the two poles occur in complex conjugate locations. In fact, as we discuss in Section 9.5.5, the complex poles (and zeros) for a real-valued signal al- ways occur in complex conjugate pairs. From the figure-partiJularly when { is small, so that the poles are close to the jw-axis-as w approaches Wn 1 - {2 , the behavior of the frequency response is dominated by the pole vector in the second quadrant, and in Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 679 gm gm s-plane s-plane (a) (b) gm gm s-plane cos e = ~ X (c) (d) Figure 9.19 (a) Pole-zero plot for a second-order system with ( > 1; (b) pole vec- tors for ( >> 1; (c) pole-zero plot for a second-order system with 0 < ( < 1; (d) pole vectors for 0 < ( < 1 and for w = wn~ and w = wn~ :±: (wn. 680 The Laplace Transform Chap.9 particular, the length of that pole vector has a minimum at w = wn~· Thus, qual- itatively, we would expect the magnitude of the frequency response to exhibit a peak in the vicinity of that frequency. Because of the presence of the other pole, the peak will occur not exactly at w = Wn~, but at a frequency slightly less than this. A careful sketch of the magnitude of the frequency response is shown in Figure 9.20(a) for w n = 1 and several values of ( where the expected behavior in the vicinity of the poles is clearly evident. This is consistent with our analysis of second-order systems in Section 6.5.2. JH(jw)l -1 w (a) <l:H(jw) w (b) Figure 9.20 (a) Magnitude and (b) phase of the frequency response for a second-order system with 0 < ~ < 1. Sec. 9.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 681 Thus, for 0 < ( < 1, the second-order system is a nonideal bandpass filter, with the parameter ( controlling the sharpness and width of the peak in the frequency response. In particular, from the geometry in Figure 9 .19( d), we see that the length of the pole vector from the second-quadrant pole increases by a factor of j2 from its minimum at w = w n ~when w increases or decreases from this value by ( w n. Consequently, for small (,and neglecting the effect of the distant third-quadrant pole, IH(jw )I is within a factor of J2 of its peak value over the frequency range Wn~- (wn < W < Wn~ + (wn. If we define the relative bandwidth Bas the length of this frequency interval divided by the undamped natural frequency w n, we see that B = 2(. Thus, the closer ( is to zero, the sharper and narrower the peak in the frequency response is. Note also that B is the reciprocal of the quality measure Q for second-order systems defined in Section 6.5.2. Thus, as the quality increases, the relative bandwidth decreases and the filter becomes increasingly frequency selective. An analogous picture can be developed for <r:H (w ), which is plotted in Figure 9 .20(b) for wn = 1 and several values of(. As can be seen from Figure 9.19(d), the angle of the second-quadrant pole vector changes from -11'14 to 0 to 11'14 as w changes from Wn~- (wn toWn~ toWn~+ (wn. For small values of(, the angle for the third-quadrant pole changes very little over this frequency interval, resulting in a rapid change in <r:H(jw) of approximately 71'/2 over the interval, as captured in the figure. Varying wn with ( fixed only changes the frequency scale in the preceding discussion-i.e., IH(w)l and <r:H(w) depend only on wlwn. From Figure 9.19(c), we also can readily determine how the poles and system characteristics change as we vary (, keeping w n constant. Since cos (} = (, the poles move along a semicircle with fixed radius w n. For ( = 0, the two poles are on the imaginary axis. Correspondingly, in the time domain, the impulse response is sinusoidal with no damping. As ( increases from 0 to 1, the two poles remain complex and move into the left-half plane, and the vectors from the origin to the poles maintain a constant overall magnitude w n. As the real part of the poles becomes more negative, the associated time response will decay more quickly as t ~ oo. Also, as we have seen, as (increases from 0 toward 1, the relative bandwidth of the frequency response increases, and the frequency response becomes less sharp and less frequency selective. 9.4.3 All-Pass Systems As a final illustration of the geometric evaluation of the frequency response, let us con- sider a system for which the Laplace transform of the impulse response has the pole-zero plot shown in Figure 9.21(a). From this figure, it is evident that for any point along the jw-axis, the pole and zero vectors have equal length, and consequently, the magnitude of the frequency response is constant and independent of frequency. Such a system is com- 682 The Laplace Transform Chap.9 s-plane w -a a (a) <tH(jw) IH(jw)l w (b) Figure 9.21 (a) Pole-zero plot for an all-pass system; (b) magnitude and phase of an all-pass frequency response. monly referred to as an all-pass system, since it passes all frequencies with equal gain (or attenuation). The phase of the frequency response is 8 I - 82, or, since 8 I = 7T - 82, 1:H(jw) = 7T- 282. (9.80) From Figure 9.21(a), 82 = tan- 1( wla), and thus, -1-H(jw) 1 = 7r- 2tan- (~). (9.81) The magnitude and phase of H(jw) are illustrated in Figure 9.2l(b). 9.5 PROPERTIES OF THE LAPLACE TRANSFORM In exploiting the Fourier transform, we relied heavily on the set of properties developed in Section 4.3. In the current section, we consider the corresponding set of properties for Sec. 9.5 Properties of the Laplace Transform 683 the Laplace transform. The derivations of many of these results are analogous to those of the corresponding properties for the Fourier transform. Consequently, we will not present the derivations in detail, some of which are left as exercises at the end of the chapter. (See Problems 9.52-9.54.) 9.5.1 Linearity of the laplace Transform If with a region of convergence that will be denoted as R1 and x (s) with a region of convergence that 2 will be denoted as R2, then £ ax1 (t) + bx2(t) ~ aX1 (s) + bX2(s), with ROC (9.82) containing R 1 n R 2. As indicated, the region of convergence of X(s) is at least the intersection of R1 and R2, which could be empty, in which case X(s) has no region of convergence-i.e., x(t) has no Laplace transform. For example, for x(t) as in eq. (9.47) of Example 9.7, with b > 0 the ROC for X(s) is the intersection of the ROCs for the two terms in the sum. If b < 0, there are no common points in R1 and R2 ; that is, the intersection is empty, and thus, x(t) has no Laplace transform. The ROC can also be larger than the intersection. As a simple example, for x1 (t) = x2(t) and a = -bin eq. (9.82), x(t) = 0, and thus, X(s) = 0. The ROC of X(s) is then the entire s-plane. The ROC associated with a linear combination of terms can always be constructed by using the properties of the ROC developed in Section 9 .2. Specifically, from the inter- section of the ROCs for the individual terms (assuming that it is not empty), we can find a line or strip that is in the ROC of the linear combination. We then extend this to the right ((Re{s} increasing) and to the left (ffi-e{s} decreasing) to the nearest poles (which may be at infinity). Example 9. 1 3 In this example, we illustrate the fact that the ROC for the Laplace transform of a linear combination of signals can sometimes extend beyond the intersection of the ROCs for the individual terms. Consider X(t) = Xt (t) - X2(t), (9.83) 684 The Laplace Transform Chap.9 where the Laplace transforms of x 1 (t) and x 2(t) are, respectively, 1 X,(s) = s + 1' CRe{s} > -1, (9.84) and 1 X2(s) (s + 1)(s + 2)' CR..e{s} > -1. (9.85) = The pole-zero plot, including the ROCs for X1( s) and X2(s), is shown in Figures 9.22(a) and (b). From eq. (9.82), 1 1 s + 1 X(s) = -s- +-1 - -(s_+_1_)-(s_+_2_) (9.86) (s + l)(s + 2) - s + 2 · Thus, in the linear combination of x 1( t) and x 2(t), the pole at s = -1 is canceled by a zero at s = -1. The pole-zero plot for X(s) = X1( s)- X2(s) is shown in Figure 9.22(c). The intersection of the ROCs for X1( s) and X2(s) is ffi-e{s} > -1. However, since the ROC is always bounded by a pole or infinity, for this example the ROC for X(s) can be extended to the left to be bounded by the pole at s = -2, as a result of the pole-zero cancellation at s = - 1. !1m !1m !1m I I I I s-plane I s-plane I s-plane I I I I R1 I f\ I R I I I I I I -~ -X-X ffi-e -2 -1 ffi-e -~ I I I ffi-e I I I I I I I I I I I I (a) (b) (c) Figure 9.22 Pole-zero plots and ROCs for Example 9.13: (a) X1(s); (b) X2(s); (c) X1 (s) - X2(s). The ROC for X1 (s) - X2(s) includes the inter- section of R1 and R2, which can then be extended to be bounded by the pole at s = -2. 9.5.2 Time Shifting If .c x(t) ~ X(s), withROC = R, then .c x(t- to) ~ e-stox(s), with ROC = R. (9.87) Sec. 9.5 Properties of the Laplace Transform 685 9.5.3 Shifting in the s-Domain If £ x(t) ~ X(s), with ROC= R, then £ esot x(t) ~ X(s - s0), with ROC = R + CRe{so}. (9.88) That is, the ROC associated with X(s- s0) is that of X(s), shifted by CRe{s0}. Thus, for any values that is in R, the values + CR.e{s0} will be in R1. This is illustrated in Figure 9.23. Note that if X(s) has a pole or zero at s = a, thenX(s-s0) has a pole or zero ats-s0 = a- i.e., s = a + so. An important special case of eq. (9.88) is when s0 = jw0-i.e., when a signal x(t) is used to modulate a periodic complex exponential ejwot. In this case, eq. (9.88) becomes . £ eJwot x(t) ~ X(s - jw0 ), with ROC = R. (9.89) The right-hand side of eq. (9.89) can be interpreted as a shift in the s-plane parallel to the jw-axis. That is, if the Laplace transform of x(t) has a pole or zero at s = a, then the Laplace transform of ejwot x(t) has a pole or zero at s = a+ jw0 . 9.5.4 Time Scaling If £ x(t) ~ X(s), withROC = R, l I 1 s-plane I s-plane Rl I I I I I l l r1 I I I I r2 + CR-e (so) I l I I I I I 1 I (a) (b) Figure 9.23 Effect on the ROC of shifting in the s-domain: (a) the ROC of X(s); (b) the ROC of X(s- s0). 686 The Laplace Transform Chap.9 then x(at) ....:.._. !x(n with ROC R1 ~ aR. (9.90) 1 1 That is, for any value s in R [which is illustrated in Figure 9.24(a)], the value a/s will be in Rt. as illustrated in Figure 9.24(b) for a positive value of a < 1. Note that, for 0 <a< 1, there is a compression in the size of the ROC of X(s) by a factor of a, as depicted in Figure 9.24(b), while for a> 1, the ROC is expanded by a factor of a. Also, eq. (9.90) implies that if a is negative, the ROC undergoes a reversal plus a scaling. In particular, as depicted in Figure 9.24(c), the ROC of lliaiX(sla) for 0 >a > -1 involves s-plane s-plane R .!1 ~ a a (a) (b) 9m s-plane ~ ~ a a Figure 9.24 Effect on the ROC of time scaling: (a) ROC of X(s); (b) ROC of (1/lai}X(s/a) for 0 <a< 1; (c) ROC of (c) (1/lai}X(s/a) for 0 >a> -1. Sec. 9.5 Properties of the Laplace Transform 687 a reversal about the jw-axis, together with a change in the size of the ROC by a factor of lal· Thus, time reversal of x(t) results in a reversal of the ROC. That is, £ x(-t) ~ X(-s), with ROC= -R. (9.91) 9.5.5 Conjugation If £ x(t) ~ X(s), with ROC = R, (9.92) then x *( t) ~£ X *( s *) , with ROC = R. (9.93) Therefore, X(s) = X*(s*) when x(t) is real. (9.94) Consequently, if x(t) is real and if X(s) has a pole or zero at s = s0 (i.e., if X(s) is un- bounded or zero at s = s0), then X(s) also has a pole or zero at the complex conjugate points = s~. For example, the transform X(s) for the real signal x(t) in Example 9.4 has poles at s = 1 ± 3j and zeros at s = ( -5 ± .ifil)/2. 9. 5. 6 Convolution Property If with ROC = R1, and with ROC = R2, then (9.95) In a manner similar to the linearity property set forth in Section 9.5.1, the ROC of X1( s)X2(s) includes the intersection of the ROCs of X1( s) and X2(s) and may be larger if pole-zero cancellation occurs in the product. For example, if s + 1 XJ(S) = s + 2' CR-t:?{s} > -2, (9.96) 688 The Laplace Transform Chap. 9 and s+2 X2(s) = s + , CRe{s} > -1, (9.97) 1 then X1 (s)X2(s) = 1, and its ROC is the entire s-plane. As we saw in Chapter 4, the convolution property in the context of the Fourier transform plays an important role in the analysis of linear time-invariant systems. In Sec- tions 9.7 and 9.8 we will exploit in some detail the convolution property for Laplace trans- forms for the analysis of LTI systems in general and, more specifically, for the class of systems represented by linear constant-coefficient differential equations. 9.5.7 Differentiation in the Time Domain If £ x(t) ~ X(s), with ROC= R, then dx(t) £ ~ sX(s), with ROC containing R. (9.98) dt This property follows by differentiating both sides of the inverse Laplace transform as expressed in equation (9.56). Specifically, let 1 f<T+ jx x(t) = -. X(s)est ds. 21T 1 (T- joo Then dx(t) 1 J<T+joo -- - - . sX(s)e5tds. (9.99) dt 2 1T 1 (T- joo Consequently, dx(t)ldt is the inverse Laplace transform of sX(s). The ROC of sX(s) in- cludes the ROC of X(s) and may be larger if X(s) has a first-order pole at s = 0 that is canceled by the multiplication by s. For example, if x(t) = u(t), then X(s) = 1/s, with an ROC that is CRe{s} > 0. The derivative of x(t) is an impulse with an associated Laplace transform that is unity and an ROC that is the entire s-plane. 9.5.8 Differentiation in the s-Domain Differentiating both sides of the Laplace transform equation (9.3), i.e., X(s) ~ r~x x(t)e-""dt, we obtain dX(s) +oo -oo (-t)x(t)e-stdt. ds r Sec. 9.5 Properties of the Laplace Transform 689 Consequently, if £ x(t) ~ X(s), with ROC= R, then £ dX(s) -tx(t) ~ withROC = R. (9.100) ds ' The next two examples illustrate the use of this property. Example 9.14 Let us find the Laplace transform of x(t) = te-at u(t). (9.101) Since CRe{s} > -a, s +a' it follows from eq. (9.100) that te-atu(t) ~ _!!__ [-1-] = __1 _, CRe{s} >-a. (9.102) . ds s +a (s + a)2 In fact, by repeated application of eq. (9.100), we obtain t2 £ 1 -e-at u(t) ~--- CRe{s} >-a, (9.103) 2 (s+a)3' and, more generally, tn-I (n- 1)! e-at u(t) ffi-e{s} > -a. (9.104) (s+a)n' As the next example illustrates, this specific Laplace transform pair is particularly use- ful when applying partial-fraction expansion to the determination of the inverse Laplace transform of a rational function with multiple-order poles. Example 9. 1 5 Consider the Laplace transform 2s2 + 5s + 5 X(s) = (s + 1)2(s + 2)' ffi-e{s} > -1. Applying the partial-fraction expansion method described in the appendix, we can write 2 1 3 X(s) = (s + 1)2 - (s + 1) + s + 2' ffi-e{s} > -1. (9.105) 690 The Laplace Transform Chap.9 Since the ROC is to the right of the poles at s = -1 and -2, the inverse transform of each of the terms is a right-sided signal, and, applying eqs. (9.14) and (9.104), we obtain the inverse transform 9.5.9 Integration in the Time Domain If £ x(t) ~ X(s), with ROC= R, then L 1 x (T)dT -X(s), with ROC containing (9.106) s R n {(Re{s} > 0}. This property is the inverse of the differentiation property set forth in Section 9.5. 7. It can be derived using the convolution property presented in Section 9.5.6. Specifically, fx x(T )dT = u(t) * x(t). (9.107) From Example 9.1, with a = 0, £ u(t) ~­ ffi-e{s} > 0, (9.108) s' and thus, from the convolution property, £ * 1 u(t) x(t) ~ -X(s), (9.109) s with an ROC that contains the intersection of the ROC of X(s) and the ROC of the Laplace transform of u(t) in eq. (9.108), which results in the ROC given in eq. (9.106). 9.5.1 0 The Initial- and Final-Value Theorems Under the specific constraints that x(t) = 0 for t < 0 and that x(t) contains no impulses or higher order singularities at the origin, one can directly calculate, from the Laplace transform, the initial value x(O+)-i.e., x(t) as t approaches zero from positive values of t. Specifically the initial-value theorem states that x(O+) = lim sX(s), (9.110) s~x Also, if x(t) = 0 fort< 0 and, in addition, x(t) has a finite limit as t ~ x, then the final- value theorem says that lim x(t) = lim sX(s). (9.111) t---+x s---->0 The derivation of these results is considered in Problem 9.53. Sec. 9.5 Properties of the Laplace Transform 691 Example 9.16 The initial- and final-value theorems can be useful in checking the correctness of the Laplace transform calculations for a signal. For example, consider the signal x(t) in Example 9.4. From eq. (9.24), we see that x(O+) = 2. Also, using eq. (9.29), we find that . = . 2s3 + 5s2 + 12s hm sX(s) hm = 2, 3 2 S->OC S-->00 S + 4 S + 14 S + 20 which is consistent with the initial-value theorem in eq. (9.110). 9.5.11 Table of Properties In Table 9.1, we summarize the properties developed in this section. In Section 9. 7, many of these properties are used in applying the Laplace transform to the analysis and characterization of linear time-invariant systems. As we have illustrated in several exam- ples, the various properties of Laplace transforms and their ROCs can provide us with TABLE 9.1 PROPERTIES OF THE LAPLACE TRANSFORM Laplace Section Property Signal Transform ROC x(t) X(s) R x, (t) X1(s) R, x2(t) X2(s) R2 ---------- ----------- ------------------ 9.5.1 Linearity ax1 (t) + bx2(t) aX1 (s) + bX2(s) At least R1 n R2 9.5.2 Time shifting x(t- to) e-sto X(s) R 9.5.3 Shifting in the s-Domain esot x(t) X(s- s0 ) Shifted version of R (i.e., s is in the ROC if s - s0 is in R) 9.5.4 Time scaling x(at) Ia1! x(sa) Scaled ROC (i.e., s is in the ROC if s/a is in R) 9.5.5 Conjugation x*(t) X*(s*) R 9.5.6 Convolution x 1( t) * x2(t) X1(s)X2(s) At least R1 n R2 d 9.5. 7 Differentiation in the dix(t) sX(s) At least R Time Domain d 9.5.8 Differentiation in the -tx(t) dsX(s) R s-Domain 1 9.5.9 Integration in the Time foo X(T)d(T) -X(s) At least R n {<Re{s} > 0} Domain s Initial- and Final-Value Theorems 9.5.1 0 If x(t) = 0 for t < 0 and x(t) contains no impulses or higher-order singularities at t = 0, then x(O+) = lim sX(s) s~oo If x(t) = 0 fort< 0 and x(t) has a finite limit as t -7 oc, then lim x(t) = lim sX(s) 1-+-:xo S--t-:xo 692 The Laplace Transform Chap.9 considerable information about a signal and its transform that can be useful either in char- acterizing the signal or in checking a calculation. In Sections 9.7 and 9.8 and in some of the problems at the end of this chapter, we give several other examples of the uses of these properties. 9.6 SOME LAPLACE TRANSFORM PAIRS As we indicated in Section 9.3, the inverse Laplace transform can often be easily evaluated by decomposing X (s) into a linear combination of simpler terms, the inverse transform of each of which can be recognized. Listed in Table 9.2 are a number of useful Laplace TABLE 9.2 LAPLACE TRANSFORMS OF ELEMENTARY FUNCTIONS Transform pair Signal Transform ROC 1 8(t) 1 Ails 1 2 u(t) - ffi-c{s} > 0 s 1 3 -u( -t) - ffi-c{s} < 0 s rn-1 1 4 (n- 1)! u(t) - ffi-c{s} > 0 sn fn-1 1 5 - (n- 1)! u(-t) - ffi-c{s} < 0 sn 6 e-at u(t) 1 -- ffi-c{s} > -a s+a 7 -e-a1 1 u(-t) -- ffi-c{s} < -a s+a fn-1 1 8 (n- 1)! e-atu(t) -+- - ffi-c{s} > -a (s a)n fn-1 9 - (n _ )! e -at u ( -t) 1 --- ffi-c{s} < -a 1 (s + a)n 10 8(t- T) e-sT Ails s 11 [cosw0 t]u(t) -2 - ffi-c{s} > 0 s + w5 12 Wo [sin w0 t]u(t) ffi-c{s} > 0 s2 + w5 s+a 13 [e-at coswot]u(t) 2 ffi-c{s} > -a (s+a) +w5 14 [e-at sinwot]u(t) wo 2 ffi-c{s} > -a (s+a) +w5 15 u (t) = dno(t) sn n dtn All s 1 16 U-n(t) = u(t) * · · · * u(t) - ffi-c{s} > 0 '--.r----1 sn n times Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 693 transform pairs. Transform pair 1 follows directly from eq. (9.3). Transform pairs 2 and 6 follow directly from Example 9.1 with a = 0 and a = a, respectively. Transform pair 4 was developed in Example 9.14 using the differentiation property. Transform pair 8 follows from transform pair 4 using the property set forth in Section 9.5.3. Transform pairs 3, 5, 7, and 9 are based on transform pairs 2, 4, 6 and 8, respectively, together with the time- scaling property of section 9.5.4 with a = -1. Similarly, transform pairs 10 through 16 can all be obtained from earlier ones in the table using appropriate properties in Table 9.1 (see Problem 9.55). 9.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING THE LAPLACE TRANSFORM One of the important applications of the Laplace transform is in the analysis and character- ization of LTI systems. Its role for this class of systems stems directly from the convolution property (Section 9.5.6). Specifically, the Laplace transforms of the input and output of an LTI system are related through multiplication by the Laplace transform of the impulse response of the system. Thus, Y(s) = H(s )X(s ). (9.112) where X(s), Y(s), and H(s) are the Laplace transforms of the input, output, and impulse response of the system, respectively. Equation (9 .112) is the counterpart, in the context of Laplace transforms, of eq. (4.56) for Fourier transform. Also, from our discussion in Section 3.2 on the response of LTI systems to complex exponentials, if the input to an LTI system is x(t) = es1 , with sin the ROC of H(s), then the output will be H(s)est; i.e., est is an eigenfunction of the system with eigenvalue equal to the Laplace transform of the impulse response. If the ROC of H(s) includes the imaginary axis, then for s = jw, H(s) is the frequency response of the LTI system. In the broader context of the Laplace transform, H(s) is commonly referred to as the system function or, alternatively, the transfer function. Many properties of LTI systems can be closely associated with the characteristics of the system function in the s-plane. We illustrate this next by examining several important properties and classes of systems. 9. 7. 1 Causality For a causal LTI system, the impulse response is zero for t < 0 and thus is right sided. Consequently, from the discussion in Section 9.2, we see that The ROC associated with the system function for a causal system is a right-half plane. It should be stressed, however, that the converse of this statement is not necessarily true. That is, as illustrated in Example 9.19 to follow, an ROC to the right of the rightmost 694 The Laplace Transform Chap.9 pole does not guarantee that a system is causal; rather, it guarantees only that the impulse response is right sided. However, if H (s) is rational, then, as illustrated in Examples 9.17 and 9.18 to follow, we can determine whether the system is causal simply by checking to see if its ROC is a right-half plane. Specifically, For a system with a rational system function, causality of the system is equivalent to the ROC being the right-half plane to the right of the rightmost pole. Example 9. 1 7 Consider a system with impulse response h(t) = e-tu(t). (9.113) Since h(t) = 0 fort < 0, this system is causal. Also, the system function can be obtained from Example 9.1: H(s) = s + 1' CRe{s} > -1. (9.114) In this case, the system function is rational and the ROC in eq. (9 .114) is to the right of the rightmost pole, consistent with our statement that causality for systems with rational system functions is equivalent to the ROC being to the right of the rightmost pole. Example 9. 1 8 Consider a system with impulse response Since h(t) =I= 0 fort < 0, this system is not causal. Also, from Example 9.7, the system function is -2 H(s) = s _ , -1 < CRe{s} < +1. 2 1 Thus, H(s) is rational and has an ROC that is not to the right of the the rightmost pole, consistent with the fact that the system is not causal. Example 9.19 Consider the system function es H(s) = s + 1' CRe{s} > -1. (9.115) For this system, the ROC is to the right of the rightmost pole. Therefore, the impulse response must be right sided. To determine the impulse response, we first use the result Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 695 of Example 9.1: e-r u(t) CRe{s} > -1. (9.116) s + 1' Next, from the time-shifting property of Section 9.5.2 [eq. (9.87)], the factor e' in eq. (9.115) can be accounted for by a shift in the time function in eq. (9.116). Then CRe{s} > -1, (9.117) so that the impulse response associated with the system is h(t) = e-u+ 1 )u(t + 1), (9.118) which is nonzero for - 1 < t < 0. Hence, the system is not causal. This example serves as a reminder that causality implies that the ROC is to the right of the rightmost pole, but the converse is not in general true, unless the system function is rational. In an exactly analogous manner, we can deal with the concept of anticausality. A system is anticausal if its impulse response h(t) = 0 for t > 0. Since in that case h(t) would be left sided, we know from Section 9.2 that the ROC of the system function H(s) would have to be a left-half plane. Again, in general, the converse is not true. That is, if the ROC of H(s) is a left-half plane, all we know is that h(t) is left sided. However, if H(s) is rational, then having an ROC to the left of the leftmost pole is equivalent to the system being anticausal. 9.7.2 Stability The ROC of H(s) can also be related to the stability of a system. As mentioned in Sec- tion 2.3.7, the stability of an LTI system is equivalent to its impulse response being abso- lutely integrable, in which case (Section 4.4) the Fourier transform ofthe impulse response converges. Since the Fourier transform of a signal equals the Laplace transform evaluated along the jw-axis, we have the following: An LTI system is stable if and only if the ROC of its system function H(s) includes the entire jw-axis [i.e., ~e(s) = 0]. Example 9.20 Let us consider an LTI system with system function s - 1 H (s) = -(s_+_1) -(s---2-) (9.119) Since the ROC has not been specified, we know from our discussion in Section 9.2 that there are several different ROCs and, consequently, several different system impulse re- sponses that can be associated with the algebraic expression for H(s) given in eq. (9.119). 696 The Laplace Transform Chap.9 If, however. we have information about the causality or stability of the system, the ap- propriate ROC can be identified. For example, if the system is known to be causal, the ROC will be that indicated in Figure 9.25(a), with impulse response lz(t) = (3 2 e r + 31 e-,.,r ) u(t). (9.120) Note that this particular choice of ROC does not include the jw-axis, and consequently, the corresponding system is unstable (as can be checked by observing that h(t) is not absolutely integrable). On the other hand, if the system is known to be stable, the ROC is that given in Figure 9.25(b). and the corresponding impulse response is 2 . 1 ,., lz(t) = -e -ru(t)- - e-1u(-t), 3 3 which is absolutely integrable. Finally, for the ROC in Figure 9.25(c), the system is anticausal and unstable, with 9m 9m I I I I s-plane I s-plane I I I I I I I I X X X X -1 2 <R-c -1 2 <R-c I I I I I I I I I I I I I I I I I I (a) (b) 9m I I s-plane I I I xI X ~1 2 <R-c (c) Figure 9.25 Possible ROCs for the system function of Example 9.20 with poles at s = -1 and s = 2 and a zero at s = 1: (a) causal, unstable system; (b) noncausal, stable system; (c) anticausal, unstable system. Sec. 9.7 Analysis And Characterization of LTI Systems Using the Laplace Transform 697 It is perfectly possible, of course, for a system to be stable (or unstable) and have a system function that is not rational. For example, the system function in eq. (9.115) is not rational, and its impulse response in eq. (9 .118) is absolutely integrable, indicating that the system is stable. However, for systems with rational system functions, stability is easily interpreted in terms of the poles of the system. For example, for the pole-zero plot in Figure 9.25, stability corresponds to the choice of an ROC that is between the two poles, so that the jw-axis is contained in the ROC. For one particular and very important class of systems, stability can be characterized very simply in terms of the locations of the poles. Specifically, consider a causal LTI system with a rational system function H(s). Since the system is causal, the ROC is to the right of the rightmost pole. Consequently, for this system to be stable (i.e., for the ROC to include the jw-axis), the rightmost pole of H(s) must be to the left of the jw-axis. That is, A causal system with rational system function H(s) is stable if and only if all of the poles of H(s) lie in the left-half of the s-plane-i.e., all of the poles have negative real parts. Example 9.21 Consider again the causal system in Example 9.17. The impulse response in eq. (9 .113) is absolutely integrable, and thus the system is stable. Consistent with this, we see that the pole of H(s) in eq. (9.114) is at s = -1, which is in the left-half of the s-plane. In contrast, the causal system with impulse response h(t) = e21 u(t) is unstable, since h(t) is not absolutely integrable. Also, in this case 1 H(s) = --, ffi-R{s} > 2, s-2 so the system has a pole at s = 2 in the right half of the s-plane. Example 9.22 Let us consider the class of causal second-order systems previously discussed in Sec- tions 9.4.2 and 6.5.2. The impulse response and system function are, respectively, (9.121) and (9.122) where CJ = -~wn +wn~• (9.123) C2 = -~wn - Wn~' (9.124) M = Wn (9.125) 2~· 698 The Laplace Transform Chap. 9 9m I I s-plane I I I ~ I I I I I I I Figure 9.26 Pole locations and ROC for a causal second-order system with?< 0. In Figure 9.19, we illustrated the pole locations for ( > 0. In Figure 9.26, we illustrate the pole locations for ( < 0. As is evident from the latter figure and from eqs. (9.124) and (9.125), for ( < 0 both poles have positive real parts. Consequently, for ( < 0, the causal second-order system cannot be stable. This is also evident in eq. (9.121), since, with (Jl.e{c 1} > 0 and <Re{c2} > 0, each term grows exponentially as t increases, and thus h(t) cannot be absolutely integrable. 9. 7.3 LTI Systems Characterized by Linear Constant-Coefficient Differential Equations In Section 4. 7, we discussed the use of the Fourier transform to obtain the frequency re- sponse of an LTI system characterized by a linear constant -coefficient differential equation without first obtaining the impulse response or time-domain solution. In an exactly anal- ogous manner, the properties of the Laplace transform can be exploited to directly obtain the system function for an LTI system characterized by a linear constant-coefficient dif- ferential equation. We illustrate this procedure in the next example. Example 9.23 Consider an LTI system for which the input x(t) and output y(t) satisfy the linear constant -coefficient differential equation dy(t) -----;[( + 3y(t) = x(t). (9.126) Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 699 Applying the Laplace transform to both sides of eq. (9.126), and using the linearity and differentiation properties set forth in Sections 9.5.1 and 9.5.7, respectively [(eqs. (9.82) and (9.98)], we obtain the algebraic equation sY(s) + 3Y(s) = X(s). (9.127) Since, from eq. (9.112), the system function is H( ) = Y(s) s X(s)' we obtain, for this system, H(s) = s + 3"" (9.128) This, then, provides the algebraic expression for the system function, but not the region of convergence. In fact, as we discussed in Section 2.4, the differential equation itself is not a complete specification of the LTI system, and there are, in general, differ- ent impulse responses, all consistent with the differential equation. If, in addition to the differential equation, we know that the system is causal, then the ROC can be inferred to be to the right of the rightmost pole, which in this case corresponds to CRe{s} > -3. If the system were known to be anticausal, then the ROC associated with H(s) would be (Jl.e{s} < -3. The corresponding impulse response in the causal case is h(t) = e-3r u(t), (9.129) whereas in the anticausal case it is h(t) = -e-3t u( -t). (9.130) The same procedure used to obtain H (s) from the differential equation in Exam- ple 9.23 can be applied more generally. Consider a general linear constant-coefficient dif- ferential equation of the form ~ dky(t) _ ~ b dk x(t) Lak-dk - L k-dk · (9.131) k=O t k=O t Applying the Laplace transform to both sides and using the linearity and differenti- ation properties repeatedly, we obtain (9.132) or H(s) = (9.133) 700 The Laplace Transform Chap.9 Thus, the system function for a system specified by a differential equation is always ratio- nal, with zeros at the solutions of (9.134) and poles at the solutions of (9.135) Consistently with our previous discussion, eq. (9.133) does not include a specification of the region of convergence of H(s), since the linear constant-coefficient differential equa- tion by itself does not constrain the region of convergence. However, with additional in- formation, such as know ledge about the stability or causality of the system, the region of convergtmce can be inferred. For example, if we impose the condition of initial rest on the system, so that it is causal, the ROC will be to the right of the rightmost pole. Example 9.24 t An RLC circuit whose capacitor voltage and inductor current are initially zero constitutes an LTI system describable by a linear constant-coefficient differential equation. Consider the series RLC circuit in Figure 9.27. Let the voltage across the voltage source be the input signal x(t), and let the voltage measured across the capacitor be the output signal y(t). Equating the sum of the voltages across the resistor, inductor, and capacitor with the source 'voltage, we obtain 2 Rc dy(t) + LCd y(t) dt dt2 + () = () y t X t . (9.136) Applying eq. (9.133), we obtain l!LC H(s) = s2 + (RIL)s + (1/LC)' (9.137) As shown in Problem 9.64, if the values of R, L, and C are all positive, the poles of this system fupction will have negative real parts, and consequently, the system will be stable. R L Figure 9.27 A series RLC circuit. Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 701 9.7.4 Examples Relating System Behavior to the System Function As we have seen, system properties such as causality and stability can be directly related to the system function and its characteristics. In fact, each of the properties of Laplace transforms that we have described can be used in this way to relate the behavior of the system to the system function. In this section, we give several examples illustrating this. Example 9.25 Suppose we know that if the input to an LTI system is x(t) = e-3' u(t), then the output is As we now show, from this knowledge we can determine the system function for this system and from this can immediately deduce a number of other properties of the system. Taking Laplace transforms of x(t) and y(t), we get 1 X(s) = -- , ffi-e{s} > -3, s+ 3 and 1 Y(s) ffi-e{s} > -1. = (s + l)(s + 2)' From eq. (9.112), we can then conclude that H(s) = Y(s) = s + 3 s+3 X(s) (s + l)(s + 2) s2 + 3s + 2· Furthermore, we can also determine the ROC for this system. In particular, we know from the convolution property set forth in Section 9.5.6 that the ROC of Y(s) must include at least the intersections of the ROCs of X(s) and H(s). Examining the three possible choices for the ROC of H(s) (i.e., to the left of the pole at s = -2, between the poles at -2 and -1, and to the right of the pole at s = -1), we see that the only choice that is consistent with the ROCs of X(s) and Y(s) is ffi-e{s} > -1. Since this is to the right of the rightmost pole of H (s ), we conclude that H (s) is causal, and since both poles of H(s) have negative real parts, it follows that the system is stable. Moreover, from the relationship between eqs. (9.131) and (9.133), we can specify the differential equation that, together with the condition of initial rest, characterizes the system: d 2y(t) dy(t) 2 () _ dx(t) () d t-? + 3 d t + yt- d + 3 xt. t Example 9.26 Suppose that we are given the following information about an LTI system: 1. The system is causal. 2. The system function is rational and has only two poles, at s = -2 and s = 4. 702 The Laplace Transform Chap.9 3. If x(t) = 1, then y(t) = 0. 4. The value of the impulse response at t = o+ is 4. From this information we would like to determine the system function of the system. From the first two facts, we know that the system is unstable (since it is causal and has a pole at s = 4 with positive real part) and that the system function is of the form p(s) p(s) H(s) = (s + 2)(s- 4) s2 - 2s- 8' where p(s) is a polynomial ins. Because the response y(t) to the input x(t) = 1 = e0·r must equal H(O) · e0'' = H(O), we conclude, from fact 3, that p(O) = 0-i.e., that p(s) must have a root at s = 0 and thus is of the form p(s) = sq(s), where q(s) is another polynomial ins. Finally, from fact 4 and the initial-value theorem in Section 9.5.10, we see that 1' H( 1' s2q(s) 4 s~ s s) = s~ s2 - 2s- 8 = · (9.138) Ass ~ oo, the terms of highest power ins in both the numerator and the denominator of sH(s) dominate and thus are the only ones of importance in evaluating eq. (9.138). Furthermore, if the numerator has higher degree than the denominator, the limit will diverge. Consequently, we can obtain a finite nonzero value for the limit only if the degree of the numerator of sH(s) is the same as the degree of the denominator. Since the degree of the denominator is 2, we conclude that, for eq. (9.138) to hold, q(s) must be a constant-i.e., q(s) = K. We can evaluate this constant by evaluating (9.139) Equating eqs. (9.138) and (9.139), we see that K = 4, and thus, 4s H(s) = (s + 2)(s- 4) Example 9.27 Consider a stable and causal system with impulse response h(t) and system function H(s). Suppose H(s) is rational, contains a pole at s = -2, and does not have a zero at the origin. The location of all other poles and zeros is unknown. For each of the following statements let us determine whether we can definitely say that it is true, whether we can definitely say that it is false, or whether there is insufficient information to ascertain the statement's truth: (a) ~ { h(t)e3'} converges. (b) L+: h(t) dt = o. (c) th(t) is the impulse response of a causal and stable system. Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 703 (d) d h(t)ldt contains at least one pole in its Laplace transform. (e) h(t) has finite duration. (f) H(s) = H( -s). (g) lim.1 _,x H(s) = 2. Statement (a) is false, since ty{h(t)e3t} corresponds to the value of the Laplace transform of h(t) at s = -3. If this converges, it implies that s = -3 is in the ROC. A causal and stable system must always have its ROC to the right of all of its poles. However, s = -3 is not to the right of the pole at s = -2. Statement (b) is false, because it is equivalent to stating that H(O) = 0. This con- tradicts the fact that H(s) does not have a zero at the origin. Statement (c) is true. According to Table 9.1, the property set forth in Section 9.5.8, the Laplace transform of th(t) has the same ROC as that of H(s). This ROC includes the jw-axis, and therefore, the corresponding system is stable. Also, h(t) = 0 fort < 0 implies that th(t) = 0 fort < 0. Thus, th(t) represents the impulse response of a causal system. Statement (d) is true. According to Table 9.1, dh(t)ldt has the Laplace transform sH(s). The multiplication by s does not eliminate the pole at s = -2. Statement (e) is false. If h(t) is of finite duration, then if its Laplace transform has any points in its ROC, the ROC must be the entire s-plane. However, this is not consistent with H(s) having a pole at s = -2. Statement (f) is false. If it were true, then, since H(s) has a pole at s = -2, it must also have a pole at s = 2. This is inconsistent with the fact that all the poles of a causal and stable system must be in the left half of the s-plane. The truth of statement (g) cannot be ascertained with the information given. The statement requires that the degree of the numerator and denominator of H(s) be equal, and we have insufficient information about H(s) to determine whether this is the case. 9. 7. 5 Butterworth Filters In Example 6.3 we briefly introduced the widely-used class of LTI systems known as Butterworth filters. The filters in this class have a number of properties, including the characteristics of the magnitude of the frequency response of each of these filters in the passband, that make them attractive for practical implementation. As a further illustration of the usefulness of Laplace transforms, in this section we use Laplace transform tech- niques to determine the system function of a Butterworth filter from the specification of its frequency response magnitude. An Nth-order lowpass Butterworth filter has a frequency response the square of whose magnitude is given by 1 + (9.140) (jwl jwc)2N' where N is the order of the filter. From eq. (9.140), we would like to determine the system function B(s) that gives rise to IB(Jw )1 2. We first note that, by definition, IB(Jw )1 2 = B(jw )B*(jw ). (9.141) 704 The Laplace Transform Chap.9 If we restrict the impulse response of the Butterworth filter to be real, then from the prop- erty of conjugate symmetry for Fourier transforms, B*(jw) = B(- jw), (9.142) so that 1 B(jw )B(- jw) = 1 + (jwl jwc)2N. (9.143) Next, we note that B(s)ls=jw = B(jw), and consequently, from eq. (9.143), 1 B(s)B( -s) = 1 + ( I . )2N. (9.144) S ]We The roots of the denominator polynomial corresponding to the combined poles of B(s)B( -s) are at s = ( -1)112N (jwc). (9.145) Equation (9 .145) is satisfied for any value s = s P for which lspl = We (9.146) and 7T(2k + 1) 7T <.sp = 2N + 2' k an integer; (9.147) that is, s = w exp 1. [7T(2kN + 1) P c ( + 7T 12 ]) . (9.148) 2 In Figure 9.28 we illustrate the positions of the poles of B(s)B( -s) for N 1, 2, 3, and 6. In general, the following observations can be made about these poles: 1. There are 2N poles equally spaced in angle on a circle of radius We in the s-plane. 2. A pole never lies on the jw-axis and occurs on the 0'-axis for N odd, but not for N even. 3. The angular spacing between the poles of B(s)B( -s) is 7T/N radians. To determine the poles of B(s) given the poles of B(s)B( -s), we observe that the poles of B(s)B( -s) occur in pairs, so that if there is a pole at s = sp, then there is also a pole at s = -sp. Consequently, to construct B(s), we choose one pole from each pair. If we restrict the system to be stable and causal, then the poles that we associate with B(s) are the poles along the semicircle in the left-half plane. The pole locations specify B(s) only to within a scale factor. However, from eq. (9.144), we see that B2(s)ls=O = 1, or equivalently, from eq. (9 .140), the scale factor is chosen so that the square of the magnitude of the frequency response has unity gain at w = 0. To illustrate the determination of B(s), let us consider the cases N = 1, N = 2, and N = 3. In Figure 9.28 we showed the poles of B(s)B( -s), as obtained from eq. (9.148). Sec. 9.7 Analysis and Characterization of LTI Systems Using the Laplace Transform 705 9m N=1 .... ..... / "" ' \ I \ ----X X------ \ 'W \ I c ' ..... _ .... ""/ 9m Figure 9.28 Position of the poles of B(s)B( -s) for N = 1, 2, 3, and 6. In Figure 9.29 we show the poles associated with B(s) for each of these values of N. The corresponding transfer functions are: N = 1: We . B(s) = (9.149) S +We' N = 2: (9.150) N = 3: (9.151) (s + wc)(s2 + wcs + w~) w~ Based on the discussion in Section 9.7.3, from B(s) we can determine the associated linear constant-coefficient differential equation. Specifically, for the foregoing three values 706 The Laplace Transform Chap. 9 N=1 N=2 ; ,. iwc / I I --X--+------ - we\ CRo£ -we\ CRo£ \ ' ' X .... .. .... x- iwc / I ---w--c *\ --r--------CR-o£ \ 'x_ Figure 9.29 Position of the poles of B(s) for N = 1, 2, and 3. of N, the corresponding differential equations are: N = 1: (9.152) N = 2: (9.153) N = 3: 9.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS The use of the Laplace transform allows us to replace time-domain operations such as differentiation, convolution, time shifting, and so on, with algebraic operations. We have already seen many of the benefits of this in terms of analyzing LTI systems, and in this section we take a look at another important use of system function algebra, namely, in analyzing interconnections of LTI systems and synthesizing systems as interconnections of elementary system building blocks. Sec. 9.8 System Function Algebra and Block Diagram Representations 707 9.8.1 System Functions for Interconnections of LTI Systems Consider the parallel interconnection of two systems, as shown in Figure 9.30(a). The impulse response of the overall system is (9.155) and from the linearity of the Laplace transform, H(s) = H1 (s) + H2(s). (9.156) Similarly, the impulse response of the series interconnection in Figure 9 .30(b) is (9.157) and the associated system function is (9.158) (a) Figure 9.30 (a) Parallel intercon- nection of two LTI systems; (b) series (b) combination of two LTI systems. The utility of the Laplace transform in representing combinations of linear systems through algebraic operations extends to far more complex interconnections than the simple parallel and series combinations in Figure 9.30. To illustrate this, consider the feedback interconnection of two systems, as indicated in Figure 9 .31. The design, applications, and analysis of such interconnections are treated in detail in Chapter 11. While analysis of the system in the time domain is not particularly simple, determining the overall system func- tion from input x(t) to output y(t) is a straightforward algebraic manipulation. Specifically, from Figure 9.31, Y(s) = H 1 (s)E(s), (9.159) E(s) = X(s) - Z(s), (9.160) 708 The Laplace Transform Chap.9 + x(t)--~ 1---------.... y(t) Figure 9.31 Feedback interconnec- tion of two LTI systems. and Z(s) = H2(s)Y(s), (9.161) from which we obtain the relation Y(s) = H1 (s)[X(s) - H2(s)Y(s)], (9.162) or Y(s) = H(s) = H 1( s) + (9.163) X(s) 1 H1 (s)H2(s) · 9.8.2 Block Diagram Representations for Causal LTI Systems Described by Differential Equations and Rational System Functions In Section 2.4.3, we illustrated the block diagram representation of an LTI system de- scribed by a first-order differential equation using the basic operations of addition, multi- plication by a coefficient, and integration. These same three operations can also be used to build block diagrams for higher order systems, and in this section we illustrate this in several examples. Example 9.28 Consider the causal LTI system with system function 1 H(s) = --. s+3 From Section 9.7.3, we know that this system can also be described by the differential equation dy(t) ---;[( + 3y(t) = x(t), together with the condition of initial rest. In Section 2.4.3 we constructed a block diagram representation, shown in Figure 2.32, for a first-order system such as this. An equiva- lent block diagram (corresponding to Figure 2.32 with a = 3 and b = 1) is shown in Sec. 9.8 System Function Algebra and Block Diagram Representations 709 x(t) 1----.......- ~ y(t) (a) (b) Figure 9.32 (a) Block diagram representation of the causal LTI system in Example 9.28; (b) equivalent block diagram representation. Figure 9.32(a). Here, lis is the system function of a system with impulse response u(t), i.e., it is the system function of an integrator. Also, the system function -3 in the feed- back path in Figure 9.32(a) corresponds to multiplication by the coefficient -3. The block diagram in the figure involves a feedback loop much as we considered in the pre- vious subsection and as pictured in Figure 9.31, the sole difference being that the two signals that are the inputs to the adder in Figure 9.32(a) are added, rather than sub- tracted as in Figure 9.31. However, as illustrated in Figure 9.32(b), by changing the sign of the coefficient in the multiplication in the feedback path, we obtain a block diagram representation of exactly the same form as Figure 9.31. Consequently, we can apply eq. (9.163) to verify that lis H(s) = 1 + 3/s s + 3' Example 9.29 Consider now the causal LTI system with system function H(s) = -s+-2 = ( -1- ) (s + 2). (9.164) s+3 s+3 As suggested by eq. (9.164), this system can be thought of as a cascade of a system with system function ll(s + 3) followed by a system with system functions + 2, and 710 The Laplace Transform Chap. 9 we have illustrated this in Figure 9.33(a), in which we have used the block diagram in Figure 9.32(a) to represent 1/(s + 3). It is also possible to obtain an alternative block diagram representation for the system in eq. (9.164). Using the linearity and differentiation properties of the Laplace transform, we know that y(t) and z(t) in Figure 9.33 (a) are related by dz(t) y(t) = --;[( + 2z(t). However, the input e(t) to the integrator is exactly the derivative of the output z(t), so that y(t) = e(t) + 2z(t), which leads directly to the alternative block diagram representation shown in Fig- ure 9.33(b). Note that the block diagram in Figure 9.33(a) requires the differentiation of z(t), since dz(t) y(t) = --;[( + 2z(t) In contrast, the block diagram in Figure 9.33(b) does not involve the explicit differenti- ation of any signal. r---------------------------------------1 I I I _x_(t.-) ---~ + 1----e(;..;.t)_-+-1 z(t) y(t) (a) r---........ y(t} __x( -t)--!~ + l---e(;..;.t). .......... __- +-1 (b) Figure 9.33 (a) Block diagram representations for the system in Exam- ple 9.29; (b) equivalent block diagram representation. Sec. 9.8 System Function Algebra and Block Diagram Representations 711 Example 9.30 Consider next a causal second-order system with system function 1 H(s) = (s + 1)(s + 2) + + (9.165) s2 3s 2· The input x(t) and output y(t) for this system satisfy the differential equation 2 d y(t) 3dy(t) 2 ()- () '""' + d (9.166) d t~ t + yt -xt. By employing similar ideas to those used in the preceding examples, we obtain the block diagram representation for this system shown in Figure 9.34(a). Specifically, since the y(t) (a) x(t) ·t y(t) t ·~ ·~ (b) x(t) y(t) (c) Figure 9.34 Block diagram representations for the system in Exam- ple 9.30: (a) direct form; (b) cascade form; (c) parallel form. 712 The Laplace Transform Chap. 9 input to an integrator is the derivative of the output of the integrator, the signals in the block diagram are related by f(t) = d;~t), e(t) = df(t) dt Also, eq. (9.166) can be rewritten as dy(t) -3----;[t - 2y(t) + x(t), or e(t) = -3f(t)- 2y(t) + x(t), which is exactly what is represented in Figure 9.34(a). The block diagram in this figure is sometimes referred to as a direct-form repre- sentation, since the coefficients appearing in the diagram can be directly identified with the coefficients appearing in the system function or, equivalently, the differential equa- tion. Other block diagram representations of practical importance also can be obtained after a modest amount of system function algebra. Specifically, H(s) in eq. (9.165) can be rewritten as H(s) ~ (s ~ 1) (s ~ 2). which suggests that this system can be represented as the cascade of two first-order sys- tems. The cascade-form representation corresponding to H (s) is shown in Figure 9 .34(b ). Alternatively, by performing a partial-fraction expansion of H(s), we obtain 1 1 H(s) = --1 - --2, s + s + which leads to the parallel-form representation depicted in Figure 9.34(c). Example 9.31 As a final example, consider the system function 2 H(s) = 2s + 4s- 6. (9.167) s2 + 3s + 2 Once again, using system function algebra, we can write H(s) in several different forms, each of which suggests a block diagram representation. In particular, we can write H(s) ~ (sz + !s + 2 2 )(2s + 4s- 6), which suggests the representation of H(s) as the cascade of the system depicted in Fig- ure 9.34(a) and the system with system function 2s2 + 4s- 6. However, exactly as we Sec. 9.8 System Function Algebra and Block Diagram Representations 713 did in Example 9.29, we can extract the derivatives required for this second system by ""tapping"" the signals appearing as the inputs to the integrators in the first system. The details qf this construction are examined in Problem 9.36, and the result is the direct- form block diagram shown in Figure 9.35. Once again, in the direct-form representation the coefficients appearing in the block diagram can be determined by inspection from the coefficients in the system function in eq. (9.167). y(t) x(t) Figure 9.35 Direct-form representation for the system in Example 9.31. Alternatively, we can write H(s) in the form H(s) = (2(s- 1))(~) (9.168) s+2 s+1 or 6 8 H(s) = 2+ -----. (9.169) s+2 s+1 The first of these suggests a cascade-form representation, while the second leads to a parallel-form block diagram. These are also considered in Problem 9.36. The methods for constructing block diagram representations for causal LTI systems described by differential equations and rational system functions can be applied equally well to higher order systems. In addition, there is often considerable flexibility in how this is done. For example, by reversing the numerators in eq. (9.168), we can write H(s) = (~)(2(s ~ 1) ), s+2 s+2 which suggests a different cascade form. Also, as illustrated in Problem 9.38, a fourth- order system function can be written as the product of two second-order system functions, each of which can be represented in a number of ways (e.g., direct form, cascade, or par- allel), and it can also be written as the sum of lower order terms, each of which has sev- eral different representations. In this way, simple low-order systems can serve as building blocks for the implementation of more complex, higher order systems. 714 The Laplace Transform Chap.9 9.9 THE UNilATERAl lAPlACE TRANSFORM In the preceding sections of this chapter, we have dealt with what is commonly called the bilateral Laplace transform. In this section, we introduce and examine a somewhat different transform, the unilateral Laplace transform, which is of considerable value in analyzing causal systems and, particularly, systems specified by linear constant-coefficient differential equations with nonzero initial conditions (i.e., systems that are not initially at rest). The unilateral Laplace transform of a continuous-time signal x(t) is defined as ~(s) ~ r(IO x(t)e-st dt, Jo- (9.170) where the lower limit of integration, o-, signifies that we include in the interval of integration any impulses or higher order singularity functions concentrated at t = 0. Once again we adopt a convenient shorthand notation for a signal and its unilateral Laplace transform: 'U£ x(t) ~ ~(s) = 11£{ x(t)}. (9.171) Comparing eqs. (9.170) and (9.3), we see that the difference in the definitions of the unilateral and bilateral Laplace transform lies in the lower limit on the integral. The bilateral transform depends on the entire signal from t = -oo tot = +oo, whereas the uni- lateral transform depends only on the signal from t = o- to oo. Consequently, two signals that differ for t < 0, but that are identical for t ~ 0, will have different bilateral Laplace transforms, but identical unilateral transforms. Similarly, any signal that is identically zero fort < 0 has identical bilateral and unilateral transforms. Since the unilateral transform of x(t) is identical to the bilateral transform of the signal obtained from x(t) by setting its value to 0 for all t < 0, many of the insights, concepts, and results pertaining to bilateral transforms can be directly adapted to the unilateral case. For example, using Property 4 in Section 9.2 for right-sided signals, we see that the ROC for eq. (9.170) is always a right-half plane. The evaluation of the inverse unilateral Laplace transforms is also the same as for bilateral transforms, with the constraint that the ROC for a unilateral transform must always be a right-half plane. 9. 9. 1 Examples of Unilateral Laplace Transforms To illustrate the unilateral Laplace transform, let us consider the following examples: Example 9. 32 Consider the signal t""-1 x(t) = (n _ l)! e-ar u(t). (9.172) Sec. 9.9 The Unilateral Laplace Transform 715 Since x(t) = 0 fort < 0, the unilateral and bilateral transforms are identical. Thus, from Table 9.2, X(s) = ar' (Jl.e{s} > -a. (9.173) (s + Example 9.33 Consider next x(t) = e-a(t+ l)u(t + 1). (9.174) The bilateral transform X (s) for this example can be obtained from Example 9.1 and the time-shifting property (Section 9.5.2): es X(s) = --, (Jl.e{s} >-a. (9.175) s+a By contrast, the unilateral transform is X(s) = L~ e-a(l+ l)u(t + 1)e-s1 dt (9.176) -a 1 e s +a' ffic{s} > -a. = Thus, in this example, the unilateral and bilateral Laplace transforms are clearly dif- ferent. In fact, we should recognize X(s) as the bilateral transform not of x(t), but of x(t)u(t), consistent with our earlier comment that the unilateral transform is the bilateral transform of a signal whose values for t < o- have been set to zero. Example 9.34 Consider the signal x(t) = o(t) + 2u1 (t) + e1 u(t). (9.177) Since x(t) = 0 for t < 0, and since singularities at the origin are included in the interval of integration, the unilateral transform for x(t) is the same as the bilateral transform. Specifically, using the fact (transform pair 15 in Table 9.2) that the bilateral transform of Un(t) is sn, we have 1 s(2s- 1) X(s) = X(s) = 1 + 2s + s _ ffic{s} > 1. (9.178) 1 s- 1 ' Example 9.35 Consider the unilateral Laplace transform X - 1 (s) - (s + 1)(s + 2) (9.179) 716 The Laplace Transform Chap.9 In Example 9.9, we considered the inverse transform for a bilateral Laplace transform of the exact form as that in eq. (9.179) and for several ROCs. For the unilateral transform, the ROC must be the right-half plane to the right of the rightmost pole of X(s); i.e., in this case, the ROC consists of all points s with CR.e{s} > -1. We can then invert this unilateral transform exactly as in Example 9.9 to obtain x(t) = [e-t- e-2t]u(t) for t > o-, (9.180) where we have emphasized the fact that unilateral Laplace transforms provide us with information about signals only for t > o-. Example 9.36 Consider the unilateral transform s2 - 3 X(s) = --. (9.181) s+2 Since the degree of the numerator of X(s) is not strictly less than the degree of the de- nominator, we expand X(s) as + + -c- X(s) = A Bs . (9.182) s+ 2 Equating eqs. (9.181) and (9.182), and clearing denominators, we obtain s2 - 3 = (A+ Bs)(s + 2) + C, (9.183) and equating coefficients for each power of s yields 1 X(s) = -2 + s + --, (9.184) s+ 2 with an ROC of CRe{s} > -2. Taking inverse transforms of each term results in X(t) = -28(t) + UJ(t) + e-Ztu(t) for t > 0-. (9.185) 9.9.2 Properties of the Unilateral Laplace Transform As with the bilateral Laplace transform, the unilateral Laplace transform has a number of important properties, many of which are the same as their bilateral counterparts and sev- eral of which differ in significant ways. Table 9.3 summarizes these properties. Note that we have not included a column explicitly identifying the ROC for the unilateral Laplace transform for each signal, since the ROC of any unilateral Laplace transform is always a right-half plane. For example the ROC for a rational unilateral Laplace transform is always to the right of the rightmost pole. Contrasting Table 9.3 with Table 9.1 for the bilateral transform, we see that, with the caveat that ROCs for unilateral Laplace transforms are always right-half planes, the linearity, s-domain shifting, time-scaling, conjugation and differentiation in the s-domain Sec. 9.9 The Unilateral Laplace Transform 717 TABLE 9.3 PROPERTIES OF THE UNILATERAL LAPLACE TRANSFORM Property Signal Unilateral Laplace Transform x(t) a::(s) x,(t) a::,(s) X2(t) a::2 (s) ---------------- ----------- ------------------ Linearity ax1( t) + bx2(t) aa::,(s) + ha::2(s) Shifting in the s-domain esot x(t) a::cs - so) Time scaling x(at), a>O ~a:(~) Conjugation X* (t) x * (s) Convolution (assuming X1 (t) * X2(t) a:, (s) a::z(s) that x 1( t) and x 2(t) are identically zero for t < 0) d Differentiation in the time dt x(t) s a::cs) - xco-) domain d Differentiation in the -tx(t) ds a:(s) s-domain 1 Integration in the time L- X(T)dT - a::(s) domain s ---------------- ------------------------------ Initial- and Final-Value Theorems If x(t) contains no impulses or higher-order singularities at t = 0, then x(O+) = E.n.. : !sa::(s) limx(t) = limsa::(s) f~'X· s~o properties are identical to their bilateral counterparts. Similarly, the initial- and final-value theorems stated in Section 9.5.10 also hold for unilateral Laplace transforms.3 The deriva- tion of each of these properties is identical to that of its bilateral counterpart. The convolution property for unilateral transforms also is quite similar to the corre- sponding property for bilateral transforms. This property states that if Xt (t) = x2(t) = 0 for all t < 0, (9.186) 3In fact, the initial- and final-value theorems are basically unilateral transform properties, as they apply only to signals x(t) that are identically 0 fort < 0. 718 The Laplace Transform Chap.9 then (9.187) Equation (9 .187) follows immediately from the bilateral convolution property, since, under the conditions of eq. (9 .186), the unilateral and bilateral transforms are identical for each of the signals x 1( t) and x2(t). Thus, the system analysis tools and system function algebra developed and used in this chapter apply without change to unilateral transforms, as long as we deal with causal LTI systems (for which the system function is both the bilateral and the unilateral transform of the impulse response) with inputs that are identically zero fort< 0. An example of this is the integration property in Table 9.3: If x(t) = 0 fort< 0, then t V£ 1 i x(T) dT = x(t) * u(t) ~ X(s)'U(s) = -X(s) (9.188) o- s As a second case in point, consider the following example: Example 9.37 Suppose a causal LTI system is described by the differential equation d 2 _v.(- t) + d v(t) 3 _._ + 2 (t) = (t) dt (9.189) 2 dt )' X ' together with the condition of initial rest. Using eq. (9.133), we find that the system function for this system is J{(s) = ----=----- s2 + (9.190) 3s + 2' Let the input to this system be x(t) = a u(t). In this case, the unilateral (and bilateral) Laplace transform of the output y(t) is 'Y(s) = J{(s) X(s) = t s(s + )(s + 2) a/2 a a/2 = ----+--. (9.191) s s+1 s+2 Applying Example 9.32 to each term ofeq. (9.191) yields =a[~- 1 + ~e- 21 y(t) e- ]u(t). (9.192) It is important to note that the convolution property for unilateral Laplace transforms applies only if the signals x 1( t) and x2(t) in eq. (9.187) are both zero fort < 0. That is, while we have seen that the bilateral Laplace transform of x 1( t) * x2(t) always equals the product of the bilateral transforms of x 1( t) and x2(t), the unilateral transform of x 1( t)*x2(t) in general does not equal the product of the unilateral transforms if either x 1( t) or x2(t) is nonzero fort< 0. (See, for example, Problem 9.39). Sec. 9.9 The Unilateral Laplace Transform 719 A particularly important difference between the properties of the unilateral and bi- lateral transforms is the differentiation property. Consider a signal x(t) with unilateral Laplace transform ~(s). Then, integrating by parts, we find that the unilateral transform of dx(t)ldt is given by oo d x(t) J, --e-st dt = x(t)e-st loo + s J, oo x(t)e-st dt 0- dt 0- o- (9.193) = s~(s) - x(O-). Similarly, a second application of this would yield the unilateral Laplace transform of d2 x(t)ldt2 , i.e., (9.194) where x'(O-) denotes the derivative of x(t) evaluated at t = o-. Clearly, we can continue the procedure to obtain the unilateral transform of higher derivatives. 9. 9. 3 Solving Differential Equations Using the Unilateral laplace Transform A primary use of the unilateral Laplace transform is in obtaining the solution of linear constant-coefficient differential equations with nonzero initial conditions. We illustrate this in the following example: Example 9.38 Consider the system characterized by the differential equation (9.189) with initial con- ditions (9.195) Let x(t) = au(t). Then, applying the unilateral transform to both sides of eq. (9.189), we obtain s2'Y(s) - f3s - y + 3s'Y(s) - 3{3 + 2'Y(s) = ~. (9.196) s or C)'( ) {3(s + 3) vS = + + + y a (9.197) (s 1)(s 2) (s + 1)(s + +- 2) s(s +- ---1)(s + 2)' where 'Y(s) is the unilateral Laplace transform of y(t). Referring to Example 9.37 and, in particular, to eq. (9.191), we see that the last term on the right-hand side of eq. (9.197) is precisely the unilateral Laplace transform of the response of the system when the initial conditions in eq. (9.195) are both zero ({3 = y = 0). That is, the last term represents the response of the causal LTI system described by eq. (9.189) and the condition of initial rest. This response is often referred 720 The Laplace Transform Chap.9 to as the zero-state response-i.e., the response when the initial state (the set of initial conditions in eq. (9.195)) is zero. An analogous interpretation applies to the first two terms on the right-hand side of eq. (9.197). These terms represent the unilateral transform of the response of the system when the input is zero (a = 0). This response is commonly referred to as the zero- input response. Note that the zero-input response is a linear function of the values of the initial conditions (e.g., doubling the values of both f3 andy doubles the zero-input response). Furthermore, eq. (9.197) illustrates an important fact about the solution of linear constant-coefficient differential equations with nonzero initial conditions, namely, that the overall response is simply the superposition of the zero-state and the zero-input responses. The zero-state response is the response obtained by setting the initial condi- tions to zero-i.e., it is the response of an LTI system defined by the differential equa- tion and the condition of initial rest. The zero-input response is the response to the initial conditions with the input set to zero. Other examples illustrating this can be found in Problems 9.20, 9.40, and 9.66. Finally, for any values of a, {3, andy, we can, of course, expand 'Y(s) in eq. (9.197) in a partial-fraction expansion and invert to obtain y(t). For example, if a = 2, f3 = 3, andy = -5, then performing a partial-fraction expansion for eq. (9.197) we find that 1 1 3 'Y(s) = - - -- + -- . (9.198) s s+1 s+2 Applying Example 9.32 to each term then yields y(t) = [1 - e- 1 + 3e- 21 ]u(t) for t > 0. (9.199) 9.10 SUMMARY In this chapter, we have developed and studied the Laplace transform, which can be viewed as a generalization of the Fourier transform. It is particularly useful as an analytical tool in the analysis and study ofLTI systems. Because of the properties of Laplace transforms, LTI systems, including those represented by linear constant- coefficient differential equations, can be characterized and analyzed in the transform domain by algebraic manipulations. In addition, system function algebra provides a convenient tool both for analyzing intercon- nections of LTI systems and for constructing block diagram representations of LTI systems described by differential equations. For signals and systems with rational Laplace transforms, the transform is often con- veniently represented in the complex plane (s-plane) by marking the locations of the poles and zeros and indicating the region of convergence. From the pole-zero plot, the Fourier transform can be geometrically obtained, within a scaling factor. Causality, stability, and other characteristics are also easily identified from knowledge of the pole locations and the region of convergence. In this chapter, we have been concerned primarily with the bilateral Laplace trans- form. However, we also introduced a somewhat different form of the Laplace transform known as the unilateral Laplace transform. The unilateral transform can be interpreted as the bilateral transform of a signal whose values prior to t = o- have been set to zero. This form of the Laplace transform is especially useful for analyzing systems described by linear constant-coefficient differential equations with nonzero initial conditions. Chap.9 Problems 721 Chapter 9 Problems The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 9.1. For each of the following integrals, specify the values of the real parameter u which ensure that the integral converges: (a) Jooo e-5t e-(a+ jw)t dt (b) J~ ooe-5t e-(a+ jw)t dt (c) J~ 5e-5t e-(a+ jw)t dt (d) J~ ooe-5t e-(a+ jw)t dt (e) J- ooooe-5ltle-(a+ jw)t dt (f) J~ ooe-51tle-(a+ jw)t dt 9.2. Consider the signal x(t) = e-5tu(t -1), and denote its Laplace transform by X(s). (a) Using eq. (9.3), evaluate X(s) and specify its region of convergence. (b) Determine the values of the finite numbers A and to such that the Laplace trans- form G(s) of g(t) = Ae-51u(-t-to) has the same algebraic form as X(s). What is the region of convergence corre- sponding to G(s)? 9.3. Consider the signal x(t) = e- 51 u(t)+e-f31u(t), and denote its Laplace transform by X(s). What are the constraints placed on the real and imaginary parts of {3 if the region of convergence of X(s) is CRe{s} > - 3? 9.4. For the Laplace transform of x(t) = { et sin 2t, t :::; 0 0, t>O' indicate the location of its poles and its region of convergence. 9.5. For each of the following algebraic expressions for the Laplace transform of a signal, determine the number of zeros located in the finite s-plane and the number of zeros located at infinity: 1 1 (a) s + 1 + s + 3 s + 1 (b)-- s2- 1 s3 - 1 (c) s2 + s + 1 722 The Laplace Transform Chap.9 9.6. An absolutely integrable signal x(t) is known to have a pole at s = 2. Answer the following questions: (a) Could x(t) be of finite duration? (b) Could x(t) be left sided? (c) Could x(t) be right sided? (d) Could x(t) be two sided? 9.7. How many signals have a Laplace transform that may be expressed as (s- 1) (s + 2)(s + 3)(s2 + s + 1) in its region of convergence? 9.8. Let x(t) be a signal that has a rational Laplace transform with exactly two poles, located at s = -1 and s = -3. If g(t) = e2 t x(t) and G(jw) [the Fourier transform of g(t)] converges, determine whether x(t) is left sided, right sided, or two sided. 9.9. Given that oC 1 e-atu(t) ~ --, ffi-e{ s} > ffi-e{- a}, s+a determine the inverse Laplace transform of 2(s + 2) X(s) = s 2 + 7 ffi-e{s} > -3. s + 12' 9.10. Using geometric evaluation of the magnitude of the Fourier transform from the cor- responding pole-zero plot, determine, for each of the following Laplace transforms, whether the magnitude of the corresponding Fourier transform is approximately lowpass, highpass, or bandpass: 1 (a) H 1( s) = (s + 1)(s + 3), ffi-e{s} > -1 (b) H 2(s) = s2 +: +1 , ffi-e{s} > - i s2 (c) H 3(s) = s2 + s + 1, ffi-e{s} > -1 2 9.11. Use geometric evaluation from the pole-zero plot to determine the magnitude of the Fourier transform of the signal whose Laplace transform is specified as s2 - s + 1 1 X(s) = , ffi-e{s} > -- . s 2 + s + 1 2 9.12. Suppose we are given the following three facts about the signal x(t): 1. x(t) = 0 for t < 0. 2. x(k/80) = 0 fork = 1, 2, 3, .... 3. x(l/160) = e- 120 . Let X(s) denote the Laplace transform of x(t), and determine which of the following statements is consistent with the given information about x(t): (a) X(s) has only one pole in the finites-plane. (b) X(s) has only two poles in the finites-plane. (c) X(s) has more than two poles in the finites-plane. Chap.9 Problems 723 9.13. Let g(t) = x(t) +ax( -t), where x(t) = {3 e -t u(t) and the Laplace transform of g(t) is s G(s) = s2- 1' -1 < (ft..e{s} < 1. Determine the values of the constants a and {3. 9.14. Suppose the following facts are given about the signal x(t) with Laplace transform X(s): 1. x(t) is real and even. 2. X(s) has four poles and no zeros in the finites-plane. 3. X(s) has a pole at s = (112)ej7T/4 . 4. f~ CX)x(t) dt = 4. Determine X(s) and its ROC. 9.15. Consider two right-sided signals x(t) and y(t) related through the differential equa- tions dx(t) ---;[[ = -2y(t) + 8(t) and d~;t) = 2x(t). Determine Y(s) and X(s), along with their regions of convergence. 9.16. A causal LTI systemS with impulse response h(t) has its input x(t) and output y(t) related through a linear constant -coefficient differential equation of the form (a) If dh(t) g(t) = ---;[[ + h(t), how many poles does G(s) have? (b) For what real values of the parameter a is S guaranteed to be stable? 9.17. A causal LTI system S has the block diagram representation shown in Figure P9 .17. Determine a differential equation relating the input x(t) to the output y(t) of this system. 724 The Laplace Transform Chap.9 Figure P9.17 9.18. Consider the causal LTI system represented by the RLC circuit examined in Prob- lem 3.20. (a) Determine H(s) and specify its region of convergence. Your answer should be consistent with the fact that the system is causal and stable. (b) Using the pole-zero plot of H(s) and geometric evaluation of the magnitude of the Fourier transform, determine whether the magnitude of the corresponding Fourier transform has an approximately lowpass, highpass, or bandpass char- acteristic. (c) If the value of R is now changed to 10-3 n, determine H(s) and specify its region of convergence. (d) Using the pole-zero plot of H(s) obtained in part (c) and geometric evaluation of the magnitude of the Fourier transform, determine whether the magnitude of the corresponding Fourier transform has an approximately lowpass, highpass, or bandpass characteristic. 9.19. Determine the unilateral Laplace transform of each of the following signals, and specify the corresponding regions of convergence: (a) x(t) = e- 21 u(t + 1) (b) x(t) = o(t + 1) + o(t) + e-2 3 (1+ lu(t + 1) (c) x(t) = e- 21 u(t) + e-41 u(t) 9.20. Consider the RL circuit of Problem 3.19. (a) Determine the zero-state response of this circuit when the input current is x(t) = e- 21 u(t). (b) Determine the zero-input response of the circuit fort > o-, given that y(O-) = 1. (c) Determine the output of the circuit when the input current is x(t) = e- 21 u(t) and the initial condition is the same as the one specified in part (b). BASIC PROBLEMS 9.21. Determine the Laplace transform and the associated region of convergence and pole- zero plot for each of the following functions of time: (a) x(t) = e- 21 u(t) + e- 31 u(t) (b) x(t) = e-41 u(t) + e-51(sin5t)u(t) (c) x(t) = e21u(-t)+e31 u(-t) (d) x(t) = te-21tl Chap.9 Problems 725 (e) x(t) = ltle-2ltl (0 x(t) = ltle2 t u( -t) 1 O:St:S t 0:St:S1 (g) x(t) = { o: elsewhere (h) x(t) = { 2- t, 1 :S t :S 2 (i) x(t) = o(t) + u(t) (j) x(t) = o(3t) + u(3t) 9.22. Determine the function of time, x(t), for each of the following Laplace transforms and their associated regions of convergence: (a) s2~9' (Jl.e{s} > 0 (b) s2·:9 , CJk{s} < 0 s+ I ( C ) (s+ 1)2+9' CRe{s} < -1 (d) s+2 2 12' -4 < CR.e{s} < -3 s +7s+ (e) s+ I 2 -3 < CRe{s} < -2 s +5s+6' 2 (0 s~~}l 1' (Re{s} > 4 2 (g) s -s+l (s+ 1)2 ' CRe{s} > -1 9.23. For each of the following statements about x(t), and for each of the four pole-zero plots in Figure P9.23, determine the corresponding constraint on the ROC: 1. x(t)e- 3 t is absolutely integrable. 2. x(t) * (e-t u(t)) is absolutely integrable. 3. x(t) = 0, t > 1. 4. x(t) = 0, t < -1. X 2j X X 2j 0 -2 2 CRe -2 2 CRe X -2j X X -2j 0 0 2j X 0 2j 0 -2 -2 2 CRe 0 -2j X 0 -2j 0 Figure P9.23 726 The Laplace Transform Chap. 9 9.24. Throughout this problem, we will consider the region of convergence of the Laplace transforms always to include the jw-axis. (a) Consider a signal x(t) with Fourier transform X(jw) and Laplace transform X(s) = s + 112. Draw the pole-zero plot for X(s). Also, draw the vector whose length represents IX(jw )I and whose angle with respect to the real axis repre- sents <r_X(jw) for a given w. (b) By examining the pole-zero plot and vector diagram in part (a), determine a different Laplace transform X1 (s) corresponding to the function of time, x 1( t), so that but x 1 (t) =I= x(t). Show the pole-zero plot and associated vectors that represent X1 (jw ). (c) For your answer in part (b), determine, again by examining the related vector diagrams, the relationship between <r.X(jw) and 1:X1( jw ). (d) Determine a Laplace transform X2(s) such that but x2(t) is not proportional to x(t). Show the pole-zero plot for X2(s) and the associated vectors that represent X2(jw ). (e) For your answer in part (d), determine the relationship between IX2(jw )I and IX(jw)l. (f) Consider a signal x(t) with Laplace transform X(s) for which the pole-zero plot is shown in Figure P9.24. Determine X1( s) such that IX(jw )I = IX1( jw )I and all poles and zeros of X1 (s) are in the left-half of the s-plane [i.e., CRe{s} < 0]. Also, determine X2(s) such that <r.X(jw) = <r.X2(jw) and all poles and zeros of X2(s) are in the left-half of the s-plane. --x----~---+~·~x---------- -2 -1 1 1 CRe 2 Figure P9.24 9.25. By considering the geometric determination of the Fourier transform, as developed in Section 9.4, sketch, for each of the pole-zero plots in Figure P9.25, the magnitude of the associated Fourier transform. Chap.9 Problems 727 !1m !1m --X--+------ (Jl.e 0 -iwo X -jw 0 (a) (b) !1m !1m --X--o-~------ --X-X--+----(:r--c>---- -b -a a b (c) (d) !1m !1m 0 iwo --X--+----<>---- 0 -jw 0 (e) (f) Figure P9.25 9.26. Consider a signal y(t) which is related to two signals x 1( t) and x 2 (t) by y( t) = X 1( t - 2) * X 2 (- t + 3) where x, (t) = e-2t u(t) and 728 The Laplace Transform Chap.9 Given that .r 1 e-ar u(t) ~ --, (Jl.e{s} >a, s+a use properties of the Laplace transform to determine the Laplace transform Y(s) of y(t). 9.27. We are given the following five facts about a real signal x(t) with Laplace transform X(s): 1. X(s) has exactly two poles. 2. X (s) has no zeros in the finite s-plane. 3. X(s) has a pole at s = -1 + j. 4. e2' x(t) is not absolutely integrable. 5. X(O) = 8. Determine X(s) and specify its region of convergence. 9.28. Consider an LTI system for which the system function H(s) has the pole-zero pattern shown in Figure P9.28. --x--x--~----x---<r---- - 2 - 1 + 1 + 2 ffi-e Figure P9.28 (a) Indicate all possible ROCs that can be associated with this pole-zero pattern. (b) For each ROC identified in part (a), specify whether the associated system is stable and/or causal. 9.29. Consider an LTI system with input x(t) = e-t u(t) and impulse response h(t) = e-2t u(t). (a) Determine the Laplace transforms of x(t) and h(t). (b) Using the convolution property, determine the Laplace transform Y(s) of the output y(t). (c) From the Laplace transform of y(t) as obtained in part (b), determine y(t). (d) Verify your result in part (c) by explicitly convolving x(t) and h(t). 9.30. A pressure gauge that can be modeled as an LTI system has a time response to a unit step input given by (1 - e-r - te-t)u(t). For a certain input x(t), the output is observed to be (2 - 3e-r + e-3')u(t). For this observed measurement, determine the true pressure input to the gauge as a function of time. Chap.9 Problems 729 9.31. Consider a continuous-time LTI system for which the input x(t) and output y(t) are related by the differential equation 2 d y(t) - dy(t) - 2 ( ) = ( ) dt2 dt y t X t. Let X(s) and Y(s) denote Laplace transforms of x(t) and y(t), respectively, and let H(s) denote the Laplace transform of h(t), the system impulse response. (a) Determine H(s) as a ratio of two polynomials ins. Sketch the pole-zero pattern of H(s). (b) Determine h(t) for each of the following cases: 1. The system is stable. 2. The system is causal. 3. The system is neither stable nor causal. 9.32. A causal LTI system with impulse response h(t) has the following properties: 1. When the input to the system is x(t) = e2t for all t, the output is y(t) = (116)e2t for all t. 2. The impulse response h(t) satisfies the differential equation -d-h;([t() + . 4 2h(t) = (e- t)u(t) + bu(t), where b is an unknown constant. Determine the system function H (s) of the system, consistent with the information above. There should be no unknown constants in your answer; that is, the constant b should not appear in the answer. 9.33. The system function of a causal LTI system is s + 1 H(s) = s 2 + 2 s + 2. Determine and sketch the response y(t) when the input is -00 < t < oo. 9.34. Suppose we are given the following information about a causal and stable LTI sys- temS with impulse response h(t) and a rational system function H(s): 1. H(l) = 0.2. 2. When the input is u(t), the output is absolutely integrable. 3. When the input is tu(t), the output is not absolutely integrable. 4. The signal d2h(t)ldt2 + 2 dh(t)ldt + 2h(t) is of finite duration. 5. H(s) has exactly one zero at infinity. Determine H (s) and its region of convergence. 9.35. The input x(t) and output y(t) of a causal LTI system are related through the block- diagram representation shown in Figure P9.35. (a) Determine a differential equation relating y(t) and x(t). (b) Is this system stable? 730 The Laplace Transform Chap.9 - ........- -{ + ,___...,.._r-_.....,._----1 + )---.......- - x(t) y(t) Figure P9.35 9.36. In this problem, we consider the construction of various types of block diagram representations for a causal LTI systemS with input x(t), output y(t), and system function 2 H(s) = 2s + 4s- 6. s2 + 3s + 2 To derive the direct-form block diagram representation of S, we first consider a causal LTI system S1 that has the same input x(t) asS, but whose system function is 1 HI(s) = s2 + 3s + 2' With the output of S1 denoted by y 1 (t), the direct-form block diagram representation of S1 is shown in Figure P9.36. The signals e(t) and f(t) indicated in the figure represent respective inputs into the two integrators. (a) Express y(t) (the output of S) as a linear combination of y1 (t), dy1 (t)ldt, and d 2yl (t)ldt2 . (b) How is dy1 (t)ldt related to f(t)? (c) How is d 2y (t)ldt2 1 related to e(t)? (d) Express y(t) as a linear combination of e(t), f(t), and y1 (t). f(t) x(t) Figure P9.36 Chap. 9 Problems 731 (e) Use the result from the previous part to extend the direct-form block diagram representation of S I and create a block diagram representation of S. (0 Observing that H(s) = (2(s - 1) )(~), s+2 s+l draw a block diagram representation for S as a cascade combination of two subsystems. (g) Observing that 6 H(s) = 2 + -- - -8- s+2 s+l' draw a block-diagram representation for S as a parallel combination of three subsystems. 9.37. Draw a direct-form representation for the causal LTI systems with the following system functions: 2 ( a ) H s+ I (b) H ( ) s -5s+6 s I (s ) = s2+5s+6 2 s = s2+ 7s+ 10 (c) H3(s) = (s+2)2 9.38. Consider a fourth-order causal LTI systemS whose system function is specified as 1 H(s) = (s2 - s + l)(s2 + 2s + 1) · (a) Show that a block diagram representation for S consisting of a cascade of four first -order sections will contain multiplications by coefficients that are not purely real. (b) Draw a block diagram representation for S as a cascade interconnection of two second-order systems, each of which is represented in direct form. There should be no multiplications by nonreal coefficients in the resulting block diagram. (c) Draw a block diagram representation for S as a parallel interconnection of two second-order systems, each of which is represented in direct form. There should be no multiplications by nonreal coefficients in the resulting block diagram. 9.39. Let XJ (t) = e-2t u(t) and x2(t) = e-3(t+I)u(t + 1). (a) Determine the unilateral Laplace transform XI (s) and the bilateral Laplace transform XI (s) for the signal x1 (t). (b) Determine the unilateral Laplace transform X 2(s) and the bilateral Laplace transform X2(s) for the signal x2(t). (c) Take the inverse bilateral Laplace transform of the product X 1 (s)X2(s) to deter- mine the signal g(t) = x1( t) * x2(t). (d) Show that the inverse unilateral Laplace transform of the product X 1 (s)X2(s) is not the same as g(t) fort> o-. 9.40. Consider the systemS characterized by the differential equation 3 2 d y(t) 6d y(t) 11 dy(t) 6 () = () dt3 + dt2 + dt + )' t X t . 732 The Laplace Transform Chap.9 (a) Determine the zero-state response of this system for the input x(t) = e-4 t u(t). (b) Determine the zero-input response of the system fort > o-, given that dy(t) I = -1 d2y(t) I = 1 dt t=O- , dt2 • t=O- (c) Determine t4e output of S when the input is x(t) = e-4 t u(t) and the initial con- ditions are the same as those specified in part (b). ADVANCED PROBLEMS 9.41. (a) Show that, if x(t) is an even function, so that x(t) = x( -t), then X(s) = X( -s). (b) Show that, if x(t) is an odd function, so that x(t) = - x( -t), then X(s) = -X(-s). (c) Determine which, if any, ofthe pole-zero plots in Figure P9.41 could correspond to an even f4nction of time. For those that could, indicate the required ROC. ----x--~~--x------ -1 1 (a) (b) ----X ------1f----X------ ----X----+------<->------- -1 1 -1 -j (c) (d) Figure P9.41 Chap.9 Problems 733 9.42. Determine whether each of the following statements is true or false. If a statement is true, construct a convincing argument for it. If it is false, give a counterexample. (a) The Laplace transform of t2u(t) does not converge anywhere on the s-plane. 2 (b) The Laplace transform of et u(t) does not converge anywhere on the s-plane. (c) The Laplace transform of ejwot does not converge anywhere on the s-plane. (d) The Laplace transform of ejwot u(t) does not converge anywhere on the s-plane. (e) The Laplace transform of ltl does not converge anywhere on the s-plane. 9.43. Let h(t) be the impulse response of a causal and stable LTI system with a rational system function. (a) Is the system with impulse response dh(t)ldt guaranteed to be causal and stable? (b) Is the system with impulse response f~oc h(T)dT guaranteed to be causal and unstable? 9.44. Let x(t) be the sampled signal specified as x(t) = Le-nT 8(t - nT), n=O where T > 0. (a) Determine X(s), including its region of convergence. (b) Sketch the pole-zero plot for X(s). (c) Use geometric interpretation of the pole-zero plot to argue that X(jw) is peri- odic. 9.45. Consider the LTI system shown in Figure P9.45(a) for which we are given the fol- lowing information: s+2 X(s) = s- 2' x(t) = 0, t > 0, and [See Figure P9.45(b ).] (a) Determine H(s) and its region of convergence. (b) Determine h(t). y(t) ___.._~ x(t)~y(t) (a) (b) Figure P9.45 734 The Laplace Transform Chap. 9 (c) Using the system function H(s) found in part (a), determine the output y(t) if the input is -oo < t < +oo. 9.46. Let H(s) represent the system function for a causal, stable system. The input to the system consists of the sum of three terms, one of which is an impulse 8(t) and another a complex exponential of the form esot, where s0 is a complex constant. The output is y(t) = -6e-t u(t) + : e4t cos 3t + ~! e4t sin 3t + 8(t). 3 Determine H(s), consistently with this information. 9.47. The signal y(t) = e-2 t u(t) is the output of a causal all-pass system for which the system function is s- 1 H(s) = s + 1' (a) Find and sketch at least two possible inputs x(t) that could produce y(t). (b) What is the input x(t) if it is known that roo lx(tll dt < oo? (c) What is the input x(t) if it is known that a stable (but not necessarily causal) system exists that will have x(t) as an output if y(t) is the input? Find the im- pulse response h(t) of this filter, and show by direct convolution that it has the property claimed [i.e., that y(t) * h(t) = x(t)]. 9.48. The inverse of an LTI system H(s) is defined as a system that, when cascaded with H(s), results in an overall transfer function of unity or, equivalently, an overall im- pulse response that is an impulse. (a) If H 1( s) denotes the transfer function of an inverse system for H(s), determine the general algebraic relationship between H(s) and H1 (s). (b) Shown in Figure P9.48 is the pole-zero plot for a stable, causal system H(s). Determine the pole-zero plot for the associated inverse system. !1m -------X----r-~---------- -1 1 2 Figure P9.48 Chap. 9 Problems 735 9.49. A class of systems, referred to as minimum-delay or minimum-phase systems, is sometimes defined through the statement that these systems are causal and stable and that the inverse systems are also causal and stable. Based on the preceding definition, develop an argument to demonstrate that all poles and zeros of the transfer function of a minimum-delay system must be in the left half of the s-plane [i.e., CRc{s} < 0]. 9.50. Determine whether or not each of the following statements about LTI systems is true. If a statement is true, construct a convincing argument for it. If it is false, give a counterexample. (a) A stable continuous-time system must have all its poles in the left half of the s-plane [i.e., CRc{s} < 0]. (b) If a system function has more poles than zeros, and the system is causal, the step response will be continuous at t = 0. (c) If a system function has more poles than zeros, and the system is not restricted to be causal, the step response can be discontinuous at t = 0. (d) A stable, causal system must have all its poles and zeros in the left half of the s-plane. 9.51. Consider a stable and causal system with a real impulse response h(t) and system function H(s). It is known that H(s) is rational, one of its poles is at -1 + j, one of its zeros is at 3 + j, and it has exactly two zeros at infinity. For each of the following statements, determine whether it is true, whether it is false, or whether there is insufficient information to determine the statement's truth. (a) h(t)e- 3t is absolutely integrable. (b) The ROC for H(s) is ffi,c{s} > -1. (c) The differential equation relating inputs x(t) and outputs y(t) for S may be writ- ten in a form having only real coefficients. (d) lim.1·~ryoH(s) = 1. (e) H(s) does not have fewer than four poles. (f) H(jw) = 0 for at least one finite value of w. (g) If the input to Sis e3t sin t, the output is e3t cost. 9.52. As indicated in Section 9.5, many of the properties of the Laplace transform and their derivation are analogous to corresponding properties of the Fourier transform and their derivation, as developed in Chapter 4. In this problem, you are asked to outline the derivation of a number of the Laplace transform properties. Observing the derivation for the corresponding property in Chapter 4 for the Fourier transform, derive each of the following Laplace transform properties. Your derivation must include a consideration of the region of convergence. (a) Time shifting (Section 9.5.2) (b) Shifting in the s-domain (Section 9.5.3) (c) Time scaling (Section 9.5.4) (d) Convolution property (Section 9.5.6) 9.53. As presented in Section 9.5.10, the initial-value theorem states that, for a signal x(t) with Laplace transform X(s) and for which x(t) = 0 fort < 0, the initial value of x(t) [i.e., x(O+ )] can be obtained from X(s) through the relation x(O+) = lim sX(s). [eq. (9.11 0)] s~x 736 The Laplace Transform Chap.9 First, we note that, since x(t) = 0 for t < 0, x(t) = x(t)u(t). Next, expanding x(t) as a Taylor series at t = 0+, we obtain x(t) = [x (O+) + x(ll(O+ )I+ · · · + x<n)(O+) :~ + .. ·] u(t), (P9.53-1) where x(n)(O+) denotes the nth derivative of x(t) evaluated at t = 0+. (a) Determine the Laplace transform of an arbitrary term x(n)(O+ )(tn/n!)u(t) on the right-hand side of eq. (P9.53-1). (You may find it helpful to review Example 9.14.) (b) From your result in part (a) and the expansion in eq. (P9.53-1), show that X(s) can be expressed as 00 1 X(s) = L~ x(n)(O+)-. sn+I n=O (c) Demonstrate that eq. (9.110) follows from the result of part (b). (d) By first determining x(t), verify the initial-value theorem for each of the fol- lowing examples: (1) X(s) = - 1 s+2 (2) X(s) = (s+~;~+ 3) (e) A more general form of the initial-value theorem states that if x(n)(O+) = 0 for n < N, then x(N)(O+) = lims~oosN+I X(s). Demonstrate that this more general statement also follows from the result in part (b). 9.54. Consider a real-valued signal x(t) with Laplace transform X(s). (a) By applying complex conjugation to both sides of eq. (9.56), show that X(s) = X*(s*). (b) From your result in (a), show that if X(s) has a pole (zero) at s = s0 , it must also have a pole (zero) at s = s0; i.e., for x(t) real, the poles and zeros of X(s) that are not on the real axis must occur in complex conjugate pairs. 9.55. In Section 9.6, Table 9.2, we listed a number of Laplace transform pairs, and we indicated specifically how transform pairs 1 through 9 follow from Examples 9.1 and 9.14, together with various properties from Table 9.1. By exploiting properties from Table 9.1, show how transform pairs 10 through 16 follow from transform pairs 1 through 9 in Table 9.2. 9.56. The Laplace transform is said to exist for a specific complex s if the magnitude of the transform is finite-that is, if IX(s)l < oo. Show that a sufficient condition for the existence of the transform X(s) at s = so = o-o + }wo is that +oo J-o o lx(t)ie-O""ot dt < oo. Chap.9 Problems 737 In other words, show that x(t) exponentially weighted by e-CYot is absolutely inte- grable. You will need to use the result that, for a complex function f(t), rf (t)dtl ~ r/J (t)/ dt. (P9.56-l) 1 Without rigorously proving eq. (P9.56-1), argue its plausibility. 9.57. The Laplace transform X(s) of a signal x(t) has four poles and an unknown number of zeros. The signal x(t) is known to have an impulse at t = 0. Determine what information, if any, this provides about the number of zeros and their locations. 9.58. Let h(t) be the impulse response of a causal and stable LTI system with rational system function H(s). Show that g(t) = CRe{h(t)} is also the impulse response of a causal and stable system. 9.59. IfX(s) denotes the unilateral Laplace transform of x(t), determine, in terms ofX(s), the unilateral Laplace transform of: (a) x(t - 1) (b) x(t + 1) (c) J_ :x( r) dr (d) dd;~t) EXTENSION PROBLEMS 9.60. In long-distance telephone communication, an echo is sometimes encountered due to the transmitted signal being reflected at the receiver, sent back down the line, re- flected again at the transmitter, and returned to the receiver. The impulse response for a system that models this effect is shown in Figure P9.60, where we have as- sumed that only one echo is received. The parameter T corresponds to the one-way travel time along the communication channel, and the parameter a represents the attenuation in amplitude between transmitter and receiver. h(t) a a3 I t 1 0 T 3T Figure P9.60 (a) Determine the system function H(s) and associated region of convergence for the system. (b) From your result in part (a), you should observe that H(s) does not consist of a ratio of polynomials. Nevertheless, it is useful to represent it in terms of poles and zeros, where, as usual, the zeros are the values of s for which H (s) = 0 738 The Laplace Transform Chap.9 and the poles are the values of s for which 11 H (s) = 0. For the system function found in part (a), determine the zeros and demonstrate that there are no poles. (c) From your result in part (b), sketch the pole-zero plot for H(s). (d) By considering the appropriate vectors in the s-plane, sketch the magnitude of the frequency response of the system. 9.61. The autocorrelation function of a signal x(t) is defined as cf>xx(T) = J:oo X(t)x(t+T)dt. (a) Determine, in terms of x(t), the impulse response h(t) of an LTI system for which, when the input is x(t), the output is cf>xAt) [Figure P9.61(a)]. (b) From your answer in part (a), determine <l>xx(s), the Laplace transform of cf>xA T) in terms of X(s). Also, express <l>xxUw), the Fourier transform of cf>xx(T), in terms of X(jw ). (c) If X(s) has the pole-zero pattern and ROC shown in Figure P9.61(b), sketch the pole-zero pattern and indicate the ROC for <I> xx<s). 9m I I I I x(t)-a-. I --X~X+--------- -3 -2 -~ ffi€ <!>,(!) (a) (b) Figure P9.61 9.62. In a number of applications in signal design and analysis, the class of signals cf>n(t) = e -t/2L n(t)u(t), n = 0, 1, 2, ... , (P9.62-1) where = -et dn ( n -t) Ln(t) -d t e , (P9.62-2) n.1 tn is encountered. (a) The functions Ln(t) are referred to as Laguerre polynomials. To verify that they in fact have the form of polynomials, determine Lo(t), L1 (t), and L2(t) explicitly. (b) Using the properties of the Laplace transform in Table 9.1 and Laplace trans- form pairs in Table 9.2, determine the Laplace transform <l>n(s) of cf>n(t). (c) The set of signals cf>n(t) can be generated by exciting a network of the form in Figure P9.62 with an impulse. From your result in part (b), determine H 1( s) and H2(s) so that the impulse responses along the cascade chain are the signals cf>n(t) as indicated. Chap. 9 Problems 739 8(t) ···-Br··· </>i(t) Figure P9.62 9.63. In filter design, it is often possible and convenient to transform a lowpass filter to a high pass filter and vice versa. With H (s) denoting the transfer function of the original filter and G(s) that of the transformed filter, one such commonly used trans- formation consists of replacing s by 11 s; that is, G(s) = HG). (a) For H(s) = 11(s + 112), sketch IH(jw )I and IG(jw )1. (b) Determine the linear constant-coefficient differential equation associated with H(s) and with G(s). (c) Now consider a more general case in which H(s) is the transfer function asso- ciated with the linear constant -coefficient differential equation in the general form ~ dky(t) _ ~ b dkx(t) Lak-dk -L k-dk. (P9.63-l) k=O t k=O t Without any loss of generality, we have assumed that the number of derivatives N is the same on both sides of the equation, although in any particular case, some of the coefficients may be zero. Determine H(s) and G(s). (d) From your result in part (c), determine, in terms of the coefficients in eq. (P9.63-l), the linear constant-coefficient differential equation associated with G(s). 9.64. Consider the RLC circuit shown in Figure 9.27 with input x(t) and output y(t). (a) Show that if R, L, and Care all positive, then this LTI system is stable. (b) How should R, L, and C be related to each other so that the system represents a second-order Butterworth filter? 9.65. (a) Determine the differential equation relating vi(t) and v0 (t) for the RLC circuit of Figure P9.65. 3!1 1h v0(0+) = 1 dvo(t) I= 2 dt t = 0+ Figure P9.65 740 The Laplace Transform Chap.9 (b) Suppose that v;(t) = e- 31 u(t). Using the unilateral Laplace transform, deter- mine v0 (t) fort > 0. 9.66. Consider the RL circuit shown in Figure P9.66. Assume that the current i(t) has reached a steady state with the switch at position A. At time t = 0, the switch is moved from position A to position B. i(t) L=1H Figure P9 .66 (a) Find the differential equation relating i(t) and V2 fort > o-. Specify the initial condition (i.e., the value of i(O-)) for this differential equation in terms of v1• (b) Using the properties of the unilateral Laplace transform in Table 9.3, determine and plot the current i(t) for each of the following values of VJ and v2: (i) v1 = 0 V, v2 = 2 V (ii) v1 = 4 V, v2 = 0 V (iii) v1 = 4 V, v2 = 2 V Using your answers for (i), (ii), and (iii), argue that the current i(t) may be expressed as a sum of the circuit's zero-state response and zero-input response. 10 THE Z-TRANSFORM 10.0 INTRODUCTION In Chapter 9, we developed the Laplace transform as an extension of the continuous-time Fourier transform. This extension was motivated in part by the fact that it can be applied to a broader class of signals than the Fourier transform can, since there are many sig- nals for which the Fourier transform does not converge but the Laplace transform does. The Laplace transform allowed us, for example, to perform transform analysis of unstable systems and to develop additional insights and tools for LTI system analysis. In this chapter, we use the same approach for discrete time as we develop the z- transform, which is the discrete-time counterpart of the Laplace transform. As we will see, the motivations for and properties of the z-transform closely parallel those of the Laplace transform. Just as with the relationship between continuous-time and discrete- time Fourier transforms, however, we will encounter some important distinctions between the z-transform and the Laplace transform that arise from the fundamental differences between continuous-time and discrete-time signals and systems. 10.1 THE z-TRANSFORM As we saw in Section 3.2, for a discrete-time linear time-invariant system with impulse response h[n], the response y[n] of the system to a complex exponential input of the form zn is y[n] = H(z)zn, (10.1) 741 742 The z-Transform Chap. 10 where +oo H(z) = L h[n]z-n. (10.2) n= -oo For z = eiw with w real (i.e., with lzl = 1), the summation in eq. (10.2) corresponds to the discrete-time Fourier transform of h[n]. More generally, when lzl is not restricted to unity, the summation is referred to as the z-transform of h[n]. The z-transform of a general discrete-time signal x[n] is defined as 1 +oo X(z) ~ L x[n]z-n, (10.3) n= -oo where z is a complex variable. For convenience, the z-transform of x[n] will sometimes be denoted as Z{x[n]} and the relationship between x[n] and its z-transform indicated as z x[n] ~ X(z). (10.4) In Chapter 9, we considered a number of important relationships between the Laplace transform and the Fourier transform for continuous-time signals. In a similar, but not identical, way, there are a number of important relationships between the z-transform and the discrete-time Fourier transform. To explore these relationships, we express the complex variable z in polar form as z = reiw, (10.5) with r as the magnitude of z and was the angle of z. In terms of rand w, eq. (10.3) becomes +oo X(reiw) = L x[n](refw)-n, n= -oo or equivalently, +oo X(reiw) = L {x[n]r-n}e- Jwn. (10.6) n= -oo From eq. (10.6), we see that X(reiw) is the Fourier transform of the sequence x[n] multiplied by a real exponential r-n; that is, X(reiw) = ~{x[n]r-n}. (10.7) The exponential weighting r-n may be decaying or growing with increasing n, depending on whether r is greater than or less than unity. We note in particular that, for r = 1, or 1T he z-transform defined in eq. (1 0.3) is often referred to as the bilateral z-transform, to distinguish it from the unilateral z-transform, which we develop in Section 10.9. The bilateral z-transform involves a summation from -oo to +oo, while the unilateral transform has a form similar to eq. (10.3), but with summation limits from 0 to +oo. Since we are mostly concerned with the bilateral z-transform, we will refer to X(z) as defined in eq. (10.3) simply as the z-transform, except in Section 10.9, in which we use the words ""unilateral"" and ""bilateral"" to avoid ambiguity. Sec. 10.1 The z-Transform 743 equivalently, lzl = 1, eq. (10.3) reduces to the Fourier transform; that is, (10.8) The relationship between the z-transform and Fourier transform for discrete-time signals parallels closely the corresponding discussion in Section 9.1 for continuous-time signals, but with some important differences. In the continuous-time case, the Laplace transform reduces to the Fourier transform when the real part of the transform variable is zero. Interpreted in terms of the s-plane, this means that the Laplace transform reduces to the Fourier transform on the imaginary axis (i.e., for s = jw ). In contrast, the z-transform reduces to the Fourier transform when the magnitude of the transform variable z is unity (i.e, for z = ejw). Thus, the z-transform reduces to the Fourier transform on the contour in the complex z-plane corresponding to a circle with a radius of unity, as indicated in Figure 10 .1. This circle in the z-plane is referred to as the unit circle and plays a role in the discussion of the z-transform similar to the role of the imaginary axis in the s-plane for the Laplace transform. z-plane Figure 1 0. 1 Complex z-plane. The z-transform reduces to the Fourier transform for values of z on the unit circle. From eq. (10.7), we see that, for convergence of the z-transform, we require that the Fourier transform of x[n]r-n converge. For any specific sequence x[n], we would expect this convergence for some values of r and not for others. In general, the z-transform of a sequence has associated with it a range of values of z for which X(z) converges. As with the Laplace transform, this range of values is referred to as the region of convergence (ROC). If the ROC includes the unit circle, then the Fourier transform also converges. To illustrate the z-transform and the associated region of convergence, let us consider several examples. Example 1 0. 1 Consider the signal x[n] = anu[n]. Then, from eq. (10.3), +x X(z) = L a11 u[n]z-n = L(az- 1t. n=-x n=O For convergence of X(z), we require that L~=o laz- 1 11 1 < oo. Consequently, the region of convergence is the range of values of z for which laz- 1 1 < 1, or equivalently, lzl > lal. 744 The z-Transform Chap. 10 Then X(z) = ~(az- 1 )"" z- a' lzl > lal. (10.9) 11=0 Thus, the z-transform for this signal is well-defined for any value of a, with an ROC determined by the magnitude of a according to eq. (10.9). For example, for a = 1, x[n] is the unit step sequence with z-transform 1 X(z) = ~· lzl > 1. We see that the z-transform in eq. (10.9) is a rational function. Consequently, just as with rational Laplace transforms, the z-transform can be characterized by its zeros (the roots of the numerator polynomial) and its poles (the roots of the denominator polyno- mial). For this example, there is one zero, at z = 0, and one pole, at z = a. The pole-zero plot and the region of convergence for Example 10.1 are shown in Figure 10.2 for a value of a between 0 and 1. For Ia I > 1, the ROC does not include the unit circle, consistent with the fact that, for these values of a, the Fourier transform of a"" u[n] does not converge. 9m ....----- Unit Circle z-plane CR.e Figure 1 0.2 Pole-zero plot and region of convergence for Example 10.1 for 0 <a< 1. Example 1 0.2 Now let x[n] = -a""u[ -n- 1]. Then +·/. -I X(z) = - ~ a""u[ -n- l]z-"" ~ a""z-"" 11= --% ll = -'l~ (10.10) - ~a-""z"" I- ~(a- 1 z)"". II= I 11=0 If la- 1 zl < 1, or equivalently, lzl < Ia I, the sum in eq. ( 10.1 0) converges and 1 1 z X(z) = 1 - a- z = az- = -z - ,a lzl < lal. (10.11) 1 - 1 1 - 1 The pole-zero plot and region of convergence for this example are shown in Fig- ure 10.3 for a value of a between 0 and 1. Sec. 10.1 The z-Transform 745 9m Unit Circle z-plane Figure 1 0.3 Pole-zero plot and region of convergence for Example 10.2 for 0 <a< 1. Comparing eqs. (10.9) and (10.11), and Figures 10.2 and 10.3, we see that the al- gebraic expression for X(z) and the corresponding pole-zero plot are identical in Exam- ples 10.1 and 10.2, and the z-transforms differ only in their regions of convergence. Thus, as with the Laplace transform, specification of the z.,transform requires both the algebraic expression and the region of convergence. Also, in both examples, the sequences were exponentials and the resulting z-transforrns were rational. In fact, as further suggested by the following examples, X(z) will be rational whenever x[n] is a linear combination of real or complex exponentials: Example 10.3 Let us consider a signal that is the sum of two real exponentials: x[n] ~ 7 GJu [n]- 6G )"" u[n]. (10.12) The z-transform is then (10.13) 7 6 (10.14) 1 - .!. z-1 1 - .!. z- 1 3 2 z(z- ~) 1 1 . (10.15) (z - 3 )(z - 2) For convergence of X(z), both sums in eq. (10.13) must converge, which requires that both I0/3)z- 1 1 1 < 1 and IC112)z- 1 < 1, or equivalently, lzl > 113 and lzl > 112. Thus, the region of convergence is lzl > 112. 746 The z-Transform Chap. 10 The z-transform for this example can also be obtained using the results of Exam- ple 10.1. Specifically, from the definition of the z-transform in eq. (1 0.3), we see that the z-transform is linear; that is, if x[n] is the sum of two terms, then X(z) will be the sum of the z-transforms of the individual terms and will converge when both z-transforms converge. From Example 10.1, 1 )n z ( - u[n] ~ \z\ > ~ (10.16) 3 1 - !z- 1 ' - :l and (-1 )n u[n] z ~ \z\ > ~. (10.17) 2 l- !z-1' 2 and consequently, 11 11 7 (1) 3 u[n] - 6 (1) 2 u[n] ~z (10.18) 7 6 --,- 1 • \z\> i· 1- -3 c' 1- -2 z- 1 as we determined before. The pole-zero plot and ROC for the z-transform of each of the individual terms and for the combined signal are shown in Figure 10.4. gm gm z-plane z-plane CRe (a) (b) 9m z-plane CRe (c) Figure 1 0.4 Pole-zero plot and region of convergence for the individual terms and the sum in Example 10.3: (a)1/(1 - tr1 ), \z\ > t; (b)1/(1 - ~r 1 ), \z\ > ~; (c)?/(1 - tr1)- 6/(1 - ~z- 1 ), \z\ > ~- Sec. 10.1 The z-Transform 747 Example 1 0.4 Let us consider the signal The z-transform of this signal is (10.19) or equivalently, I X 3hz (z) = ----,-------,---- (10.20) (z- lei7TI4)(z _ le-J7TI4) 3 3 For convergence of X(z), both sums in eq. (10.19) must converge, which requires that l(l/3)ei7T/4z-'l < 1 and l(l/3)e-J7T/4z- 1 1 < 1, or equivalently, lzl > 113. The pole- zero plot and ROC for this example are shown in Figure 10.5. 9m z-plane <Re Figure 1 o.s Pole-zero plot and ROC for the z-transform in Example 10 .4. In each of the preceding four examples, we expressed the z-transform both as a ratio of polynomials in z and as a ratio of polynomials in z- 1• From the definition of the z-transform as given in eq. (10.3), we see that, for sequences which are zero for n < 0, X(z) involves only negative powers of z. Thus, for this class of signals, it is partic- ularly convenient for X(z) to be expressed in terms of polynomials in z- 1 rather than z, and 748 The z-Transform Chap. 10 when appropriate, we will use that form in our discussion. However, reference to the poles and zeros is always in terms of the roots of the numerator and denominator expressed as polynomials in z. Also, it is sometimes convenient to refer to X(z), written as a ratio of polynomials in z, as having poles at infinity if the degree of the numerator exceeds the degree of the denominator or zeros at infinity if the numerator is of smaller degree than the denominator. 10.2 THE REGION OF CONVERGENCE FOR THE z-TRANSFORM In Chapter 9, we saw that there were specific properties of the region of convergence of the Laplace transform for different classes of signals and that understanding these properties led to further insights about the transform. In a similar manner, we explore a number of properties of the region of convergence for the z-transform. Each of the following properties and its justification closely parallel the corresponding property in Section 9 .2. Property 1: The ROC of X(z) consists of a ring in the z-plane centered about the origin. This property is illustrated in Figure 10.6 and follows from the fact that the ROC consists of those values of z = rejw for which x[n]r-n has a Fourier transform that con- verges. That is, the ROC of the z-transform of x[n] consists of the values of z for which x[n]r-n is absolutely summable:2 +x L Jx[n]Jr- 11 < x. (10.21) n=-x 9m / "" ' ' ' z-plane / I .... I / ' I ' ' I ' I <R.e Figure 1 0.6 ROC as a ring in the z-plane. In some cases, the inner boundary can extend inward to the ori- gin, in which case the ROC becomes a disc. In other cases, the outer bound- ary can extend outward to infinity. 2For a thorough treatment of the mathematical properties of z-transforms, see R.V. Churchill and J.W. Brown, Complex Variables and Applications (5th ed.) (New York: McGraw-Hill, 1990), and E. I. Jury, Theory and Application of the z-Transform Method (Malabar, FL: R. E. Krieger Pub. Co., 1982). Sec. 10.2 The Region of Convergence for the z-Transform 749 Thus, convergence is dependent only on r = lzl and not on w. Consequently, if a specific value of z is in the ROC, then all values of z on the same circle (i.e., with the same magnitude) will be in the ROC. This by itself guarantees that the ROC will con- sist of concentric rings. As we will see when we discuss Property 6, the ROC must in fact consist of only a single ring. In some cases the inner boundary of the ROC may ex- tend inward to the origin, and in some cases the outer boundary may extend outward to infinity. Property 2: The ROC does not contain any poles. As with the Laplace transform, this property is simply a consequence of the fact that at a pole X(z) is infinite and therefore, by definition, does not converge. Property 3: If x[n] is of finite duration, then the ROC is the entire z-plane, except possibly z = 0 and/or z = oo. A finite-duration sequence has only a finite number of nonzero values, extending, say, from n = N 1 ton = N 2 , where N 1 and N 2 are finite. Thus, the z-transform is the sum of a finite number of terms; that is, NJ X(z) = 2:_ x[n]z- 11 • (10.22) n=N1 For z not equal to zero or infinity, each term in the sum will be finite, and conse- quently, X(z) will converge. If N 1 is negative and N2 positive, so that x[n] has nonzero values both for n < 0 and n > 0, then the summation includes terms with both positive powers of z and negative powers of z. As lzl ~ 0, terms involving negative powers of z become unbounded, and as lzl ~ oo, terms involving positive powers of z become un- bounded. Consequently, for N1 negative and N2 positive, the ROC does not include z = 0 or z = oo. If N1 is zero or positive, there are only negative powers of z in eq. (10.22), and consequently, the ROC includes z = oo. If N2 is zero or negative, there are only positive powers of z in eq. (10.22), and consequently, the ROC includes z = 0. Example 10.5 Consider the unit impulse signalo[n]. Its z-transform is given by Z +x o[n] ~ L o[n]z-n = 1, (10.23) n=-x with an ROC consisting of the entire z-plane, including z = 0 and z = oo. On the other hand, consider the delayed unit impulse 8 [ n - 1], for which Z +oo o[n- 11 ~ L o[n- 1]z-n = z- 1• (10.24) n=-x 750 The z-Transform Chap. 10 This z-transform is well defined except at z = 0. where there is a pole. Thus, the ROC consists of the entire z-plane, including z = w but excluding z = 0. Similarly, consider an impulse advanced in time, namely, 8[n + 1]. In this case, z t X 8 [ n + 1] ~ ~ 8[ n + 1] z 11 = z, (10.25) L.......,., n=- -z which is well defined for all finite values of z. Thus, the ROC consists of the entire finite z-plane (including z = 0). but there is a pole at infinity. Property 4: If x[ n] is a right -sided sequence, and if the circle \z \ = r0 is in the ROC, then all finite values of z for which \z\ > r0 will also be in the ROC. The justification for this property follows in a manner identical to that of Property 4 in Section 9.2. A right-sided sequence is zero prior to some value of n, say, N 1 • If the circle \z\ = r0 is in the ROC, then x[n]rc)"" is absolutely summable. Now consider \z\ = r 1 with r 1 > r0 , so that r("" decays more quickly than rc)"" for increasing n. As illustrated in Figure 10.7, this more rapid exponential decay will further attenuate sequence values x[n] n Illiiiiiiiirrttttrrrr'''ITtttt~· n lllll Figure 10.7 With r1 > r0, x[n]r1- n decays faster with increasing n than 1 1 0 r r >r does x[n]r0-n. Since x[n] = 0, n < N1, "", IlllllttttttTTttttttttttt''' this implies that if x[n]r0-n is abso- lutely summable, then x[n]r1 n will be n also. Sec. 10.2 The Region of Convergence for the z-Transform 751 for positive values of n and cannot cause sequence values for negative values of n to be- come unbounded, since x[n] is right sided and, in particular, x[n]z-n = 0 for n < N1• Consequently, x[n]rj 11 is absolutely summable. For right-sided sequences in general, eq. (10.3) takes the form X(z) = ~ x[n]z-n, (10.26) n=N1 where N1 is finite and may be positive or negative. If N1 is negative, then the summation in eq. (10.26) includes terms with positive powers of z, which become unbounded as lzl ~ oo. Consequently, for right-sided sequences in general, the ROC will not include infinity. However, for the particular class of causal sequences, i.e., sequences that are zero for n < 0, N1 will be nonnegative, and consequently, the ROC will include z = oo. Property 5: If x[n] is a left-sided sequence, and if the circle lzl = r0 is in the ROC, then all values of z for which 0 < lzl < r0 will also be in the ROC. Again, this property closely parallels the corresponding property for Laplace trans- forms, and the proof of it and its basis in intuition are similar to the proof and intuition for Property 4. In general, for left-sided sequences, from eq. (10.3), the summation for the z-transform will be of the form :N X(z) = Zo x[n]z- 11 , (10.27) n=-x where N2 may be positive or negative. If N2 is positive, then eq. (10.27) includes negative powers of z, which become unbounded as lzl ~ 0. Consequently, for left-sided sequences, the ROC will not in general include z = 0. However, if N2 ::; 0 (so that x[n] = 0 for all n > 0), the ROC will include z = 0. Property 6: If x[n] is two sided, and if the circle lzl = r0 is in the ROC, then the ROC will consist of a ring in the z-plane that includes the circle lzl = r0 . As with Property 6 in Section 9 .2, the ROC for a two-sided signal can be examined by expressing x[n] as the sum of a right-sided and a left-sided signal. The ROC for the right-sided component is a region bounded on the inside by a circle and extending outward to (and possibly including) infinity. The ROC for the left-sided component is a region bounded on the outside by a circle and extending inward to, and possibly including, the origin. The ROC for the composite signal includes the intersection of the ROCs of the components. As illustrated in Figure 10.8, the overlap (assuming that there is one) is a ring in the z-plane. We illustrate the foregoing properties with examples that closely parallel Exam- ples 9.6 and 9.7. 752 The z-Transform Chap. 10 9m / ' / ' / z-plane I / ' ' \ z-plane I \ I \ \ I CR-e \ ' / :' ' / (a) (b) 9m z-plane (c) Figure 10.8 (a) ROC for right-sided sequence; (b) ROC for left-sided sequence; (c) intersection of the ROCs in (a) and (b), representing the ROC for a two-sided se- quence that is the sum of the right-sided and the left-sided sequence. Example 1 0.6 Consider the signal a11 0 :::; n :::; N - 1, a > 0 x[n] = { O,' otherwise Then N-l X(z) = L anz-n n=O N-l (10.28) = L(az-'t n=O 1- (az-l)N 1 ZN- aN 1 - az- 1 - zN -t z - a · Sec. 10.2 The Region of Convergence for the z-Transform 753 Since x[h] is of finite length, it follows from Property 3 that the ROC includes the entire z- plane except possibly the origin and/or infinity. In fact, from our discussion of Property 3, since x[n] is zero for n < 0, the ROC will extend to infinity. However, since x[n] is nonzero for some positive values of n, the ROC will not include the origin. This is evident from eq. (10.28), from which we see that there is a pole of order N- 1 at z = 0. TheN roots of the numerator polynomial are at Zk = aej(27rkiN>, k = 0, 1, ... , N - 1. (10.29) The root for k = 0 cancels the pole at z = a. Consequently, there are no poles other than at the origin. The remaining zeros are at Zk = aej(27rk!N), k = 1, ... , N- 1. (10.30) The pole-zero pattern is shown in Figure 10.9. 9m z-plane (N-1 )st order pole Unit circle a Figure 1 o. 9 Pole-zero pattern for Example 10.6 with N = 16 and 0 < a < 1. The region of convergence for this example consists of all values of z except z = 0. Example 1 0. 7 Let x[n] = bini, b > 0. (10.31) This two-sided sequence is illustrated in Figure 10.10, for both b < 1 and b > 1. The z-transform for the sequence can be obtained by expressing it as the sum of a right-sided and a left-sided sequence. We have (10.32) From Example 10.1, (10.33) and from Example 10 .2, 1 lzl <b. (10.34) 754 The z-Transform Chap. 10 x[n] = bini O<b<1 n (a) x[n] =bini b>1 n (b) Figure 10.10 Sequence x[n] = bini for 0 < b < 1 and forb> 1: (a) b = 0.95; (b) b = 1.05. In Figures 10.11(a)-(d) we show the pole-zero pattern and ROC for eqs. (10.33) and (10.34), for values of b > 1 and 0 < b < 1. Forb> 1, there is no common ROC, and thus the sequence in eq. (10.31) will not have a z-transform, even though the right- sided and left-sided components do individually. Forb< 1, the ROCs in eqs. (10.33) and (10.34) overlap, and thus the z-transform for the composite sequence is 1 1 X(z) = 1-bz-I- 1-b-Iz-1' b < lzl < b' (10.35) or equivalently, b2 - 1 z 1 X(z) = -b- (z- b)(z- b- 1)' b < lzl <b. (10.36) The corresponding pole-zero pattern and ROC are shown in Figure 10.1l(e). (c) (d) ~m Unit circle "" "" / f z-plane I I 1 b I (Jl.e \ \ \ ' / "" / (e) Figure 1 o. 11 Pole-zero plots and ROCs for Example 10.7: (a) eq. (1 0.33) for b > 1; (b) eq. (10.34) forb> 1; (c) eq. (10.33) forO< b < 1; (d) eq. (10.34) for 0 < b < 1; (e) pole-zero plot and ROC for eq. (1 0.36) with 0 < b < 1. Forb> 1, the z-transform of x[n] in eq. (1 0.31) does not converge for any value of z. 756 The z-Transform Chap. 10 In discussing the Laplace transform in Chapter 9, we remarked that for a rational Laplace transform, the ROC is always bounded by poles or infinity. We observe that in the foregoing examples a similar statement applies to the z-transform, and in fact, this is always true: Property 7: If the z-transform X(z) of x[n] is rational, then its ROC is bounded by poles or extends to infinity. Combining Property 7 with Properties 4 and 5, we have Property 8: If the z-transform X(z) of x[n] is rational, and if x[n] is right sided, then the ROC is the region in the z-plane outside the outermost pole-i.e., outside the circle of radius equal to the largest magnitude of the poles of X(z). Furthermore, if x[n] is causal (i.e., if it is right sided and equal to 0 for n < 0), then the ROC also includes z = oo. Thus, for right-sided sequences with rational transforms, the poles are all closer to the origin than is any point in the ROC. Property 9: If the z-transform X(z) of x[n] is rational, and if x[n] is left sided, then the ROC is the region in the z-plane inside the innermost nonzero pole-i.e., inside the circle of radius equal to the smallest magnitude of the poles of X(z) other than any at z = 0 and extending inward to and possibly including z = 0. In particular, if x[n] is anticausal (i.e., if it is left sided and equal to 0 for n > 0), then the ROC also includes z = 0. Thus, for left-sided sequences, the poles of X(z) other than any at z = 0 are farther from the origin than is any point in the ROC. For a given pole-zero pattern, or equivalently, a given rational algebraic expression X(z), there are a limited number of different ROCs that are consistent with the preceding properties. To illustrate how different ROCs can be associated with the same pole-zero pattern, we present the following example, which closely parallels Example 9.8. Example 1 0.8 Let us consider all of the possible ROCs that can be connected with the function 1 X(z) = . (10.37) (1 - ~ z- 1 )( 1 - 2c 1) The associated pole-zero pattern is shown in Figure 10.12(a). Based on our discussion in this section, there are three possible ROCs that can be associated with this algebraic expression for the z-transform. These ROCs are indicated in Figure 10.12(b)-(d). Each corresponds to a different sequence. Figure 10.12(b) is associated with a right-sided sequence, Figure 10.12(c) with a left-sided sequence, and Figure 10.12(d) with a two- sided sequence. Since Figure 10.12(d) is the only one for which the ROC includes the unit circle, the sequence corresponding to this choice of ROC is the only one of the three for which the Fourier transform converges. Sec. 10.3 The Inverse z-Transform 757 9m 9m ' / ' "" (a) (b) 9m 9m Unit circle / / ' I ' ' z-plane I \ I CRe eRe \ I \ I / ' ' ... __ / / --' (c) (d) Figure 1 o. 12 The three possible ROCs that can be connected with the expression for the z-transform in Example 10.8: (a) pole-zero pattern for X(z); (b) pole-zero pattern and ROC if x[n] is right sided; (c) pole-zero pattern and ROC if x[n] is left sided; (d) pole-zero pattern and ROC if x[n] is two sided. In each case, the zero at the origin is a second-order zero. 10.3 THE INVERSE z-TRANSFORM In this section, we consider several procedures for determining a sequence when its z- transform is known. To begin, let us consider the formal relation expressing a sequence in terms of its z-transform. This expression can be obtained on the basis of the interpretation, developed in Section 10.1, of the z-transform as the Fourier transform of an exponentially weighted sequence. Specifically, as expressed in eq. (10.7), (10.38) for any value of r so that z = rejw is inside the ROC. Applying the inverse Fourier trans- form to both sides of eq. (10.38) yields x[n]r-n = g:-I {X(rejw)}, 758 The z-Transform Chap. 10 or x[n] = rn~-l [X(reiw)]. (10.39) Using the inverse Fourier transform expression in eq. (5.8), we have or, moving the exponential factor r"" inside the integral and combining it with the term eJwn, x[n] = -1 J X(relw. )(rel.w )'1dw. (10.40) 27T 27T That is, we can recover x[n] from its z-transform evaluated along a contour z = reiw in the ROC, with r fixed and w varying over a 27T interval. Let us now change the variable of integration from w to z. With z = reiw and r fixed, dz = jreiwdw = jzdw, or dw = (1/ j)z- 1d z. The integration in eq. (10.40) is over a 27T interval in w, which, in terms of z, corresponds to one traversal around the circle lzl = r. Consequently, in terms of an integration in the z-plane, eq. (10.40) can be rewritten as = ! '1 1 x[n] j X(z)z""- dz, (10.41) 2 where the symbol 0 denotes integration around a counterclockwise closed circular contour centered at the origin and with radius r. The value of r can be chosen as any value for which X(z) converges-i.e., any value such that the circular contour of integration lzl = r is in the ROC. Equation (10.41) is the formal expression for the inverse z-transform and is the discrete-time counterpart of eq. (9.56) for the inverse Laplace transform. As with eq. (9.56), formal evaluation of the inverse transform equation (10.41) requires the use of contour integration in the complex plane. There are, however, a number of alternative procedures for obtaining a sequence from its z-transform. As with Laplace transforms, one particularly useful procedure for rational z-transforms consists of expanding the algebraic expression into a partial-fraction expansion and recognizing the sequences associated with the individual terms. In the following examples, we illustrate the procedure. Example 1 0. 9 Consider the z-transform 3- ~z- 1 6 X(z) = lzl > *· (10.42) (1 __1 2-1 )(1 __1 z-1) , 4 3 There are two poles, one at z = 1/3 and one at z = 114, and the ROC lies outside the outermost pole. That is, the ROC consists of all points with magnitude greater than that of the pole with the larger magnitude, namely the pole at z = 1/3. From Property 4 in Section 10.2, we then know that the inverse transform is a right-sided sequence. As described in the appendix, X(z) can be expanded by the method of partial fractions. For Sec. 10.3 The Inverse z-Transform 759 this example, the partial-fraction expansion, expressed in polynomials in z- 1 , is 1 2 X(z) = 1- lz-1 + 1 - lz-1. (10.43) 4 3 Thus, x[n] is the sum of two terms, one with z-transform 11[1 - (1/4)z- 1 ] and the other with z-transform 2/[1 - (1/3)z- 1 ]. In order to determine the inverse z-transform of each of these individual terms, we must specify the ROC associated with each. Since the ROC for X(z) is outside the outermost pole, the ROC for each individual term in eq. (10.43) must also be outside the pole associated with that term. That is, the ROC for each term consists of all points with magnitude greater than the magnitude of the corresponding pole. Thus, x[n] = x1 [n] + x2[n], (10.44) where (10.45) (10.46) From Example 10.1, we can identify by inspection that x 1[ n] 1 )II = (4 u[n] (10.47) and x [n] = 2 1 )II 2 (3 u[n], (10.48) and thus, 1)11 11 x[n] = (4 u[n] + 2 (13) u[n]. (10.49) Example 1 0. 1 0 Now let us consider the same algebraic expression for X(z) as in eq. (10.42), but with the ROC for X(z) as 114 < lzl < 1/3. Equation (10.43) is still a valid partial-fraction ex- pansion of the algebraic expression for X(z), but the ROC associated with the individual terms will change. In particular, since the ROC for X(z) is outside the pole at z = 1/4, the ROC corresponding to this term in eq. (10.43) is also outside the pole and consists of all points with magnitude greater than 114, as it did in the previous example. However, since in this example the ROC for X(z) is inside the pole at z = 113, that is, since the points in the ROC all have magnitude less than 113, the ROC corresponding to this term must also lie inside this pole. Thus, the z-transform pairs for the individual components in eq. (1 0.44) are (10.50) 760 The z-Transform Chap. 10 and z 2 x,_[n] <E------;> ------=-- (10.51) 1 - !z- 1 ' 3 The signal x 1 [n] remains as in eq. (10.47), while from Example 10.2, we can identify x2[n] 1 )II = -2 (3 u[ -n- 1], (10.52) so that 11 11 x[n] = (41) u[n] - 2 (13) u[ -n- 1]. (10.53) Example 1 0. 11 Finally, consider X(z) as in eq. (10.42), but now with the ROC lzl < 1/4. In this case the ROC is inside both poles, i.e., the points in the ROC all have magnitude smaller than either of the poles at z = 1/3 or z = 1/4. Consequently the ROC for each term in the partial-fraction expansion in eq. (10.43) must also lie inside the corresponding pole. As a result, the z-transform pair for x 1 [n] is given by 1 lzl < 4' (10.54) while the z-transform pair for x2 [n] is given by eq. (10.51). Applying the result of Ex- ample 10.2 to eq. (10.54), we find that x [n] 1 )n 1 = - (4 u[ -n- 1], so that 11 11 x[n] =- (41) u[-n-1] -2 (13) u[-n-1]. The foregoing examples illustrate the basic procedure of using partial-fraction ex- pansions to determine inverse z-transforms. As with the corresponding method for the Laplace transform, the procedure relies on expressing the z-transform as a linear com- bination of simpler terms. The inverse transform of each term can then be obtained by inspection. In particular, suppose that the partial-fraction expansion of X(z) is of the form m A· X(z) = ~ 1- t -t, (10.55) i=I a1z so that the inverse transform of X(z) equals the sum of the inverse transforms of the individ- ual terms in the equation. If the ROC of X(z) is outside the pole at z = ai, the inverse trans- form of the corresponding term in eq. (10.55) is Aia?u[n]. On the other hand, if the ROC of X(z) is inside the pole at z = ai, the inverse transform of this term is -Aia?u[ -n- 1]. In general, the partial-fraction expansion of a rational transform may include terms in Sec. 10.3 The Inverse z-Transform 761 addition to the first-order terms in eq. (10.55). In Section 10.6, we list a number of other z-transform pairs that can be used in conjunction with the z-transform properties to be developed in Section 10.5 to extend the inverse transform method outlined in the preceding example to arbitrary rational z-transforms. Another very useful procedure for determining the inverse z-transform relies on a power-series expansion of X(z). This procedure is motivated by the observation that the definition of the z-transform given in eq. (10.3) can be interpreted as a power series in- volving both positive and negative powers of z. The coefficients in this power series are, in fact, the sequence values x[n]. To illustrate how a power-series expansion can be used to obtain the inverse z-transform, let us consider three examples. Example 1 0. 1 2 Consider the z-transform X(z) = 4z2 + 2 + 3z- 1 , 0 < lzl < oo. (10.56) From the power-series definition of the z-transform in eq. (10.3), we can determine the inverse transform of X(z) by inspection: 4, n = -2 i: n = 0 x[n] = n = 1 \ 0, otherwise That is, x[n] = 48[n + 2] + 28[n] + 38[n- 1]. (10.57) Comparing eqs. ( 10 .56) and ( 10 .57), we see that different powers of z serve as placehold- ers for sequence values at different points in time; i.e., if we simply use the transform pair we can immediately pass from eq. (10.56) to (10.57) and vice versa. Example 1 0. 1 3 Consider X(z) = 1 _ az-I, lzl > lal. This expression can be expanded in a power series by long division: 1 + az- 1 + a2z - 2 + · · · 1- az- 1 1 1 - az- 1 az- 1 az- 1 - a2z-2 a2z-2 762 The z-Transform Chap. 10 or --------=- = 1 + az -I + a2z - 2 + .... (10.58) 1- az- 1 The series expansion of eq. (1 0.58) converges, since lzl > Ia I, or equivalently, laz- 1 1 < 1. Comparing this equation with the definition of the z-transform in equation (10.3), we see, by matching terms in powers of z, that x[n] = 0, n < 0; x[O] = 1; x[l] = a; x[2] = a2; and in general, x[n] = anu[n], which is consistent with Example 10.1. If, instead, the ROC of X(z) is specified as lzl < lal or, equivalently, laz- 1 1 > 1, then the power-series expansion for 11(1- az- 1) in eq. (10.58) does not converge. How- ever, we can obtain a convergent power series by long division again: - az- 1 + 1 or -~---a-z-_---,-1 = -a-! z- a-2z2- .... (10.59) In this case, then, x[n] = 0, n ::::: 0; and x[ -1] = -a- 1 , x[ -2] = -a-2, ... ; that is, x[n] = -anu[ -n- 1]. This is consistent with Example 10.2. The power-series expansion method for obtaining the inverse z-transform is particu- larly useful for nonrational z-transforms, which we illustrate with the following example: Example 1 0. 14 Consider the z-transform X(z) = log(l + az- 1 ), lzl > Ia I. (10.60) With lzl > Ia I, or, equivalently, laz- 1 1 < 1, eq. ( 10 .60) can be expanded in a power series using the Taylor's series expansion Lx (-l)n+lvn log(l + v) = , Iv i < 1. (10.61) n= I n Applying this to eq. (10.60), we have (10.62) from which we can identify x[n] = { ( -l)n+ I a:, n ::::: 1 (10.63) 0, n::::; 0 Sec. 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 763 or equivalently, -(-a)n x[n] = ---u[n- 1]. n In Problem 10.63 we consider a related example with region of convergence lzl < lal. 1 0.4 GEOMETRIC EVALUATION OF THE FOURIER TRANSFORM FROM THE POLE-ZERO PLOT In Section 10.1 we noted that the z-transform reduces to the Fourier transform for lzl = (i.e., for the contour in the z-plane corresponding to the unit circle), provided that the ROC of the z-transform includes the unit circle, so that the Fourier transform converges. In a similar manner, we saw in Chapter 9 that, for continuous-time signals, the Laplace transform reduces to the Fourier transform on the jw-axis in the s-plane. In Section 9.4, we also discussed the geometric evaluation of the continuous-time Fourier transform from the pole-zero plot. In the discrete-time case, the Fourier transform can again be evaluated geometrically by considering the pole and zero vectors in the z-plane. However, since in this case the rational function is to be evaluated on the contour lzl = 1, we consider the vectors from the poles and zeros to the unit circle rather than to the imaginary axis. To illustrate the procedure, let us consider first-order and second-order systems, as discussed in Section 6.6. 1 0.4. 1 First-Order Systems The impulse response of a first-order causal discrete-time system is of the general form (10.64) and from Example 10.1, its z-transform is 1 z H(z) = 1 - az -I z- a' lzl > lal. (10.65) For Ia I < 1, the ROC includes the unit circle, and consequently, the Fourier transform of h[n] converges and is equal to H(z) for z = ejw. Thus, the frequency response for the first-order system is (10.66) Figure 10.13(a) depicts the pole-zero plot for H(z) in eq. (10.65), including the vectors from the pole (at z = a) and zero (at z = 0) to the unit circle. With this plot, the geometric evaluation of H(z) can be carried out using the same procedure as described in Section 9.4. In particular, if we wish to evaluate the frequency response in eq. (10.65), we perform the evaluation for values of z of the form z = ejw. The magnitude of the frequency response at frequency w is the ratio of the length of the vector v 1 to the length of the vector v2 shown in Figure 10.13(a). The phase of the frequency response is the an- gle ofv1 with respect to the real axis minus the angle ofv2. Furthermore, the vector v1 from 764 The z-Transform Chap. 10 20 z-plane 10 ffi£ a= 0.5 -'IT 0 'IT w (a) (b) <tH(eiw) w Figure 1 o. 13 (a) Pole and zero vectors for the geometric determina- tion of the frequency response for a first-order system for a value of a be- tween 0 and 1; (b) magnitude of the frequency response for a = 0.95 and a = 0.5; (c) phase of the frequency (c) response for a = 0.95 and a = 0.5. the zero at the origin to the unit circle has a constant length of unity and thus has no effect on the magnitude of H(ejw). The phase contributed to H(ejw) by the zero is the angle of the zero vector with respect to the real axis, which we see is equal to w. For 0 <a< 1, the pole vector has minimum length at w = 0 and monotonically increases in l~ngth as w increases from zero to 1T. Thus, the magnitude of the frequency response will be Sec. 10.4 Geometric Evaluation of the Fourier Transform from the Pole-Zero Plot 765 maximum at w = 0 and will decrease monotonically as w increases from 0 to 7T. The angle of the pole vector begins at zero and increases monotonically as w increases from zero to 7T. The resulting magnitude and phase of H(e.iw) are shown in Figures 10.13(b) and (c), respectively, for two values of a. The magnitude of the parameter a in the discrete-time first-order system plays a role similar to that of the time constant T for the continuous-time first-order system of Section 9 .4.1. Note first that, as illustrated in Figure 10 .13, the magnitude of the peak of H(e.iw) at w = 0 decreases as lal decreases toward 0. Also, as was discussed in Sec- tion 6.6.1 and illustrated in Figures 6.26 and 6.27, as Ia I decreases, the impulse response decays more sharply and the step response settles more quickly. With multiple poles, the speed of response associated with each pole is related to its distance from the origin, with those closest to the origin contributing the most rapidly decaying terms in the impulse re- sponse. This is further illustrated in the case of second-order systems, which we consider next. 1 0.4.2 Second-Order Systems Next, let us consider the class of second-order systems as discussed in Section 6.6.2, with impulse response and frequency response given in eqs. (6.64) and (6.60), which were- spectively repeat here as h[ ] = 11 Sin(n + 1)(} [ ] n r . (} un (10.67) Sill and H(ejw) = (10.68) 1 - 2r cos oe-Jw + r2e-J2w' where 0 < r < 1 and 0 ::=::: (} ::=::: 7T. Since H(e.iw) = H(z)l~=eiw, we can infer from eq. (10.68) that the system function, corresponding to the z-transform of the system impulse response, is H(z) = 1 - (2r cos O)z- 1 + r2 2 · (10.69) z- The poles of H(z) are located at (10.70) and there is a double zero at z = 0. The pole-zero plot and the pole and zero vectors with 0 < (} < 7T/2 are illustrated in Figure 10.14(a). In this case, the magnitude of the frequency response equals the square of the magnitude ofv1 (since there is a double zero at the origin) divided by the product of the magnitudes of v2 and v3• Because the length of the vector v 1 from the zero at the origin is 1 for all values of w, the magnitude of the frequency response equals the reciprocal of the product of the lengths of the two pole vectors v2 and v3 . Also, the phase of the frequency response equals twice the angle of v 1 with respect to the real axis minus the sums of the angles ofv2 and v3. In Figure 10.14(b) we show the magnitude of the frequency response for r = 0.95 and r = 0.75, while in Figure 10.14(c) we display the phase of H ( e.iw) for the same two values of r. We note in particular that, as we move 766 The z-Transform Chap. 10 z-plane Unit circle 10 r = 0.95 CRe r = 0.75 0 w (a) (b) w Figure 1 o. 14 (a) Zero vectorv1 and pole vectors v2 and v3 used in the geometric cal- culation of the frequency responses for a second-order system; (b) magnitude of the frequency response correspond- ing to the reciprocal of the product of the lengths of the pole vectors for r = 0.95 and r = 0.75; (c) phase of the frequency response for r = 0.95 (c) and r = 0.75. along the unit circle from w = 0 toward w = 7T, the length of the vector v2 first decreases and then increases, with a minimum length in the vicinity of the pole location, at w = (J. This is consistent with the fact that the magnitude of the frequency response peaks for w near 0 when the length of the vector v2 is small. Based on the behavior of the pole vectors, it is also evident that as r increases toward unity, the minimum length of the pole vectors will decrease, causing the frequency response to peak more sharply with increasing r. Also, for r near unity, the angle of the vector v2 changes sharply for w in the vicinity of e. Furthermore, from the form of the impulse response [eq. (10.67) and Figure 6.29] or the Sec. 10.5 Properties of the z-Transform 767 step response [eq. (6.67) and Figure 6.30], we see, as we did with the first-order system, that as the poles move closer to the origin, corresponding to r decreasing, the impulse response decays more rapidly and the step response settles more quickly. 10.5 PROPERTIES OF THE z-TRANSFORM As with the other transforms we have developed, the z-transform possesses a number of properties that make it an extremely valuable tool in the study of discrete-time signals and systems. In this section, we summarize many of these properties. Their derivations are analogous to the derivations of properties for the other transforms, and thus, many are left as exercises at the end of the chapter. (See Problems 10.43 and 10.51-10.54.) 1 0.5. 1 linearity If z x 1 [n] ~ X1( z), with ROC = R1, and then z ax 1 [n] + bx2[n] ~ aX1 (z) + bX2(z), with ROC containing R1 n R2. (10.71) As indicated, the ROC of the linear combination is at least the intersection of R1 and R2 . For sequences with rational z-transforms, if the poles of aX1( z) + bX2(z) consist of all of the poles of X1( z) and X2(z) (i.e., if there is no pole-zero cancellation), then the region of convergence will be exactly equal to the overlap of the individual regions of convergence. If the linear combination is such that some zeros are introduced that cancel poles, then the region of convergence may be larger. A simple example of this occurs when x1 [n] and x2 [n] are both of infinite duration, but the linear combination is of finite duration. In this case the region of convergence of the linear combination is the entire z-plane, with the possible exception of zero and/or infinity. For example, the sequences a11 u[n] and a'1u[n- 1] both have a region of convergence defined by lzl > lal, but the sequence corresponding to the difference ( a 11 u [ n] - a 11 u [ n - I]) = 8 [n ] has a region of convergence that is the entire z-plane. 1 0.5.2 Time Shifting If z x[n] ~ X(z), with ROC = R, 768 The z-Transform Chap. 10 then z x[n - n0 ] ~ z -no X(z), with ROC = R, except for the possible addition or dele- (10.72) tion of the origin or infinity. Because of the multiplication by z-no, for no > 0 poles will be introduced at z = 0, which may cancel corresponding zeros of X(z) at z = 0. Consequently, z = 0 may be a pole of z-no X(z) while it may not be a pole of X(z). In this case the ROC for z-no X(z) equals the ROC of X(z) but with the origin deleted. Similarly, if n0 < 0, zeros will be introduced at z = 0, which may cancel corresponding poles of X(z) at z = 0. Consequently, z = 0 may be a zero of z-no X(z) while it may not be a pole of X(z). In this case z = oo is a pole of z-no X(z), and thus the ROC for z-no X(z) equals the ROC of X(z) but with the z = oo deleted. 1 0.5.3 Scaling in the z-Domain If z x[n] ~ X(z), with ROC= R, then zQx[n] ~ x(z:). with ROC= IZoiR. (10.73) where lzoiR is the scaled version of R. That is, if z is a point in the ROC of X(z), then the point lzolz is in the ROC of X(zizo). Also, if X(z) has a pole (or zero) at z = a, then X(zlzo) has a pole (or zero) at z = z0a. An important special case of eq. (10.73) is when zo = eiwo. In this case, lzoiR = R and (10.74) The left-hand side of eq. (10.74) corresponds to multiplication by a complex exponential sequence. The right-hand side can be interpreted as a rotation in the z-plane; that is, all pole-zero locations rotate in the z-plane by an angle of w 0 , as illustrated in Figure 10.15. This can be seen by noting that if X(z) has a factor of the form 1 - az- 1, then X(e- Jwo z) will have a factor 1- aeiwo z- 1, and thus, a pole or zero at z = a in X(z) will become a pole or zero at z = aeiwo in X(e- Jwo z). The behavior of the z-transform on the unit circle will then also shift by an angle of w 0 . This is consistent with the frequency-shifting property set forth in Section 5.3.3, where multiplication with a complex exponential in the time domain was shown to correspond to a shift in frequency of the Fourier transform. Also, in the more general case when zo = r0 eiwo in eq. (10.73), the pole and zero locations are rotated by w 0 and scaled in magnitude by a factor of r0 . Sec. 10.5 Properties of the z-Transform 769 z-plane CR-e (a) (b) Figure 1 o. 1 s Effect on the pole-zero plot of time-domain multiplica- tion by a complex exponential sequence eiwon: (a) pole-zero pattern for the z-transform for a signal x[n]; (b) pole-zero pattern for the z-transform of x[n]eiwon. 1 0.5.4 Time Reversal If z x[n] ~ X(z), with ROC = R, then z I I x[ -n] ~X(), with ROC = f?· (10.75) That is, if zo is in the ROC for x[n], then 1/zo is in the ROC for x[ -n]. 1 0.5.5 Time Expansion As we discussed in Section 5.3.7, the continuous-time concept of time scaling does not directly extend to discrete time, since the discrete-time index is defined only for integer values. However, the discrete-time concept of time expansion -i.e., of inserting a number of zeros between successive values of a discrete-time sequence x[n]--can be defined and does play an important role in discrete-time signal and system analysis. Specifically, the sequence X(k)[n], introduced in Section 5.3.7 and defined as _ { x[nlk], if n is a multiple of k X(k) [ n ] - 0, (10.76) if n is not a multiple of k has k - 1 zeros inserted between successive values of the original signal. In this case, if z x[n] ~ X(z), with ROC = R, 770 The z-Transform Chap. 10 then z k x(k)[n] ~ X(z ), with ROC = R 11 (10.77) k. That is, if z is in the ROC of X(z), then the point z11k is in the ROC of X(l). Also, if X(z) has a pole (or zero) at z = a, then X(zk) has a pole (or zero) at z = a 11k. The interpretation of this result follows from the power-series form of the z- transform, from which we see that the coefficient of the term z- n equals the value of the signal at time n. That is, with +oo X(z) = .:Z x[n]z-n, n= -oo it follows that +oo +oo X(l) = .:Z x[n](l)-n = .:Z x[n]z-kn_ (10.78) n= -oo n= -oo Examining the right-hand side of eq. (10.78), we see that the only terms that appear are of the form z-kn. In other words, the coefficient of the term z-m in this power series equals 0 if m is not a multiple of k and equals x[ml k] if m is a multiple of k. Thus, the inverse transform of eq. (10.78) is x(k)[n]. 1 0.5.6 Conjugation If z x[n] ~ X(z), with ROC = R, (10.79) then x * [n] ~z X* (z * ), with ROC= R. (10.80) Consequently, if x[n] is real, we can conclude from eq. (10.80) that X(z) = X*(z*). Thus, if X(z) has a pole (or zero) at z = z0 , it must also have a pole (or zero) at the com- plex conjugate point z = z0. For example, the transform X(z) for the real signal x[n] in Example 10.4 has poles at z = (113)e±f7TI4 . 1 0.5.7 The Convolution Property If z x 1[n] ~ X1(z), with ROC= R1, Sec. 10.5 Properties of the z-Transform 771 and then z x 1 [n] * x2[n] ~ X1 (z)X2(z), with ROC containing R1 n R2. (10.81) Just as with the convolution property for the Laplace transform, the ROC of X1( z)X2(z) includes the intersection of R1 and R2 and may be larger if pole-zero can- cellation occurs in the product. The convolution property for the z-transform can be derived in a variety of different ways. A formal derivation is developed in Problem 10.56. A derivation can also be carried out analogous to that used for the convolution property for the continuous-time Fourier transform in Section 4.4, which relied on the interpretation of the Fourier transform as the change in amplitude of a complex exponential through an LTI system. For the z-transform, there is another often useful interpretation of the convolution property. From the definition in eq. (10.3), we recognize the z-transform as a series in z- 1 where the coefficient of z-n is the sequence value x[n]. In essence, the convolution property equation (10.81) states that when two polynomials or power series X1( z) and X2(z) are multiplied, the coefficients in the polynomial representing the product are the convolution of the coefficients in the polynomials X1( z) and X2(z). (See Problem 10.57). Example 1 0. 1 5 Consider an LTI system for which y[n] = h[n] * x[n], (10.82) where h[n] = 8[n] - 8[n- 1]. Note that z 8[n]- 8[n- 1] ~ 1 - z- 1, (10.83) with ROC equal to the entire z-plane except the origin. Also, the z-transform in eq. (10.83) has a zero at z = 1. From eq. (10.81), we see that if z x[n] ~ X(z), with ROC = R, then z y[n] ~ (1- z- 1)X(z), (10.84) with ROC equal to R, with the possible deletion of z = 0 and/or addition of z = 1. Note that for this system y[n] = [o[n] - o[n- 1]] * x[n] = x[n] - x[n- 1]. 772 The z-Transform Chap. 10 That is, y[n] is the first difference ofthe sequence x[n]. Since the first-difference opera- tion is commonly thought of as a discrete-time counterpart to differentiation, eq. (10.83) can be thought of as the z-transfoi'IlJ. counterpart of the Laplace transform differentiation property presented in Section 9.5.7. Example 1 0. 16 Suppose we now consider the inverse of first differencing, namely, accumulation or sum- mation. Specifically, let w[n] be the running sum of x[n]: n w[n] = L x[k] = u[n] * x[n]. (10.85) k=-00 Then, using eq. (10.81) together with the z-transform of the unit step in Example 10.1, we see that n Z 1 w[n] = L x[k] ~ _ z-l X(z), (10.86) 1 k=-00 with ROC including at least the intersection of R with lzl > 1. Eq. (10.86) is the discrete- time z-transform counterpart of the integration property in Section 9.5.9. 10 .5.8 Differentiation in the z-Domain If z x[n] ~ X(z), with ROC = R, then z nx[n] ~ - zdX(z) with ROC = R. (10.87) dz ' This property follows in a straightforward manner by differentiating both sides of the expression for the z-transform given in eq. ( 10.3). As an example of the use of this property, let us apply it to determining the inverse z-transform considered in Example 10.14. Example 1 0. 1 7 If X(z) = log(l + az- 1 ), lzl > lal, (10.88) then z dX(z) az- 1 nx[n] ~ -z-- = + 1 lzl > lal. (10.89) dz 1 az- ' By differentiating, we have converted the z-transform to a rational expression. The inverse z-transform of the right-hand side of eq. (10.~9) can be obtained by using Exam- ple 10.1 together with the time-shifting property, eq: (~0.72), set forth in Section 10.5.2. Sec. 10.5 Properties of the z-Transform 773 Specifically, from Example 10.1 and the linearity property, z a a( -atu[n] ~ _1 , lzl > lal. (10.90) 1 + az Combining this with the time-shifting property yields 1 z az- 1 a(-at- u[n- 1] ~ _ , lzl > lal. 1 + az 1 Consequently, -( -a)n x[n] = --u[n- 1]. (10.91) n Example 1 0. 18 As another example of the use of the differentiation property, consider determining the inverse z-transform for az- 1 X(z) = (1 _ az-l )2 , lzl > lal. (10.92) From Example 10.1, z an u[n] ~ (10.93) 1- az-I' lzl > lal, and hence, (10.94) 1 0.5.9 The Initial-Value Theorem If x[n] = 0, n < 0, then x[O] = lim X(z). (10.95) z~cc This property follows by considering the limit of each term individually in the ex- pression for the z-transform, with x[n] zero for n < 0. With this constraint, X(z) = L x[n]z-n. n=O As z ~ oo, z-n ~ 0 for n > 0, whereas for n = 0, z-n = 1. Thus, eq. (10.95) follows. As one consequence of the initial-value theorem, for a causal sequence, if x[O] is finite, then limz__.oo X(z) is finite. Consequently, with X(z) expressed as a ratio of polyno- mials in z, the order of the numerator polynomial cannot be greater than the order of the denominator polynomial; or, equivalently, the number of finite zeros of X(z) cannot be greater than the number of finite poles. 774 The z-Transform Chap. 10 Example 1 0. 19 The initial-value theorem can also be useful in checking the correctness of the z- transform calculation for a signal. For example, consider the signal x[n] in Example 10.3. From eq. (10.12), we see that x[O] = 1. Also, from eq. (10.14), 1- 3 -1 limX(z) = lim 2z = 1, z->oc z->""' (1- ~z-1)(1- ~z-1) which is consistent with the initial-value theorem. 1 0.5.1 0 Summary of Properties In Table 10.1, we summarize the properties of the z-transform. 1 0.6 SOME COMMON z-TRANSFORM PAIRS As with the inverse Laplace transform, the inverse z-transform can often be easily evalu- ated by expressing X(z) as a linear combination of simpler terms, the inverse transforms of which are recognizable. In Table 10 .2, we have listed a number of useful z-transform pairs. Each of these can be developed from previous examples in combination with the proper- ties of the z-transform listed in Table 10.1. For example, transform pairs 2 and 5 follow directly from Example 10.1, and transform pair 7 is developed in Example 10.18. These, together with the time-reversal and time-shifting properties set forth in Sections 10.5.4 and 10.5.2, respectively, then lead to transform pairs 3, 6, and 8. Transform pairs 9 and 10 can be developed using transform pair 2 together with the linearity and scaling properties developed in Sections 10.5.1 and 10.5.3, respectively. 10.7 ANALYSIS AND CHARACTERIZATION OF LTI SYSTEMS USING z-TRANSFORMS The z-transform plays a particularly important role in the analysis and representation of discrete-time LTI systems. From the convolution property presented in Section 10.5.7, Y(z) = H(z)X(z), (10.96) where X(z), Y(z), and H(z) are the z-transforms of the system input, output, and impulse response, respectively. H (z) is referred to as the system function or transfer function of the system. For z evaluated on the unit circle (i.e., for z = ejw), H(z) reduces to the frequency response of the system, provided that the unit circle is in the ROC for H(z). Also, from our discussion in Section 3.2, we know that if the input to an LTI system is the complex exponential signal x[n] = zn, then the output will be H(z)zn. That is, zn is an eigenfunction of the system with eigenvalue given by H(z), the z-transform of the impulse response. Many properties of a system can be tied directly to characteristics of the poles, zeros, and region of convergence of the system function, and in this section we illustrate some of these relationships by examining several important system properties and an important class of systems. TABLE 10.1 PROPERTIES OF THE z-TRANSFORM Section Property Signal z-Transform ROC X(z) XJ(Z) X2(z) 10.5.1 Linearity ax1 [n] + bx2[n] aX1 (z) + bXz(z) At least the intersection of R 1 and R 2 10.5.2 Time shifting x[n- no] z-no X(z) R, except for the possible addition or deletion of the origin 10.5.3 Scaling in the z-domain ejwon x[n] X(e- jwo z) R z0x[n] x(~) zoR a 11 x[n] X(a- 1 z) Scaled version of R (i.e., [a[R = the set of points {[a[z} for z in R) 10.5.4 Time reversal x[ -n] Inverted R (i.e., R- 1 = the set of points z- 1, where z is in R) x[r], n = rk 10.5.5 Time expansion X(kl[n] = { for some integer r R 11k (i.e., the set of points z11k, where 0, n #- rk z is in R) 10.5.6 Conjugation x*[n] X*(z*) R 10.5.7 Convolution x 1[ n] * Xz[n] X1 (z)Xz(z) At least the intersection of R 1 and R2 10.5.7 First difference x[n] - x[n - 1] (1 - z- 1 )X(z) At least the intersection of R and lzl > 0 1 10.5.7 Accumulation 1 - z-1 X(z) At least the intersection of R and lzl > 1 dX(z) 10.5.8 Differentiation nx[n] -zdZ R in the z-domain 10.5.9 Initial Value Theorem If x[n] = 0 for n < 0, then x[O] = limX(z) z->x 776 The z-Transform Chap. 10 TABLE 10.2 SOME COMMON z-TRANSFORM PAIRS Signal Transform ROC 1. 8[n] Allz 2. u[n] I- z lzl >I I 1 3.-u[-n-I] I - z lzl <I I 4. 8[n- m] All z, except 0 (if m > 0) or x (if m < 0) 5. a 11 u[n] 1 - az- 1 lzl > lal 6. -a11 U[ -n- I] lzl < lal 7. na 11 u[n] lzl > lal 8. -na""u[ -n- I] (l- az-1)2 lzl < lal 1 - [cos w 1 9. [cos w n]u[n] 0]z- 0 1- [2cosw ]z- 1 + z- 2 lzl >I 0 [sinwo]z- 1 10. [sinw0 n]u[n] 1 - [2 cos wo]z- 1 + z-2 lzl >I I - [rcos w0]z 1 11. [r11 coswon]u[n] 1- [2rcoswo]z- 1 + 2z- 2 lzl > r r [r sin w ]z- 1 12. [r11 sinw0 n]u[n] 0 1- [2rcosw ]z- 1 + r2z- 2 lzl > r 0 1 0. 7. 1 Causality A causal LTI system has an impulse response h[n] that is zero for n < 0, and therefore is right-sided. From Property 4 in Section 10.2 we then know that the ROC of H(z) is the exterior of a circle in the z-plane. For some systems, e.g., if h[n] = o[n], so that H(z) = 1, the ROC can extend all the way in to and possibly include the origin. Also, in general, for a right-sided impulse response, the ROC may or may not include infinity. For example, if h[n] = o[n + 1], then H(z) = z, which has a pole at infinity. However, as we saw in Property 8 in Section 10 .2, for a causal system the power series H(z) = L h[n]z-n n=O does not include any positive powers of z. Consequently, the ROC includes infinity. Sum- marizing, we have the follow principle: A discrete-time LTI system is causal if and only if the ROC of its system function is the exterior of a circle, including infinity. Sec. 10.7 Analysis and Characterization of LTI z-Transforms 777 If H(z) is rational, then, from Property 8 in Section 10.2, for the system to be causal, the ROC must be outside the outermost pole and infinity must be in the ROC. Equivalently, the limit of H(z) as z ~ oo must be finite. As we discussed in Section 10.5.9, this is equivalent to the numerator of H(z) having degree no larger than the denominator when both are expressed as polynomials in z. That is: A discrete-time LTI system with rational system function H(z) is causal if and only if: (a) the ROC is the exterior of a circle outside the outermost pole; and (b) with H(z) expressed as a ratio of polynomials in z, the order of the numerator cannot be greater than the order of the denominator. Example 1 0.20 Consider a system with system function whose algebraic expression is = 3 2 2 + H(z) z - z z. z2 + lz + l 4 8 Without even knowing the ROC for this system, we can conclude that the system is not causal, because the numerator of H(z) is of higher order than the denominator. Example 1 0.21 Consider a system with system function 1 1 H(z) = 1 - lz-t + 1 - 2z-t, lzl > 2 (10.97) 2 Since the ROC for this system function is the exterior of a circle outside the outermost pole, we know that the impulse response is right-sided. To determine if the system is causal, we then need only check the other condition required for causality, namely that H (z), when expressed as a ratio of polynomials in z, has numerator degree no larger than the denominator. For this example, (10.98) z2 - ~z + 1' 2 so that the numerator and denominator of H(z) are both of degree two, and consequently we can conclude that the system is causal. This can also be verified by calculating the inverse transform of H(z). In particular, using transform pair 5 in Table 10.2, we find that the impulse response of this system is h[n] ~ [ (U + 2""] u[n]. (10.99) Since h[n] = 0 for n < 0, we can confirm that the system is causal. 10.7 .2 Stability As we discussed iJ1 Section 2.3.7, the stability of a discrete-time LTI system is equivalent to its impulse response being absolutely summable. In this case the Fourier transform of h[n] 778 The z-Transform Chap. 10 converges, and consequently, the ROC of H(z) must include the unit circle. Summarizing, we obtain the following result: An LTI system is stable if and only if the ROC of its system function H(z) includes the unit circle, lzl = 1. Example 1 0.22 Consider again the system function in eq. (10.97). Since the associated ROC is the region \z\ > 2, which does not include the unit circle, the system is not stable. This can also be seen by noting that the impulse response in eq. (10.99) is not absolutely summable. If, however, we consider a system whose system function has the same algebraic expression as in eq. (10.97) but whose ROC is the region 112 < \z\ < 2, then the ROC does contain the unit circle, so that the corresponding system is noncausal but stable. In this case, using transform pairs 5 and 6 from Table 10 .2, we find that the corresponding impulse response is 11 h[n] = 1 ) u[n] - 211 (2 u[ -n- 1], (10.100) which is absolutely summable. Also, for the third possible choice of ROC associated with the algebraic expression for H(z) in eq. (10.97), namely, \z\ < 112, the corresponding system is neither causal (since the ROC is not outside the outermost pole) nor stable (since the ROC does not include the unit circle). This can also be seen from the impulse response, which (using transform pair 6 in Table 10.2) is h[nj ~ - [ GJ + 2'}[ -n- 1]. As Example 10.22 illustrates, it is perfectly possible for a system to be stable but not causal. However, if we focus on causal systems, stability can easily be checked by examining the locations of the poles. Specifically, for a causal system with rational system function, the ROC is outside the outermost pole. For this ROC to include the unit circle, lzl = 1, all of the poles of the system must be inside the unit circle. That is: A causal LTI system with rational system function H(z) is stable if and only if all of the poles of H(z) lie inside the unit circle-i.e., they must all have magnitude smaller than 1. Example 1 0.23 Consider a causal system with system function 1 H(z) = 1 - az -1, which has a pole at z = a. For this system to be stable, its pole must be inside the unit circle, i.e., we must have \a\ < 1. This is consistent with the condition for the absolute summability of the corresponding impulse response h[n] = a11 u[n]. Sec. 10.7 Analysis and Characterization of LTI z-Transforms 779 Example 1 0.24 The system function for a second-order system with complex poles was given in eq. (10.69), specifically, H(z) = 1- (2rcos8)z~ 1 + (10.101) r2z-2' with poles located at z1 = rei8 and z2 = re~ JO. Assuming causality, we see that the ROC is outside the outermost pole (i.e., lzl > lrl). The pole-zero plot and ROC for this system are shown in Figure 10.16 for r < 1 and r > 1. For r < 1, the poles are inside the unit circle, the ROC includes the unit circle, and therefore, the system is stable. For r > 1, the poles are outside the unit circle, the ROC does not include the unit circle, and the system is unstable. !lm !lm Unit circle Unit circle z-plane - j z-plane ,. .. ,~-- --~X I / ' I \ I \ I I I I \ 1 : ffi..e (a) (b) Figure 1 o. 16 Pole-zero plot for a second-order system with complex poles: (a) r < 1; (b) r > 1. 1 0. 7. 3 LTI Systems Characterized by linear Constant-Coefficient Difference Equations For systems characterized by linear constant-coefficient difference equations, the proper- ties of the z-transform provide a particularly convenient procedure for obtaining the system function, frequency response, or time-domain response of the system. Let us illustrate this with an example. Example 10.25 Consider an LTI system for which the input x[n] and output y[n] satisfy the linear constant-coefficient difference equation 1 1 y[n]- ly[n- 1] = x[n] + x[n- 1]. (10.102) 3 780 The z-Transform Chap. 10 Applying the z-transform to both sides of eq. ( 10.1 02), and using the linearity property set forth in Section 10.5.1 and the time-shifting property presented in Section 10.5.2, we obtain or 1+.!3. z- 1 ] Y(z) = X(z) . (10.103) [ 1- .!.z- 1 2 From eq. (10.96), then, Y(z) 1 + .!.z- 1 H(z) = X( ) = ~ _ . (10.104) z 1 - 2z 1 This provides the algebraic expression for H(z), but not the region of convergence. In fact, there are two distinct impulse responses that are consistent with the difference equation (10.102), one right sided and the other left sided. Correspondingly, there are two different choices for the ROC associated with the algebraic expression (10.104). One, lzl > 112, is associated with the assumption that h[n] is right sided, and the other, lzl < 112, is associated with the assumption that h[n] is left sided. Consider first the choice of ROC equal to lzl > 1. Writing H(z) = ( 1 + 1 3z -1) 1 1 _ , 1- 2z I we can use transform pair 5 in Table 10.2, together with the linearity and time-shifting properties, to find the corresponding impulse response 11 h[n] = 1) u[n] + 1 (1)n-l (2 3 2 u[n- 1]. For the other choice of ROC, namely, lzl < 1, we can use transform pair 6 in Table 10.2 and the linearity and time-shifting properties, yielding 11 h[n] =- 1) (2 u[-n- 1]- 1 (1)n-l 3 2 u[-n]. In this case, the system is anticausal (h[n] = 0 for n > 0) and unstable. For the more general case of an Nth-order difference equation, we proceed in a man- ner similar to that in Example 10.25, applying the z-transform to both sides of the equation and using the linearity and time-shifting properties. In particular, consider an LTI system for which the input and output satisfy a linear constant-coefficient difference equation of the form N M L aky[n - k] = L bkx[n - k]. (10.105) k=O k=O Sec. 10.7 Analysis and Characterization of LTI z-Transforms 781 Then taking z-transforms of both sides of eq. (10.105) and using the linearity and time- shifting properties, we obtain N M .L, akz-kY(z) = .L, bkz-kX(z), k=O k=O or N M Y(z) .L, akz-k = X(z) .L, bkz-k, k=O k=O so that H( 2 ) = Y(z) (10.106) X(z) We note in particular that the system function for a system satisfying a linear constant- coefficient difference equation is always rational. Consistent with our previous example and with the related discussion for the Laplace transform, the difference equation by itself does not provide information about which ROC to associate with the algebraic expression H(z). An additional constraint, such as the causality or stability of the system, however, serves to specify the region of convergence. For example, if we know in addition that the system is causal, the ROC will be outside the outermost pole. If the system is stable, the ROC must include the unit circle. 1 0.7.4 Examples Relating System Behavior to the System Function As the previous subsections illustrate, many properties of discrete-time LTI systems can be directly related to the system function and its characteristics. In this section, we give several additional examples to show how z-transform properties can be used in analyzing systems. Example 1 0.26 Suppose that we are given the following information about an LTI system: 1. If the input to the system is x1 [n] = (116r u[n], then the output is y,[n] = HU + w(U]u[n], where a is a real number. 2. If x2[n] = ( -l)n, then the output is Y2[n] = ~( -l)n. As we now show, from these two pieces of information, we can determine the system function H(z) for this system, including the value of the number a, and can also immediately deduce a number of other properties of the system. The z-transforms of the signals specified in the first piece of information are 782 The z-Transform Chap. 10 1 X 1 ( z) = -----,---- 1 - !z-1' lzl > 6' (10.107) 6 a 10 YI(Z) = + --.,.--- 1 - ! z- 1 1 - ! z- 1 2 3 (10.108) (a+ 10)- (5 + })z- 1 1 (1 _ ! z- I )(1 _ ! z- I ) ' lzl > 2· 2 3 From eq. (10.96), it follows that the algebraic expression for the system function is Y1(z) [(a+ 10)- (5 + ~)z- 1 ][1- -1z - 1] H(z) = -- = - 6 (10.109) X1(z) (1- !z-1)(1- !z-1) 2 3 Furthermore, we know that the response to x2 [n] = ( -l)n must equal ( -l)n multiplied by the system function H(z) evaluated at z = -1. Thus from the second piece of infor- mation given, we see that 7 _ H _ _ [(a+ 10) + 5 + }][~] 1 (10.110) 4 - ( ) - ( ~ )( ~) Solving eq. (10.110), we find that a = -9, so that (1- 2z- 1)(1- !z- 1) H(z) = 6 4z- (10.111) (1- 1)(1- ~z- 1 )' or (10.112) or, finally, z2 - .!lz + ! H(z) = 6 3 (10.113) z2 - ~z + ! · 6 6 Also, from the convolution property, we know that the ROC of Y1 (z) must include at least the intersections of the ROCs of X1 (z) and H(z). Examining the three possible ROCs for H(z) (namely, lzl < 113, 113 < lzl < 112, and lzl > 112), we find that the only choice that is consistent with the ROCs of X1 (z) and Y1( z) is lzl > 112. Since the ROC for the system includes the unit circle, we know that the system is stable. Furthermore, from eq. (10.113) with H(z) viewed as a ratio of polynomials in z, the order of the numerator does not exceed that of the denominator, and thus we can conclude that the LTI system is causal. Also, using eqs. (10.112) and (10.106), we can write the difference equation that, together with the condition of initial rest, characterizes the system: 5 1 13 1 y[n]- 6y[n- 1] + 6y[n- 2] = x[n]- 6 x[n- 1] + 3x[n- 2]. Example 1 0.27 Consider a stable and causal system with impulse response h[n] and rational system function H(z). Suppose it is known that H(z) contains a pole at z = 112 and a zero somewhere on the unit circle. The precise number and locations of all of the other poles Sec. 10.8 System Function Algebra and Block Diagram Representations 783 and zeros are unknown. For each of the following statements, let us determine whether we can definitely say that it is true, whether we can definitely say that it is false, or whether there is insufficient information given to determine if it is true or not: (a) ~ { (112)Hh[nl} converges. (b) H(ejw) = 0 for some w. (c) h[n] has finite duration. (d) h[n] is real. (e) g[n] = n[h[n] * h[n]] is the impulse response of a stable system. Statement (a) is true.~ { (112)"" h[nl} corresponds to the value ofthe z-transform of h[n] at z = 2. Thus, its convergence is equivalent to the point z = 2 being in the ROC. Since the system is stable and causal, all of the poles of H(z) are inside the unit circle, and the ROC includes all the points outside the unit circle, including z = 2. Statement (b) is true because there is a zero on the unit circle. Statement (c) is false because a finite-duration sequence must have an ROC that includes the entire z-plane, except possibly z = 0 and/or z = oo. This is not consistent with having a pole at z = 112. Statement (d) requires that H(z) = H*(z*). This in tum implies that if there is a pole (zero) at a nonreallocation z = z0 , there must also be a pole (zero) at z = z~. Insufficient information is given to validate such a conclusion. Statement (e) is true. Since the system is causal, h[n] = 0 for n < 0. Conse- quently, h[n] * h[n] = 0 for n < 0; i.e., the system with h[n] * h[n] as its impulse re- sponse is causal. The same is then true for g[n] = n[h[n] * h[n]]. Furthermore, by the convolution property set forth in Section 10.5.7, the system function corresponding to the impulse response h[n] * h[n] is H 2(z), and by the differentiation property presented in Section 10.5.8, the system function corresponding to g[n] is (10.114) From eq. (10.114), we can conclude that the poles of G(z) are at the same locations as those of H (z), with the possible exception of the origin. Therefore, since H (z) has all its poles inside the unit circle, so must G(z). It follows that g[n] is the impulse response of a causal and stable system. 1 0.8 SYSTEM FUNCTION ALGEBRA AND BLOCK DIAGRAM REPRESENTATIONS Just as with the Laplace transform in continuous time, the z-transform in discrete time allows us to replace time-domain operations such as convolution and time shifting with algebraic operations. This was exploited in Section 10.7.3, where we were able to replace the difference-equation description of an LTI system with an algebraic description. The use of the z-transform to convert system descriptions to algebraic equations is also helpful in analyzing interconnections ofLTI systems and in representing and synthesizing systems as interconnections of basic system building blocks. 784 The z-Transform Chap. 10 1 0.8.1 System Functions for Interconnections of LTI Systems The system function algebra for analyzing discrete-time block diagrams such as series, parallel, and feedback interconnections is exactly the same as that for the corresponding continuous-time systems in Section 9 .8.1. For example, the system function for the cascade of two discrete-time LTI systems is the product of the system functions for the individual systems in the cascade. Also, consider the feedback interconnection of two systems, as shown in Figure 10.17. It is relatively involved to determine the difference equation or im- pulse response for the overall system working directly in the time domain. However, with the systems and sequences expressed in terms of their z-transforms, the analysis involves only algebraic equations. The specific equations for the interconnection of Figure 10.17 exactly parallel eqs. (9.159)-(9.163), with the final result that the overall system function for the feedback system of Figure 10.17 is Y(z) = H(z) = H,(z) (10.115) X(z) 1 + H, (z)H2(Z). + e[n] H1(z) x[n] "" + h y[n] - 1[n] ,"" H2(z) h2[n] - Figure 10.17 Feedback intercon- nection of two systems. 1 0.8.2 Block Diagram Representations for Causal LTI Systems Described by Difference Equations and Rational System Functions As in Section 9.8.2, we can represent causal LTI systems described by difference equations using block diagrams involving three basic operations-in this case, addition, multiplica- tion by a coefficient, and a unit delay. In Section 2.4.3, we described such a block diagram for a first-order difference equation. We first revisit that example, this time using system function algebra, and then consider several slightly more complex examples to illustrate the basic ideas in constructing block diagram representations. Example 1 0.28 Consider the causal LTI system with system function (10.116) Using the results in Section 10.7.3, we find that this system can also be described by the difference equation 1 y[n] - 4 y[n - 1] = x[n], Sec. 10.8 System Function Algebra and Block Diagram Representations 785 together with the. condition of initial rest. In Section 2.4.3 we constructed a block diagram representation for a first-order system of this form, and an equivalent block diagram (corresponding to Figure 2.28 with a = -114 and b = 1) is shown in Figure 10.18(a). Here, z- 1 is the system function of a unit delay. That is, from the time-shifting property, the input and output of this system are related by w[n] = y[n- 1]. The block diagram in Figure 10.18(a) contains a feedback loop much as for the sys- tem considered in the previous subsection and pictured in Figure 10.17. In fact, with some minor modifications, we can obtain the equivalent block diagram shown in Fig- ure 10.18(b), which is exactly in the form shown in Figure 10.17, with H 1(z) = 1 and H2(z) = -114z- 1• Then, applying eq. (10.115), we can verify that the system function of the system in Figure 10.18 is given by eq. (10.116). • y[n] (a) + x[n] __. ...,.. . , + 1---------~--.- y[n] (b) Figure 1 o. 18 (a) Block diagram representations of the causal LTI system in Example 10 .28; (b) equivalent block diagram representation. Example 1 0.29 Suppose we now consider the causal LTI system with system function H(z) = 1 - 2z-I = ( 1 )(1 - 2z-1). (10.117) 1 - ! z- 1 1 - ! z- 1 4 4 As eq. (1 0.117) suggests, we can think of this system as the cascade of a system with system function 1/[1 - (1/4)z-1 ] and one with system function 1 - 2z-1• We have il- lustrated the cascade in Figure 10 .19(a), in which we have used the block diagram in Figure 10.18(a) to represent 1/[1 - (1/4)z-1 ]. We have also represented 1 - 2z-1 using a unit delay, an adder, and a coefficient multiplier. Using the time-shifting property, we then see that the input v[n] and output y[ n] of the system with the system function 1 - 2z-1 786 The z-Transform Chap. 10 are related by y[n] = v[n] - 2v[n- 1]. While the block diagram in Figure 10.19(a) is certainly a valid representation of the system in eq. ( 10 .117), it has an inefficiency whose elimination leads to an alternative block-diagram representation. To see this, note that the input to both unit delay elements in Figure 10 .19(a) is v[n], so that the outputs of these elements are identical; i.e., w[n] = s[n] = v[n- 1]. Consequently, we need not keep both of these delay elements, and we can simply use the output of one of them as the signal to be fed to both coefficient multipliers. The result is the block diagram representation in Figure 10 .19(b ). Since each unit delay element requires a memory register to store the preceding value of its input, the representation in Figure 10 .19(b) requires less memory than that in Figure 10.19(a). -----------------1 f:\ v[n] x[n] -......:-----o~~~f---------,l~:;....;:..~___,l------•~1 + 1-T-----!~ y[n] k_2 ,____..... (a) x[n] -----~1 + 1------------------~1 + 1------1~ y[n] 1 4 (b) Figure 1 o. 19 (a) Block-diagram representations for the system in Exam- ple 10 .29; (b) equivalent block-diagram representation using only one unit de- lay element. Example 1 0.30 Next, consider the second-order system function (10.118) 1 + !z- 1 - !s z-2 ' 4 which is also described by the difference equation 1 1 y[n] + 4y[n- 1]- sy[n- 2] = x[n]. (10.119) Sec. 10.8 System Function Algebra and Block Diagram Representations 787 Using the same ideas as in Example 10.28, we obtain the block-diagram representation for this system shown in Figure 10.20(a). Specifically, since the two system function blocks in this figure with system function z- 1 are unit delays, we have f[n] = y[n- 1], e[n] = f[n- 1] = y[n- 2], so that eq. (10.119) can be rewritten as 1 1 y[n] = - 4y[n- 1] + gy[n- 2] + x[n], (a) x[n] y[n] -2 ~3 (b) y[n] x[n]~ Figure 1 0.20 Block-diagram representations for the system in Exam- ple 10.30: (a) direct form; (b) cascade form; (c) parallel form. 788 The z-Transform Chap. 10 or y[n] = -41 1 f[n] + se[n] + x[n], which is exactly what the figure represents. The block diagram in Figure 10.20(a) is commonly referred to as a direct-form representation, since the coefficients appearing in the diagram can be determined by inspection from the coefficients appearing in the difference equation or, equivalently, the system function. Alternatively, as in continuous time, we can obtain both cascade- form and parallel-form block diagrams with the aid of a bit of system function algebra. Specifically, we can rewrite eq. (10.118) as H(~ = ( 1 )( 1 ) , (10.120) 1 + ~z- 1 1- ~z-1 which suggests the cascade-form representation depicted in Figure 10.20(b) in which the system is represented as the cascade of two systems corresponding to the two factors in eq. (10.120). Also, by performing a partial-fraction expansion, we obtain ~ I H(z) = 3 + 3 1 + _!_ z- I 1 - _!_ z- I ' 2 4 which leads to the parallel-form representation depicted in Figure 10.20(c). Example 10.31 Finally, consider the system function (10.121) Writing 1+~z-11 -~z-2 )( 7 - 1 1 -2) H(z)= 1-4z -2z (10.122) ( suggests representing the system as the cascade of the system in Figure 10.20(a) and the system with system function 1 - ~z- 1 - ~z- 2 • However, as in Example 10.29, the unit delay elements needed to implement the first term in eq. (10.122) also produce the de- layed signals needed in computing the output of the second system. The result is the direct-form block diagram shown in Figure 10.21, the details of the construction of which are examined in Problem 10.38. The coefficients in the direct-form representation can be determined by inspection from the coefficients in the system function of eq. (10.121). We can also write H (z) in the forms 1 H(z) = (1+ 1 l4 z- )(1- 22 - ) (10.123) 1 + ! z- 1 1 - ! z- 1 2 4 Sec. 10.9 The Unilateral z-Transform 789 y[n] Figure 1 0.21 Direct-form representation for the system in Example 10 .31. and 5/3 14/3 H(z) = 4+ -+- - (10.124) 1 _!_ z- 1 1 - _!_ z- 1 • 2 4 Eq. (10.123) suggests a cascade-form representation, while eq. (10.124) leads to a parallel-form block diagram. These are also considered in Problem 10.38. The concepts used in constructing block-diagram representations in the preceding examples can be applied directly to higher order systems, and several examples are con- sidered in Problem 10.39. As in continuous time, there is typically considerable flexibility in doing this-e.g., in how numerator and denominator factors are paired in a product rep- resentation as in eq. (10.123), in the way in which each factor is implemented, and in the order in which the factors are cascaded. While all of these variations lead to representa- tions of the same system, in practice there are differences in the behavior of the different block diagrams. Specifically, each block-diagram representation of a system can be trans- lated directly into a computer algorithm for the implementation of the system. However, because the finite word length of a computer necessitates quantizing the coefficients in the block diagram and because there is numerical roundoff as the algorithm operates, each of these representations will lead to an algorithm that only approximates the behavior of the original system. Moreover, the errors in each of these approximations will be somewhat different. Because of these differences, considerable effort has been put into examining the relative merits of the various block-diagram representations in terms of their accuracy and sensitivity to quantization effects. For discussions of this subject, the reader may tum to the references on digital signal processing in the bibliography at the end of the book. 10.9 THE UNILATERAl z-TRANSFORM The form of the z-transform considered thus far in this chapter is often referred to as the bilateral z-transform. As was the case with the Laplace transform, there is an alterna- tive form, referred to as the unilateral z-transform, that is particularly useful in analyzing causal systems specified by linear constant-coefficient difference equations with nonzero initial conditions (i.e., systems that are not initially at rest). In this section, we introduce the unilateral z-transform and illustrate some of its properties and uses, paralleling our discussion of the unilateral Laplace transform in Section 9.9. 790 The z-Transform Chap. 10 The unilateral z-transform of a sequence x[ n] is defined as ~(z) = L x[n]z-n. (10.125) n=O As in previous chapters, we adopt a convenient shorthand notation for a signal and its unilateral z-transform: 'UZ x[n] ~ ~(z) = 11Z{ x[nJ}. (10.126) The unilateral z-transform differs from the bilateral transform in that the summation is carried out only over nonnegative values of n, whether or not x[n] is zero for n < 0. Thus the unilateral z-transform of x[n] can be thought of as the bilateral transform of x[n]u[n] (i.e., x[n] multiplied by a unit step). In particular, then, for any sequence that is zero for n < 0, the unilateral and bilateral z-transforms will be identical. Referring to the discussion of regions of convergence in Section 10.2, we also see that, since x[n]u[n] is always a right-sided sequence, the region of convergence of ~(z) is always the exterior of a circle. Because of the close connection between bilateral and unilateral z-transforms, the calculation of unilateral transforms proceeds much as for bilateral transforms, with the caveat that we must take care to limit the range of summation in the transform to n ~ 0. Similarly, the calculation of inverse unilateral transforms is basically the same as for bilat- eral transforms, once we take into account the fact that the ROC for a unilateral transform is always the exterior of a circle. 1 0.9.1 Examples of Unilateral z-Transforms and Inverse Transforms Example 1 0.32 Consider the signal x[n] = anu[n]. (10.127) Since x[ n] = 0, n < 0, the unilateral and bilateral transforms are equal for this example, and thus, in particular, X(z) = 1 - art, lzl > lal. (10.128) Example 1 0.33 Let x[n] = an+ 1 u[n + 1]. (10.129) In this case the unilateral and bilateral transforms are not equal, since x[ -1] = 1 ¥- 0. The bilateral transform is obtained from Example 10.1 and the time-shifting property set forth in Section 10.5.2. Specifically, z X(z) = 1 - az -t, lzl > lal. (10.130) Sec. 10.9 The Unilateral z-Transform 791 In contrast, the unilateral transform is ~(z) = L x[n]z-n n=O X = Lan+lz-"", n=O or ~(z) = a 1- az- 1 lzl > lal. (10.131) ' Example 1 0.34 Consider the unilateral z-transform (10.132) In Example 10.9, we considered the inverse transform for a bilateral z-transform X(z) of the same form as in eq. (10.132) and for several different ROCs. In the case of the unilateral transform, the ROC must be the exterior of the circle of radius equal to the largest magnitude of the poles of ~(z)-in this instance, all points z with lzl > 1/3. We can then invert the unilateral transform exactly as in Example 10.9, yielding x[n] = (41) "" u[n] + 2 (13) "" u[n] for n 2: 0. (10.133) In eq. (10.133), we have emphasized the fact that inverse unilateral z-transforms provide us with information about x[n] only for n 2: 0. Another approach to inverse transforms introduced in Section 10.3, namely, iden- tifying the inverse transforms from the coefficients in the power-series expansion of the z-transform, also can be used for unilateral transforms. However, in the unilateral case, a constraint which must be satisfied is that, as a consequence of eq. (10.125), the power- series expansion for the transform cannot contain terms with positive powers of z. For instance, in Example 10.13 we performed long division on the bilateral transform 1 X(z) = 1 -t (10.134) - az in two ways, corresponding to the two possible ROCs for X(z). Only one of these choices, namely, that corresponding to the ROC lzl > Ia!, led to a series expansion without positive powers of z, i.e., (10.135) 1 - az-I 792 The z-Transform Chap. 10 and this is the only choice for the expansion if eq. (1 0.134) represents a unilateral trans- form. Note that the requirement that X(z) have a power-series expansion with no terms with positive powers of z implies that not every function of z can be a unilateral z-transform. In particular, if we consider a rational function of z written as a ratio of polynomials in z (not in z~ 1) , i.e., p(z) (10.136) q(z)' then for this to be a unilateral transform (with the appropriately chosen ROC as the ex- terior of a circle), the degree of the numerator must be no bigger than the degree of the denominator. Example 1 0.35 A simple example illustrating the preceding point is given by the rational function in eq. (10.130), which we can write as a ratio of polynomials in z: z2 (10.137) z-a There are two possible bilateral transforms that can be associated with this function, namely those corresponding to the two possible ROCs, lzl < lal and lzl > lal. The choice lzl > Ia Ic orresponds to a right-sided sequence, but not to a signal that is zero for all n < 0, since its inverse transform, which is given by eq. (10.129), is nonzero for n = -1. More generally, if we associate eq. (10.136) with the bilateral transform with the ROC that is the exterior of the circle with radius given by the magnitude of the largest root of q(z), then the inverse transform will certainly be right sided. However, for it to be zero for all n < 0, it must also be the case that degree(p(z)) ::::; degree(q(z)). 1 0.9.2 Properties of the Unilateral z-Transform The unilateral z-transform has many important properties, some of which are identical to their bilateral counterparts and several of which differ in significant ways. Table 10.3 sum- marizes these properties. Note that we have not included a column explicitly identifying the ROC for the unilateral z-transform for each signal, since the ROC of any unilateral z- transform is always the exterior of a circle. For example, the ROC for a rational unilateral z-transform is always outside the outermost pole. By contrasting this table with the corresponding Table 10.1 for bilateral z-transforms, we can gain considerable insight into the nature of the unilateral transform. In particular, several properties-namely, linearity, scaling in the z-domain, time expansion, conjuga- tion, and differentiation in the z-domain -are identical to their bilateral counterparts, as is the initial-value theorem stated in Section 10.5.9, which is fundamentally a unilateral transform property, since it requires x[n] = 0 for n < 0. One bilateral property, namely, the time-reversal property set forth in Section 10.5.4, obviously has no meaningful coun- terpart for the unilateral transform, while the remaining properties differ in important ways between the bilateral and unilateral cases. Sec. 10.9 The Unilateral z-Transform 793 TABLE 10.3 PROPERTIES OF THE UNILATERAL z-TRANSFORM Property Signal Unilateral z-Transform x[n] x 1[n] x2[n] Linearity ax1 [n] + bx2[n] a~, (z) + b~2(z) Time delay x[n- 1] z-'~(z) + x[ -1] Time advance x[n + 1] z~(z) - zx[O] Scaling in the z-domain e1w 011 x[n] ~(e- Jwo z) z0x[n] ~(z/zo) 11 Q X[n] ~(a- 1 z) Time expansion xk[n] = { x[m], n = mk ~(zk) 0, n =I= mk for any m Conjugation x*[n] ~*(z*) Convolution (assuming x 1[ n] * x2[n] ~,(z)~2(z) that x 1 [n] and x2 [n] are identically zero for n < 0) First difference x[n] - x[n- 1] (1- z- 1 )~(z)- x[-1] 1 Accumulation 1 - z-I ~(z) d~(z) Differentiation in the nx[n] - z-----;IZ z-domain Initial Value Theorem x[O] = lim ~(z) 7--'+X Let us examine the difference in the convolution property first. Table 10.3 states that if XI [n] = x2 [n] = 0 for all n < 0, then (10.138) Since in this case the unilateral and bilateral transforms are identical for each of these signals, eq. (10.138) follows from the bilateral convolution property. Thus, the system analysis and system function algebra developed and used in this chapter apply without change to unilateral transforms, as long as we are considering causal LTI systems (for which the system function is both the bilateral and the unilateral transform of the impulse response) with inputs that are identically zero for n < 0. An exa~ple of such application is to the accumulation or summation property in Table 10.3. Specifically, if x[n] = 0 for n < 0, then Ln 'UZ · 1 x[k] = x[n] * u[n] ~ X(z)'U(z) = X(z) __ . (10.139) k=O 1 z 1 As a second example, consider the following: 794 The z-Transform Chap. 10 Example 1 0.36 Consider the causal LTI system described by the difference equation y[n] + 3y[n- 1] = x[n], (10.140) together with the condition of initial rest. The system function for this system is 1 J{(z) = 1 + 3z- • (10.141) 1 Suppose that the input to the system is x[n] = au[n], where a is a given constant. In this case, the unilateral (and bilateral) z-transform of the output y[n] is 'Y(z) = J{( z)X(z) = (1 + 3z-l~(l - z-1) (10.142) = (3/4)a + (1/4)a . 1+3z-1 1-z-1 Applying Example 10.32 to each term of eq. (10.142) yields y[n] ~ a[~ + (~ )<-3)"" ]u[n] (10.143) An important point to note here is that the convolution property for unilateral z- transforms applies only if the signals x 1 [n] and x2[n] in eq. (10.138) are both identically zero for n < 0. While it is generally true that the bilateral transform of x 1 [n] * x2[n] equals the product of the bilateral transforms of x1 [n] and x2 [n], the unilateral transform of x1 [n]* x2 [n] in general does not equal the product of the unilateral transforms if x 1 [n] or x2 [n] is nonzero for n < 0. This point is explored further in Problem 10 .41. Much of the importance of the unilateral z-transform lies in its application to analyz- ing causal systems and, in particular, systems characterized by linear constant- coefficient difference equations with possibly nonzero initial conditions. In Section 10.7 we saw how the bilateral transform-particularly the shifting property for bilateral z-transforms- could be used to analyze and compute solutions for LTI systems characterized by such difference equations, together with the assumption of initial rest. As we will now see, the shifting property for unilateral transforms, which differs from its bilateral counterpart, plays an analogous role for initialized systems. To develop the shifting property for the unilateral transform, consider the signal y[n] = x[n- 1]. (10.144) Then 00 'Y(z) = L x[n- 1]z-n n=O = x[ -1] + L x[n- 1]z-n n=l 00 = x[ -1] + L x[n]z-(n+l), n=O Sec. 10.9 The Unilateral z-Transform 795 or 'Y(z) = x[ -1] + z -I~ x[n]z-n, (10.145) n=O so that 'Y(z) = x[-1] + z- 1X(z). (10.146) By repeated application of eq. (10.146), the unilateral transform of w[n] = y[n - 1] = x[n - 2] (10.147) is W(z) = x[ -2] + x[ -l]z- 1 + z-2X(z). (10.148) Continuing this iterative procedure, we can also determine the unilateral transform of x[n- m] for any positive value of m. Eq. (10.146) is sometimes referred to as the time delay property, since y[n] in eq. (10.144) is a delayed version of x[n]. There is also a time advance property for unilateral transforms that relates the transform of an advanced version of x[n] to X(z). Specifically, as shown in Problem 10.60, 'UZ x[n + 1] ~ zX(z) - zx[O]. (10.149) 1 0.9.3 Solving Difference Equations Using the Unilateral z-Transform The following example illustrates the use of unilateral z-transforms and the time delay property to solve linear constant-coefficient difference equations with nonzero initial con- ditions: Example 1 0.37 Consider again the difference equation (10.140) with x[n] = au[n] and with the initial condition y[-1] = {3. (10.150) Applying the unilateral transform to both sides of eq. ( 10 .140) and using the linearity and time delay properties, we obtain (10.151) Solving for 'Y(z) yields cy z = _ 3{3 + a ( ) 1 + 3z- 1 (1 + 3z- z- (10.152) 1)(1- 1)' 796 The z-Transform Chap. 10 Referring to Example 10.36 and, in particular, eq. (10.142), we see that the second term on the right-hand side of eq. (10.152) equals the unilateral z-transform of there- sponse of the system when the initial condition in eq. (10.150) is zero (/3 = 0). That is, this term represents the response of the causal LTI system described by eq. (10.140), to- gether with the condition of initial rest. As in continuous-time, this response is frequently referred to as the zero-state response, i.e., the response when the initial condition or state is zero. The first term on the right-hand side of eq. (10.152) is interpreted as the unilateral transform of the zero-input response-i.e., the response of the system when the input is zero (a = 0). The zero-input response is a linear function of the value f3 of the ini- tial condition. Moreover, eq. (10.152) illustrates the fact that the solution of a linear constant-coefficient difference equation with nonzero initial state is the superposition of the zero-state a~d zero-input responses. The zero-state response, obtained by setting the initial condition to zero, corresponds to the response of the causal LTI system defined by the difference equation and the condition of initial rest. The zero-input response is there- sponse to the initial condition alone with the input set to zero. Problems 10.20 and 10.42 provide other examples illustrating the use of unilateral transforms to solve difference equations with nonzero initial conditions. Finally, for any values of a and {3, we can expand cy(z) in eq. (10.152) by the method of partial fractions and invert the result to obtain y[n]. For example, if a = 8 and {3 = 1, 3 2 cy(z) = 1 + 3z- (10.153) 1 + 1 - z- 1 ' and applying the unilateral transform pair in Example 10.32 to each term yields y[n] = [3(-3t + 2]u[n], for n 2 0. (10.154) 1 0. 1 0 SUMMARY In this chapter, we have developed the z-transform for discrete-time signals and systems. The discussion and development closely paralleled the corresponding treatment of the Laplace transform for continuous-time signals, but with some important differences. For example, in the complex s-plane the Laplace transform reduces to the Fourier transform on the imaginary axis, whereas in the complex z-plane the z-transform reduces to the Fourier transform on the unit circle. For the Laplace transform the ROC consists of a strip or half-plane (i.e., a strip extending to infinity in one direction), whereas for the z-transform the ROC is a ring, perhaps extending outward to infinity or inward to include the origin. As with the Laplace transform, time-domain characteristics such as the right -sided, left- sided, or two-sided nature of a sequence and the causality or stability of an LTI sy&tem can be associated with properties of the region of convergence. In particular, for rational z-transforms, these time-domain characteristics can be associated with the pole locations in relation to the region of convergence. Because of the properties of z-transforms, LTI systems, including those described by linear constant-coefficient difference equations, can be analyzed in the transform do- main by algebraic manipulations. System function algebra also is a very usefpl tool for the analysis of interconnections of LTI systems and for the construction of block diagram representations of LTI systems described qy difference equations. Chap. 10 Problems 797 For the most part in this chapter, we have focused on bilateral z-transforms. However, as with the Laplace transform, we have also introduced a second form of the z-transform known as the unilateral z-transform. The unilateral transform, which can be viewed as the bilateral transform of a signal whose values for n < 0 have been set to zero, is particularly useful for analyzing systems described by linear constant-coefficient difference equations with nonzero initial conditions. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 10.1. Determine the constraint on r = \z\ for each of the following sums to converge: (a) ~ (~)n+1 2 -n (b) ~(~)-n+lzn n=-1 n=l (c) ~{ l+(~l)""}z-n (d) ~ (4)1n1 cos(in)z-n n=O n= -oc 10.2. Consider the signal x[n] = S1 ) II ( u[n - 3]. Use eq. (10.3) to evaluate the z-transform of this signal, and specify the corre- sponding region of convergence. 10.3. Let Determine the constraints on the complex number a and the integer n0 , given that the ROC of X(z) is 1 < \z\ < 2. 10.4. Consider the signal x[n] = { C1)n cos( in), n~O 0, n>O Determine the poles and ROC for X(z). 10.5. For each of the following algebraic expressions for the z-transform of a signal, determine the number of zeros in the finite z-plane and the number of zeros at infinity. 798 The z-Transform Chap. 10 z-2(1- z-1) (c) (1- ±z- 1)(1 + ±z- 1) 10.6. Let x[n] be an absolutely summable signal with rational z-transform X(z). If X(z) is known to have a pole at z = 112, could x[n] be (a) a finite-duration signal? (b) a left- sided signal? (c) a right-sided signal? (d) a two-sided signal? 10.7. Suppose that the algebraic expression for the z-transform of x[n] is 1 - lz-2 X(z) = 4 . (1 + ±z-2)(1 + iz-1 + ~z-2) How many different regions of convergence could correspond to X(z)? 10.8. Let x[n] be a signal whose rational z-transform X(z) contains a pole at z = 112. Given that x 1 [n] ~ (U x[n] is absolutely summable and = S1 ) fl x2[n] ( x[n] is not absolutely summable, determine whether x[n] is left sided, right sided, or two sided. 10.9. Using partial-fraction expansion and the fact that z 1 anu[n] ~ - az _1 , lzl > lal, 1 find the inverse z-transform of 10.10. Consider the following algebraic expression for the z-transform X(z) of a signal x[n]: 1 + z- 1 X (z) = --+-- =--- 1 lz-1. 3 Chap. 10 Problems 799 (a) Assuming the ROC to be lzl > 1/3, use long division to determine the values of x[O], x[1], and x[2]. (b) Assuming the ROC to be lzl < 113, use long division to determine the values of x[O], x[ -1], and x[ -2]. 10.11. Find the inverse z-transform of 10 1 [ 1' 024 - z- ] X(z) = 1,024 1 - ~z-1 , lzl > 0. 10.12. By considering the geometric interpretation of the magnitude of the Fourier trans- form from the pole-zero plot, determine, for each of the following z-transforms, whether the corresponding signal has an approximately lowpass, bandpass, or highpass characteristic: -1 (a) X(z) = z 8 _ , lzl > ~ 1 + gZ 1 1 + _§_z- 1 (b) X(z) = 9 + lzl > ~ 1 - ~z- 1 ~z-2 ' 9 81 (c) X(z) = 1 _ , I 64 zl > 98 1 + SfZ 2 10.13. Consider the rectangular signal O::sn::s5 x[n] = { ~ otherwise Let g[n] = x[n] - x[n - 1]. (a) Find the signal g[n] and directly evaluate its z-transform. (b) Noting that n x[n] = L g[k], k=-X use Table 10.1 to determine the z-transform of x[n]. 10.14. Consider the triangular signal n-1 2::sn::s7 g[n] = 13- ~. 8 :::; n :::; 12 . { 0, otherwise (a) Determine the value of no such that g[n] = x[n] * x[n - no], where x[n] is the rectangular signal considered in Problem 10.13. (b) Use the convolution and shift properties in conjunction with X(z) found in Problem 10.13 to determine G(z). Verify that your answer satisfies the initial- value theorem. 800 The z-Transform Chap. 10 10.15. Let y[n] = g1 )II ( u[n]. Determine two distinct signals such that each has a z-transform X(z) which satisfies both of the following conditions: 1. [X(z) +X(- z)]/2 = Y(z2). 2. X(z) has only one pole and only one zero in the z-plane. 10.16. Consider the following system functions for stable LTI systems. Without utilizing the inverse z-transform, determine in each case whether or not the corresponding system is causal. 1 4 -1 + I -2 (a) - 3z 2z z- 1(1- 4z- 1)(1- ~z- 1 ) z-! (b) 2 z2 + I z _ 3 2 16 z + 1 (c) z +:!- !z-2 - ~z- 3 3 2 3 10.17. Suppose we are given the following five facts about a particular LTI systemS with impulse response h[n] and z-transform H(z): 1. h[n] is real. 2. h[n] is right sided. 3. limH(z) = 1. z---->oo 4. H(z) has two zeros. 5. H (z) has one of its poles at a nonreallocation on the circle defined by lzl = 3/4. Answer the following two questions: (a) IsS causal? (b) IsS stable? 10.18. Consider a causal LTI system whose input x[n] and output y[n] are related through the block diagram representation shown in Figure Pl0.18. x[n] ~8-----•~l----~·~~ y[n] Figure P1 0.18 (a) Determine a difference equation relating y[n] and x[n]. (b) Is this system stable? 10.19. Determine the unilateral z-transform of each of the following signals, and specify the corresponding regions of convergence: Chap. 10 Problems 801 (a) XJ [n] = Ci)nu[n + 5] (b) x2[n] = S[n + 3] + S[n] + 2nu[ -n] (c) x3[n] = (4)1nl 10.20. Consider a system whose input x[n] and output y[n] are related by y[n - 1] + 2y[n] = x[n]. (a) Determine the zero-input response of this system if y[ -1] = 2. (b) Determine the zero-state response of the system to the input x[ n] = ( 1/4) n u [ n]. (c) Determine the output of the system for n 2: 0 when x[n] = (114)nu[n] and y[ -1] = 2. BASIC PROBLEMS 10.21. Determine the z-transform for each of the following sequences. Sketch the pole- zero plot and indicate the region of convergence. Indicate whether or not the Fourier transform of the sequence exists. (a) S[n + 5] (b) S[n- 5] (c) ( -l)nu[n] (d) C4Y+ 1 u[n + 3] (e) (-~)nu[-n- 2] (0 Ci)nu[3- n] (g) 2nu[ -n] + (i)nu[n- 1] (h) (~)n-2u[n- 2] 10.22. Determine the z-transform for the following sequences. Express all sums in closed form. Sketch the pole-zero plot and indicate the region of convergence. Indicate whether the Fourier transform of the sequence exists. (a) (4)n{u[n + 4]- u[n- 5]} (b) n(4)1nl (c) lnl(4)1nl (d) 4n case; n + *]u[ -n- 1] 10.23. Following are several z-transforms. For each one, determine the inverse z-transform using both the method based on the partial-fraction expansion and the Taylor's se- ries method based on the use of long division. 1- z- 1 1 X(z) = 1-.!.z-2' lzl > 2· 4 1- z- 1 1 X(z) = 1- .!.z-2' lzl < 2· 4 z-1-.!. 2 1 X(z) = 1- .!.z- 1' lzl > 2· 2 z-1-.!. X(z) = 2 1 1- .!.z- 1' lzl < 2· 2 z- 1 -.!. 1 X(z) = (1 - 4z-~)2' lzl > 2. z- 1 -.!. 1 X(z) = (1- 4z-~)2' lzl < 2. 802 The z-Transform Chap. 10 10.24. Using the method indicated, determine the sequence that goes with each of the following z-transforms: (a) Partial fractions: X(z) = + ~z-I + z_ 2, and x[n] is absolutely summable. 1 (b) Long division: 1 - !z- 1 X(z) = 2 and x[n] is right sided. 1 + !z- 1' 2 (c) Partial fractions: 3 X(z) = _ , and x[n] is absolutely summable. z--1 4 - -18 z 1 10.25. Consider a right-sided sequence x[n] with z-transform 1 X(z) = • (P10.25-1) ( 1 - 21 z-1 )(I - z-1) (a) Carry out a partial-fraction expansion of eq. (P10.25-1) expressed as a ratio of polynomials in z- 1, and from this expansion, determine x[n]. (b) Rewrite eq. (P10.25-1) as a ratio of polynomials in z, and carry out a partial- fraction expansion of X(z) expressed in terms of polynomials in z. From this expansion, determine x[n], and demonstrate that the sequence obtained is identical to that obtained in part (a). 10.26. Consider a left-sided sequence x[n] with z-transform 1 X(z) = . ( 1 - ~ z-1 )(1 - z- I) (a) Write X(z) as a ratio of polynomials in z instead of z- 1• (b) Using a partial-fraction expression, express X(z) as a sum of terms, where each term represents a pole from your answer in part (a). (c) Determine x[n]. 10.27. A right-sided sequence x[n] has z-transform 3z- 10 + z-7 - sz-2 + 4z-l + 1 X(z) = Determine x[n] for n < 0. 10.28. (a) Determine the z-transform of the sequence x[n] = B[n] - 0.95 B[n- 6]. (b) Sketch the pole-zero pattern for the sequence in part (a). (c) By considering the behavior of the pole and zero vectors as the unit circle is traversed, develop an approximate sketch of the magnitude of the Fourier transform of x[n]. 10.29. By considering the geometric determination of the frequency response as discussed in Section 10.4, sketch, for each of the pole-zero plots in Figure P10.29, the mag- nitude of the associated Fourier transform. Chap. 10 Problems 803 9m 9m Unit circle CRc (a) (b) 9m 9m Unit circle CRc CRc (c) 9m (d) eRe Circle of radius 0.9 (e) Figure P1 0.29 804 The z-Transform Chap. 10 10.30. Consider a signal y[n] which is related to two signals x 1 [n] and x2 [n] by y[n] = x,[n + 3] * x2 [-n + 1] where x, [n] = (4 )"" u[n] and x2[n] = (~ )"" u[n]. Given that z an u[n] ~ 1 - az-t' lzl > Ia I, use properties of the z-transform to determine the z-transform Y(z) of y[n]. 10.31. We are given the following five facts about a discrete-time signal x[n] with z- transform X(z): 1. x[n] is real and right-sided. 2. X(z) has exactly two poles. 3. X(z) has two zeros at the origin. 4. X(z) has a pole at z = ~ej7T13 • 5. X(l) = ~· Determine X(z) and specify its region of convergence. 10.32. Consider an LTI system with impulse response ~~· n2:0 h[n] = { n<O and input Q:::;n:::;N-1 x[n] = { ~: otherwise (a) Determine the output y[n] by explicitly evaluating the discrete convolution of x[n] and h[n]. (b) Determine the output y[n] by computing the inverse z-transform of the product of the z-transforms of the input and the unit sample response. 10.33. (a) Determine the system function for the causal LTI system with difference equa- tion 1 1 y[n]- 2y[n- 1] + 4y[n- 2] = x[n]. (b) Using z-transforms, determine y[n] if x[n] = (4 )"" u[n]. Chap. 10 Problems 805 10.34. A causal LTI system is described by the difference equation y[n] = y[n- 1] + y[n- 2] + x[n- 1]. (a) Find the system function H(z) = Y(z)/X(z) for this system. Plot the poles and zeros of H(z) and indicate the region of convergence. (b) Find the unit sample response of the system. (c) You should have found the system to be unstable. Find a stable (noncausal) unit sample response that satisfies the difference equation. 10.35. Consider an LTI system with input x[n] and output y[n] for which 5 y[n - 1] - 2y [n] + y[n + 1] = x[n]. The system may or may not be stable or causal. By considering the pole-zero pattern associated with the preceding differ- ence equation, determine three possible choices for the unit sample response of the system. Show that each choice satisfies the difference equation. 10.36. Consider the linear, discrete-time, shift-invariant system with input x[ n] and output y[n] for which 10 y[n- 1] - 3 y[n] + y[n + 1] = x[n]. The system is stable. Determine the unit sample response. 10.37. The input x[n] and output y[n] of a causal LTI system are related through the block-diagram representation shown in Figure Pl0.37. x[n] ----.- + 1-----....,.------~ y[n] Figure P1 0.37 (a) Determine a difference equation relating y[n] and x[n]. (b) Is this system stable? 10.38. Consider a causal LTI systemS with input x[n] and a system function specified as H(z) = H1 (z)H2(z), where 806 The z-Transform Chap. 10 and A block diagram corresponding to H(z) may be obtained as a cascade connection of a block diagram for H 1( z) followed by a block diagram for H2(z). The result is shown in Figure PI 0.38, in which we have also labeled the intermediate signals e1 [n], e2[n], !1 [n], and f2[n]. x[n] y[n] Figure P1 0.38 (a) How is e1 [n] related to / 1 [n]? (b) How is e2[n] related to f2[n]? (c) Using your answers to the previous two parts as a guide, construct a direct- form block diagram for S that contains only two delay elements. (d) Draw a cascade-form block diagram representation for S based on the obser- vation that 1+ 1 H(z) = ( l4 z- )(1- 2z-I) 1 + ~ z- 1 1 - ~ z- 1 • (e) Draw a parallel-form block diagram representation for S based on the obser- vation that 5/3 14/3 H(z) = 4 + ----=--- 1 + lz-I 1- lz-I. 2 4 10.39. Consider the following three system functions corresponding to causal LTI sys- tems: H1 (z) = ---------,-----=------ (1 - z- 1 + ~z- 2)(1 - ~z- 1 + ~z-2 )' 1 H2(z) = -----,------,------- (1- z-I + ~z-2)(1 - ~z-I + z-2)' 1 H3(z) = ---------,------- (1- z-I + ~z-2)(1- z-I + ~z-2). Chap. 10 Problems 807 (a) For each system function, draw a direct-form block diagram. (b) For each system function, draw a block diagram that corresponds to the cas- cade connection of two second-order block diagrams. Each second-order block diagram should be in direct form. (c) For each system function, determine whether there exists a block diagram rep- resentation which is the cascade of four first-order block diagrams with the constraint that all the coefficient multipliers must be real. 10.40. Determine the unilateral z-transform for each of the sequences in Problem 10.21. 10.41. Consider the following two signals: 1 )n+ I XJ [n] = (2 u[n + 1], x2[n] = 1 )n (4 u[n]. Let X 1( z) and X1( z) respectively be the unilateral and bilateral z-transforms of x 1 [n], and let X 2(z) and X2(z) respectively be the unilateral and bilateral z- transforms of x2 [ n]. (a) Take the inverse bilateral z-transform of X1( z)X2(z) to determine g[n] = XJ [n] * x2[n]. (b) Take the inverse unilateral z-transform ofX 1(z)X2(z) to obtain a signal q[n] for n 2: 0. Observe that q[n] and g[n] are not identical for n 2: 0. 10.42. For each of the following difference equations and associated input and initial con- ditions, determine the zero-input and zero-state responses by using the unilateral z-transform: (a) y[n] + 3y[n- 1] = x[n], x[n] = 1 )ll (2 u[n], y[-1] = 1. 1 1 (b) y[n] - 2y [n- 1] = x[n] - 2x[n- 1], x[n] = u[n], y[-1]=0. 1 1 (c) y[n]- 2y[n- 1] = x[n]- 2x[n- 1], x[n] = u[n], y[-1] = 1. ADVANCED PROBLEMS 10.43. Consider an even sequence x[n] (i.e., x[n] = x[ -n]) with rational z-transform X(z). 808 The z-Transform Chap. 10 (a) From the definition of the z-transform, show that X(z) = x(H (b) From your results in part (a), show that if a pole (zero) of X(z) occurs at z = zo, then a pole (zero) must also occur at z = 1/z0 . (c) Verify the result in part (b) for each of the following sequences: (1) o[n + 1] + o[n - 1] (2) o[n + 1] - ~o[n] + o[n- 1] 10.44. Let x[n] be a discrete-time signal with z-transform X(z). For each of the following signals, determine the z-transform in terms of X(z): (a) Llx[n], where Ll is the first-difference operator defined by Llx[n] = x[n] - x[n- 1] (b) XI [n] = { x[n/2], n even 0, n odd (c) x 1 [n] = x[2n] 10.45. Determine which of the following z-transforms could be the transfer function of a discrete-time linear system that is not necessarily stable, but for which the unit sample response is zero for n < 0. State your reasons clearly. (1- z-1)2 (b) (z- 1)2 (a) 1 - .! z- 1 z - .! 2 2 (z- ~)5 (z- ~)6 (c) (d) (z - ~ )6 (z - ~ )5 10.46. A sequence x[n] is the output of an LTI system whose input is s[n]. The system is described by the difference equation x[n] = s[n] - e8a s[n - 8], where 0 < a < 1. (a) Find the system function X(z) H1 (z) = S(z), and plot its poles and zeros in the z-plane. Indicate the region of convergence. (b) We wish to recover s[n] from x[n] with an LTI system. Find the system func- tion H ( ) = Y(z) 2 z X(z) Chap. 10 Problems 809 such that y[n] = s[n]. Find all possible regions of convergence for H2(z), and for each, tell whether or not the system is causal or stable. (c) Find all possible choices for the unit impulse response h2 [n] such that y[n] = h2[n] * x[n] = s[n]. 10.47. The following is known about a discrete-time LTI system with input x[n] and out- put y[n]: 1. If x[n] = ( -2)n for all n, then y[n] = 0 for all n. 2. If x[n] = (112)nu[n] for all n, then y[n] for all n is of the form y[n] = ll[n] +a(~)"" u[n], where a is a constant. (a) Determine the value of the constant a. (b) Determine the response y[n] if the input x[n] is x[n] = 1, for all n. 10.48. Suppose a second-order causal LTI system has been designed with a real impulse response hi [n] and a rational system function HI (z). The pole-zero plot for HI (z) is shown in Figure P10.48(a). Now consider another causal second-order system with impulse response h2 [n] and rational system function H2(z). The pole-zero plot for H2(z) is shown in Figure P10.48(b). Determine a sequence g[n] such that the following three conditions hold: (1) h2[n] = g[n]h 1 [n] (2) g[n] = 0 for n < 0 (3) _Lig[kJI = 3 k=O 1 (fl.e 1 <Re (a) (b) Figure Pl 0.48 810 The z-Transform Chap. 10 10.49. In Property 4 of Section 10.2, it was stated that if x[n] is a right-sided sequence and if the circle lzl = r0 is in the ROC, then all finite values of z for which 1z1 > r0 will also be in the ROC. In this discussion an intuitive explanation was given. A more formal argument parallels closely that used for Property 4 of Section 9 .2, relating to the Laplace transform. Specifically, consider a right-sided sequence x[n] = 0, n < NJ, and for which L, ix[n]jr0n L, jx[n]jr0n < oo. n=-oo n=N1 Then if ro :::; r1, (P10.49-1) where A is a positive constant. (a) Show that eq. (Pl0.49-l) is true, and determine the constant A in terms of r0 , r 1, andN1• (b) From your result in part (a), show that Property 4 of Section 10.2 follows. (c) Develop an argument similar to the foregoing one to demonstrate the validity of Property 5 of Section 10 .2. 10.50. A discrete-time system with the pole-zero pattern shown in Figure Pl0.50(a) is referred to as a first-order all-pass system, since the magnitude of the frequency response is constant regardless of frequency. (a) Demonstrate algebraically that jH(ejw)i is constant. To demonstrate the same property geometrically, consider the vector dia- gram in Figure P10.50(b). We wish to show that the length ofv2 is proportional to the length of v1 independently of the frequency w. Unit circle ROC: izl>a (a) Figure P1 O.SOa Chap. 10 Problems 811 (b) Figure P1 O.SOb (b) Express the length of v 1 using the law of cosines and the fact that v 1 is one leg of a triangle for which the other two legs are the unit vector and a vector of . length a. (c) In a manner similar to that in part (b), determine the length of v2 and show that it is proportional in length to v1 independently of w. 10.51. Consider a real-valued sequence x[n] with rational z-transform X(z). (a) From the definition of the z-transform, show that X(z) = X*(z*). (b) From your result in part (a), show that if a pole (zero) of X(z) occurs at z = z0 , then a pole (zero) must also occur at z = z~. (c) Verify the result in part (b) for each of the following sequences: (1) x[n] = (4)nu[n] (2) x[n] = o[n] - 4o[n- 1] + io[n- 2] (d) By combining your results in part (b) with the result of Problem 10.43(b), show that for a real, even sequence, if there is a pole (zero) of H (z) at z = peF1, then there is also a pole (zero) of H(z) at z = (11 p)ei8 and at z = (11 p)e- i 8 . 10.52. Consider a sequence xdn] with z-transform X1( z) and a sequence x2 [n] with z- transform X2(z), where Show that X2(z) = X1 (11z), and from this, show that if X1 (z) has a pole (or zero) at z = zo, then X2(z) has a pole (or zero) at z = 1/zo. 10.53. (a) Carry out the proof for each of the following properties in Table 10.1: (1) Property set forth in Section 10.5.2 (2) Property set forth in Section 10.5.3 (3) Property set forth in Section 10.5.4 812 The z-Transform Chap. 10 (b) WithX(z) denoting the z-trarisform of x[n] and Rx the ROC of X(z), determine, in terms of X(z) and Rx. the z-transform and associated ROC for each of the following sequences: (1) x*[n] (2), z0x[n], where zo is a complex number 10.54. In Section 10.5.9, we stated and proved the initial-value theorem for causal se- quences. (a) State and prove the corresponding theorem if x[n] is anticausal (i.e., if x[n] = 0, n > 0). (b) Show that if x[n] = 0, n < 0, then x[1] = lim z(X(z) - x[O]). z~oo 10.55. Let x[n] denote a causal sequence (i.e., if x[n] = 0, n < 0) for which x[O] is nonzero and finite. (a) Using the initial-value theorem, show that there are no poles or zeros of X(z) at z = oo. (b) Show that, as a consequence of your result in part (a), the number of poles of X(z) in the finite z-plane equals the number of zeros of X(z) in the finite z-plane. (The finite z-plane excludes z = oo.) 10.56. In Section 10.5.7, we stated the convolution property for the z-transform. To show that this property holds, we begin with the convolution sum expressed as 00 X3[n] = XI [n] * X2[n] = ~ XI [k]x2[n- k]. (P10.56-l) k= -00 (a) By taking the z-transform of eq. (P10.56-1) and using eq. (10.3), show that X3(Z) = ~ XI [k]X2(Z), k= -00 where X2(z) is the transform of x2 [n - k]. (b) Using your result in part (a) and property 10.5.2 in Table 10.1, show that 00 X3(z) = X2(z) ~ xi [k]z-k. k= -00 (c) From part (b), show that as stated in eq. (10.81). 10.57. Let XI(z) = xi[O] + xi[1]z-I + · · · + xi[NJ]z-N', X2(z) = x2[0] + x2[l]z-I + · · · + x2[N2]z-N2 • Define Chap. 10 Problems 813 and let M f(z) = L, y[k]z-k. k=O (a) Express Min terms of N1 and N2. (b) Use polynomial multiplication to determine y[O], y[l], and y[2]. (c) Use polynomial multiplication to show that, for 0 :::; k :::; M, y[k] = L, XI [m]x2[k- m]. m= -oo 10.58. A minimum-phase system is a system that is causal and stable and for which the inverse system is also causal and stable. Determine the necessary constraints on the location in the z-plane of the poles and zeros of the system function of a minimum- phase system. 10.59. Consider the digital filter structure shown in Figure P10.59. k k 3 4 Figure P1 0.59 (a) Find H(z) for this causal filter. Plot the pole-zero pattern and indicate there- gion of convergence. (b) For what values of the k is the system stable? (c) Determine y[n] if k = 1 and x[n] = (2/3)n for all n. 10.60. Consider a signal x[n] whose unilateral z-transform is ~(z). Show that the unilat- eral z-transform of y[n] = x[n + 1] may be specified as 'Y(z) = z~(z) - zx[O]. 10.61. If~(z) denotes the unilateral z-transform of x[n], determine, in terms of~(z), the unilateral z-transform of: (a) x[n + 3] (b) x[n - 3] (c) L ~= _ 00 x[k] EXTENSION PROBLEMS 10.62. The autocorrelation sequence of a sequence x[n] is defined as cf>xAn] = L, x[k]x[n + k]. k= -oo Determine the z-transform of cf>xx[n] in terms of the z-transform of x[n]. 814 The z-Transform Chap. 10 10.63. By using the power-series expansion oo wi log(l - w) = - L --:-, lwl < 1, i = 1 l determine the inverse of each of the following two z-transforms: (a) X(z) = log(l - 2z), lzl < 1 (b) X(z) = log(l -1z- 1 ), lzl > 1 10.64. By first differentiating X(z) and using the appropriate properties of the z-transform, determine the sequence for which the z-transform is each of the following: (a) X(z) = log(l - 2z), lzl < 1 (b) X(z) = log(l-1z- 1), lzl > 1 Compare your results for (a) and (b) with the results obtained in Problem 10.63, in which the power-series expansion was used. 10.65. The bilinear transformation is a mapping for obtaining a rational z-transform Hd(Z) from a rational Laplace transform Hc(s). This mapping has two important proper- ties: 1. If Hc(s) is the Laplace transform of a causal and stable LTI system, then Hd(Z) is the z-transform of a causal and stable LTI system. 2. Certain important characteristics of iHc(jw )I are preserved in iHd(eiw)i. In this problem, we illustrate the second of these properties for the case of all-pass filters. (a) Let a-s Hc(s) = --, s+a where a is real and positive. Show that (b) Let us now apply the bilinear transformation to Hc(s) in order to obtain Hd(Z). That is, Show that Hd(Z) has one pole (which is inside the unit circle) and one zero (which is outside the unit circle). (c) For the system function Hd(Z) derived in part (b), show that iHd(eiw)i = 1. 10.66. The bilinear transformation, introduced in the previous problem, may also be used to obtain a discrete-time filter, the magnitude of whose frequency response is sim- ilar to the magnitude of the frequency response of a given continuous-time low- pass filter. In this problem, we illustrate the similarity through the example of a continuous-time second-order Butterworth filter with system function Hc(s). Chap. 10 Problems 815 (a) Let Hd(Z) = Hc(s)Js= 1-z-1 • l+z-1 Show that (b) Given that 1 Hc(s) = (s + e/rr1 4 )(s + e- j1r 14 ) and that the corresponding filter is causal, verify that Hc(O) = 1, that IHc(Jw )J decreases monotonically with increasing positive values of w, that IHc(j)J2 = 112 (i.e., that We = 1 is the half-power frequency), and that Hc(oo) = 0. (c) Show that if the bilinear transformation is applied to Hc(s) of part (b) in order to obtain Hd(Z), then the following may be asserted about Hd(Z) and Hd(ejw): 1. Hd(Z) has only two poles, both of which are inside the unit circle. 2. Hd(ej0 ) = 1. 3. IHd(ejw)l decreases monotonically as w goes from 0 to TT. 4. The half-power frequency of Hd(ejw) is TT/2. 1 1 lJNEARFEEDBACKSYSTEMS 11.0 INTRODUCTION It has long been recognized that in many situations there are particular advantages to be gained by using feedback-that is, by using the output of a system to control or modify the input. For example, it is common in electromechanical systems, such as a motor whose shaft position is to be maintained at a constant angle, to measure the error between the desired and the true position and to use this error in the form of a signal to tum the shaft in the appropriate direction. This is illustrated in Figure 11.1, where we have depicted the use of a de motor for the accurate pointing of a telescope. In Figure 11.1 (a) we have indicated pictorially what such a system would look like, where v(t) is the input voltage to the motor and 8(t) is the angular position of the telescope platform. The block diagram for the motor-driven pointing system is shown in Figure 11.1 (b). A feedback system for controlling the position of the telescope is illustrated in Figure 11.1 (c), and a block diagram equivalent to this system is shown in Figure 11.1(d). The external, or reference, input to this feedback system is the desired shaft angle 8 D· A potentiometer is used to convert the angle into a voltage K1 8 D proportional to 8 D· Similarly, a second potentiometer produces a voltage K18 (t) proportional to the actual platform angle. These two voltages are compared, producing an error voltage K1 (8 D - 8(t)), which is amplified and then used to drive the electric motor. Figure 11.1 suggests two different methods for pointing the telescope. One of these is the feedback system of Figures 11.1(c) and (d). Here, the input that we must provide is the desired reference angle 8 D· Alternatively, if the initial angle, the desired angle, and the detailed electrical and mechanical characteristics of the motor-shaft assembly were known exactly, we could specify the precise history of the input voltage v(t) that would first accelerate and then decelerate the shaft, bringing the platform to a stop at the desired 816 v(t) ..__ ______ ___. Cl:) 8(t) I (a) v(t)----~1 Motor 1---.........;~ 8(t) Input Platform voltage angular position (b) K1[8 0 - e (t)] So - Potentiometer Comparator Amplifier (gain K2) Potentiometer (c) 9o~v K H Motor • 8(t) I (d) Figure 11. 1 Use of feedback to control the, angular position of a telescope: (a) de motor-driven telescope platform; (b) block diagram of the system in (a); (c) feedback system for pointing the telescope; (d) block diagram of the system in (c) (here, K = K1K2). 817 818 Linear Feedback Systems Chap. 11 position without the use of feedback, as in Figures 11.1 (a) and (b). A system operating in accordance with Figures 11.1 (a) and (b) is typically referred to as an open-loop system, in contrast to the closed-loop system of Figures ll.l(c) and (d). In a practical environ- ment, there are clear advantages to controlling the motor-shaft angle with the closed-loop system rather than with the open-loop system. For example, in the closed-loop system, when the shaft has been rotated to the correct position, any disturbance from this position will be sensed, and the resulting error will be used to provide a correction. In the open- loop system, there is no mechanism for providing a correction. As another advantage of the closed-loop system, consider the effect of errors in modeling the characteristics of the motor-shaft assembly. In the open -loop system, a precise characterization of the system is required to design the correct input. In the closed-loop system, the input is simply the de- sired shaft angle and does not require precise knowledge of the system. This insensitivity of the closed-loop system to disturbances and to imprecise knowledge of the system are two important advantages of feedback. The control of an electric motor is just one of a great many examples in which feed- back plays an important role. Similar uses of feedback can be found in a wide variety of applications, such as chemical process control, automotive fuel systems, household heating systems, and aerospace systems, to name just a few. In addition, feedback is also present in many biological processes and in the control of human motion. For example, when a person reaches for an object, it is usual during the reaching process to monitor visually the distance between the hand and the object so that the velocity of the hand can be smoothly decreased as the distance (i.e., the error) between the hand and the object decreases. The effectiveness of using the system output (hand position) to control the input is clearly demonstrated by alternatively reaching with and without the use of visual feedback. In addition to its use in providing an error-correcting mechanism that can reduce sen- sitivity to disturbances and to errors in the modeling of the system that is to be controlled, another important characteristic of feedback is its potential for stabilizing a system that is inherently unstable. Consider the problem of trying to balance a broomstick in the palm of the hand. If the hand is held stationary, small disturbances (such as a slight breeze or inadvertent motion of the hand) will cause the broom to fall over. Of course, if one knows exactly what disturbances will occur, and if one can control the motion of the hand per- fectly, it is possible to determine in advance how to move the hand to balance the broom. This is clearly unrealistic; however, by always moving the hand in the direction in which the broom is falling, the broom can be balanced. This, of course, requires feedback in order to sense the direction in which the broom is falling. A second example that is closely related to the balancing of a broom is the problem of controlling a so-called inverted pen- dulum, which is illustrated in Figure 11.2. As shown, an inverted pendulum consists of a thin rod with a weight at the top. The bottom of the rod is mounted on a cart that can move in either direction along a track. Again, if the cart is kept stationary, the inverted pendulum Figure 11 .2 An inverted pendulum. Sec. 11.1 Linear Feedback Systems 819 will topple over. The problem of stabilizing the pendulum is one of designing a feedback system that will move the cart to keep the pendulum vertical. This example is examined in Problem 11.56. A third example, which again bears some similarity to the balancing of a broom, is the problem of controlling the trajectory of a rocket. In this case, much as the movement of the hand is used to compensate for disturbances in the position of the broom, the direction of the thrust of the rocket is used to correct for changes in aerodynamic forces and wind disturbances that would otherwise cause the rocket to deviate from its course. Again, feedback is important, because these forces and disturbances are never precisely known in advance. The preceding examples provide some indication of why feedback may be useful. In the next two sections we introduce the basic block diagrams and equations for linear feedback systems and discuss in more detail a number of applications of feedback and control, both in continuous time and in discrete time. We also point out how feedback can have harmful as well as useful effects. These examples of the uses and effects of feedback will give us some insight into how changes in the parameters in a feedback control system lead to changes in the behavior of the system. Understanding this relationship is essential in designing feedback systems that have certain desirable characteristics. With this material as background, we will then develop, in the remaining sections of the chapter, several specific techniques that are of significant value in the analysis and design of continuous- time and discrete-time feedback systems. 11. 1 LINEAR FEEDBACK SYSTEMS The general configuration of a continuous-time LTI feedback system is shown in Fig- ure 11.3(a) and that of a discrete-time LTI feedback system in Figure 11.3(b ). Because of x(t) ~~e(t) ----+. .-~ H(s) y(t) r(t) G(s) (a) x[n]e[n] + ~ H(z) y[n] r[n] G(z) Figure 11.3 Basic feedback system configurations in (a) continuous time (b) and (b) discrete time. 820 Linear Feedback Systems Chap. 11 the typical applications in which feedback is utilized, it is natural to restrict the systems in these figures to be causal. This will be our assumption throughout the chapter. In that case, the system functions in Figure 11.3 can be interpreted either as unilateral or as bilateral transforms, and, as a consequence of causality, the ROC's associated with them will always be to the right of the rightmost pole for Laplace transforms and outside the outermost pole for z-transforms. It should also be noted that the convention used in Figure 11.3(a) is that r(t), the signal fed back, is subtracted from the input x(t) to form e(t). The identical convention is adopted in discrete time. Historically, this convention arose in tracking-system applica- tions, where x(t) represented a desired command and e(t) represented the error between the command and the actual response r(t). This was the case, for example, in our discus- sion of the pointing of a telescope. In more general feedback systems, e(t) and e[n], the disCrete-time counterpart of e(t), may not correspond to or be directly interpretable as error signals. The system function H(s) in Figure 11.3(a) or H(z) in Figure 11.3(b) is referred to as the system function of the forward path and G(s) or G(z) as the system function of the feedback path. The system function of the overall system of Figure 11.3(a) or (b) is referred to as the closed-loop system function and will be denoted by Q(s) or Q(z). In Sections 9.8.1 and 10 .8.1, we derived expressions for the system functions of feedback interconnections of LTI systems. Applying these results to the feedback systems of Figure 11.3, we obtain Q(s) = _Y(_s) = __H _(_s_) _ (11.1) X(s) 1 + G(s)H(s)' Q(z) = Y(z) = H(z) (11.2) X(z) 1 + G(z)H(z) · Equations ( 11.1) and ( 11.2) represent the fundamental equations for the study of LTI feed- back systems. In the following sections, we use these equations as the basis for gaining insight into the properties of feedback systems and for developing several tools for their analysis. 11.2 SOME APPLICATIONS AND CONSEQUENCES OF FEEDBACK In the introduction, we provided a brief, intuitive look at some of the properties and uses of feedback systems. In this section, we examine a number of the characteristics and ap- plications of feedback in somewhat more quantitative terms, using the basic feedback equations (11.1) and (11.2) as a starting point. Our purpose is to provide an introduction to and an appreciation for the applications of feedback, rather than to develop any of these applications in detail. In the sections that follow, we focus in more depth on several specific techniques for analyzing feedback systems that are useful in a wide range of problems, including many of the applications that we are about to describe. 11.2.1 Inverse System Design In some applications, one would like to synthesize the inverse of a given continuous-time system. Suppose that this system has system function P(s), and consider the feedback system shown in Figure 11.4. Applying equation ( 11.1) with H (s) = K and G(s) = P(s ), Sec. 11.2 Some Applications and Consequences of Feedback 821 + K y(t) Figure 11 .4 Form of a feedback system used in implementing the in- P(s) verse of the system with system func- tion P(s). we find that the closed-loop system function is K Q(s) = 1 + KP(s) (11.3) If the gain K is sufficiently large so that K P(s) >> 1, then 1 Q(s) = P(s)' (11.4) in which case the feedback system approximates the inverse of the system with system function P(s). It is important to note that the result in eq. (11.4) requires that the gain K be suffi- ciently high, but is otherwise not dependent on the precise value of the gain. Operational amplifiers are one class of devices that provide this kind of gain and are widely used in feedback systems. One common application of the inversion inherent in eq. ( 11.4) is in the implementation of integrators. A capacitor has the property that its current is proportional to the derivative of the voltage. By inserting a capacitor in the feedback path around an operational amplifier, the differentiation property of the capacitor is inverted to provide integration. This specific application is explored in more detail in Problems 11.50-11.52. Although our discussion is for the most part restricted to linear systems, it is worth pointing out that this same basic approach is commonly used in inverting a nonlinearity. For example, systems for which the output is the logarithm of the input are commonly im- plemented by utilizing the exponential current-voltage characteristics of a diode as feed- back around an operational amplifier. This is explored in more detail in Problem 11.53. 11 .2.2 Compensation for Nonideal Elements Another common use of feedback is to correct for some of the nonideal properties of the open -loop system. For example, feedback is often used in the design of amplifiers to pro- vide constant-gain amplification in a given frequency band, and in fact, it is this applica- tion, pioneered by H. S. Black at Bell Telephone Laboratories in the 1920s, that is generally considered to have been the catalyst for the development of feedback control as a practical and useful system design methodology. Specifically, consider an open-loop frequency response H(jw) which provides am- plification over the specified frequency band, but which is not constant over that range. For example, operational amplifiers or the vacuum tube amplifiers of concern to Black and his colleagues typically provide considerable, but not precisely controlled, amplification. 822 Linear Feedback Systems Chap. 11 While such devices can provide raw amplification levels of several orders of magnitude, the price one pays for this includes uncertain levels of amplification that can fluctuate with frequency, time, temperature, etc., and that can also introduce unwanted phase and nonlinear distortions. What Black proposed was placing such a powerful, but uncertain and erratic, amplifier in a feedback loop as in Figure 11.3(a) with G(s) chosen to be constant, i.e., G(s) = K. In this case, assuming the closed-loop system is stable, its frequency response is H(jw) Q(jw) = 1 + KH(jw )' (11.5) If, over the specified frequency range, IKH(jw)l >> 1, (11.6) then Q(jw) = ~· (11.7) That is, the closed-loop frequency response is constant, as desired. This of course assumes that the system in the feedback path can be designed so that its frequency response G(jw) has a constant gain Kover the desired frequency band, which is precisely what we assumed we could not ensure for H (jw ). The difference between the requirement on H (jw) and that on G(jw ), however, is that H(jw) must provide amplification, whereas, from eq. (11.7), we see that for the overall closed-loop system to provide a gain greater than unity, K must be less than 1. That is, G(jw) must be an attenuator over the specified range of frequencies. In general, an attenuator with approximately flat frequency characteristics is considerably easier to realize than an amplifier with approximately flat frequency response (since an attenuator can be constructed from passive elements). The use of feedback to flatten the frequency response incurs some cost, however, and it is this fact that led to the considerable skepticism with which Black's idea was met. In particular, from eqs. (11.6) and (11.7), we see that IH(jw)l >> ~ = Q(jw), (11.8) so that the closed-loop gain l!K will be substantially less than the open-loop gain iH(jw)i. This apparently significant loss of gain, attributable to what Black referred to as degen- erative or negative feedback, was initially viewed as a serious weakness in his negative- feedback amplifier. Indeed, the effect had been known for many years and had led to the conviction that negative feedback was not a particularly useful mechanism. However, Black pointed out that what one gave up in overall gain was often more than offset by the reduced sensitivity of the overall closed-loop amplifier: The closed-loop system function is essentially equal to eq. (11.7), independently of variations in H(jw), as long as IH(jw)l is large enough. Thus, if the open-loop amplifier is initially designed with considerably more gain than is actually needed, the closed-loop amplifier will provide the desired lev- els of amplification with greatly reduced sensitivity. This concept and its application to extending the bandwidth of an amplifier are explored in Problem 11.49. Sec. 11.2 Some Applications and Consequences of Feedback 823 11.2.3 Stabilization of Unstable Systems As mentioned in the introduction, one use of feedback systems is to stabilize systems that, without feedback, are unstable. Examples of this kind of application include the control of the trajectory of a rocket, the regulation of nuclear reactions in a nuclear power plant, the stabilization of an aircraft, and the natural and regulatory control of animal populations. To illustrate how feedback can be used to stabilize an unstable system, let us consider a simple first-order continuous-time system with b H(s) = --. (11.9) s-a With a > 0, the system is unstable. Choosing the system function G(s) to be a constant gain K, we see that the closed-loop system function in eq. (11.1) becomes H(s) Q(s) = 1 + KH(s) b (11.10) s-a+Kb. The closed-loop system will be stable if the pole is moved into the left half of the s-plane. This will be the case if Kb>a. (11.11) Thus, we can stabilize the system with a constant gain in the feedback loop if that gain is chosen to satisfy eq. ( 11.11 ). This type of feedback system is referred to as a proportional feedback system, since the signal that is fed back is proportional to the output of the system. As another example, consider the second-order system b H(s) = --. (11.12) s2 +a If a > 0, the system is an oscillator (i.e., H(s) has its poles on the jw-axis), and the impulse response of the system is sinusoidal. If a < 0, H(s) has one pole in the left-half plane and one in the right-half plane. Thus, in either case, the system is unstable. In fact, as considered in Problem 11.56, the system function given in eq. (11.12) with a < 0 can be used to model the dynamics of the inverted pendulum described in the introduction. Let us first consider the use of proportional feedback for this second-order system; that is, we take G(s) = K. (11.13) Substituting into eq. ( 11.1 ), we obtain b Q(s) = s2 +(a+ Kb). (11.14) 824 Linear Feedback Systems Chap. 11 In our discussion of second-order systems in Chapter 6, we considered a transfer function of the form (11.15) For such a system to be stable, wn must be real and positive (i.e., w~ > 0), and C must be positive (corresponding to positive damping). From eqs. (11.14) and (11.15), it follows that with proportional feedback we can only influence the value of w~, and consequently, we cannot stabilize the system because we cannot introduce any damping. To suggest a type of feedback that can be used to stabilize this system, recall the mass-spring-dashpot mechanical system described in our examination of second-order systems in Section 6.5.2. We saw that damping in that system was the result of the inclusion of a dashpot, which provided a restoring force proportional to the velocity of the mass. This suggests that we consider proportional-plus-derivative feedback, that is, a G(s) of the form (11.16) which yields b (11.17) Q(s) = s2 + bK2s +(a+ Ktb)"" The closed-loop poles will be in the left-half plane, and hence, the closed-loop system will be stable as long as we choose K 1 and K2 to guarantee that (11.18) The preceding discussion illustrates how feedback can be used to stabilize con tin- uous-time systems. The stabilization of unstable systems is an important application of feedback for discrete-time systems as well. Examples of discrete-time systems that are unstable in the absence of feedback are models of population growth. To illustrate how feedback can prevent the unimpeded growth of populations, let us consider a simple model for the evolution of the population of a single species of animal. Let y[n] denote the number of animals in the nth generation, and assume that without the presence of any impeding influences, the birthrate is such that the population would double each generation. In this case, the basic equation for the population dynamics of the species is y[n] = 2y[n- 1] + e[n], (11.19) where e[n] represents any additions to or deletions from the population that are caused by external influences. This population model is obviously unstable, with an impulse response that grows exponentially. However, in any ecological system, there are a number of factors that will inhibit the growth of a population. For example, limits on the food supply for the species will manifest themselves through a reduction in population growth when the number of animals becomes large. Similarly, if the species has natural enemies, it is often reasonable to assume that the population of the predators will grow when the population of the prey increases and, consequently, that the presence of natural enemies will retard population growth. In addition to natural influences such as these, there may be effects introduced by Sec. 11.2 Some Applications and Consequences of Feedback 825 humans that are aimed at population control. For example, the food supply or the predator population may fall under human regulation. In addition, stocking lakes with fish or im- porting animals from other areas can be used to promote growth, and the control of hunting or fishing can also provide a regulative effect. Because all of these influences depend on the size of the population (either naturally or by design), they represent feedback effects. Based on the preceding discussion, we can separate e[n] into two parts by means of the equation e[n] = x[n] - r[n], (11.20) where r[n] represents the effect of the regulative influences described in the previous para- graph and x[ n] incorporates any other external effects, such as the migration of animals or natural disasters or disease. Note that we have included a minus sign in eq. (11.20). This is consistent with our convention of using negative feedback, and here it also has the physical interpretation that, since the uninhibited growth of the population is unstable, the feedback term plays the role of a retarding influence. To see how the population can be controlled by the presence of this feedback term, suppose that the regulative influences ac- count for the depletion of a fixed proportion f3 of the population in each generation. Since, according to our model, the surviving fraction of each generation will double in size, it follows that y[n] = 2(1 - f3)y[n - 1] + x[n]. (11.21) Comparing eq. (11.21) with eqs. (11.19) and (11.20), we see that r[n] = 2{3 y[n - 1]. (11.22) The factor of 2 here represents the fact that the depletion of the present population de- creases the number of births in the next generation. This example of the use of feedback is illustrated in Figure 11.5. Here, the system function of the forward path is obtained from eq. (11.19) as 1 H(z) = 1 - 2z-I, (11.23) while from eq. ( 11.22) the system function of the feedback path is G(z) = 2{3z- 1 • (11.24) + e[n] x[n] 1 + y[n] 1 -2z- 1 - 1 Figure 11.5 Block diagram of a 2~z- 2~y[n-1] simple feedback model of population dynamics. 826 Linear Feedback Systems Chap. 11 Consequently, the closed-loop system function is H(z) Q(z) = 1 + G(z)H(z) (11.25) 1- 2(1 - {3)z- 1 • If f3 < 1/2, the closed-loop system is still unstable, whereas it is stable1 if 112 < f3 < 3/2. Clearly, this example of population growth and control is extremely simplified. For instance, the feedback model of eq. (11.22) does not account for the fact that the part of r[ n] which is due to the presence of natural enemies depends upon the population of the predators, which in tum has its own growth dynamics. Such effects can be incorporated by making the feedback model more complex to reflect the presence of other dynamics in an ecological system, and the resulting models for the evolution of interacting species are extremely important in ecological studies. However, even without the incorporation of these effects, the simple model that we have described here does illustrate the basic ideas of how feedback can prevent both the unlimited proliferation of a species and its extinction. In particular, we can see at an elementary level how human-induced factors can be used. For example, if a natural disaster or an increase in the population of natural enemies causes a drastic decrease in the population of a species, a tightening of limits on hunting or fishing and accelerated efforts to increase the population can be used to decrease f3 in order to destabilize the system to allow for rapid growth, until a normal-size population is again attained. Note also that for this type of problem, it is not usually the case that one wants strict stability. If the regulating influences are such that f3 = 112, and if all other external influences are zero (i.e., if x[n] = 0), then y[n] = y[n - 1]. Therefore, as long as x[n] is small and averages to zero over several generations, a value of f3 = 1/2 will result in an essentially constant population. However, for this value of f3 the system is unstable, since eq. ( 11.21) then reduces to y[n] = y[n - 1] + x[n]. (11.26) That is, the system is equivalent to an accumulator. Thus, if x[n] is a unit step, the output grows without bound. Consequently, if a steady trend is expected in x[n], caused, for example, by a migration of animals into a region, a value of f3 > 1/2 would need to be used to stabilize the system and thus to keep the population within bounds and maintain an ecological balance. 11.2.4 Sampled-Data Feedback Systems In addition to dealing with problems such as the one just described, discrete-time feedback techniques are of great importance in a wide variety of applications involving continuous- time systems. The flexibility of digital systems has made the implementation of sampled- data feedback systems an extremely attractive option. In such a system, the output of a continuous-time system is sampled, some processing is done on the resulting sequence of samples, and a discrete sequence of feedback commands is generated. This sequence 1A lthough, in the context of our population example, f3 could never exceed unity, since f3 > 1 corre- sponds to removing more than 100% of the population. Sec. 11.2 Some Applications and Consequences of Feedback 827 is then converted to a continuous-time signal that is fed back to and subtracted from the external input to produce the actual input to the continuous-time system. Clearly, the constraint of causality on feedback systems imposes a restriction on the process of converting the discrete-time feedback signal to a continuous-time signal (e.g., ideal lowpass filtering or any noncausal approximation of it is not allowed). One of the most widely used conversion systems is the zero-order hold (introduced in Section 7 .1.2). The structure of a sampled-data feedback system involving a zero-order hold is depicted in Figure 11.6(a). In the figure, we have a continuous-time LTI system with system function H(s) that is sampled to produce a discrete-time sequence p[n] = y(nT). (11.27) The sequence p[n] is then processed by a discrete-time LTI system with system function G(z), and the resulting output is put through a zero-order hold to produce the continuous-time signal z(t) = d[n] for nT :::; t < (n + l)T. (11.28) This signal is subtracted from the external input x(t) to produce e(t). -----~+~e(t~) + x(t) H(s) y(t) z(t) Zero-order Ideal C/D hold p[n] G(z) (a) F(z) I + e[n]: Z d r[n] --~ + 1---...1. ~ er~~f~ er ----. H(s) f---+ Ideal C/D 1---:---...----t~ p[n] I I ~--------------------------------- G(z) (b) Figure 11.6 (a) A sampled-data feedback system using a zero-order hold; (b) equivalent discrete-time system. 828 Linear Feedback Systems Chap. 11 Suppose also that x(t) is constant over intervals of length T. That is, x(t) = r[n] for nT ::; t < (n + l)T, (11.29) where r[n] is a discrete-time sequence. This is an approximation that is usually valid in practice, as the sampling rate is typically fast enough so that x(t) does not change appre- ciably over intervals of length T. Furthermore, in many applications, the external input is itself actually generated by applying a zero-order hold operation to a discrete sequence. For example, in systems such as advanced aircraft, the external inputs represent human operator commands that are themselves first processed digitally and then converted back to continuous-time input signals. Because the zero-order hold is a linear operation, the feedback system of Figure 11.6(a) when x(t) is given by eq. (11.29) is equivalent to the system of Figure 11.6(b ). As shown in Problem 11.60, the discrete-time system with input e[n] and output p[n] is an LTI system with system function F(z) that is related to the continuous-time system function H(s) by means of a step-invariant transformation. That is, if s(t) is the step response of the continuous-time system, then the step response q[n] of the discrete- time system consists of equally spaced samples of s(t). Mathematically, q[n] = s(nT) for all n. (11.30) Once we have determined F(z), we have a completely discrete-time feedback system model (Figure 11.6(b)) exactly capturing the behavior of the continuous-time feedback system (Figure 11.6(a)) at the sampling instants t = nT, and we can then consider de- signing the feedback system function G(z) to achieve our desired objectives. An example of designing such a sampled-data feedback system to stabilize an unstable continuous-time system is examined in detail in Problem 11.60. 11.2.5 Tracking Systems As mentioned in Section 11.0, one of the important applications of feedback is in the design of systems in which the objective is to have the output track or follow the input. There is a broad range of problems in which tracking is an important component. For example, the telescope-pointing problem discussed in Section 11.0 is a tracking problem: The fe~dback system of Figures 11.1 (c) and (d) has as its input the desired pointing angle, and the purppse of the feedback loop is to provide a mechanism for driving the telescope to follow' the input. In airplane autopilots the input is the desired flight path of the vehicle, and the autopilot feedback system uses the aircraft control surfaces (rudder, ailerons, and elevator) and thrust control in order to keep the aircraft on the prescribed course. To illtH;trate some of the issues that arise in the design of tracking systems, con- sider the discrete-time feedback system depicted in Figure 11.7(a). The examination of discrete-time tracking systems of this form often arises in analyzing the characteristics of sampled-data tracking systems for continuous-time applications. One example of such a sy~tem is a digital autopilot. In Figure 11.7(a), H p(Z) denotes the system function of the systerp. whose output is to be controlled. This system is often referred to as the plant, a term that can be traced to applications such as the control of power plant~, heating systems, and chemical-processing plants. The system function l[c(z) represents a compensator, which is the element to be designed. Here, the input to the compensator is the tracking error- Sec. 11.2 Some Applications and Consequences of Feedback 829 (a) x[n] 1---....--~y[n] Figure 11.7 (a) Discrete-time tracking system; (b) tracking sys- d[n] tern of (a) with a disturbance d[n] in the feedback path accounting for the (b) presence of measurement errors. that is, the difference e[n] between the input x[n] and the output y[n]. The output of the compensator is the input to the plant (for example, the actual voltage applied to the motor in the feedback system of Figures 11.1 (c) and (d) or the actual physical input to the drive system of the rudder of an aircraft). To simplify notation, let H(z) = Hc(z)Hp (Z). In this case, the application of eq. ( 11.2) yields the relationship H(z) Y(z) = 1 + H(z)X(z). (11.31) Also, since Y(z) = H(z)E(z), it follows that 1 E(z) = 1 + H(z) X(z), 0 1.32) or, specializing to z = eiw, we obtain jw) 1 X( jw) E(e· = 1 + H(eJw) e . (11.33) Equation ( 11.33) provides us with some insight into the design of tracking systems. Specif- ically, for good tracking performance, we would like e[n] or, equivalently, E(eiw) to be small. That is, 1 X jw) ~ 0 1 + H(eiw) (e - · (11.34) Consequently, for that range of frequencies for which X(eiw) is nonzero, we would like IH (e Jw )I to be large. Thus, we have one of the fundamental principles of feedback system design: Good tracking performance requires a large gain. This desire for a large gain, however, must typically be tempered, for several reasons. One reason is that if the gain is too large, the closed-loop system may have undesirable characteristics (such as too little 830 Linear Feedback Systems Chap. 11 damping) or might in fact become unstable. This possibility is discussed in the next section and is also addressed by the methods developed in subsequent sections. In addition to the issue of stability, there are other reasons for wanting to limit the gain in a tracking system. For example, in implementing such a system, we must measure the output y[n] in order to compare it to the command input x[n], and any measuring device used will have inaccuracies and error sources (such as thermal noise in the electronics of the device). In Figure 11.7(b), we have included these error sources in the form of a disturbance input d[n] in the feedback loop. Some simple system function algebra yields the following relationship between Y(z) and the transforms X(z) and D(z) of x[n] and d[n]: H(z) ] [ H(z) ] Y(z) = [ 1 + H(z) X(z) - 1 + H(z) D(z) . (11.35) From this expression, we see that in order to minimize the influence of d[n] on y[n], we would like H(z) to be small so that the second term on the right-hand side of eq. (11.35) is small. From the preceding development, we see that the goals of tracking and of minimiz- ing the effect of measurement errors are conflicting, and one must take this into account in coming up with an acceptable system design. In general, the design depends on more de- tailed information concerning the characteristics of the input x[n] and the disturbance d[n]. For example, in many applications x[n] has a significant amount of its energy concentrated at low frequencies, while measurement error sources such as thermal noise have a great deal of energy at high frequencies. Consequently, one usually designs the compensator Hc(Z) so that IH(ejw)l is large at low frequencies and is small for w near ±7T. There are a variety of other issues that one must consider in designing tracking sys- tems, such as the presence of disturbances at other points in the feedback loop. (For exam- ple, the effect of wind on the motion of an aircraft must be taken into account in designing an autopilot.) The methods of feedback system analysis introduced in this chapter provide the necessary tools for examining each of these issues. In Problem 11.57, we use some of these tools to investigate several other aspects of the problem of designing tracking systems. 11.2.6 Destabilization Caused by Feedback As well as having many applications, feedback can have undesirable effects and can in fact cause instability. For example, consider the telescope-pointing system illustrated in Figure 11.1. From the discussion in the preceding section, we know that it would be de- sirable to have a large amplifier gain in order to achieve good performance in tracking the desired pointing angle. On the other hand, as we increase the gain, we are likely to obtain faster tracking response at the expense of a reduction in system damping, resulting in sig- nificant overshoot and ringing in response to changes in the desired angle. Furthermore, instability can result if the gain is increased too much. Another common example of the possible destabilizing effect of feedback is feed- back in audio systems. Consider the situation depicted in Figure 11.8(a). Here, a loud- speaker produces an audio signal that is an amplified version of the sounds picked up by a microphone. Note that in addition to other audio inputs, the sound coming from the speaker itself may be sensed by the microphone. How strong this particular signal is depends upon Sec. 11.2 Some Applications and Consequences of Feedback 831 Speaker Amplifier (a) Total audio input to the microphone audio 1-----..... ---1~ Speaker inputs K1 output + (b) Ext~rnal 5J audio + ·-----~ K 1-----..----t•~ Speaker 1 o~p~ 1 -~ Figure 11 .8 (a) Pictorial repre- inputs L.___ sentation of the phenomenon of audio 1 feedback; (b) block diagram represen- - .-I_-K_2_e_ _s_T ...,I .....•. f---..... tation of (a); (c) block diagram in (b) redrawn as a negative feedback sys- tem. (Note: e-sr is the system function (c) of a T-second time delay.) the distance between the speaker and the microphone. Specifically, because of the attenu- ating properties of air, the strength of the signal reaching the microphone from the speaker decreases as the distance between the speaker and the microphone increases. In addition, due to the finite speed of propagation of sound waves, there is time delay between the signal produced by the speaker and that sensed by the microphone. This audio feedback system is represented in block diagram form in Figure 11.8(b). Here, the constant K2 in the feedback path represents the attenuation, and Tis the prop- agation delay. The constant K 1 is the amplifier gain. Also, note that the output from the feedback path is added to the external input. This is an example of positive feedback. As discussed at the beginning of the section, the use of a negative sign in the definition of the basic feedback system of Figure 11.3 is purely conventional, and positive and nega- tive feedback systems can be analyzed using the same tools. For example, as illustrated in Figure 11.8(c), the feedback system of Figure 11.8(b) can be written as a negative feedback 832 Linear Feedback Systems Chap. 11 system by adding a minus sign to the feedback-path system function. From this figure and from eq. (11.1), we can determine the closed-loop system function: Q(s) = K1 . (11.36) 1- K1K2e-sT Later we will return to this example, and, using a technique that we will develop in Section 11.3, we will show that the system of Figure 11.8 is unstable if (11.37) Since the attenuation due to the propagation of sound through the air decreases (i.e., K2 increases) as the distance between the speaker and the microphone decreases, if the mi- crophone is placed too close to the speaker, so that eq. (11.37) is satisfied, the system will be unstable. The result of this instability is an excessive amplification and distortion of audio signals. It is interesting to note that positive, or what Black referred to as regenerative, feed- back had also been known for some time before he invented his negative feedback am- plifier and, ironically, had been viewed as a very useful mechanism (in contrast to the skeptical view of negative feedback). Indeed, positive feedback can be useful. For exam- ple, it was already known in the 1920s that the destabilizing influence of positive feedback could be used to generate oscillating signals. This use of positive feedback is illustrated in Problem 11.54. In this section, we have described a number of the applications of feedback. These and others, such as the use of feedback in the implementation of recursive discrete-time filters (see Problem 11.55), are considered in more detail in the problems at the end of the chapter. From our examination of the uses of feedback and the possible stabilizing and destabilizing effects that it can have, it is clear that some care must be taken in designing and analyzing feedback systems to ensure that the closed-loop system behaves in a desir- able fashion. Specifically, in Sections 11.2.3 and 11.2.6, we have seen several examples of feedback systems in which the characteristics of the closed-loop system can be signif- icantly altered by changing the values of one or two parameters in the feedback system. In the remaining sections of this chapter, we develop several techniques for analyzing the effect of changes in such parameters on the closed-loop system and for designing systems to meet desired objectives such as stability, adequate damping, etc. 11 .3 ROOT-LOCUS ANALYSIS OF LINEAR FEEDBACK SYSTEMS As we have seen in a number of the examples and applications we have discussed, a useful type of feedback system is that in which the system has an adjustable gain K as- sociated with it. As this gain is varied, it is of interest to examine how the poles of the closed-loop system change, since the locations of these poles tell us a great deal about the behavior of the system. For example, in stabilizing an unstable system, the adjustable gain is used to move the poles into the left-half plane for a continuous-time system or inside the unit circle for a discrete-time system. In addition, in Problem 11.49, we show that feedback can be used to broaden the bandwidth of a first-order system by moving the pole so as to decrease the time constant of the system. Furthermore, just as feedback can be used Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 833 to relocate the poles to improve system performance, as we saw in Section 11.2.6, there is the potential danger that with an improper choice of feedback a stable system can be destabilized, which is usually undesirable. In this section, we discuss a particular method for examining the locus (i.e., the path) in the complex plane of the poles of the closed-loop system as an adjustable gain is varied. The procedure, referred to as the root-locus method, is a graphical technique for plotting the closed-loop poles of a rational system function Q(s) or Q(z) as a function of the value of the gain. The technique works in an identical manner for both continuous-time and discrete-time systems. 11 .3. 1 An Introductory Example To illustrate the basic nature of the root-locus method for analyzing a feedback system, let us reexamine the discrete-time example considered in the preceding section and specified by the system functions z [eq. (11.23)] H(z) = 1 - 2z-' (11.38) z-2 and [e q. ( II. 24)] G(z) = 2{3z- 1 = 213 , (11.39) z where {3 now is viewed as an adjustable gain. Then, as we noted earlier, the closed-loop system function is 1 z [eq. (11.25)] Q(z) = 1- 2(1- {3)z- 1 z- (11.40) 2(1 - {3). In this example, it is straightforward to identify the closed-loop pole as being located at z = 2(1 - {3). In Figure 11.9(a), we have plotted the locus of the pole for the system as {3 varies from 0 to + oo. In part (b) of the figure, we have plotted the locus as {3 varies from 0 to -oo. In each plot, we have indicated the point z = 2, which is the open -loop pole [i.e., it is the pole of Q(z) for {3 = 0]. As {3 increases from 0, the pole moves to the left of the point z = 2 along the real axis, and we have indicated this by including an arrow on the thick line to show how the pole changes as {3 is increased. Similarly, for {3 < 0, the pole of Q(z) moves to the right of z = 2, and the direction of the arrow in Figure 11.9(b) indicates how the pole changes as the magnitude of {3 increases. For 1/2 < {3 < 3/2, the pole lies inside the unit circle, and thus, the system is stable. As a second example, consider a continuous-time feedback system with s H(s) = -- (11.41) s-2 and 2{3 G(s) = -, (11.42) s where {3 again represents the adjustable gain. Since H (s) and G(s) in this example are algebraically identical to H(z) and G(z), respectively, in the preceding example, the same 834 Linear Feedback Systems Chap. 11 Unit circle - -,/ I \ I I 2 <R.e (a) Unit circle - -,/ I \ I I I 2 (Jl.e Figure 1 1 . 9 Root locus for the closed-loop system of eq. (11.40) as the value of {3 is varied: (a) {3 > 0; (b) {3 < 0. Note that we have marked the point z = 2 that corresponds to (b) the pole location when {3 = 0. will be true for the closed-loop system function s Q(s) = s- 2(1 - {3) (11.43) vis-a-vis Q(z), and the locus of the pole as a function of f3 will be identical to the locus in that example. The relationship between these two examples stresses the fact that the locus of the poles is determined by the algebraic expressions for the system functions of the for- ward and feedback paths and is not inherently associated with whether the system is a continuous-time or discrete-time system. However, the interpretation of the result is inti- mately connected with its continuous-time or discrete-time context. In the discrete-time case it is the location of the poles in relation to the unit circle that is important, whereas in the continuous-time case it is their location in relation to the imaginary axis. Thus, as we have seen for the discrete-time example in eq. ( 11.40), the system is stable for 112 < f3 < 3/2, while the continuous-time system of eq. ( 11.43) is stable for f3 > 1. 11.3.2 Equation for the Closed-Loop Poles In the simple example considered in the previous section the root locus was easy to plot, since we could first explicitly determine the closed-loop pole as a function of the gain parameter and then plot the location of the pole as we changed the gain. For more complex Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 835 systems, one cannot expect to find such simple closed-form expressions for the closed-loop poles. However, it is still possible to sketch accurately the locus of the poles as the value of the gain parameter is varied from -oo to +oo, without actually solving for the location of the poles for any specific value of the gain. This technique for determining the root locus is extremely useful in gaining insight into the characteristics of a feedback system. Also, as we develop the method, we will see that once we have determined the root locus, there is a relatively straightforward procedure for determining the value of the gain parameter that produces a closed-loop pole at any specified location along the root locus. We will phrase our discussion in terms of the Laplace transform variables, with the understanding that it applies equally well to the discrete-time case. Consider a modification of the basic feedback system of Figure 11.3(a), where either G(s) or H(s) is cascaded with an adjustable gain K. This is illustrated in Figure 11.10. In ei- ther of these cases, the denominator of the closed-loop system function is 1 + KG(s)H(s).2 Therefore, the equation for the poles of the closed-loop system are the solutions of the equation 1 + KG(s)H(s) = 0. (11.44) x(t) + + K ~ H(s) y(t) - G(s) Q(s) = 1 + ~~~:~G(s) (a) x(t) --..:. ~+8 • H(s) y(t) - K ~ G(s) - Figure 11.1 0 Feedback systems H(s) containing an adjustable gain: (a) sys- Q(s) = 1 + KH(s)G(s) tem in which the gain is located in the forward path; (b) system with the gain (b) in the feedback path. 21n the following discussion, we assume for simplicity that there is no pole-zero cancellation in the product G(s)H(s). The presence of such pole-zero cancellations does not cause any real difficulties, and the procedure we will outline here is easily extended to that case (Problem 11.32). In fact, the simple example at the start ofthis section [e qs. (11.41) and ( 11.42)] does involve a pole-zero cancellation, at s = 0. 836 Linear Feedback Systems Chap. 11 Rewriting eq. (11.44), we obtain the basic equation determining the closed-loop poles: 1 G(s)H(s) = - K' (11.45) The technique for plotting the root locus is based on the properties of this equation and its solutions. In the remainder of this section, we will discuss some of these properties and indicate how they can be exploited in determining the root locus. 11.3.3 The End Points of the Root Locus: The Closed-Loop Poles for K = 0 and IK l = +oo Perhaps the most immediate observation that one can make about the root locus is that obtained by examining eq. (11.45) forK = 0 and IKI = oo. ForK = 0, the solution of this equation must yield th~ poles of G(s)H(s), since l!K = oo. To illustrate, recall the example given by eqs. (11.41) and (11.42). If we let {3 play the role of K, we see that eq. (11.45) becomes 2 1 (11.46) s-2 73' Therefore, for {3 = 0, the pole of the system will be located at the pole of 2/(s - 2) (i.e., at s = 2), which agrees with what we depicted in Figure 11.9. Suppose now that IKI = oo. Then l!K = 0, so that the solutions of eq. (11.45) must approach the zeros of G(s)H(s). If the order of the numerator of G(s)H(s) is smaller than that of the denominator, then some of these zeros, equal in number to the difference in order between the denominator and numerator, will be at infinity. Referring again to eq. (11.46), since the order of the denominator of 2/(s- 2) is 1, while the order of the numerator is zero, we conclude that\n this example there is one zero at infinity and no zeros in the finite s-plane. Thus, as lf31 ~ oo, the closed-loop pole approaches infinity. Again, this agrees with Figure 11.9, in which the magnitude of the pole increases without bound as lf31 ~ oo for either {3 > 0 or {3 < 0. While the foregoing observations provide us with basic information as to the closed- loop pole locations for the extreme values of K, the following result is the key to our being able to plot the root locus without actually solving for the closed-loop poles as explicit functions of the gain. 11 .3.4 The Angle Criterion Consider again eq. (11.45). Since the right-hand side of this equation is real, a point s0 can be a closed-loop pole only if the left-hand side of the equation, i.e., G(s0)H(s0), is also real. Writing G(so)H(so) = IG(so)H(so)l ei<tG(so)H(so), (11.47) we see that, for G(s0)H(s0) to be real, it must be true that ei<tG(so)H(so) = ± 1. (11.48) Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 837 That is, for s0 to be a closed-loop pole, we must have <r.G (so) H (so) = integer multiple of 7T. (11.49) Returning to eq. (11.46), we see immediately that in order for 2/(so- 2) to be real, it is necessary that s0 be real. For more complex system functions, it is not as easy to determine the values of so for which G (so) H (so) is real. However, as we will see, the use of the angle criterion given by eq. (11.49), together with the geometric method described in Chapter 9 for evaluating <r.G (so) H (so), greatly facilitates the determination of the root locus. The angle criterion given by eq. (11.49) provides us with a direct method for deter- mining whether a point s0 could be a closed-loop pole for some value of the gain K. A further examination of eq. (11.45) gives us a way in which to calculate the value of the gain corresponding to any point on the root locus. Specifically, suppose that s0 satisfies <r.G (so) H (so) = odd multiple of 7T. (11.50) Then ei-t.G(so)H(so) = -1, and from eq. (11.47) we see that G(so)H(so) = -IG(so)H(so)l. (11.51) Substituting eq. (11.51) into eq. (11.45), we find that if K = 1 (11.52) IG(so)H(so)l' then s0 is a solution of the equation and hence a closed-loop pole. Similarly, if s0 satisfies the condition <r.G (so) H (so) = even multiple of 7T, (11.53) then G (so) H (so) = IG (so) H (so) I. (11.54) Thus, if K =- 1 (11.55) IG (so) H (so)l' then s0 is a solution of eq. (11.45) and hence a closed-loop pole. For the example given in eq. (11.46), if so is on the real line and s0 < 2, then <r. (-2 )= -7T, (11.56) so- 2 and from eq. (11.52), the value of {3 for which s0 is the closed-loop pole is {3 = _1_ = 2 - so (11.57) ls0~21 2 838 Linear Feedback Systems Chap. 11 That is, so = 2(1 - {3), (11.58) which agrees with eq. (11.43). Summarizing the last two observations that we have made, we see that the root locus for the closed-loop system, that is, the set of points in the complex s-plane that are closed- loop poles for some value of K asK varies from -oc., to +oc.,, are precisely those points that satisfy the angle condition of eq. ( 11.49). Furthermore: 1. A point so for which <t.G (so) H (so) = odd multiple of 7T (11.59) is on the root locus and is a closed-loop pole for some value of K > 0. The value of the gain that makes s0 a closed-loop pole is given by eq. ( 11.52). 2. A point s0 for which <t.G (so) H (so) = even multiple of 7T (11.60) is on the root locus and is a closed-loop pole for some value of K < 0. The value of the gain that makes s0 a closed-loop pole is given by eq. ( 11.55). Therefore, we have now reduced the problem of determining the root locus to that of searching for points that satisfy the angle requirements given by eqs. (11.59) and (11.60). These equations can be refined further to a set of properties that aid in sketching the root locus. Before discussing these properties, however, let us consider a simple example. Example 1 1 . 1 Let H(s) = s + 1, G(s) = s + 2"" (11.61) Recall that in Section 9.4 we discussed the geometric evaluation of Laplace transforms. In that section, we saw that the angle of the rational Laplace transform k=l n (11.62) ll (s- ak) k=l evaluated at some point s0 in the complex plane equals the sum of the angles of the vectors from each of the zeros to s0 minus the sum of the angles from each of the poles to s0 . Applying this to the product of G(s)H(s), where G(s) and H(s) are as given in eq. ( 11.61 ), we can determine geometrically those points in the s-plane that satisfy the angle criteria, eqs. (11.59) and (11.60), and therefore can sketch the root locus. In Figure 11.11, we have plotted the poles of G(s )H (s) and have denoted by () and <P the angles from each of the poles to the point s0 . Let us first test the angle criterion Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 839 -2 -1 CRe Figure 11.11 Geometric procedure for evaluating angle criterion in Exam- ple 11.1. for points so on the real axis. To begin with, the angle contribution from both poles is zero when so is on the real axis to the right of - 1. Thus, <r..G (so) H (so) = 0 = 0 · 1T, s0 real and greater than - 1, (11.63) and by eq. (11.60), these points are on the root locus for K < 0. For points between the two poles, the pole at -1 contributes an angle of -1T, and the pole at -2 contributes 0. Thus, <r..G(so)H(so) = -1T, so real, -2 < so < - 1. (11.64) These points are on the locus for K > 0. Finally, each pole contributes an angle of -7r when s0 is real and less than -2, so that <r..G (so) H (so) = -21T, s0 real and less than -2. Therefore, these points are on the locus for K < 0. Let us now examine points in the upper half of the s-plane. (Since the impulse responses are real-valued, the complex poles occur in conjugate pairs. Therefore, we can immediately determine the poles in the lower half-plane after we have examined the upper half.) From Figure 11.11, the angle of G (so) H (so) at the point s0 is <r..G (so) H (so) = -((} + cp). (11.65) Also, it is clear that as so ranges over the upper half-plane (but not the real axis), we have 0 < (} < 1T, 0 < 4J < 1T. (11.66) Thus, -21T < <r..G(s)H(s) < 0, (11.67) Therefore, we see immediately that no point in the upper half-plane can be on the locus forK< 0 [since <r..G(s)H(s) never equals an even multiple of 1r]. In addition, if s0 is to be on the locus for K > 0, we must have <r..G (so) H (so) = -((} + cp) = -1T, (11.68) 840 Linear Feedback Systems Chap. 11 or () = 7T- ¢. (11.69) Examining the geometry of Figure 11.11, we see that this occurs only for those points located on the straight line that is parallel to the imaginary axis and that bisects the line joining the poles at - 1 and -2. We have now examined the entire s-plane and have determined all those points on the root locus. In addition, we know that for K = 0, the closed-loop poles equal the poles of G(s)H(s), and as IKI ~ oo, the closed-loop poles go to the zeros of G(s)H(s), which in this case are both at infinity. Putting these results together, we can draw the entire root locus, depicted in Figure 11.12, in which we have indicated the direction of increasing IKI, both for K > 0 and for K < 0. -2 -1 (a) -2 -1 (b) Figure 11.12 Root locus for Example 11.1: (a) K > 0; (b) K < 0. The poles of G(s)H(s), which are located at s = -1 and s = -2, are indicated. Note from the figure that forK > 0 there are two branches of the root locus and that the same is true for K < 0. The reason for the existence of two branches is that in this example the closed-loop system is a second-order system and consequently has two poles for any specified value of K. Therefore, the root locus has two branches, each of which traces the location of one of the closed-loop poles asK is varied, and for any particular value of K, there is one closed-loop pole on each branch. Again, if we wish to calculate the value of K for which a specific point s0 on the locus is a closed-loop pole, we can use eqs. (11.52) and (11.55). Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 841 11 . 3. 5 Properties of the Root Locus The procedure outlined in the preceding subsection provides us, in principle, with a method for determining the root locus for any continuous-time or discrete-time LTI feedback sys- tem. That is, we simply determine, graphically or otherwise, all those points that satisfy eq. (11.59) or eq. (11.60). Fortunately, there are a number of other geometric properties concerning root loci that make the sketching of a locus far less tedious. To begin our dis- cussion of these properties, let us assume that we have placed G(s)H(s) in the standard form nm (s- f3k) Sm + bm-ISm-l + ... + bo G(s)H(s) = k=l (11.70) sn + an-ISn-l + ... + ao nn (s- ak) k=l where the f3k's denote the zeros and the ak's denote the poles. In general, these may be complex. Also, from eq. (11.70), we see that we are assuming that the leading coeffi- cient in both the numerator anp the denominator of G(s)H(s) is + 1. This can always be achieved by dividing the numerator and denominator by the denominator coefficient of sn and absorbing the resulting numerator coefficient of sm into the gain K. For example, K 2s + 1 =K ~s + _!_ (2) s + _!_ 2 3 3 =-K 2 (11.71) 3s + 5s + 2 s2 + 2_s + ~ 3 s2 + 2_s + ~' 3 3 3 3 and the quantity (2/3 )K i~ then regarded as the overall gain that is varied in determining the root locus. We assume, in addition, that m $ n, (11.72) which is the case that is usually encountered in practice. (Problem 11.33 considers the case m > n.) The following are some properties that include earlier observations and that aid in sketching the root locus. Property 1: ForK = 0, the solutions of eq. (11.45) are the poles of G(s)H(s). Since we are assuming n poles, the root locus has n bra~ches, each one starting (for K = 0) at a pole of G(s)H(s). Property 1 includes the g~neral version of a fact which we noted in Example 11.1: that there is one branch of the root locus for each closed-loop pole. The next property is also simply a restatement of one of our earlier observations. Property 2: As IKI ~ oo, each branch of the root locus approaches a zero of G(s)H(s). Since we are assuming that m $ n, n- m of these zeros are at infinity. 842 Linear Feedback Systems Chap. 11 Property 3: Parts of the real s-axis that lie to the left of an odd number of real poles and zeros of G(s)H(s) are on the root locus for K > 0. Parts of the real s-axis that lie to the left of an even number (possibly zero) of poles and zeros of G(s)H(s) are on the root locus forK < 0. We can show that Property 3 is true as follows: From our discussion in Example 11.1 and from Figure 11.13(a), we see that if a point on the real s-axis is to the right of a real pole or zero of G(s)H(s), that pole or zero contributes zero to <t-G(s0)H(s0 ). On the other hand, if s0 is to the left of a zero, that zero contributes +7T, whereas if s0 is to the left of a pole, we get a contribution of -7r (since we subtract the pole angles). Hence, if s0 is to the left of an odd number of real poles and zeros, the total contribution of these poles and zeros is an odd multiple of 7T, whereas if s0 is to the left of an even number of real poles and zeros, the total contribution is an even multiple of 7T. From eqs. (11.59) and (11.60), we will have the result stated in Property 3 if we can show that the total contribution from all poles and zeros with nonzero imaginary parts is an even multiple of 7T. The key here is that such poles and zeros occur in complex-conjugate pairs, and we can consider the contribution from each such pair, as illustrated in Figure 11.13(b). The symmetry in the picture clearly indicates that the sum of the angles from this pair to any point s0 on the real axis is precisely 27T. Summing over all conjugate zero pairs and subtracting the sum over all conjugate pole pairs, we get the desired result. Thus, any segment of the real line Angle of zero radians Angle of 7r radians t So (a) Figure 11. 13 (a) Angle contribu- tion from real poles and zeros to a point on the real axis; (b) total angle contribution from a complex-conjugate (b) pole pair to a point on the real axis. Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 843 between real poles or zeros is on the root locus either forK > 0 or forK < 0, depending on whether it lies to the left of an odd or an even number of poles and zeros of G(s)H(s). As one consequence of Properties 1 through 3, consider a segment of the real axis between two poles of G(s )H (s ), with no zeros between these poles. From Property 1 the root locus begins at the poles, and from Property 3 the entire portion of the real axis between the two poles will lie on the root locus for a positive or negative range of values of K. Therefore, as IKI increases from zero, the two branches of the root locus that begin at these poles move toward each other along the segment of the real axis between the poles. From Property 2, as IKI increases toward infinity, each branch of the root locus must approach a zero. Since there are no zeros along that portion of the real axis, the only way that this can happen is if the branches break off into the complex plane for IKI sufficiently large. This is illustrated in Figure 11.12, in which the locus for K > 0 has a portion between two real poles. AsK is increased, the root locus eventually leaves the real axis, forming two complex-conjugate branches. Summarizing this discussion, we have the following property of the root locus: Property 4: Branches of the root locus between two real poles must break off into the complex plane for IKilarge enough. Properties 1-4 serve to illustrate how characteristics of the root locus can be de- duced from eqs. (11.45), (11.59), and (11.60). In many cases, plotting the poles and zeros of G(s)H(s) and then using these four properties suffices to provide a reasonably accurate sketch of the root locus. (See Examples 11.2 and 11.3.) In addition to these properties, however, there are numerous other characteristics of the root locus that allow one to ob- tain sketches of increasing accuracy. For example, from Property 2, we know that n - m branches of the root locus approach infinity. In fact, these branches approach infinity at specific angles that can be calculated, and therefore, the branches are asymptotically par- allel to lines at these angles. Moreover, it is possible to draw in the asymptotes and then determine the point at which they intersect. These two properties and several others are illustrated in Problems 11.34-11.36 and 11.41-11.42. A more detailed development of the root-locus method can be found in more advanced texts such as those listed in the bibliography at the end of the book. In the remainder of this section we present two examples, one in continuous time and one in discrete time, that illustrate how the four properties that we have just described allow us to sketch the root locus and to deduce the stability characteristics of a feedback system as the gain K is varied. Example 11 .2 Let s- 1 G(s)H(s) = (s + 1)(s + 2) (11.73) From Properties 1 and 2, the root locus for either K positive or K negative starts at the points s = - 1 and s = -2. One branch terminates at the zero at s = 1 and the other at infinity. Let us first consider K > 0. The root locus in this case is illustrated in Fig- ure 11.14(a). From Property 3, we can identify the regions of the real axis that are on 844 Linear Feedback Systems Chap. 11 9m -2 -1 (a) 9m (b) Figure 11.14 Root locus for Example 11.2: (a) K > 0; (b) K < 0. The poles of G(5)H(5) at 5 = -1 and 5 = -2 and the zero of G(5)H(5) at 5 = 1 are indicated in the figure. the root locus for K > 0-specifically , CRe{ s} < - 2 and - 1 < CRe{ s} < 1. Therefore, one branch of the root locus forK > 0 originates at s = - 1 and approaches s = 1 as K ~ oo. The other begins at s = -2 and extends to the left toward CRe{s} = -oo asK ~ +oo. Thus, we see that for K > 0, if K is sufficiently large, the system will become unstable, as one of the closed-loop poles moves into the right-half plane. The procedure that we have used for sketching the root locus does not, of course, indicate the value of K for which this instability develops. However, for this particular example, we see that the value of K for which the instability occurs corresponds to the root locus passing through s = 0. Consequently, from eq. (11.52), the corresponding value of K is 1 K= =2 (11.74) IG(O)H(O)I . Thus, the system is stable for 0 ::; K < 2, but is unstable for K ~ 2. ForK < 0, the portions of the real axis lying on the root locus are CR..e{s} > 1 and -2 < CR..e{s} < -1. Thus, the root locus again starts at the points s = -2 and s = -1, moving into the region -2 < CR..e{s} < -1. At some point, it breaks off into the complex plane and follows a trajectory such that it returns to the real axis for s > 1. Upon the root locus' return to the real axis, one branch moves to the left toward the zero at s = 1 and the other to the right towards = oo, as indicated in Figure 11.14(b), in which we have displayed an accurate plot of the root locus for K < 0. Sec. 11.3 Root-Locus Analysis of Linear Feedback Systems 845 Rules can also be developed to indicate the locations at which the root locus leaves and enters the real axis. Even without that precise a description, however, we can sketch the general shape of the root locus in Figure 11.14(b) and can therefore deduce that for K < 0, the system also becomes unstable for IKI sufficiently large. Example 1 1 . 3 Consider the discrete-time feedback system illustrated in Figure 11.15. In this case, (11.75) + K x[n] 1---.....- ~ y[n] Figure 11. 1 5 Discrete-time feedback system of Example 11.3. As discussed at the beginning of this section, the techniques for sketching the root locus of a discrete-time feedback system are identical to those used in the continuous-time case. Therefore, in a manner exactly analogous to that of the preceding example, we can deduce the basic form of the root locus for this example, which is illustrated in Figure 11.16. In this case the portion of the real axis between the two poles of G(z)H(z) (at z = 1/4 and z = 112) is on the root locus forK > 0, and asK increases the locus breaks off into the complex plane and returns to the axis at some point in the left-half plane. From there, one branch approaches the zero of G(z)H(z) at z = 0, and the other approaches infinity as K ~ oo. The form of the root locus for K < 0 consists of two branches on the real axis, one approaching 0 and the other infinity. As we remarked earlier, while the form of the root locus does not depend on whether the system is a continuous-time or discrete-time system, any conclusion re- garding stability based on examining the locus certainly does. For this example, we can conclude that for IKI sufficiently large the system is unstable, since one of the two poles has magnitude greater than 1. In particular, from the K > 0 root locus in Figure 11.16(a), we see that the transition from stability to instability occurs when one of the closed-loop poles is at z = -1. From eq. (11.52), the corresponding value of K is K= 1 =_!2 (11.76) IG ( -l)H (- 1) I 8 . Similarly, from Figure 11.16(b ), the stability-instability transition occurs when one of the closed-loop poles is at z = 1, and from eq. (11.55), the corresponding value of K is 1 3 K = - = -- (11.77) IG(l)H(l)l 8' Putting this together, we see that the closed-loop system in Figure 11.16 is stable if 846 Linear Feedback Systems Chap. 11 9m. - - - - - , , , ',/Unit circle // / I ' \ \ I I I \ I CR.e I I (a) 9m. - - - - - , , , , /Unit circle ' I ' \ I \ I \ 1 1 CRe 4 2 (b) Figure 11 .16 Root locus for Example 11.3: (a) K > 0; (b) K < 0. The poles of G(z)H(z) at z = 1/4 and z = 1/2 and the zero of G(z)H(z) at z = 0 are indicated in the figure. 3 15 -- < K <- (11.78) 8 8 and is unstable for K outside this range. 11 .4 THE NYQUIST STABILITY CRITERION As developed in Section 11.3, the root-locus technique provides detailed information con- ceming the location of closed-loop poles as the system gain is varied. From root-locus plots, one can determine the damping and the stability characteristics of the system as K is varied. Determination of the root locus requires the analytic description of the system Sec. 11.4 The Nyquist Stability Criterion 847 functions of the forward and feedback paths and is applicable only when these transforms are rational. For example, it cannot be directly applied in situations in which our knowledge of the system functions is obtained purely from experimentation. In this section, we introduce another method for the determination of the stability of feedback systems as a function of an adjustable gain parameter. This technique, referred to as the Nyquist criterion, differs from the root-locus method in two basic ways. Unlike the root-locus method, the Nyquist criterion does not provide detailed information concerning the location of the closed-loop poles as a function of K, but rather, simply determines whether or not the system is stable for any specified value of K. On the other hand, the Nyquist criterion can be applied to nonrational system functions and in situations in which no analytic description of the forward and feedback path system functions is available. Our objective in this section is to outline the basic ideas behind the Nyquist criterion for both continuous-time and discrete-time systems. As we will see, both the discrete- time and continuous-time Nyquist tests are the result of the same fundamental concept, although, as with the root-locus method, the actual criteria for stability differ because of the differences between continuous and discrete time. More detailed developments of the ideas behind the Nyquist criterion and its use in the design of feedback systems can be found in texts on the analysis and synthesis of feedback systems and automatic control systems, including those listed in the bibliography at the end of the book. To introduce the method, let us recall that the poles of the closed-loop systems of Figure 11.10 and their discrete-time counterparts are the solutions of the equations 1 + KG(s)H(s) = 0 (continuous time) (11.79) and 1 + KG(z)H(z) = 0 (discrete time). (11.80) For discrete-time systems, we want to determine whether any of the solutions of eq. (11.80) lie outside the unit circle, and for continuous-time systems whether any of the solutions of eq. (11.79) lie in the right half of the s-plane. The Nyquist criterion fixes this by examina- tion of the values of G(s)H(s) along the jw-axis and the values of G(z)H(z) along the unit circle. The basis for this is the encirclement property, which we develop in the following subsection. 11.4.1 The Encirclement Property Consider a general rational function W(p), where pis a complex variable,3 and suppose that we plot W (p) for values of p along a closed contour in the p-plane, which we traverse in a clockwise direction. This is illustrated in Figure 11.17 for a function W (p) that has two zeros and no poles. In Figure 11.17(a) we show a closed contour C in the p-plane, and in Figure 11.17 (b) we plot the closed contour of the values of W (p) asp varies around C. 3Because we will use the property we are about to develop for both continuous-time and discrete-time feedback systems, we have chosen to describe it in terms of a general complex variable p. In the next subsection we use this property to analyze continuous-time feedback systems, where the complex variable iss. Following this, in Section 11.4.3 we use the encirclement property for discrete-time feedback systems, in which context the complex variable is z. 848 Linear Feedback Systems Chap. 11 p-plane (a) W-plane Figure 11 . 1 7 Basic encirclement property. The closed curve in (b) rep- resents a plot of the values of W(p) ffi-e for values of p along the curve C in (a). Here, the arrow on the curve C in (a) indicates the direction in which C is traversed, and the arrow in (b) indicates the corresponding direction (b) along the contour of values of W(p). In this example, there is one zero of W(p) inside the contour and one zero of W(p) outside the contour. At any point p on C, the angle of W(p) is the sum of the angles of the two vectors v1 and v2 to the point p. As we traverse the contour once, the angle 4> 1 of the vector from the zero inside the contour encounters a net change of -21T radians, whereas the angle 4>2 of the vector from the zero outside the contour encounters no net change. Thus, on the plot of W(p), there is a net change in angle of -21T. Said another way, the plot of W (p) in Figure 11.17 (b) encircles the origin once in the clockwise direction. More generally, for an arbitrary rational W(p), as we traverse a closed contour in the clock- wise direction, any poles and zeros of W (p) outside the contour will contribute no net change to the angle of W(p), whereas each zero inside the contour will contribute a net change of -21T and each pole inside will contribute a net change of +21T. Since each net change of -21T in W(p) corresponds to one clockwise encirclement of the origin in the plot of W(p), we can state the following basic encirclement property: Encirclement Property: As a closed contour C in the p-plane is traversed once in the clockwise direction, the corresponding plot of W (p) for values of p along the contour encircles the origin in the clockwise direction a net number of times equal to the number of zeros minus the number of poles contained within the contour. In applying this statement, a counterclockwise encirclement is interpreted as the neg- ative of one clockwise encirclement. For example, if there is one pole and no zeros inside the contour, there will be one counterclockwise, or equivalently, minus-one clockwise, encirclement. Sec. 11.4 The Nyquist Stability Criterion 849 Example 11 .4 Consider the function p-1 w( p) = (p + 1) (p2 + p + 1). (11.81) 9m 9m X (a) 9m 9m p-plane W-plane X (b) 9m 9m p-plane W-plane (c) Figure 11. 18 Basic encirclement property for Example 11.4: (a) the con- tour encircles no poles or zeros and consequently W(p) has no encirclements of the origin; (b) the contour encircles one pole and therefore W(p) has one encirclement of the origin; (c) the contour encircles three poles and therefore W(p) has three encirclements of the origin; 850 Linear Feedback Systems Chap. 11 !1m !1m p-plane W-plane <R.e X (d) !1m !1m p-plane W-plane CRe CRe (e) Figure 11.18 Continued (d) the contour encircles one pole and one zero and therefore W(p) has no encirclements of the origin; (e) the contour encir- cles three poles and one zero. W(p) has two encirclements of the origin. In Figure 11.18, we depict several closed contours in the complex p-plane and the corre- sponding plots of W(p) along each of these contours. In Figure 11.18(a), the contour C1 does not encircle any of the poles or zeros of W(p), and consequently, the plot of W(p) has no net encirclements of zero. In Figure 11.18(b ), only the pole at p = - 1 is contained within the contour Cz, and the plot of W(p) encircles the origin once in the counterclock- wise direction (minus-one clockwise encirclements). In Figure 11.18( c), C3 encircles all three poles, and the plot of W(p) encircles the origin three times in a counterclockwise direction. In Figure 11.18( d), C4 encircles one pole and one zero, and therefore, the plot of W (p) has no net encirclements of the origin. Finally, in Figure 11.18( e), all of the poles and the one zero of W(p) are contained within C5, and thus, the plot of W(p) along this contour has two net counterclockwise encirclements of the origin. 11.4.2 The Nyquist Criterion for Continuous-Time LTI Feedback Systems In this section, we exploit the encirclement property in examining the stability of the continuous-time feedback system of Figure 11.10. Stability of this system requires that Sec. 11.4 The Nyquist Stability Criterion 851 no zeros of 1 + KG(s)H(s), or equivalently, of the function 1 R(s) = K + G(s)H(s) (11.82) lie in the right half of the s-plane. Thus, in applying the general result developed in the preceding subsection, we can consider the contour indicated in Figure 11.19. From the plot of R(s), as s traverses the contour C we can obtain a count of the number of zeros minus the number of poles of R(s) contained within the contour by counting the number of clockwise encirclements of the origin. As M increases to infinity, this then corresponds to the number of zeros minus the number of poles of R(s) in the right half of the s-plane. 9m jM CRe Figure 11. 19 Closed contour con- taining a portion of the right-half plane; -jM as M ~ oo, the contour encloses the entire right-half plane. Let us examine the evaluation of R(s) along the contour in Figure 11.19 as M in- creases to infinity. Along the semicircular portion of the contour extending into the right- half plane, we must ensure that R(s) remains bounded as M increases. Accordingly, we will assume that R(s) has at least as many poles as zeros. In that case, = b sn + b -1 sn - 1 + . . . + bo R(s) n n (11.83) ansn + an-1sn- 1 + ... + ao and lim R(s) = bn = constant. (11.84) isl~oo an Therefore, as M increases to infinity, the value of R(s) does not change as we traverse the semicircular part of the contour, and consequently, the constant value along this part is equal to the value of R(s) at the end points [i.e., R(jw) at w = ±oo]. Therefore, the plot of R(s) along the contour of Figure 11.19 can be obtained by plotting R(s) along the part of the contour that coincides with the imaginary axis-that is, the plot of R(jw) as w varies from -oo to +oo. Since R(jw) equals 1/K + G(jw)H(jw), R(s) along the contour can be drawn from knowledge of G(jw) and H(jw ). If both the forward- and feedback-path systems are stable, G(jw) and H(jw) are simply the frequency-response functions of these systems. However, the encirclement property for the general function W(p) is simply a property of complex functions; it has nothing to 852 Linear Feedback Systems Chap. 11 do with wnether W(p) arose as the Laplace or z-transform of any signal and, conse- quently, has nothing to do with regions of convergence. Thus, even if the forward- and feedback-path systems are unstable, if we examine the plot of the function R(jw) = 1/ K + G(jw )H(jw) for -oo < w < oo, we know that the net number of clockwise en- circlements of the origin will equal the number of zeros minus the number of poles of R(s) that lie in the right-half plane. Furthermore, from eq. (11.82), we see that the poles of R(s) are simply the poles of G(s)H(s), while the zeros of R(s) are the closed-loop poles. In addition, since G(jw)H(jw) = R(jw)- 1/K, it follows that the plot of G(jw)H(jw) encircles the point -1/ K exactly as many times as R(jw) encircles the origin. The plot of G(jw )H(jw) as w varies from -oo to +oo is called the Nyquist plot. From the encirclement property, we then see that The number of right-half The number of clockwise plane closed-loop poles encirclements of the point (11.85) minus the number of right- -1/K by the Nyquist plot half plane poles of G(s)H(s). While the open-loop system G(s)H(s) may have unstable poles, for the closed-loop system to be stable we require no right-half plane closed-loop poles. This yields the continuous- time Nyquist stability criterion: Continuous-Time Nyquist Stability Criterion: For the closed-loop system to be stable, the net number of clockwise encirclements of the point - 1/ K by theNy quist plot of G(jw )H(jw) must equal minus the number of right-half-plane poles of G(s)H(s). Equivalently, the net number of counterclockwise encirclements of the point -1/ K by the Nyquist plot of G(jw )H(jw) must equal the number of right-half-plane poles of G(s)H(s). For example, if the forward- and feedback-path systems are stable, then the Nyquist plot is simply the plot of the frequency response of the cascade of these two systems. In this case, since there are no poles of G(s)H(s) in the right-half plane, the Nyquist criterion requires that, for stability, the net number of encirclements of the point -1/K must be zero. Example 11.5 Let 1 1 G(s) = -- H(s) = --. (11.86) s + 1' !s + 1 2 The Bode plot for G(jw )H(jw) is shown in Figure 11.20. The Nyquist plot depicted in Figure 11.21 is constructed directly from the plots of the log magnitude and phase of G(jw)H(jw). That is, each point on the Nyquist plot has polar coordinates con- sisting of the magnitude IG (j w )H (j w) I and angle <r G(jw )H (j w) for some value of w. The coordinates of G(jw )H(jw) for w < 0 are obtained from the values for w > 0 through the use of the conjugate symmetry property of G(jw )H(jw ). This property manifests itself geometrically in a very simple way, which facilitates the sketching of the Nyquist plot for any feedback system composed of systems with Sec. 11.4 The Nyquist Stability Criterion 853 20 0 dB 3 -20 I 3 -40 (9 -60 0 Cl -80 .Q 0 C\J -100 -120 0.01 0.1 10 100 w 0 3- I -TI/2 3 (9 -3TI/4 \1 -TI w Figure 11.20 Bode plot for G(jw )H(jw) in Example 11.5. gm{G(jw)H(jw)} w=O ffi.e{G (jw) H (jw)} Figure 11 .21 Nyquist plot of G(jw )H(jw) for Example 11.5. The arrow on the curve indicates the direction of increasing w. 854 Linear Feedback Systems Chap. 11 real impulse responses. Specifically, since IG(-jw)H(-jw)l = IG(jw)H(jw)l and <t.G(- jw )H(- jw) = - <t.G(jw )H(jw ), the Nyquist plot of G(jw )H(jw) for w :s 0 is a reflection about the real axis of the plot for w ~ 0. Note also that we have included an arrow on the Nyquist plot in Figure 11.21. This arrow indicates the direction of in- creasing w. That is, it indicates the direction in which the Nyquist plot is traversed (as w varies from -x to +x) for the counting of encirclements in the application of the Nyquist criterion. In this example there are no right-half-plane open-loop poles, and consequently, the Nyquist criterion requires that, for stability, there be no net encirclements of the point -11K. Thus, by inspection of Figure 11.21, we see that the closed-loop system will be stable if the point - 11 K falls outside the Nyquist contour-that is, if 1 1 - :s 0 or -- K K > 1, (11.87) which is equivalent to or O>K>-1. (11.88) Combining these two conditions, we obtain the result that the closed-loop system will be stable for any choice of K greater than - 1. Example 11 .6 Consider now s + 1 G(s)H(s) = (11.89) (s - 1) ( ~ s + 1) · The Nyquist plot for this system is indicated in Figure 11.22. For this example, G(s)H(s) has one right-half-plane pole. Thus, for stability we require one counterclockwise encir- clement of the point - 11 K, which in turn requires that the point - 11 K fall inside the contour. Hence, we will have stability if and only if - 1 < - 1/ K < 0, that is, if K > 1. gm{G(jw)H(jw)} -1 / ffi~{G(jw)H(jw)} w=O Figure 11.22 Nyquist plot for Example 11.6. The arrow on the curve indi- cates the direction of increasing w. In the foregoing discussion, we have introduced and illustrated a form for the Nyquist stability criterion that applies to an extremely large class of feedback systems. In addition, Sec. 11.4 The Nyquist Stability Criterion 855 there are a number of refinements and extensions of the criterion that allow it to be used for many other feedback systems as well. For example, as we have developed it, the Nyquist plot can be drawn without any difficulties for stable or unstable G(s )H (s ), as long as there are no poles of G(s)H(s) exactly on the jw-axis. When such poles do occur, the value of G(jw )H(jw) is infinite at those points. However, as considered in Problem 11.44, the Nyquist criterion can be modified to allow for poles of G(s)H(s) on the jw-axis. In ad- dition, as mentioned at the beginning of this section, the Nyquist criterion can also be extended to the case in which G(s) and H(s) are not rational. For example, it can be shown that if the forward- and feedback-path systems are both stable, the Nyquist criterion is the same when the system functions are nonrational as it is when they are rational. That is, the closed-loop system is stable if there are no net encirclements of the point -1/K. To illustrate the application of the Nyquist criterion for nonrational system functions, we present the following example: Example 11.7 Consider the acoustic feedback example discussed in Section 11.2.6. Referring to Fig- ure 11.8(b), let K = K1K2 and G(s)H(s) = -e-sT = e-(sT+j1T)' (11.90) where we have used the fact that e-J1T = -1. In this case, G(jw)H(jw) = e-j(wT+1T)' (11.91) and as w varies from -oo to oo, G(jw )H(jw) traces out a circle of radius 1 in the clock- wise direction, with one full revolution for every change of 21TIT in w. This is illus- trated in Figure 11.23. Since the forward- and feedback-path systems are stable [the cascade G(s)H(s) is simply a time delay], the Nyquist stability criterion indicates that the closed-loop system will be stable if and only if -11 K does not fall inside the unit circle. Equivalently, we require for stability that IKI < 1. (11.92) 9m{G(jw)H(jw)} Figure 11.23 Nyquist plot for Example 11.7. Since K 1 and K 2 represent an acoustic gain and attenuation, respectively, they are both positive, which yields the stability criterion (11.93) 856 Linear Feedback Systems Chap. 11 11.4.3 The Nyquist Criterion for Discrete-Time LTI Feedback Systems As in the continuous-time case, the Nyquist stability criterion for discrete-time systems is based on the fact that the difference in the number of poles and zeros inside a contour, for a rational function, can be determined by examining a plot of the value of the function along the contour. The difference between the continuous-time and discrete-time cases is the choice of the contour. For the discrete-time case, stability of the closed-loop feedback system requires that no zeros of 1 R(z) = K + G(z)H(z) (11.94) lie outside the unit circle. Recall that the encirclement property relates to poles and zeros inside any specified contour. On the other hand, in examining the stability of a discrete-time system, we are concerned with the zeros of R(z) outside the unit circle. Therefore, in order to make use of the encirclement property, we first make a simple modification. Let us consider the rational function (11.95) obtained by replacing z by its reciprocal. As seen in Problem 10.43, if zo is a zero (pole) of R(z), then 1/z0 is a zero (pole) of R(z). Since 1/ \zo\ is less than 1 if \zo\ > 1, any zero or pole of R(z) outside the unit circle corresponds to a zero or pole of R(z) inside the unit circle. From the basic encirclement property, we know that as z traverses the unit circle in a clockwise direction, the net number of clockwise encirclements of the origin by R(z) equals the difference between the number of its zeros and poles inside the unit circle. However, from the previous paragraph, this equals the difference between the number of zeros and poles of R(z) outside the unit circle. Furthermore, on the unit circle, z = ejw and 1I z = e- jw . Therefore, (11.96) From this, we see that evaluating R(z) as z traverses the unit circle in the clockwise di- rection is identical to evaluating R(z) as z traverses the unit circle in the counterclockwise direction. In sum, then, The number of clockwise encirclements of the origin by the plot of as the The number of zeros of R(z) R( e.iw) unit circle is traversed in the outside the unit circle minus (11.97) counterclockwise direction the number of poles of R(z) (e.g., as w increases from 0 outside the unit circle. to 27T) Much as in the continuous-time case, counting the encirclements of the origin by R(e.iw) is equivalent to counting the number of encirclements of the point -1/K by the Sec. 11.4 The Nyquist Stability Criterion 857 plot of G(efw)H(efw), again referred to as the Nyquist plot, which is graphed as w varies from 0 to 27T. Also, the poles of R(z) are precisely the poles of G(z)H(z), and the zeros of R(z) are the closed-loop poles. Therefore, the encirclement property stated in the preceding paragraph implies that the net number of clockwise encirclements by the Nyquist plot of the point -1/K equals the numtier of closed-loop poles outside the unit circle minus the number of poles of G( z)H ( z) outside the unit circle. In order that the closed-loop system be stable, we require no closed-loop poles outside the unit circle. This yields the discrete-time Nyquist stability criterion: Discrete-Time Nyquist Stability Criterion: For the closed-loop system to be sta- ble, the net number of clockwise encirclements of the point -1/K by the Nyquist plot of G(efw)H(efw) as w varies from 0 to 27T must equal minus the number of poles of G(z)H(z) that lie outside the unit circle. Equivalently, the net number of counterclock- wise encirclements of the point -1/K by the Nyquist plot of G(efw)H(efw) as w varies from 0 to 27T must equal the number of poles of G(z)H(z) outside the unit circle. Example 1 1 . 8 Let z-2 G(z)H (z) = -~- (11.98) 1 + lz-1 2 The Nyquist plot of this curve is shown in Figure 11.24. Since G(z)H(z) has no poles outside the unit circle, for the stability of the closed-loop system there must be no encir- clements of the point -1/K. From the figure, we see that this will be the case either if -1/K < -1 or if -1/K > 2. Thus, the system is stable for -112 < K < 1. 9m {G(eiw)H(eiw)} -1 Figure 11 .24 Nyquist plot for Example 11.8. The arrow on the curve in- dicates the direction in which the curve is traversed as w increases from 0 to 27T. Just as in continuous time, if the forward and feedback pdths are stable, then the Nyquist plot can be obtained from the frequency responses H ( efw) and G( efw) of these 858 Linear Feedback Systems Chap. 11 systems. If the forward and feedback paths are unstable, then the frequency responses are not defined. Nevertheless, the function G(z)H(z) can still be evaluated on the unit circle, and the Nyquist stability criterion can be applied. As we have seen in this section, the Nyquist stability criterion provides a useful method for determining the range of values of the gain K for which a continuous-time or discrete-time feedback is stable (or unstable). This criterion and the root-locus method are extremely important tools in the design and implementation of feedback systems, and each has its own uses and limitations. For example, the Nyquist criterion can be applied to nonrational system functions, whereas the root-locus method cannot. On the other hand, root-locus plots allow us to examine not only stability, but also other characteristics of the closed-loop system response, such as damping, oscillation frequency, and so on, which are readily identifiable from the location of the poles of the closed-loop system. In the next section, we introduce an additional tool for the analysis of feedback systems that highlights another important characteristic of closed-loop system behavior. 11.5 GAIN AND PHASE MARGINS In this section, we introduce and examine the concept of the margin of stability in a feed- back system. It is often of interest not only to know whether a feedback system is stable, but also to determine how much the gain in the system can be perturbed and how much additional phase shift can be added to the system before it becomes unstable. Information such as this is important because in many applications the forward and feedback system functions are known only approximately or may change slightly during operation because of wear, the effect of high temperatures on components, or similar influences. As an example, consider the telescope-pointing system described in Section 11.0 and illustrated in Figures 11.1 (c) and (d). This system consists of a motor, a potentiometer converting the shaft angle to a voltage, and an amplifier that is used to amplify the voltage representing the difference between the desired and the actual shaft angles. Assuming that we have obtained approximate descriptions of each of these components, we can set the amplifier gain so that the system will be stable if these descriptions are accurate. However, the amplifier gain and the constant of proportionality that describes the angle-voltage char- acteristic of the potentiometer are never known exactly, and therefore, the actual gain in the feedback system may differ from the nominal value assumed in designing the system. Furthermore, the damping characteristics of the motor cannot be determined with absolute precision, and thus, the actual time constant of the motor response may differ from the ap- proximate value in the specification of the system. For example, if the actual motor time constant is larger than the nominal value used in the design, the motor will respond more sluggishly than anticipated, thereby producing an effective time delay in the feedback system. As we have discussed in earlier chapters, and as we will again in Example 11.11, time delays have the effect of increasing the negative phase in the frequency response of a system, and this phase shift can have a destabilizing influence on the system. Because of the possible presence of gain and phase errors such as those we have just described, it is clearly desirable to set the amplifier gain so that there is some margin for error-that is, so that the actual system will remain stable even if it differs somewhat from the approximate model used in the design process. Sec. 11.5 Gain and Phase Margins 859 In this section, we introduce one method for quantifying the margin of stability in a feedback system. To do this, we consider a closed-loop system, depicted in Figure 11.25, that has been designed to be stable based on nominal values for the forward- and feedback- path system functions. For our discussion here, we let H(s) and G(s) denote these nominal values. Also, since the basic concepts are identical for both continuous-time and discrete- time systems, we will again focus our development on the continuous-time case, and at the end of the section we illustrate the application of these ideas to a discrete-time example. x(t) -~--u--.+- ...­.-~ H(s) y(t) Figure 11.25 Typical feedback system designed to be stable, as- G(s) suming nominal descriptions for H(s) and G(s). To assess the margin of stability in our feedback system, suppose that the actual system is as depicted in Figure 11.26, where we have allowed for the possibility of a gain K and phase shift cp in the feedback path. In the nominal system K is unity and cp is zero, but in the actual system either or both may have a different value. Therefore, it is of interest to know how much variation can be tolerated in these quantities without losing closed-loop system stability. In particular, the gain margin of the feedback system is defined as the minimum amount of additional gain K, with cp = 0, that is required so that the closed- loop system becomes unstable. Similarly, the phase margin is the additional amount of phase shift, with K = 1, that is required for the system to be unstable. By convention, the phase margin is expressed as a positive quantity; that is, it equals the magnitude of the additional negative phase shift at which the feedback system becomes unstable. x(t+) --ta • H(s) ""' y(t) G(s)~- K ~ Figure 11 .26 Feedback system containing possible gain and phase devia- tions from the nominal description depicted in Figure 11.25. Since the closed-loop system of Figure 11.25 is stable, the system of Figure 11.26 can become unstable if, asK and cp are varied, at least one pole of the closed-loop system crosses the jw-axis. If a pole of the closed-loop system is on the jw-axis at, say, w = w 0 , then at this frequency (11.99) 860 Linear Feedback Systems Chap. 11 or (11.100) Note that with K = 1 and <P = 0, by our assumption of stability for the nominal feedback system of Figure 11.25, there is no value of w 0 for which eq. (11.100) is satisfied. The gain margin of this system is the minimum value of K > 1 for which eq. (11.100) has a solution for some w 0 with <P = 0. That is, the gain margin is the smallest value of K for which the equation KG (jwo)H (jwo) = -1 (11.101) has a solution w 0 . Similarly, the phase margin is the minimum value of <P for which eq. (11.1 00) has a solution for some w 0 when K = 1. In other words, the phase margin is the smallest value of <P > 0 for which the equation (11.102) has a solution. To illustrate the calculation and graphical interpretation of gain and phase margins, we consider the following example. Example 11.9 Let 4(1 + ls) G(s)H(s) = 2 . (11.103) s(l + 2s)(l + 0.05s + (0.125s)2 ) The Bode plot for this example is shown in Figure 11.27. Note that, as discussed in Problem 6.3·1, the factor of lljw in G(jw)H(jw) contributes -90° (-'TT/2 radians) of phase shift and a 20-dB-per-decade increase in IG(jw )H(jw )1. To determine the gain margin, we observe that, with 4> = 0, the only frequency at which eq. (11.101) can be satisfied is that for which -9:G(jw0 )H(jw0 ) = -'TT. At this frequency, the gain mar- gin in decibels can be identified by inspection of Figure 11.27. We first examine Fig- ure 11.27(b) to determine the frequency w1 at which the angle curve crosses the line -7r radians. Locating the point at this same frequency in Figure 11.27(a) provides us with the value of IG(JwJ)H(jwl)l. For eq. (11.101) to be satisfied for wo = w1, K must equall/IG(Jw 1) H(jw 1) I. This value is the gain margin. As illustrated in Figure 11.27(a), the gain margin expressed in decibels can be identified as the amount the log-magnitude curve woulq have to be shifted up so that the curve intersects the 0-dB line at the fre- quency w 1• In a similar fashion, we can determine the phase margin. Note first that the only frequency at which eq. (11.102) can be satisfied is that for which IG(jw0)H(jw0 )1 = 1, or equivalently, 20 log10 IG(jw0)H(jw0 )1 = 0. To determine the phase margin, we first find the frequency w 2 in Figure 11.27(a) at which the log-magnitude curve crosses the 0-dB line. Locating the point at this same frequency in Figure 11.27(b) then provides us with the value of -9:G (iw 2 )H (iw2). For eq. (11.102) to be satisfied for w0 = w 2 , the angle of the left-hand side of this equation must be -'TT. The value of 4> for which this is true is the phase margin. As illustrated in Figure 11.27(b), the phase margin can be identified as the amount the angle curve would have to be lowered so that the curve intersects the line -7r at the frequency w2. Sec. 11.5 Gain and Phase Margins 861 40 3 20 I 3 OdB (9 0 Oi -20 ..Q 0 N -40 -60 0.1 1.0 (J) 10.0 7T 2 0 3 I 3 7T 2 (9 \:f -'IT -----------~--------- (1)2 (J)\! -237T 0.1 1.0 (J) 10.0 Figure 11 .27 Use of Bode plots to calculate gain and phase margins for the system of Example 11.9. In determining gain and phase margins, it is not always of interest to identify ex- plicitly the frequency at which the poles will cross the jw-axis. As an alternative, we can identify the gain and phase margins from a log magnitude-phase diagram. For example, the log magnitude-phase diagram for the system of Figure 11.27 is shown in Figure 11.28. In this figure, we plot 20 log 10 /G(jw)H(jw)/ versus <r.G(jw)H(jw) as w varies from 0 to +oo. Therefore, because of the conjugate symmetry of G(jw )H(jw ), the plot contains the same information as the Nyquist plot, in which (Jlc{G(jw )H(jw )} is plotted versus dm{G(jw )H(jw )} for -oo < w < oo. As we have indicated, the phase margin can be read off by locating the intersection of the log magnitude-phase plot with the 0-dB line. That is, the phase margin is the amount of additional negative phase shift required to shift the log magnitude-phase curve so that it intersects the 0-dB line with exactly 180° ( 7r ra- dians) of phase shift. Similarly, the gain margin is directly obtained from the intersec- tion of the log magnitude-phase curve with the line -7r radians, and this represents the amount of additional gain needed so that the curve crosses the line -7r with a magnitude ofOdB. The following examples provide several other elementary illustrations of log mag- nitude-phase diagrams: 862 Linear Feedback Systems Chap. 11 40 20 3 I OdB 3 (9 0 o; -20 .2 0 N -40 -60 -21T 31T -1T 1T 0 2 2 Figure 11 .28 Log magnitude- phase plot for the system of Exam- 1:G(jw)H(jw) ple 11.9. Example 11. 1 0 Let G(s)H(s) = TS + , T > 0. (11.104) 1 20 Phase margin 3 I 0 dB 3 (9 0 o; -20 _Q 0 N -40 1:G(jw)H(jw) Figure 11 .29 Log magnitude-phase plot for the first-order system of Ex- ample 11.10. Sec. 11.5 Gain and Phase Margins 863 In this case, we obtain the log magnitude-phase plot depicted in Figure 11.29. This has a phase margin of 71', and since the curve does not intersect the line -71', the system has infinite gain margin (i.e., we can increase the gain as much as we like and maintain stability). This is consistent with the conclusion that we can draw by examining the system illustrated in Figure 11.30( a). In Figure 11.30(b ), we have depicted the root locus for this system with 4> = 0 and K > 0. From the figure, it is evident that the system is stable for any positive value of K. In addition, if K = 1 and 4> = 71', so that ei¢ = -1, the closed-loop system function for the system of Figure 11.30(a) is l!TS, which has a pole at s = 0, so that the system is unstable. + 1 x(t)~ + , -- __, TS+1 y(t) - e-i<P (a) T Cfl.e (b) Figure 11 .30 (a) First-order feedback system with possible gain and phase variations in the feedback path; (b) root locus for this system with 1> = 0, K > 0. Example 1 1 . 1 1 Suppose we now consider the second-order system 1 H(s) = 2 1, G(s) = 1. (11.105) s + s + The system H (s) has an undamped natural frequency of 1 and a damping ratio of 0.5. The log magnitude-phase plot for this system is illustrated in Figure 11.31. Again we have in- finite gain margin, but a phase margin of only 7T/2, since it can be shown by a straightfor- ward calculation that jH(jw )j = 1 for w = 1, and at this frequency <r.H(jw) = -71'/2. We can now illustrate the type of problem that can be solved using the concepts of gain and phase margins. Suppose that the feedback system specified by eq. (11.105) cannot be realized. Rather, some unavoidable time delay is introduced into the feedback 864 Linear Feedback Systems Chap. 11 path. That is, (11.106) where T is the time delay. What we would like to know is how small this delay must be to ensure the stability of the closed-loop system. 20 I OdB ------------------,I..-__- --_--_-~ ~-----1 3 1 0 o; -20 .Q 0 N -40 -60 L-------L-------~------~---~ -21T 31T 0 2 <tG(jw)H(jw) Figure 11.31 Log magnitude-phase plot for the second-order system of Example 11.11. The first point to note is that (11.107) so the delay does not change the magnitude of H(jw )G(jw ). On the other hand, <r.e-iWT = -wrradians. (11.108) Thus, every point on the curve in Figure 11.31 is shifted to the left. The amount of the shift is proportional to the value of w for each point on the log magnitude-phase curve. From this discussion, we see that instability will occur once the phase margin is reduced to zero, and this will happen when the phase shift introduced by the delay is equal to - 'TT'/2 at w = 1. That is, the critical value r* of the time delay satisfies (11.109) or (assuming that the units of w are radians/second) r* = 1.57 seconds. (11.110) Thus, for any time delay T < r*, the system remains stable. Example 1 1 . 1 2 Consider again the acoustic feedback system discussed in Section 11.2.6 and Exam- ple 11.7. Here, we assume that the system of Figure 11.8 has been designed with K 1 K2 < 1, so that the closed-loop system is stable. In this case, the log magnitude-phase plot for Sec. 11.5 Gain and Phase Margins 865 40.------,~-------,~-------~~------~.-----~ 20 r- - Gain margin 1 ~ 0 dB r---------------!-------------- :------ -- .s2_ 20 log10 (K 1 K 2)-r-------....,.~------.... 0 Oi :I ""' w=O .Q -201- - 0 C\J I -40~----~1------~1 -------~~------l~----~ -4TI -3TI -2TI -'IT 0 <tG(jw)H(jw) Figure 11 .32 Log magnitude-phase plot for Example 11.12. G(s)H(s) = K 1K2e-(sT+ j7T) is illustrated in Figure 11.32. From the figure, we see that the system has infinite phase margin and a gain margin in decibels of -20 log 10 (K1K2) (i.e., this is precisely the gain factor that, when multiplied by K 1K2, equals 1). As indicated at the start of the section, the definitions of the gain and phase margin are the same for discrete-time feedback systems as for continuous-time systems. Specifi- cally, if we have a stable discrete-time feedback system, the gain margin is the minimum amount of additional gain required in the feedback system such that the closed-loop sys- tem becomes unstable. Similarly, the phase margin is the minimum amount of additional negative phase shift required for the feedback system to be unstable. The following ex- ample illustrates the graphical calculation of phase and gain margins f~r a discrete-time feedback system; the procedure is essentially the same as for continuous-time systems. Example 1 1 . 1 3 In this example, we illustrate the concept of gain and phase margin for the discrete-time feedback system shown in Figure 11.33. Here, 7 J2 -1 2 G(z)H(z) = 4 , (11.111) 1 - -7 J-2z - I + 49 z -2 8 64 and by direct calculation we can check that the feedback system is stable for K = 1 and cf> = 0. In Figure 11.34, we have displayed the log magnitude-phase diagram for + 1 x[n] ~ + 1_ 7V2 + 49 -2 y[n] -1 8 2 64 2 7BV2z - 1 -EJ- K ~ Figure 11.33 Discrete-time feedback system of Example 11.13. 866 Linear Feedback Systems Chap. 11 40~----------~----------~------------~----------~ 3 -~ 20 I 3 -~ (!) 0 Oi Phase margin = 0.0685 rad ..Q _f ____________________ _ o OdB ----------------------- C\1 Gain margin = 1.68dB -20~----------~----------~------------~----------~ -2'TT 0 Figure 11 .34 Log magnitude-phase diagram for the discrete-time feedback system of Example 11.13. the system; that is, we have plotted 20 log 10 IG(ejw)H(ejw)l versus <t.G(ejw)H(ejw) as w varies from 0 to 27T. The system has a gain margin of 1.68 dB and a phase margin of 0.0685 radians (3.93°). In concluding this section, it should be stressed that the gain margin is the minimum value of gain that moves one or more of the closed-loop poles onto the jw-axis in continu- ous time or the unit circle in discrete time and, consequently, causes the system to become unstable. It is important to note, however, that this does not imply that the system is un- stable for all values of gain above the value specified by the gain margin. For example, as illustrated in Problem 11.47, asK increases, the root locus may move from the left-half plane into the right-half plane and then cross back into the left-half plane. The gain margin provides us with the information about how much the gain can be increased until the poles first reach the jw-axis, but it tells us nothing about the possibility that the system may again be stable for even larger values of the gain. To obtain such information, we must either refer to the root locus or use the Nyquist stability criterion. (See Problem 11.47.).4 11 .6 SUMMARY In this chapter, we have examined a number of the applications and several techniques for the analysis of feedback systems. We have seen how the use of Laplace and z-transforms allows us to analyze these systems algebraically and graphically. In Section 11.2 we in- dicated several of the applications of feedback, including the design of inverse systems, 4For detailed discussions of this point and also of gain and phase margins and log magnitude-phase diagrams in general, see the texts on feedback listed in the bibliography at the end of the book. Chap. 11 Problems 867 the stabilization of unstable systems, and the design of tracking systems. We also saw that feedback can destabilize, as well as stabilize, a system. In Section 11.3, we described the root-locus method for plotting the poles of the closed-loop system as a function of a gain parameter. Here, we found that the geometric evaluation of the phase of a rational Laplace transform or z-transform allowed us to gain a significant amount of insight into the properties of the root locus. These properties of- ten permit us to obtain a reasonably accurate sketch of the root locus without performing complex calculations. In contrast to the root-locus method, the Nyquist criterion of Section 11.4 is a tech- nique for determining the stability of a feedback system, again as a function of a variable gain, without obtaining a detailed description of the location of the closed-loop poles. The Nyquist criterion is applicable to nonrational system functions and thus can be used when all that is available are experimentally determined frequency responses. The same is true of the gain and phase margins described in Section 11.5. These quantities provide a measure of the margin of stability in a feedback system and therefore are of importance to design- ers in that they allow them to determine how robust a feedback system is to discrepancies between estimates of the forward- and feedback-path system functions and their actual values. The first section of problems belongs to the basic category, and the answers are pro- vided in the back of the book. The remaining three sections contain problems belonging to the basic, advanced, and extension categories, respectively. BASIC PROBLEMS WITH ANSWERS 11.1. Consider the interconnection of discrete-time LTI systems shown in Figure P11.1. Express the overall system function for this interconnection in terms of H0(z), H1 (z), and G(z). H0(z) + + 1 x[n] + H1(z) + y[n] - I G(z) Figure P11.1 868 Linear Feedback Systems Chap. 11 11.2. Consider the interconnection of discrete-time LTI systems shown in Figure P11.2. Express the overall system function for this interconnection in terms of H 1( s), H2(s), G1 (s), and G2(s). + + x(t) + H2(s) + H1(s) y(t) - - G1(s) G2(s) Figure P11.2 11.3. Consider the continuous-time feedback system depicted in Figure 11.3(a) with 1 H(s) = s- 1 and G(s) = s- b. For what real values of b is the feedback system stable? 11.4. A causal LTI system S with input x(t) and output y(t) is represented by the differ- ential equation S is to be implemented using the feedback configuration of Figure 11.3(a) with H(s) = 1/(s + 1). Determine G(s). 11.5. Consider the discrete-time feedback system depicted in Figure 11.3(b) with 1 H(z) = 1 - .!.z-1 and G(z) = 1- bz- 1 • 2 For what real values of b is the feedback system stable? 11.6. Consider the discrete-time feedback system depicted in Figure 11.3(b) with . -1 H(z) 1- z-N z = and G(z) = 1- z-N· Is this system IIR or FIR? 11.7. Suppose the closed-loop poles of a feedback system satisfy 1 1 (s + 2)(s + 3) K' Use the root-locus method to determine the values of K for which the feedback system is guaranteed to be stable. Chap. 11 Problems 869 11.8. Suppose the closed-loop poles of a feedback system satisfy s- 1 1 (s + l)(s + 2) K"" Use the root-locus method to determine the negative values of K for which the feedback system is guaranteed to be stable. 11.9. Suppose the closed-loop poles of a feedback system satisfy (s + l)(s + 3) 1 (s + 2)(s + 4) K"" Use the root-locus method to determine whether there are any values of the ad- justable gain K for which the system's impulse response has an oscillatory compo- nent of the form e-at cos( w 0t + cf> ), where w 0 ~ 0. 11.10. The root locus corresponding to G(s)H(s) = -1/K is illustrated in Figure Pl1.10. In this figure, the start (K = 0) and end of each branch of the root locus are marked by a 'e' symbol. Specify the poles and zeros of G(s)H(s). -1 <Re -j K<O -1 CRe Figure P11.1 0 870 Linear Feedback Systems Chap. 11 11.11. Suppose the closed-loop poles of a discrete-time feedback system satisfy Using the root-locus method, determine the positive values of K for which this system is stable. 11.12. Each of the four locations z = 1/2, z = 1/4, z = 0, and z = - 112 is a single-order pole or zero of G(z)H(z). Furthermore, G(z)H(z) is known to have only two poles. What information can you deduce about the poles and zeros of G(z)H(z) from the fact that for all K, the root locus corresponding to G(z)H(z) = K is on the real axis. 11.13. Consider the block diagram of Figure P 11.13 for a discrete-time system. Use the root-locus method to determine the values of K for which the system is guaranteed to be stable. x[n+] ~ + y[n] D 1 + D 4 K Figure P11.13 11.14. Let C be a closed path that lies on the unit circle in the p-plane and that is traversed in the clockwise direction in order to evaluate W(p). For each of the following expressions for W(p), determine the net number of times the plot of W(p) encircles the origin in a clockwise direction: (1-kp- 1 ) (a) W(p) = o-±p-1> b (l-2p-l) ( ) W(p) = (t-~p-IJ0-2p-1+4p-2J Chap. 11 Problems 871 11.15. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 1 G(s)H(s) = (s + l) = - K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. Hint: In sketching the Nyquist plot, you may find it useful to sketch the corresponding Bode plot first. It also is helpful to determine the values of w for which G(jw )H(jw) is real. 11.16. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 G(s)H(s) = (s + 1)(s/10 + 1) K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.17. Consider a continuous-time feedback system whose closed-loop poles satisfy 1 G(s)H(s) = (s + 1)4 K"" Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.18. Consider a discrete-time feedback system whose closed-loop poles satisfy -3 1 G(z)H(z) = z = - K. Use the Nyquist plot and the Nyquist stability criterion to determine the range of values of K for which the closed-loop system is stable. 11.19. Consider a feedback system, either in continuous-time or discrete-time, and sup- pose that the Nyquist plot for the system passes through the point - 1/ K. Is the feedback system stable or unstable for this value of the gain? Explain your answer. 11.20. Consider the basic continuous-time feedback system of Figure 11.3(a). Determine the phase and gain margin for the following specification of H(s) and G(s): s + 1 H(s) = , G(s) = 1. s 2 + s + 1 BASIC PROBLEMS 11.21. Consider the feedback system of Figure P11.21. Find the closed-loop poles and zeros of this system for the following values of K: (i) K = 0.1 (ii) K = 1 (iii) K = 10 (iv) K = 100 872 Linear Feedback Systems Chap. 11 + x(t) --~~~ 1----.....- -~ y(t) s+1 s+100 Figure P11.21 11.22. Consider the basic feedback system of Figure 11.3(a). Determine the closed-loop system impulse response for each of the following specifications of the system functions in the forward and feedback paths: (a) H(s) = (s+ l)l(s+ 3), G(s) = 1 (b) H(s) = s~ 3 ,G(s) = s~l (c) H(s) = 4, G(s) = e-s/3 11.23. Consider the basic feedback systems of Figure 11.3(b). Determine the closed-loop system impulse response for each of the following specifications of the system functions in the forward and feedback paths: 1 ( a ) H( ) _ z- G( ) _ 2 I -1 z - l-~z- 1 , z - 3 - 6z (b) H(z) = ~ - ~z- 1 , G(z) = 1 _z~~-~ 2 11.24. Sketch the root loci for K > 0 and K < 0 for each of the following: (a) G(s)H(s) = s~ 1 (b) G(s)H(s) = (s-l)l(s+ 3) 1 (c) G(s)H(s) = s2 +s+ 1 (d) G(s)H(s) = s.~ 1 2 (e) G(s)H(s) = (s:}> (f) G(s)H(s) = s2+2s+2 s2(s-l) (g) G(s)H(s) = (s+ l)(s-l) s(s2+2s+2) (h) G(s)H(s) = (s;~~;·~ 3 ) 11.25. Sketch the root loci for K > 0 and K < 0 for each of the following: (a) G(z)H(z) = ~- 1 z 1 -4 (b) G(z)H(z) = 2 zL~ (c) G(z)H(z) = z-.~~~tz~~~) 4 (d) G(z)H(z) = z- 1 - z-2 (e) G(z)H(z) is the system function of the causal LTI system described by the difference equation y[n] - 2y[n - 1] = x[n - 1] - x[n - 2]. Chap. 11 Problems 873 11.26. Consider a feedback system with G(s)H(s) = (s- a)(s- b) s(s + 3)(s + 6) Sketch the root locus for K > 0 and K < 0 for the following values of a and b: (a) a = 1, b = 2 (b) a = -2, b = 2 (c) a = -4, b = 2 (d) a = -7, b = 2 (e) a = -1, b = -2 (f) a = -4, b = -2 (g) a = -7, b = -2 (h) a = -5, b = -4 (i) a = -7, b = -4 (j) a= -7, b = -8 11.27. Consider a feedback system with s+2 H(s) = , G(s) = K. s 2 + 2 s + 4 (a) Sketch the root locus forK> 0. (b) Sketch the root locus forK < 0. (c) Find the smallest positive value of K for which the closed-loop impulse re- sponse does not exhibit any oscillatory behavior. 11.28. Sketch the Nyquist plot for each of the following specifications of G(s)H(s), and use the continuous-time Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Note: In sketch- ing the Nyquist plots, you may find it useful to sketch the corresponding Bode plots first. It also is helpful to determine the values of w for which G(jw )H(jw) is real. (a) G(s)H(s) = s~ I (b) G(s)H(s) = s2 ~ I (c) G(s)H(s) = (s] 1)2 (d) G(s)H(s) = (s] 03 (e) G(s)H(s) = (}+-t)2 (f) G(s)H(s) = c:~/)2 (g) G(s)H(s) = ;;~~ (h) G(s)H(s) = s2+~s+2 (i) G(s)H(s) = ~ (j) G(s)H(s) = s+ I s2-2s+2 (s+IOO)(s-I)2 s2 (k) G(s)H(s) = (s+ I)3 11.29. Consider the basic continuous-time feedback system of Figure 11.3(a). Sketch the log magnitude-phase diagram, and roughly determine the phase and gain margin, for each of the following choices of G(s) and H(s). You may find it useful to use the straight- line anproximations to the Bode plots developed in Chapter 6 to aid you in sketching the log magnitude-phase diagrams. Be careful, however, to take into account how the actual frequency response deviates from its approximation near break frequencies when there are underdamped second-order terms present. (See Section 6.5.2.) (a) H(s) = 2lOs+ I G(s) = 1 s +s+ I? (b) H(s) = s2ilO+ I G(s) = 1 s +s+ I' (c) H(s) = (s+I/(s+IO)' G(s) = 100 (d) H(s) = (s-!1)3, G(s) = s~ I (e) H(s) = (s+ I\;~~ IO), G(s) = 1 874 Linear Feedback Systems Chap. 11 (f) H(s) = 1-s/100 G(s) = lOs+ 1 (s+J)2' s/10+1 (g) H(s) = - 1 - G(s) = - 1 s(s+ I)' s+l Note: Your sketch for part (g) should reflect the fact that for this feedback system IG(jw )H(jw )I ~ oo as w ~ 0; what is the phase of G(jw )H(jw) for w = o+' i.e., for w an infinitesimal amount larger than 0? 11.30. Sketch the Nyquist plot for each of the following specifications of G(z)H(z), and use the discrete-time Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. [Note: In sketch- ing the Nyquist plots, you may find it useful to first sketch the magnitude and phase plots as a function of frequency or at least calculate IG(efw)H(efw)l and <r.G(efw)H(efw) at several points. Also, it is helpful to determine the values of w for which G( efw )H ( efw) is real.] (a) G(z)H(z) = z~l (b) G(z)H(z) = 2 ~ 2 2 (c) G(z)H(z) = z- 1 (d) G(z)H(z) = z- 2 1 (e) G(z)H(z) = (z+ ~ ) ~ (f) G( )H( ) - z-/3 (z- > Z Z - z(z+ 1/,/3) (g) G(z)H(z) = 22_~+~ (h) G(z)H(z) = z- ~ z(z-2) 2 (i) G(z)H(z) = c=:} > 11.31. Consider the basic discrete-time system in Figure 11.3(b ). Sketch the log magnitude- phase diagram, and roughly determine the phase and gain margin, for each of the following choices of G(z) and H(z). You may find it useful to determine the values of w for which either IG(efw)H(efw)l = 1 or <r.G(efw) = -7T. (a) H(z) = z- 1, G(z) = ~ 7-1 I (b) H(z) = 1 _~lz 1 , G(z) = 2 2 (c) H(z) = (l-~:::- 1 )1( 1 +h-l)' 2 G(z) = z- (d) H(z) = :::_: 2: G(z) ~ 1 (e) H(z) = -1-~ , G(z) = z+ 2 ~7 ~ l 2 (f) H(z) = ~ 1 ,.+', G(z) = 1 - ~z- -I (g) H(z) = _2_:+l'G(z) = 1 ,, ~ 3 (h) H(z) = 2~ 1 ,G(z) = iz- 1 (Note: Your sketch for part (h) should reflect the fact that, for this feedback system, G(z)H(z) has a pole at z = 1; what are the values of <r.G(efw)H(efw) for eiw just on either side of the point z = 1? ) ADVANCED PROBLEMS 11.32. (a) Consider the feedback system of Figure 11.1 O(b) with H() = N1(s) (P11.32-1) s D1 (s)' Chap. 11 Problems 875 Assume that there is no pole-zero cancellation in the product G(s)H(s). Show that the zeros of the closed-loop system function consist of the zeros of H(s) and the poles of G(s). (b) Use the result of part (a) together with the appropriate property of the root locus to confirm that, with K = 0, the closed-loop system zeros are the zeros of H(s) and the closed-loop poles are the poles of H(s). (c) While it is usual for H(s) and G(s) in eq. (P11.32-1) to be in reduced form [i.e., the polynomials N 1( s) and D 1( s) have no common factors, and the same is true of N2(s) and D2(s)], it may happen that N 1( s) and D2(s) have common factors or N2(s) and D 1( s) have common factors. To see what occurs when such common factors are present, let p(s) denote the greatest common factor of N2(s) and D 1( s). That is, NJ(S) and p(s) are both polynomials and have no common factors. Similarly, DJ(S) and q(s) are polynomials and have no common factors. Show that the closed-loop sys- tem function can be written as Q(s) = p(s) [ H__<s) "" ]· (P11.32-2) q(s) 1 + KG(s)H(s) where H(s) = N1 (s)/ p(s) D1 (s)/q(s) and G(s) = N2(s)/q(s). D2(s)/p(s) Therefore, from eq. (P11.32-2) and part (a), we see that the zeros of Q(s) are the zeros of p(s), the zeros of H(s), and the poles of G(s), while the poles of Q(s) are the zeros of q(s) and the solutions of 1 + KG(s)H(s) = 0. (P11.32-3) By construction, there is no pole-zero cancellation in the product G(s)H(s), and thus, we can apply the root-locus method described in Section 11.3 to sketch the locations of the solutions of eq. (P11.32-3) asK is varied. (d) Use the procedure outlined in part (c) to determine the closed-loop zeros, any closed-loop poles whose locations are independent of K, and the locus of the remaining closed-loop poles for K > 0 when s + 1 s+2 H(s) = (s + 4)(s + 2)' G(s) = s +I' 876 Linear Feedback Systems Chap. 11 (e) Repeat part (d) for 1 + z- 1 z-I H(z) = 1 - lz-I, G(z) = 1 + z-I. 2 (f) Let z2 1 H(z) = (z- 2)(z + 2)' G(z) = 2 . z (i) Sketch the root locus for K > 0 and for K < 0. (ii) Find all the values of K for which the overall system is stable. (iii) Find the impulse response of the closed-loop system when K = 4. 11.33. Consider the feedback system of Figure 11.10(a), and suppose that m ncs -/h) G(s)H(s) = _k:-1 -- n(s- ak) k=I where m > n.5 In this case G(s)H(s) has m- n poles at infinity (see Chapter 9), and we can adapt the root-locus rules given in the text by noting that (1) there are m branches of the root locus and (2) forK = 0, all branches of the root locus begin at poles of G(s)H(s), m- n of which are at infinity. Furthermore, as IKI ~ 00, these branches converge to them zeros of G(s)H(s), namely, {3 1, {32, .•• , f3m· Use these facts to assist you in sketching the root locus (for K > 0 and for K < 0) for each of the following: (a) G(s)H(s) = s - 1 (b) G(s)H(s) = (s + l)(s + 2) (c) G(s)H(s) = 2 (s+ ~~~+ ) 11.34. In Section 11.3, we derived a number of properties that can be of value in deter- mining the root locus for a feedback system. In this problem, we develop several additional properties. We derive these properties in terms of continuous-time sys- tems, but, as with ali root-locus properties, they hold as well for discrete-time root loci. For our discussion of these properties, we refer to the basic equation satisfied by the closed-loop poles, namely, 1 G(s)H(s) = (Pl1.34-1) K' 5Note that for a continuous-time system, the condition m > n implies that the system with system func- tion G(s)H(s) involves differentiation of the input. [In fact, the inverse transform of G(s)H(s) includes singu- larity functions up to the order m - n.] In discrete time, if G(z)H (z), written as a ratio of polynomials in z, has m > n, it is necessarily the system function of a noncausal system. [In fact, the inverse transform of G(z)H(z) has a nonzero value at time n- m < 0.] Thus, the case considered in this problem is actually of interest only for continuous-time systems. Chap. 11 Problems 877 where m n<s- {3d G(s)H(s) = _k:-1 -- (P11.34-2) n(s- ak) k=l s Figure P11.34 Throughout this problem, we assume that m ~ n. (a) From Property 2, we know that n- m branches of the root locus go to ze- ros of G(s)H(s) located at infinity. In this first part, we demonstrate that it is straightforward to determine the angles at which these branches approach in- finity. Specifically, consider searching the remote part of the s-plane [i.e., the region where lsi is extremely large and far from any of the poles and zeros of G(s)H(s)]. This region is illustrated in Figure Pll.34. Use the geometry of the picture, together with the angle criterion for K > 0 and for K < 0, to deduce that: • For K > 0, the n - m branches of the root locus that approach infinity do so at the angles (2k + 1)7T k = 0, 1, ... , n - m - 1. n-m 878 Linear Feedback Systems Chap. 11 • For K < 0, the n - m branches of the root locus that approach infinity do so at the angles 2k1T' k = 0, 1, ... , n - m - 1. n- m' Thus, the branches of the root locus that approach infinity do so at specified angles that are arranged symmetrically. For example, for n - m = 3 and K > 0, we see that the asymptotic angles are 1T'I3, 1T', and 57T'/3. The result of part (a), together with one additional fact, allows us to draw in the asymptotes for the branches of the root locus that approach infinity. Specifically, all of the n - m asymptotes intersect at a single point on the real axis. This is derived in the next part of the problem. (b) (i) As a first step, consider a general polynomial equation sr+fr-tSr-l +···+fo = (s-gt)(s-6)···(s-gr) = 0. Show that r J,--1 - Lgi· i= I (ii) Perform long division on 1/G(s)H(s) to write 1 n-m n-m-1 G(s)H(s) = S + 'Yn-m-1 S + . . . . (Pll.34-3) Show that Ill II 'Yn-m-1 =an-I- bm-1 = Lf3k- LCik. k= I k= I [See eq. (Pll.34-2).] (iii) Argue that the solution of eq. (Pll.34-l) for large sis an approximate solution of the equation sn-m + 'Yn-m-ISn-m-1 + 'Yn-m-2Sn-m-2 + ... +'Yo+ K = 0. (iv) Use the results of (i)-(iii) to deduce that the sum of then- m closed-loop poles that approach infinity is asymptotically equal to Thus, the center of gravity of these n - m poles is n- m which does not depend on K. Consequently, we haven- m closed-loop poles that approach lsi = oo at evenly spaced angles and that have a center of gravity that is independent of K. From this, we can deduce that: Chap. 11 Problems 879 The asymptotes of the n- m branches of the root locus that approach infinity intersect at the point n m Lak- Lf3k bm-1 -an-I k=l k=l n-m n-m This point of intersection of the asymptotes is the same for K > 0 and K<O. (c) Suppose that 1 G(s)H(s) = (s + 1)(s + 3)(s + 5) (i) What are the asymptotic angles for the closed-loop poles that approach infinity for K > 0 and for K < 0? (ii) What is the point of intersection of the asymptotes? (iii) Draw in the asymptotes, and use them to help you sketch the root locus forK> 0 and forK< 0. (d) Repeat part (c) for each of the following: (i) G(s)H(s) = s(s~~2~(~+4) (ii) G(s)H(s) = I 7 (iii) G(s)H(s) = s(s+ I )(s+5)(s+6) (iv) G(s)H(s) = 1 (s+2)2(s-1)2 (v) G(s)H(s) = s+3 (s+ I )(s2 + 2s+ 2) (vi) G(s)H(s) = s+l (s+ 2)2(s2 + 2s+ 2) (vii) G(s)H(s) = (s+ 100{c;~ l)(s- 2) (e) Use the result of part (a) to explain why the following statement is true: For any continuous-time feedback system, with G(s)H(s) given by eq. (P11.34- 2), if n- m ~ 3, we can make the closed-loop system unstable by choosing IKilarge enough. (f) Repeat part (c) for the discrete-time feedback system specified by z-3 G(z)H(z) = . (1- z- 1)(1 + ~z- 1 ) (g) Explain why the following statement is true: For any discrete-time feedback system with G(z)H(z) = zm + bm-IZm-1 + ... + bo' zn + an-IZn-l + ' .. + ao if n > m, we can make the closed-loop system unstable by choosing IKilarge enough. 11.35. (a) Consider again the feedback system of Example 11.2: s- 1 G(s)H(s) = (s + 1)(s + 2) 880 Linear Feedback Systems Chap. 11 The root locus forK< 0 is plotted in Figure 11.14(b). For some value of K, the closed-loop poles are on the jw-axis. Determine this value of K and the corresponding locations of the closed-loop poles by examining the real and imaginary parts of the equation 1 G(jw)H(jw) = K' which must be satisfied if the point s = jw is on the root locus for any given values of K. Use this result plus the analysis in Example 11.2 to find the full range of values of K (positive and negative) for which the closed-loop system is stable. (b) Note that the feedback system is unstable for IKI sufficiently large. Explain why this is true in general for continuous-time feedback systems for which G(s)H(s) has a zero in the right-half plane and for discrete-time feedback sys- tems for which G(z)H(z) has a zero outside the unit circle. 11.36. Consider a continuous-time feedback system with 1 G(s)H(s) = s(s + 1)(s + 2) (P11.36-1) (a) Sketch the root locus for K > 0 and for K < 0. (Hint: The results of Problem 11.34 are useful here.) (b) If you have sketched the locus correctly, you will see that for K > 0, two branches of the root locus cross the jw-axis, passing from the left-half plane into the right-half plane. Consequently, we can conclude that the closed-loop system is stable for 0 < K < K0 , where K0 is the value of the gain for which the two branches of the root locus intersect the jw-axis. Note that the sketch of the root locus does not by itself tell us what the value of K0 is or the exact point on the jw-axis where the branches cross. As in Problem 11.35, deter- mine Ko by solving the pair of equations obtained as the real and imaginary parts of G(jw )H(jw) = - ; . (P11.36-2) 0 Determine the corresponding two values of w (which are the negatives of each other, since poles occur in complex-conjugate pairs). From your root-locus sketches in part (a), note that there is a segment of the real axis between two poles which is on the root locus for K > 0, and a different segment is on the locus for K < 0. In both cases, the root locus breaks off from the real axis at some point. In the next part of this problem, we illustrate how one can calculate these breakaway points. (c) Consider the equation denoting the closed-loop poles: 1 G(s)H(s) = (P11.36-3) K"" Chap. 11 Problems 881 s '~I I I I : ~ I p(s) (a) ~p(s) IC2S==t', s I ' I (b) ' Figure P11.36 Using eq. (P11.36-1), show that an equivalent equation for the closed loop poles is p(s) = s3 + 3s2 + 2s = - K. (P11.36-4) Consider the segment of the real axis between 0 and - 1. This segment is on the root locus forK ~ 0. ForK = 0, two branches of the locus begin at 0 and - 1 and approach each other as K is increased. (i) Use the facts stated, together with eq. (P11.36-4), to explain why the function p(s) has the form shown in Figure P11.36(a) for -1 ~ s ~ 0 and why the point s + where the minimum occurs is the breakaway point (i.e., it is the point where the two branches of the K > 0 locus break from the segment of the real axis between -1 and 0). Similarly, consider the root locus for K < 0 and, more specifically, the segment of the real axis between -1 and -2 that is part of this locus. ForK = 0, two branches of the root locus begin at -1 and -2, and asK is decreased, these poles approach each other. (ii) In an analogous fashion to that used in part (i), explain why the function p(s) has the form shown in Figure P11.36(b) and why the points- where the maximum occurs is the breakaway point for K < 0. Thus, the breakaway points correspond to the the maxima and min- ima of p(s) as s ranges over the negative real line. (iii) The points at which p(s) has a maximum or minimum are the solutions of the equation dp(s) = 0 ds · Use this fact to find the breakaway points s+ and s-, and then use eq. (P11.36-4) to find the gains at which these points are closed-loop poles. In addition to the method illustrated in part (c), there are other, partially analytical, partially graphical methods for determining breakaway points. It is also possible to use a procedure similar to the one just illustrated in part (c) to find the 882 Linear Feedback Systems Chap. 11 ""break-in"" points, where two branches of the root locus merge onto the real axis. These methods plus the one illustrated are described in advanced texts such as those listed in the bibliography at the end of the book. 11.37. One issue that must always be taken into account by the system designer is the possible effect of unmodeled aspects of the system one is attempting to stabilize or modify through feedback. In this problem, we provide an illustration of why this is the case. Consider a continuous-time feedback system, and suppose that 1 H(s) = (P11.37-1) (s + 10)(s- 2) and G(s) = K. (P11.37-2) (a) Use root-locus techniques to show that the closed-loop system will be stable if K is chosen large enough. (b) Suppose that the system we are trying to stabilize by feedback actually has a system function 1 H(s) = + + . (P11.37-3) (s 10)(s- 2)(10-3s 1) The added factor can be thought of as representing a first-order system in cas- cade with the system of eq. (P11.37-1). Note that the time constant of the added first order system is extremely small and thus will appear to have a step response that is almost instantaneous. For this reason, one often neglects such factors in order to obtain simpler and more tractable models that capture all of the important characteristics of the system. However, one must still keep these neglected dynamics in mind in obtaining a useful feedback design. To see why this is the case, show that if G(s) is given by eq. (P11.37-2) and H(s) is as in eq. (P11.37-3), then the closed-loop system will be unstable if K is chosen too large. Hint: See Problem 11.34. (c) Use root -locus techniques to show that if G(s) = K(s + 100), then the feedback system will be stable for all values of K sufficiently large if H(s) is given by eq. (P11.37-1) or eq. (P11.37-3). 11.38. Consider the feedback system of Figure 11.3(b) with H(z) = Kz-1 1- z- 1 and G(z) = 1- az- 1• (a) Sketch the root locus for K > 0 and K < 0 when a = 112. (b) Repeat part (a) when a = -112. Chap. 11 Problems 883 (c) With a = -112, find a value of K for which the closed-loop impulse response is of the form (A+ Bn)an for some values of the constants A, B, and a, with Ia I < 1. (Hint: What must the denominator of the closed-loop system function look like in this case?) 11.39. Consider the feedback system of Figure P11.39 with 1 H(z) = _ l.z-I, G(z) = K. (P11.39-1) 1 2 e[n] + H(z) y[n] - G(z) Figure P11.39 (a) Plot the root locus forK > 0. (b) Plot the root locus forK < 0. (Note: Be careful with this root locus. By ap- plying the angle criterion on the real axis, you will find that asK is decreased from zero, the closed loop approaches z = +oo along the positive real axis and then returns along the negative real axis from z = -oo. Check that this is in fact the case by explicitly solving for the closed-loop pole as a function of K. At what value of K is the pole at lzl = oo?) (c) Find the full range of values of K for which the closed-loop system is stable. (d) The phenomenon observed in part (b) is a direct consequence of the fact that in this example the numerator and denominator of G(z)H(z) have the same degree. When this occurs in a discrete-time feedback system, it means that there is a delay-free loop in the system. That is, the output at a given point in time is being fed back into the system and in tum affects its own value at the same point in time. To see that this is the case in the system we are considering here, write the difference equation relating y[n] and e[n]. Then write e[n] in terms of the input and output for the feedback system. Contrast this result with that of the feedback system with 1 H(z) = G(z) = Kz- 1 _ , • (P11.39-2) 1- 1 z 1 2 The primary consequence of having delay-free loops is that such feed- back systems cannot be implemented in the form depicted. For example, for the system of eq. (P11.39-1), we cannot first calculate e[n] and then y[n], 884 Linear Feedback Systems Chap. 11 because e[n] depends on y[n]. Note that we can perform this type of calcula- tion for the system of eq. (P11.39-2), since e[n] depends on y[n- 1]. (e) Show that the feedback system of eq. (P11.39-1) represents a causal system, except for the value of K for which the closed-loop pole is at lzl = oo. 11.40. Consider the discrete-time feedback system depicted in Figure P11.40. The system in the forward path is not very well damped, and we would like to choose the feedback system function so as to improve the overall damping. By using the root- locus method, show that this can be done with G(z) = 1- 1 -1 2z . 1 + K(z-4) x[n] -,. + 7V2 + 49 ""' y[n] z2- z - 8 64 G(z) Figure P11.40 Specifically, sketch the root locus for K > 0, and specify the value of the gain K for which a significant improvement in damping is obtained. 11.41. (a) Consider a feedback system with H(z) = z + 1 , K z2 + z +.!. G(z) = z- 1"" 4 (i) Write the closed-loop system function explicitly as a ratio of two polyno- mials. (The denominator polynomial will have coefficients that depend onK.) (ii) Show that the sum of the closed-loop poles is independent of K. (b) More generally, consider a feedback system with system function Show that if m :s n- 2, the sum of the closed-loop poles is independent of K. 11.42. Consider again the discrete-time feedback system of Example 11.3: z G(z)H(z) = . (z- 21 )(z- 41 ) The root loci for K > 0 and K < 0 are depicted in Figure 11.16. Chap. 11 Problems 885 (a) Consider the root locus for K > 0. In this case, the system becomes unstable when one of the closed-loop poles is less than or equal to -1. Find the value of K for which z = -1 is a closed-loop pole. (b) Consider the root locus for K < 0. In this case, the system becomes unstable when one of the closed-loop poles is greater than or equal to 1. Find the value of K for which z = 1 is a closed-loop pole. (c) What is the full range of values of K for which the closed-loop system is stable? 11.43. Consider a discrete-time feedback system with 1 G(z)H(z) = z(z _ l) (a) Sketch the root locus forK> 0 and forK< 0. (b) If you have sketched the root locus correctly for K > 0, you will see that the two branches of the root locus cross and exit from the unit circle. Consequently, we can conclude that the closed-loop system is stable for 0 < K < K0 , where K 0 is the value of the gain for which the two branches intersect the unit circle. At what points on the unit circle do the branches exit from it? What is the value of Ko? 11.44. As mentioned in Section 11.4, the continuous-time Nyquist criterion can be ex- tended to allow for poles of G(s)H(s) on the jw-axis. In this problem, we will illustrate the general technique for doing this by means of several examples. Con- sider a continuous-time feedback system with 1 G(s)H(s) = s(s + l) (Pl1.44-l) When G(s )H( s) has a pole at s = 0, we modify the contour of Figure 11.19 by avoiding the origin. To do this, we indent the contour by adding a semicircle of infinitesimal radius E into the right-half plane. [See Figure Pll.44(a).] Thus, only a small part of the right-half plane is not enclosed by the modified contour, and its area goes to zero as we let E ~ 0. Consequently, as M ~ oo, the contour will enclose the entire right-half plane. As in the text, G(s )H (s) is a constant (in this case zero) along the circle of infinite radius. Thus, to plot G(s)H(s) along the contour, we need only plot it for the portion of the contour consisting of the jw-axis and the infinitesimal circle. (a) Show that 7T 2 and where s = jO- is the point where the infinitesimal semicircle meets the jw- axis just below the origin and s = jO+ is the corresponding point just above the origin. (b) Use the result of part (a) together with eq. (Pl1.44-l) to verify that Figure P11.44(b) is an accurate sketch of G(s)H(s) along the portions of the contour CRe (a) t w=o- w=±oo (b) Figure P11.44 886 Chap. 11 Problems 887 from - joo to jO- and jo+ to joo. In particular, check that <f._G(jw )H(jw) and IG(jw )H(jw )I behave in the manner depicted in the figure. (c) All that remains to be done is to determine the plot of G(s)H(s) along the small semicircle about s = 0. Note that as E ~ 0, the magnitude of G(s)H(s) along this contour goes to infinity. Show that as E ~ 0, the contribution of the pole at s = -1 to <f._G(s)H(s) along the semicircle is zero. Then show that as E ~ 0, <r._G(s)H(s) = -8, where 8 is as defined in Figure Pl1.44(a). Thus, since 8 varies from -Tr/2 at s = jO- to +Tr/2 at s = jO- in the counterclockwise direction, <f._G(s)H(s) must go from + Trl2 at s = jO+ to -Tr/2 at s = jO+ in the clockwise direction. The result is the complete Nyquist plot depicted in Figure P11.44(c). CRe (c) Figure P11.44 Continued 888 Linear Feedback Systems Chap. 11 (d) Using the Nyquist plot of Figure Pl1.44(c), find the range of values of K for which the closed-loop feedback system is stable. (Note: As presented in the text, the continuous-time Nyquist criterion states that, for closed-loop sys- tem stability, the net number of clockwise encirclements of the point -1/K must equal minus the net number of right-half plane poles of G(s)H(s). In the present example, note that the pole of G(s)H(s) at s = 0 is outside the modi- fied contour. Consequently, it is not included in counting the poles of G(s)H(s) in the right-half plane [i.e., only poles of G(s)H(s) strictly inside the right-half plane are counted in applying the Nyquist criterion]. Thus, in this case, since G(s)H(s) has no poles strictly inside the right-half plane, we must have no encirclements of the points = -11K for closed-loop system stability.) (e) Follow the steps outlined in parts (a)-(c) to sketch the Nyquist plots for each of the following: (i) G(s)H(s) = (.I/IO)+ 1 s(s+ I) (ii) G(s)H(s) = s(.l~ ) 1 2 (iii) G(s)H(s) = ~ [be careful in calculating 1:-G(s)H(s) along the infinites- imal semicircle] (iv) G(s)H(s) = .,f't-1n [be careful in calculating 1:-G(jw )H(jw) as w is var- ied; make sure to take the minus sign in the denominator into account] (v) G(s)H(s) = s~ 1 [same remark as for (iii)] s- In each case, use the Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Also, use another method (root locus or direct calculation of the closed-loop poles as a function of K) to provide a partial check of the correctness of your Nyquist plot. [Note: In sketching the Nyquist plots, you may find it useful to sketch the Bode plots of G(s)H(s) first. It may also be helpful to determine the values of w for which G(jw )H(jw) is real.] (f) Repeat part (e) for: (i) G(s)H(s) = .\ 2 ~ 1 (ii) G(s)H(s) = ;~:\ Note: In these cases there are trvo poles on the imaginary axis; accordingly, you will need to modify the contour of Figure 11.19 to avoid each of them. Use infinitesimal semicircles, as in Figure P11.44(a). 11.45. Consider a system with system function 1 H(s) = (P11.45-1) (s+1)(s-2) Because this system is unstable, we would like to devise some method for its sta- bilization. (a) Consider first a series compensation scheme as illustrated in Figure P11.45(a). Show that the overall system of this figure is stable if the system function s-2 C(s) = s + 3. In practice, this is not considered to be a particularly useful way to attempt to stabilize a system. Explain why. Chap. 11 Problems 889 x(t) ---t•~' C(s) H(s) 1---....,·~ y(t) (a) x(t) --~+ ~--~8--~._H_(_s)_:----+-~ y(t) (b) Fi~ure P 11 .45 (b) Suppose that instead we use a feedback system, as depicted in Figure P11.45(b ). Is it possible to stabilize this system using a constant gain, that is, C(s) = K, for the stabilizing element? Justify your answer using Nyquist techniques. (c) Show that the system of Figure P11.45(b) can be stabilized if C(s) is a propor- tional plus derivative system-that is, if C(s) = K(s + a). Consider both the case 0 < a < 1 and the case a > 1. (d) Suppose that C(s) = K(s + 2). Choose the value of K such that the closed-loop system has a pair of complex poles with a damping ratio { = 112. (Hint: In this case, the denominator of the closed-loop system must have the form s2 + w s + w 2 n n for some value of Wn > 0.) (e) Pure derivative compensation is both impossible to obtain and undesirable in practice. This is because the required amplification of arbitrarily high frequen- cies neither can be obtained nor is advisable, as all real systems are subject to some level of high-frequency disturbances. Thus, suppose that we consider a compensator of the form = ss ++ a) C(s) K ( b , a,b > 0. (P11.45-2) 890 Linear Feedback Systems Chap. 11 If b < a, this is a lag network: <:f_C(jw) < 0 for all w > 0, so that the phase of the output of the system lags the phase of the input. If b > a, <:f_C(jw) > 0 for all w > 0, and the system is then called a lead network. (i) Show that it is possible to stabilize the system with the lead compensator s+_!_ C(s) = K--2 (Pll.45-3) s+2 if K is chosen large enough. (ii) Show that it is not possible to stabilize the feedback system of Figure P11.45(b) using the lag network C(s) = Ks + 3 . s+2 Hint: Use the results of Problem 11.34 in sketching the root locus. Then determine the points on the jw-axis that are on the root locus and the values of K for which each of these points is a closed-loop pole. Use this information to prove that for no value of K are all of the closed-loop poles in the left-half plane. 11.46. Consider the continuous-time feedback system depicted in Figure P11.46(a). s+10 + y(t) (s+1)2 - 10 (s/1 00+1)2 (a) (s+ 1 O)e-sT x(t) -~~++ ~ y(t) (s+1)2 - 10 (s/100+1)2 (b) Figure P 11 .46 Chap. 11 Problems 891 (a) Use the straight-line approximations to Bode plots developed in Chapter 6 to obtain a sketch of the log magnitude-phase plot of this system. Estimate the phase and gain margins from your plot. (b) Suppose that there is an unknown delay within the feedback system, so that the actual feedback system is as shown in Figure P11.46(b ). Approximately what is the largest delay T that can be tolerated before the feedback system becomes unstable? Use your results from part (a) for this calculation. (c) Calculate more precise values of the phase and gain margins, and compare these to your results in part (a). This should give you some idea of the size of the errors that are incurred in using the approximate Bode plots. 11.47. As mentioned at the end of Section 11.5, the phase and gain margins may provide sufficient conditions to ensure that a stable feedback system remains stable. For example, we showed that a stable feedback system will remain stable as the gain is increased, until we reach a limit specified by the gain margin. This does not imply (a) that the feedback system cannot be made unstable by decreasing the gain or (b) that the system will be unstable for all values of gain greater than the gain margin limit. In this problem, we illustrate these two points. (a) Consider a continuous-time feedback system with 1 G(s)H(s) = (s- 1)(s + 2)(s + 3) Sketch the root locus for this system for K > 0. Use the properties of the root locus described in the text and in Problem 11.34 to help you draw the locus accurately. Once you do so, you should see that for small values of the gain K the system is unstable, for larger values of K the system is stable, while for still larger values of K the system again becomes unstable. Find the range of values of K for which the system is stable. Hint: Use the same method as is employed in Example 11.2 and Problem 11.35 to determine the values of K at which branches of the root locus pass through the origin and cross the jw-axis. If we set our gain somewhere within the stable range that you have just found, we can increase the gain somewhat and maintain stability, but a large enough increase in gain causes the system to become unstable. This maximum amount of increase in gain at which the closed-loop system just becomes un- stable is the gain margin. Note that if we decrease the gain too much, we can also cause instability. (b) Consider the feedback system of part (a) with the gain K set at a value of 7. Show that the closed-loop system is stable. Sketch the log magnitude-phase plot of this system, and show that there are two nonnegative values of w for which <r-G (jw)H(jw) = -7T. Further, show that, for one of these values 7IG(jw)H(jw)l < 1, and for the other 7IG(jw)H(jw)l > 1. The first value provides us with the usual gain margin-that is, the factor 11I7G(jw )H(jw )I by which we can increase the gain and cause instability. The second provides us with the factor 11I7G(jw )H(jw )I by which we can decrease the gain and just cause instability. 892 Linear Feedback Systems Chap. 11 (c) Consider a feedback system with (s/100 + 1)2 G(s)H(s) = (s + l)3 Sketch the root locus forK > 0. Show that two branches of the root locus begin in the left-half plane and, asK is increased, move into the right-half plane and then back into the left-half plane. Do this by examining the equation G(jw)H(jw) = - ~- Specifically, by equating the real and imaginary parts of this equation, show that there are two values of K 2: 0 for which the closed-loop poles lie o~ the jw-axis. Thus, if we set the gain at a small enough value so that the system is sta- ble, then we can increase the gain up until the point at which the two branches of the root locus intersect the jw-axis. For a range of values of gain beyond this point, the closed-loop system is unstable. However, if we continue to increase the gain, the system will again become stable for K large enough. (d) Sketch the Nyquist plot for the system of part (c), and confirm the conclusions reached in part (c) by applying the Nyquist criterion. (Make sure to count the net number of encirclements of -1/K .) Systems such as that considered in parts (c) and (d) of this problem are often referred to as being conditionally stable systems, because their stability properties may change several times as the gain is varied. 11.48. In this problem, we illustrate the discrete-time counterpart of the technique de- scribed in Problem 11.44. Specifically, the discrete-time Nyquist criterion can be extended to allow for poles of G(z)H(z) on the unit circle. Consider a discrete-time feedback system with z_ '""-) G(z)H(z) = _ z-I (Pll.48-l) 1 z(z- 1) · In this case, we modify the contour on which we evaluate G(z)H(z), as illustrated in Figure Pll.48(a). (a) Show that 1T 2 and ''"") - ""'"") - 1T 1:-G(e1-1T )H(e1-1T ) = , 2 where z = ei2 1T is the point below the real axis at which the small semicircle intersects the unit circle and z = ei0 + is the corresponding point above the real axis. (b) Use the results of part (a) together with eq. (Pll.48-l) to verify that Fig- ure P11.48(b) is an accurate sketch of G(z)H(z) along the portion of the Chap. 11 Problems 893 (a) fw=2'TT !w =O+ (b) Figure P11.48 894 Linear Feedback Systems Chap. 11 contour z = ejw as w varies from o+ to 27T- in a counterclockwise direc- tion. In particular, verify that the angular variation of G( ejw )H ( ejw) is as indicated. (c) Find the value of w for which <t.G(ejw)H(ejw) = -1r, and verify that at this point. [Hint: Use the geometrical method for evaluating <t G( ejw )H ( ejw) together with some elementary geometry to determine the value of w.] (d) Consider next the plot of G(z)H(z) along the small semicircle about z = 1. Note that as E ~ 0, the magnitude of G(z)H(z) along this contour goes to in- finity. Show that as E ~ 0, the contribution of the pole at z = 0 to <t.G(z)H(z) along the semicircle is zero. Then show that as E ~ 0, <t.G(z)H(z) = -(}, where(} is as defined in Figure P11.48(a). Thus, since (} varies from -7T/2 to + 7T/2 in the counterclockwise direc- tion, <t.G(z)H(z) varies from +1rl2 to -1r12 in the clockwise direction. The result is the complete Nyquist plot of Figure P11.48( c). (e) Using the Nyquist plot, find the range of values of K for which the closed- loop feedback system is stable. [Note: Since the pole of G(z)H(z) at z = 1 is !1m CRe (c) Figure P11.48 Continued Chap. 11 Problems 895 inside the modified contour, it is not included in counting the poles of G(z)H (z) outside the unit circle. That is, only poles strictly outside the unit circle are counted in applying the Nyquist criterion. Thus, in this case, since G(z)H(z) has no poles strictly outside the unit circle, we must have no encirclements of the point z = -11 K for closed-loop stability.] (f) Follow the steps outlined in parts (a), (b), and (d) to sketch the Nyquist plots for each of the following: (i) z+ f_+/3 (ii) I (z-1 )(z+ ~ + }3) (iii) z+ 1 z(z-1) (iv) ~~211~ [be careful in calculating <f-G(z)H(z) along the infinitesimal semi- circle] For each of the preceding, use the Nyquist criterion to determine the range of values of K (if any such range exists) for which the closed-loop system is stable. Also, use another method (root locus or direct calculation of the closed- loop poles as a function of K) to provide a partial check of the correctness of your Nyquist plot. Note: In sketching the Nyquist plots, you may find it useful to first sketch the magnitude and phase plots as a function of frequency or at least calculate IG(ejw)H(ejw)l and <f-G(ejw)H(ejw) at several points. Also, it is helpful to determine the values of w for which G( ejw )H ( ejw) is real. (g) Repeat part (f) for G(z)H(z) = z2 _ . 1 In this case there are two poles on the unit circle, and thus, you must modify the contour around each of these by including an infinitesimal semicircle that extends outside the unit circle, thereby placing the pole inside the contour. EXTENSION PROBLEMS 11.49. In this problem, we provide an illustration of how feedback can be used to increase the bandwidth of an amplifier. Consider an amplifier whose gain falls off at high frequencies. That is, suppose the system function of this amplifier is Ga H(s) = --. s+a (a) What is the de gain of the amplifier (i.e., the magnitude of its frequency re- sponse at 0 frequency)? (b) What is the system time constant? (c) Suppose we define the bandwidth of the system as the frequency at which the magnitude of the amplifier frequency response is 1/ J2 times its magnitude at de. What is the bandwidth of the amplifier? 896 Linear Feedback Systems Chap. 11 (d) Suppose we place the amplifier in a feedback loop as depicted in Figure P11.49. What is the de gain of the closed-loop system? What are the time constant and the bandwidth of the closed-loop system? (e) Find the value of K that leads to a closed-loop bandwidth that is exactly double the bandwidth of the open -loop amplifier. What are the corresponding closed- loop system time constant and de gain? x(t)--.....,.+~1 1----......- ---l~ y(t) Figure P 11 .49 11.50. As mentioned in the text, an important class of devices used in the implementation of feedback systems is the class of operational amplifiers. A model for such an amplifier is depicted in Figure P11.50(a). The amplifier's input is the difference >---10+ v0(t) + r Figure P11.50a between two voltages v2(t) and v1( t), and the output voltage is an amplified version of the input; that is, Va(t) = K[v2(t)- Vt (t)]. (Pll.S0-1) Consider an operational amplifier connection shown in Figure P11.50(b ). In this figure, Z1 (s) and Z2(s) are impedances. (That is, each is the system function of an LTI system whose input is the current flowing through the impedance element and whose output is the voltage across the element.) Making the approximation that the input impedance of the operational amplifier is infinite and that its output impedance is zero, we obtain the following relationship between V1( s), Vi(s), and V0 (s), the Laplace transforms of v1( t), vi(t), and v 0 (t), respectively: v, = [z,(s~si2(s)] V;(s) + [z,(s~~si2(s)] Vn(s). (Pll.50-2) Chap. 11 Problems 897 Z2 (s) Z1(s) + + v0 (t) vi(t) v1 (t) r- - Figure P11.50b Also, from eq. (Pl1.50-1) and Figure P11.50(b), we see that Va(s) = -KVt(s). (P11.50-3) (a) Show that the system function H(s) = Va(s) V;(s) for the interconnection of Figure P 11.50(b) is identical to the overall closed- loop system function for the system of Figure P11.50(c). Z2 (s) + -K Z1 (s) + + Z2 (s) + Z1(s) Z1 (s) + Z2 (s) Figure P11.50c (b) Show that if K >> 1, then H(s) = _ Z2(s). Zt(S) 11.51. (a) Suppose that in Figure P11.50(b) Z1( s) and Z2(s) are both pure resistances, say, R1 and R2 , respectively. A typical value for R2/R 1 is in the range 1 to 103, while a typical value forK is 106 . Using the results of Problem 11.50(a), calculate the actual system function for this value of K and for R2! R1 equal to 1 and then to 103, and compare each resulting value to -R2/R1• This should give you some idea of how good the approximation of Problem 11.50(b) typically is. 898 Linear Feedback Systems Chap. 11 (b) One of the important uses of feedback is in the reduction of system sensitivity to variations in parameters. This is particularly important for circuits involv- ing operational amplifiers, which have high gains that may be known only approximately. (i) Consider the circuit discussed in part (a), with R2/R 1 = 102. What is the percentage change in the closed-loop gain of the system if K changes from 106 to 5 X 105? (ii) How large must K be so that a 50% reduction in its value results in only a 1% reduction in the closed-loop gain? Again, take R 1R = 102 2 1 . 11.52. Consider the circuit of Figure P11.52. This circuit is obtained by using 1 Zis) = CS in Figure Pll.50(b) . Using the results from Problem 11.50, show that the system behaves approximately like an integrator. In what frequency range (expressed in terms of K, R, and C) does this approximation break down? c >-~. ..........- u+ v0 (t) r Figure P11.52 11.53. Consider the circuit depicted in Figure P11.53(a), which is obtained from the circuit of Figure P11.50(b) by using Z1 (s) = Rand by replacing Z2(s) with a diode that has an exponential current-voltage relationship. Assume that this relationship is of the form (P11.53-l) where M is a constant that depends upon the construction of the diode, q is the charge of an electron, k is Boltzmann's constant, and Tis absolute temperature. Note that the idealized relationship of eq. (P11.53-1) assumes that there is no pos- sibility of a negative diode current. Usually, there is some small maximum negative value of diode current, but we will neglect this possibility in our analysis. (a) Assuming that the input impedance of the operational amplifier is infinite and that its output impedance is zero, show that the following relations hold: (Pll.53-2) (Pll.53-3) Chap. 11 Problems 899 >--.......- 0+ v (t) r 0 Figure P11.53a (b) Show that forK large, the relationship between v0 (t) and vi(t) is essentially the same as in the feedback system of Figure P11.53(b), in which the system in the feedback path is a nonlinear memoryless system with input v0 (t) and output w(t) = RMe qv0 (t)lkT_ (c) Show that for K large, v (t ) ~~ -kT l n ( -V-i(-t)) (Pl1.53-4) o q RM. -K + w(t) Figure P11.53b Note that eq. (Pll.53-4) makes sense only for a negative vi(t), which is con- sistent with the requirement that the diode current cannot be negative. If a positive Vi(t) is applied, the current id(t) cannot balance the current through the resistor. Thus, a nonnegligible current is fed into the amplifier, causing it to saturate. 11.54. In this problem, we explore the use of positive feedback for generating oscillating signals. (a) Consider the system illustrated in Figure Pll.54(a). Show that x 1(t) = xi(t) if G(s)H(s) = -1. (Pll.54-1) Suppose that we connect terminals 1 and 2 in Figure P11.54(a) and make Xi(t) = 0. Then the output of the system should remain unchanged if we 900 Linear Feedback Systems Chap. 11 Xj(t) H(s) y(t) 3 1 -1 ~ G(s) (a) + xi(t+ )=O~ H(s) y(t) + ---- -1 G(s) I f+- (b) Figure P11.54 satisfy eq. (Pll.54-1). The system now produces an output without any input. Therefore, the system shown in Figure Pll.54(b) is an oscillator, provided that eq. (Pll.54-l) is satisfied. (b) A commonly used oscillator in practice is the sinusoidal oscillator. For such an oscillator, we may rewrite the condition of ~q. (Pll.54-l) as G(jwo)H(jwo) = -1. (Pll.54-2) What is the value of the closed-loop gain for the system shown in Figure Pll.54(b) at w0 when eq. (Pll.54-2) is satisfied? (c) A sinusoidal oscillator may be constructed on the basis of the principle out- lined above by using the circuit shown in Figure Pll.54(c). The input to the Figure P11.54c Chap. 11 Problems 901 amplifier is the difference between the voltages v1( t) and v2(t). In this circuit, the amplifier has a gain of A and an output resistance of R0 . Z1( s), Z2(s), and Z3(s) are impedances. (That is, each is the system function of an LTI system whose input is the current flowing through the impedance element and whose output is the voltage across the element.) It can be shown that, for this circuit, H(s) = -AZL(s) ZL(s) + Ro' where Also, we can show that (i) Show that G(s)H(s) = AZ1 (s)Z2(s) Ro(ZI (s) + Z2(s) + Z3(s)) + Z2(s)(Z1 (s) + Z3(s)) (ii) If Z1( s), Z2(s), and Z3(s) are pure reactances (i.e., inductances or ca- pacitances), we can write Z1 (jw) = jX1( jw ), Z2(jw) = jX2(jw ), and Z3(jw) = jX3(jw), where X;(jw), i = 1, 2, 3, are all real. Using there- sults of parts (b) and (i), show that a necessary condition for the circuit to produce oscillations is X1(jw) + X2(jw) + X3(jw) = 0. (iii) Show also that, in addition to the constraint of part (ii), the constraint AX1( jw) = X2(jw) has to be satisfied for the circuit to produce os- cillations. [Since X;(jw) is positive for inductances and negative for capacitances, the latter constraint requires that Z1( s) and Z2(s) be reac- tances of the same type (i.e., both should be inductances or both should be capacitances).] (iv) Let us assume that Z1( s) and Z2(s) are both inductances such that X1(jw) = X2(jw) = wL. Let us also assume that X3(jw) = -ll(wC) is a capacitance. Use the condition derived in (ii) to determine the fre- quency (in terms of L and C) at which the circuit oscillates. 11.55. (a) Consider the nonrecursive discrete-time LTI filter depicted in Figure P11.55(a). Through the use of feedback around this nonrecursive system, a recursive filter can be implemented. To do so, consider the configuration shown in Figure Pll.55(b), in which H(z) is the system function of the nonrecursive LTI system of Figure Pll.55(a). Detenninetheovernllsystemftmctionofthisfeedback 902 Linear Feedback Systems Chap. 11 system, and find the difference equation relating the input to the output of the overall system. x[n] D D D ... ~ D ~ L...-----+-l + 1----....,.~ + ~-- ... --~ y[n] (a) + K .. y[n] - H(z) (b) Figure P11.55 (b) Now suppose that H(z) in Figure P11.55(b) is the system function of a recur- sive LTI system. Specifically, suppose that H(z) = Show how one can find values of the coefficients K, c 1, ••• , cN , and do, ... , d N, such that the closed-loop system function is where the ai and hi are specified coefficients. Chap. 11 Problems 903 In this problem, we have seen that the use of feedback provides us with alterna- tive implementations of LTI systems specified by linear constant-coefficient dif- ference equations. The implementation in part (a), consisting of feedback around a nonrecursive system, is particularly interesting, as some technologies are ide- ally suited to implementing tapped delay-line structures (i.e., systems consisting of chains of delays with taps at each delay whose outputs are weighted and then summed). 11.56. Consider an inverted pendulum mounted on a movable cart, as depicted in Figure P11.56. Here, we have modeled the pendulum as consisting of a massless rod of length L with a mass m attached at the end. The variable O(t) denotes the pendu- lum's angular deflection from the vertical, g is gravitational acceleration, s(t) is the position of the cart with respect to some reference point, a(t) is the acceler- ation of the cart, and x(t) represents the angular acceleration resulting from any disturbances, such as gusts of wind. a(t) I J I s(t) Figure P11.56 Our goal in this problem is to analyze the dynamics of the inverted pendulum and, more specifically, to investigate the problem of balancing the pendulum by a judicious choice of the acceleration a(t) of the cart. The differential equation relating O(t), a(t), and x(t) is d 20(t) . LdT = g sm[O(t)] - a(t) cos[O(t)] + Lx(t). (P11.56-l) This relation merely equates the actual acceleration of the mass along a direction perpendicular to the rod to the applied accelerations [gravity, the disturbance ac- celeration due to x(t), and the cart's acceleration] along this direction. Note that eq. (P11.56-1) is a nonlinear differential equation. The detailed, exact analysis of the behavior of the pendulum requires that we examine this equa- tion; however, we can obtain a great deal of insight into the dynamics of the pendu- lum by performing a linearized analysis. Specifically, let us examine the dynamics of the pendulum when it is nearly vertical [i.e., when O(t) is small]. In this case, we 904 Linear Feedback Systems Chap. 11 can make the approximations sin[O(t)] = O(t), cos[O(t)] = 1. (P11.56-2) (a) Suppose that the cart is stationary [i.e., a(t) = 0], and consider the causal LTI system with input x(t) and output O(t) described by eq. (P11.56-1), together with the approximations given in eq. (P11.56-2). Find the system function for this system, and show that it has a pole in the right-half of the plane, implying that the system is unstable. (b) The result of part (a) indicates that if the cart is stationary, any minor angular disturbance caused by x(t) will lead to growing angular deviations from the vertical. Clearly, at some point, these deviations will become sufficiently large so that the approximations of eq. (Pl1.56-2) will no longer be valid. At this point the linearized analysis is no longer accurate, but the fact that it is ac- curate for small angular displacements allows us to conclude that the vertical equilibrium position is unstable, since small angular displacements will grow rather than diminish. We now wish to consider the problem of stabilizing the vertical position of the pendulum by moving the cart in an appropriate fashion. Suppose we try proportional feedback-that is, a(t) = KO(t). Assume that O(t) is small, so that the approximations in eq. (P11.56-2) are valid. Draw a block diagram of the linearized system with O(t) as the output, x(t) as the external input, and a(t) as the signal that is fed back. Show that the resulting closed-loop system is unstable. Find a value of K such that if x(t) = S(t), the pendulum will sway back and forth in an undamped oscillatory fashion. (c) Consider using the proportional-plus-derivative (PD) feedback, Show that one can find values of K1 and K2 that stabilize the pendulum. In fact, using g = 9.8 m/sec2 and (P11.56-3) L = 0.5 m, choose values of Kt and K2 so that the damping ratio of the closed loop system is 1 and the natural frequency is 3 rad/sec. 11.57. In this problem, we consider several examples of the design of tracking systems. Consider the system depicted in Figure P11.57. Here, Hp(s) is the system whose output is to be controlled, and Hc(s) is the compensator to be designed. Our objec- tive in choosing Hc(s) is that we would like the output y(t) to follow the input x(t). In particular, in addition to stabilizing the system, we would also like to design the system so that the error e(t) decays to zero for certain specified inputs. Chap. 11 Problems 905 --~+11'7\ e(t) d(t) x(t) V+ ·I H,(s) I • Hp(s) y(t) ..,. Figure P11.57 (a) Suppose that Hp(s) = -a- , a =1:- 0. (P11.57-1) s+a Show that if Hc(s) = K (which is known as proportional or P control), we can choose K so as to stabilize the system and so that e(t) ~ 0 if x(t) = 8(t). Show that we cannot get e(t) ~ 0 if x(t) = u(t). (b) Again let Hp(s) be as in eq. (P11.57-l), and suppose that we use proportional- plus-integral (Pl) ~ontrol-that is, Show that we can choose K1 and K2 so as tp stabilize the system, and we can also get e(t) ~ 0 if x(t) = u(t). Thus, the system can track a step. In fact, this illustrates a basic and important principle in feedback system design: To track a step [X(s) = lis], we need an integrator (1/s) in the feedback system. An extension of this principle is considered in the next problem. (c) Suppose that 1 Hp(s) = (s - 1)2. Show that we cannot stabilize this system with a PI controller, but that we can stabilize it and have it track a step if we use proportional-plus-integral-plus- differential (PID) control, i.e., 11.58. In Problem 11.57, we discussed how the presence of an integrator in a feedback system can make it possible for the system to track a step input with zero error in the steady state. In this problem, we extend the idea. Consider the feedback system depicted in Figure Pll.58, and suppose that the overall closed-loop system is stable. SupP.ose ~lso that m Kfl(s -{h) H(s) = _k_=_l- -- n-1 s1fl(s- ak) k=l 906 Linear Feedback Systems Chap. 11 e(t) + H(s) y(t) ~ Figure P11.58 where the ak and f3k are given nonzero numbers and lis a positive integer. The feedback system of Figure P11.58 is often referred to as a Type l feedback system. (a) Use the final-value theorem (Section 9.5.10) to show that a Type 1 feedback system can track a step-that is, that e(t) ~ 0 if x(t) = u(t). (b) Similarly, show that a Type 1 system cannot track a ramp, but rather, that e(t) ~ a finite constant if x(t) = U-2(t). (c) Show that, for a Type 1 system, unbounded results ensue if x(t) = U-k(t) with k > 2. (d) More generally, show that, for a Type l system: (i) e(t) ~ 0 if x(t) = U-k(t) with k :::::.; l (ii) e(t) ~a finite constant if x(t) = U(-l+l)(t) (iii) e(t) ~ oo if x(t) = U-k(t) with k > l + 1 11.59. (a) Consider the discrete-time feedback system of Figure P11.59. Suppose that 1 H(~ = . (z- 1)(z + ~) x[n] ____;_~ H(z) 1--__. ..,..____...,.... y[n] Figure P11.59 Show that this system can track a unit step in the sense that if x[n] = u[n], then lim e[n] = 0. (P11.59-l) n~oc (b) More generally, consider the feedback system of Figure P11.59, and assume that the closed-loop system is stable. Suppose that H(z) has a pole at z = 1. Chap. 11 Problems 907 Show that the system can track a unit step. [Hint: Express the transform E(z) of e[n] in terms of H(z) and the transform of u[n]; explain why all the poles of E(z) are inside the unit circle.] (c) The results of parts (a) and (b) are discrete-time counterparts of the results for continuous-time systems discussed in Problems 11.57 and 11.58. In discrete time, we can also consider the design of the systems that track specified inputs peifectly after a finite number of steps. Such systems are known as deadbeat feedback systems. Consider the discrete-time system of Figure P 11.59 with z-I H(z)= 1-z-I' Show that the overall closed-loop system is a deadbeat feedback system with the property that it tracks a step input exactly after one step: that is, if x[n] = u[n], then e[n] = 0, n ~ 1. (d) Show that the feedback system of Figure P11.59 with is a deadbeat system with the property that the output tracks a unit step per- fectly after a finite number of steps. At what time step does the error e[n] first settle to zero? (e) More generally, for the feedback system of Figure P11.59, find H(z) so that y[n] perfectly tracks a unit step for n ~ Nand, in fact, so that N-l e[n] = .L, ako[n- k], (P11.59-2) k=O where the ai are specified constants. Hint: Use the relationship between H(z) and E(z) when the input is a unit step and e[n] is given by eq. (P11.59-2). (f) Consider the system of Figure P11.59 with Show that this system tracks a ramp x[n] = (n + 1)u[n] exactly after two time steps. 11.60. In this problem, we investigate some of the properties of sampled-data feedback systems and illustrate the use of such systems. Recall from Section 11.2.4 that in a sampled-data feedback system the output of a continuous-time system is sam- pled. The resulting sequence of samples is processed by a discrete-time system, the output of which is converted to a continuous-time signal that in tum is fed back and subtracted from the external input to produce the actual input to the continuous- time system. 908 Linear Feedback Systems Chap. 11 (a) Consider the system within dashed lines in Figure 11.6(b ). This is a discrete- time system with input e[n] and output p[n]. Show that it is an LTI system. As we have indicated in the figure, we will let F(z) denote the system function of this system. (b) Show that in Figure 11.6(b) the discrete-time system with system function F(z) is related to the continuous-time system with system function H(s) by means of a step-invariant transformation. That is, if s(t) is the step response of the continuous-time system and q[n] is the step response of the discrete-time system, then q[n] = s(nT) for all n. (c) Suppose that 1 H(s) = --, ffi..e{s} > 1. s- 1 Show that (d) Suppose that H(s) is as in part (c) and that G(z) = K. Find the range of values of K for which the closed-loop discrete-time system of Figure 11.6(b) is stable. (e) Suppose that K G(z) = ----,------- 1 + !z- 1 • 2 Under what conditions on T can we find a value of K that stabilizes the overall system? Find a particular pair of values for K and T that yield a stable closed- loop system. Hint: Examine the root locus, and find the values for which the poles enter or leave the unit circle. APPENDIX pARTIAL-FRACTION EXPANSION A. 1 INTRODUCTION The purpose of this appendix is to describe the technique of partial-fraction expansion. This tool is of great value in the study of signals and systems; in particular, it is very useful in inverting Fourier, Laplace, or z-transforms and in analyzing LTI systems de- scribed by linear constant-coefficient differential or difference equations. The method of partial-fraction expansion consists of taking a function that is the ratio of polynomials and expanding it as a linear combination of simpler terms of the same type. The determination of the coefficients in the linear combination is the basic problem to be solved in obtaining the expansion. As we will see, this is a relatively straightforward problem in algebra that can be solved very efficiently with a bit of ""bookkeeping."" To illustrate the basic idea behind and role of partial-fraction expansion, consider the analysis developed in Section 6.5.2 for a second-order continuous-time LTI system specified by the differential equation d 2 y(t) d y(t) 2 2 ----;[i2 + 2{wn----;[{ + wny(t) = wnx(t). (A.l) The frequency response of this system is w~ H(jw) = (jw)2 + 2{wn(jw) + w~' (A.2) or, if we factor the denominator, (A.3) where CJ = -{wn + WnJ(2=I, c2 = -{wn - WnJ(2=I. (A.4) Having H(jw ), we are in a position to answer a variety of questions related to the system. For example, to determine the impulse response of the system, recall that for any number a with CRe{s} < 0, the Fourier transform of XJ (t) = eat u(t) (A.5) is 1 X 1(jw) = (A.6) jw -a' 909 910 Appendix while if (A.7) then 1 X2(jw) = (jw - a)2. (A.8) Therefore, if we can expand H(jw) as a sum of terms of the form of eq. (A.6) or (A.8), we can determine the inverse transform of H (jw) by inspection. For example, in Section 6.5.2 we noted that when c1 -# c2, H(jw) in eq. (A.3) could be rewritten in the form H( ).W ) _- ( w~ ) . 1 + ( w~ ) . 1 . (A.9) CJ - C2 )W - CJ C2 - Ct )W - C2 In this case, the Fourier transform pair of eqs. (A.5) and (A.6) allows us to write down immediately the inverse transform of H(jw) as h(t) = (A.10) While we have phrased the preceding discussion in terms of continuous-time Fourier transforms, similar concepts also arise in discrete-time Fourier analysis and in the use of Laplace and z-transforms. In all of these cases, we encounter the important class of rational transforms-that is, transforms that are ratios of polynomials in some variable. Also, in each of these contexts, we find reasons for expanding these transforms as sums of simpler terms such as in eq. (A.9). In this section, in order to develop a general procedure for calculating the expansions, we consider rational functions of a general variable v; that is, we examine functions of the form H(v) = f3mvm + f3m-IVm-I + ... + f3tv + f3o. (A.ll) a V 11 11 + a 11 -JVn-I + ... + a1v + ao For continuous-time Fourier analysis (jw) plays the role of v, while for Laplace transforms that role is played by the complex variables. In discrete-time Fourier analysis, vis usually taken to bee- jw, while for z-transforms, we can use either z- 1 or z. After we have developed the basic techniques of partial-fraction expansion, we will illustrate their application to the analysis of both continuous-time and discrete-time LTI systems. A.2 PARTIAL-FRACTION EXPANSION AND CONTINOUS-TIME SIGNALS AND SYSTEMS For our purposes, it is convenient to consider rational functions in one of two standard forms. The second of these, which is often useful in the analysis of discrete-time signals and systems, will be discussed shortly. The first of the standard forms is (A.12) V 11 + an-Ivn-I + ... + a1v + ao Appendix 911 In this form the coefficient of the highest order term in the denominator is 1, and the order of the numerator is at least one less than the order of the denominator. (The order of the numerator will be less than n- 1 if bn- 1 = 0.) If we are given H(v) in the form of eq. (A.11), we can obtain a rational function of the form of eq. (A.12) by performing two straightforward calculations. First, we divide both the numerator and the denominator of H ( v) by an. This yields (A.13) where f3m f3m-1 Ym = Ym-1 = an an an-1 an-2 an-1 = an-2 = an an If m < n, H ( v) is called a strictly proper rational function, and in this case, letting bo = Yo, b1 = Y1, ... , bm = Ym, and setting any remaining b's equal to zero, we see that H(v) in eq. (A.13) is already of the form of eq. (A.12). In most of the discussions in this book in which rational functions are considered, we are concerned primarily with strictly proper rational functions. However, if H(v) is not proper (i.e., if m ~ n), we can perform a preliminary calculation that allows us to write H ( v) as the sum of a polynomial in v and a strictly proper rational function. That is, H(v) = Cm-nVm-n + Cm-n-1Vm-n- 1 + ... + C1V +Co bn-1Vn- 1 + bn-2Vn-2 + ... + b1v + bo (A.14) + ------------~--------------- vn + an-1vn-1 + ... + a1V + ao The coefficients co, c1, ... , Cm-n and bo, b1, ... , bn-1 can be obtained by equating eqs. (A.13) and (A.14) and then multiplying through by the denominator. This yields (A.15) + (Cm-nVm-n + ... + Co)(vn + an-1 Vn-I + ... + ao). By equating the coefficients of equal powers of v on both sides of eq. (A.15), we can determine the c's and b's in terms of the a's andy's. For example, if m = 2 and n = 1, so that Y2v2 + Y1v +Yo bo H(v) = = c1v +co+--, (A.16) v + a1 v + a1 then eq. (A.15) becomes Y2v2 + YIV +Yo = bo + (civ + co)(v +at) = b + c1v2 0 +(co+ a1 c1)v + a1 co. Equating the coefficients of equal powers of v, we obtain the equations Y2 = C], YI = co+ a1c1, Yo= bo + a1co. 912 Appendix The first equation yields the value of c 1, which can then be used in the second to solve for c0 , which in tum can be used in the third to solve for b0 . The result is CJ = ')'2, Co = /'I - GJ')'2, bo = 'Yo-ai('YI-ai/'2). The general case of eq. (A.15) can be solved in an analogous fashion. Our goal now is to focus on the proper rational function G(v) in eq. (A.l2) and to expand it into a sum of simpler proper rational functions. To see how this can be done, consider the case of n = 3, so that eq. (A.l2) reduces to 2 G(v) = b2v + b1v + bo . (A.17) v 3 +a2v2 +a1v+ao As a first step, we factor the denominator of G( v) in order to write it in the form 2 G(v) = b2v + b1v + bo (A.l8) (v - PI )(v - P2)(v - P3) Assuming for the moment that the roots p 1, p2, and p3 of the denominator are all distinct, we would like to expand G(v ) into a sum of the form G(v) = -A1- + -A2- + -A-3 . (A.l9) v - p 1 v - P2 v - P3 The problem, then, is to determine the constants A 1, A2, and A3 . One approach is to equate eqs. (A.l8) and (A.19) and to multiply through the denominator. In this case, we obtain the equation b2v2 + b1v + bo = A1(v- P2)(v- P3) + A2(v - PI )(v - P3) (A.20) + A3(v - PI )(v - P2). By expanding the right-hand side of eq. (A.20) and then equating coefficients of equal powers of v, we obtain a set of linear equations that can be solved for A 1, A2, and A 3. Although this approach always works, there is a much easier method. Consider eq. (A.l9), and suppose that we would like to calculate A1• Then, multiplying through by v - p 1, we obtain (v- PI)G(v) = AI + A2(v- pJ) + A3(v- pi). (A.21) v- P2 v- P3 Since p1, p2, and p3 are distinct, the last two terms on the right-hand side of eq. (A.21) are zero for v = p 1• Therefore, A1 = [(v- PI)G(v)]Jv=p"" (A.22) or, using eq. (A.l8), b2pf + b1P1 + bo (A.23) (PI - P2)(PI - P3). Appendix 913 Similarly, b2p~ + htP2 + bo A2 = [(v- P2)G(v)Jiv=p2 = ( )( )' (A.24) P2- Pt P2- P3 b2p~ + htP3 + bo A3 = [(v- P3)G(v)Jiv=p = ( )( ) (A.25) 3 P3- Pt P3- P2 Suppose now that Pt = P3 ¥= p2; that is, 2 G(v) = b2v + btv + bo . (A.26) (v- pt)2(v- P2) In this case, we look for an expansion of the form G(v) = ~ + A 12 + ~. (A.27) v-pl (v-pt)2 v-p2 Here, we need the ll(v- p1)2 term in order to obtain the correct denominator in eq. (A.26) when we collect terms over a least common denominator. We also need to include the ll(v- p1) term in general. To see why this is so, consider equating eqs. (A.26) and (A.27) and multiplying them througH by the denominator of eq. (A.26): b2v2 + btv + bo = A11(v- pt)(v- P2) (A.28) + A12(v- P2) + A21(v- Pt)2. Again, if we equate coefficients of equal powers of v, we obtain three equations (for the coefficients of the v 0 , v 1, and v 2 terms). If we omit the A11 term in eq. (A.27), we will then have three equations in two unknowns, which in general will not have a solution. By including this term, we can always find a solution. In this case also, however, there is a much simpler method. Consider eq. (A.27) and multiply through by (v- p 1)2: 2 2 (v - Pt) G(v) = All (v - Pt) + A12 + A21 (v- Pt) (A.29) v- P2 From the preceding example, we see immediately how to determine A12 : A 12 = [( _ Pl )2G( )JI = b2PT + htPt + bo V V v=p1 • (A.30) Pt- P2 As for All, suppose that we differentiate eq. (A.29) with respect to v: d 2 [2(v- Pt) (v- Pt)2] -d [(v - pt) G(v)] = All + A21 - ( ) . (A.31) v v- P2 v- P2 2 It is then apparent that the final term in eq. (A.31) is zero for v = p1, and therefore, Au [:)v- Pt)2= G(v)Jiv~p, (A.32) 2b2Pt + bt b2PT + bt Pt + bo Pt - P2 (Pt - P2)2 914 Appendix Finally, by multiplying eq. (A.27) by v - p2, we find that A = [( _ )G( )JI = b2p~ + b1 P2 + bo 21 v P2 v v = P2 ( )2 (A.33) P2- PI This example illustrates all of the basic ideas behind partial-fraction expansion in the general case. Specifically, suppose that the denominator of G(v) in eq. (A.12) has distinct roots p 1, ••• , p7 with multiplicities a 1, ••• , a 7 ; that is, G(v) = bn-JVn-l + ... + blv + bo (A.34) (v- pJ)<T 1 (v- P2)cT2 ••• (v- Pr)<Tr In this case, G(v) has a partial-fraction expansion of the form v _- -A-11 G( ) - + A12 2 + ... + __A1<_ T__1_ ;___ v- PI (v- pi) (v- PI)cTi + ~ + ... + A2<T2 v - P2 (v - P2)<T2 (A.35) + ... +-Ar-! + ... + ... +( Am, V - p r V - Pr )CT r r <T; A;k = L,L, <v _ p·)k' i= I k= I I where the A; k are computed from the equation 1 A;k = (<T;I- k)! [ ddv""""; ;--""k [(v- p;)< T ;G(v)] ll v=p; (A.36) This result can be checked much as in the example: Multiply both sides of eq. (A.35) by (v- p;)<T; and differentiate repeatedly, until A;k is no longer multiplied by a power of v- p;. Then set v = p;. Example A.1 In Example 4.25, we examine an LTI system described by the differential equation d2y(t) + 4 dy(t) + 3y(t) = dx(t) 2 () (A.37) dt2 dt ----;[{ + X t . The frequency response of this system is . jw + 2 H(;w) = ( . )2 + 4 . (A.38) )W )W + 3. To determine the impulse response for this system, we expand H(jw) into a sum of simpler terms whose inverse transforms can be obtained by inspection. Making the subsitution of v for jw, we obtain the function 1H ere, we use the factorial notation r! for the product r(r- I )(r- 2) ... 2 · I. The quantity 0! is defined to be equal to I. Appendix 915 v+2 v+2 G(v) = v 2 + 4 (A.39) v + 3 (v + 1)(v + 3)"" The partial-fraction expansion for G(v) is then _ Au A21 G(v ) - --+-- (A.40) v+1 v+3' where -1 + 2 1 Au = [(v + 1)G(v)] lv=-1 = _ + = 2' (A.41) 1 3 A21 = [(v + 3)G(v)] lv= -3 + 2 1 -3 = _ + = 2· (A.42) 3 1 Thus, I I H(jw) = jw 2+ 1 + jw 2+ 3' (A.43) and the impulse response of the system, obtained by inverting eq. (A.43), is (A.44) The system described by eq. (A.37) can also be analyzed using the techniques of Laplace transform analysis, as developed in Chapter 9. The system function for this system is s+2 H(s) = s (A.45) 2 + 4s + 3' and if we substitute v for s, we obtain the same G(v) given in eq. (A.39). Thus, the partial-fraction expansion proceeds exactly as in eqs. (A.40)-(A.42), with the result that I I H(s) = _L_ + _L_. (A.46) s+1 s+3 Inverting this transform, we again obtain the impulse response, as given in eq. (A.44). ExampleA.2 We now illustrate the method of partial-fraction expansion when there are repeated fac- tors in the denominator. In Example 4.26, we considered the response of the system described in eq. (A.37) when the input was x(t) = e-1 u(t). (A.47) From eq. 4.81, the Fourier transform of the output of the system is . jw +2 Y(jw) = (jw + 1)2(jw + 3)"" (A.48) Substituting v for jw, we obtain the rational function v+2 G(v) = (v + 1)2(v + 3) · (A.49) 916 Appendix The partial-fraction expansion for this function is _ A,, A12 A21 G(v ) --- (A. 50) v + + 1 (v + + 1)2 v + 3' where, from eq. (A.36), 1 d + 7 I 1 A,, = (Z _ 1)! dv[(v 1tG(v)] v=-l = 4, (A.51) A 12 = [(v + 1)7- G(v)] I 1 v= -I = 2' (A.52) 1 A12 = [(v + 3)G(v)] lv= -3 = - 4· (A.53) Therefore, I I I 4 Y(jw) = jw ~+ 1 + (jw ~ 1)2 (A.54) jw + 3' and taking inverse transforms, we get (A.55) Again, this analysis could also have been performed using Laplace transforms, and the algebra would be identical to that given in eqs. (A.49)-(A.55). A.3 PARTIAL-FRACTION EXPANSION AND DISCRETE-TIME SIGNALS AND SYSTEMS As mentioned previously, ih performing partial-fraction expansions for discrete-time Fourier transforms or for z-transforms, it is often more convenient to deal with a slightly different form for rational functions. Suppose, then, that we have a rational function in the form (A.56) This form for G(v) can be obtained from G(v) in eq. (A.l2) by dividing the numerator and denominator by a0 . With G(v) as in eq. (A.56), the corresponding factorization of the denominator is of the form G(v) = dn-tVn-l + ... + dtV +do (A.57) (1- P]lv)a1(1 _ P2lv)a2 ••• (1 _ p;lv)ar' and the form of the partial-fraction expansion that results is (A.58) Appendix 917 The B i k can be calculated in a manner similar to that used earlier: (A.59) As before, the validity of eq. (A.59) can be determined by multiplying both sides of eq. (A.58) by (1- pj 1vYJ"";, then differentiating repeatedly with respect to v, until Bik is no longer multiplied by a power of 1 - pj 1v, and finally, setting v = Pi· Example A.3 Consider the causal LTI system in Example 5.19 characterized by the difference equation 3 1 y[n] - 4 y[n- 1] + Sy [n - 2] = 2x[n]. (A.60) The frequency response of the system is (A.61) For discrete-time transforms such as this, it is most convenient to substitute v fore- jw. Making this substitution, we obtain the rational function 2 2 G(v) = -----;;----:-- (A.62) 1- lv + !v2 (1 - !v)(l - ~v) · 4 8 Using the partial-fraction expansion specified by eqs. (A.57)-(A.59), we obtain G(v) = ~ + ____!!]J__' (A.63) 1- !v 1- !v 2 4 [( 1- ~v )a(v)t 2 -- =4 (A.64) 1- _!_ ' 2 2 [(I- HG(v)t, 2 1-2 = - 2. (A.65) Thus, 2 (A.66) and taking the inverse transform of eq. (A.66), we obtain the unit impulse response: h[n] = 4 (~ )n u[n] - 2 (~ Ju[ n]. (A.67) In Section 10.7, we developed the tools of z-transform analysis for the exami- nation of discrete-time LTI systems specified by linear constant-coefficient difference 918 Appendix equations. Applying those techniques to this example, we find that the system function can be determined by inspection from eq. (A.60) and is (A.68) Then, substituting v for z- 1, we obtain G(v) as in eq. (A.62). Thus, using the partial- fraction expansion calculations in eqs. (A.63)-(A.65), we find that 4 2 H(z) = 1- !z-1 (A.69) 2 which, when inverted, again yields the unit impulse response of eq. (A.67). ExampleA.4 Suppose that the input to the system considered in Example A.3 is x[n] ~ (~ )"" u[n]. (A.70) Then from Example 5.20, the Fourier transform of the output is (A.71) Substituting v fore- Jw yields 2 G(v) = -------=--------=-- (A.72) (1 - ~v)(l - ±v)2 • Thus, using eqs. (A.58) and (A.59), we obtain the partial-fraction expansion B11 B12 B21 G(v ) = 1 - ! v + ( 1 - ! v)2 + 1 - ! v (A.73) 4 4 2 and find B 11 ~ (-4) [:v (1- ~v) G(v)L -4, (A.74) 4 B12 ~ [(1- ~v )' G(v)t -2, (A.75) 4 821 [(1- ~v )c(v)L ~ 8. (A.76) 2 Therefore, (A.77) Appendix 919 which can be inverted by inspection as follows, using the Fourier transform pairs in Table 4.2: 11 11 11 y[n] = {- 4 (41 ) - 2(n + 1) (14) + 8 (1.2 ) } u[n]. (A.78) Example A.S Improper rational functions are often encountered in the analysis of discrete-time sys- tems. To illustrate this, and also to show how they can be analyzed using the techniques developed here, consider the causal LTI system characterized by the difference equation 5 1 11 1 y[n] + 6y[n- 1] + 6y[n- 2] = x[n] + 3x[n- 1] + 6 x[n- 2] + 3x[n- 3]. The frequency response of this system is . 1 + 3e-Jw + l..!.e- j2w + !e-J3w H(elw) = 6 3 (A.79) 1 + 1e-Jw + !e- j2w 6 6 Substituting v fore- Jw, we obtain (A. SO) This rational function can be written as the sum of a polynomial and a proper rational function: (A.81) Equating eqs. (A. SO) and (A.81 ), and multiplying by 1 + ~v + iv2 , we obtain 11 1 + 3v + v2 6 + ~v3 =(co+ bo) +(~co+ c1 + b1)v (A.82) Equating coefficients, we see that 1 5 11 -Co+ -C] 6 ~co= 1, 6 6 (A.83) 5 1 6co + C] + b1 = 3 ~ b1 6, co + bo = 1 ~ bo = 0. Thus, (A.84) 920 Appendix Also, we can use the method developed here to expand the proper rational function in eq. (A.81): --=B-1-1 ----+ B21 . (A.85) (1 + }v)(l + 4v) (1 + }v) (1 + .!.v) The coefficients are B11 = (__l;-)1 = 1, 1 + 2v v= -3 _f._!.v1 _ )I = -1 ( 1 + }v v= -2 . Therefore, we find that (A.86) and by inspection, we can determine the impulse response of this system: h [ n] = 8 [ n] + 28 [ n - 1] + [ (- ~ J- (- ~ J]u [ n]. (A.87) BIBLIOGRAPHY The purpose of this bibliography is to provide the reader with sources for additional and more advanced treatments of topics in signal and system analysis. This is by no means meant to be an exhaustive list but rather it is intended to indicate directions for further study and several references for each. We have divided the bibliography into sixteen different subject areas. The first few deal with the mathematical techniques of signal and system analysis including texts on background mathematics (calculus, differential and difference equations, and complex variables), the theory of Fourier series and of Fourier, Laplace, and z-transforms, and addi- tional topics in mathematics that are commonly encountered and used in signal and system analysis. Several of the sections that follow deal with more thorough and specialized treat- ments of topics in signals and systems introduced in this text, including filtering, sampling and discrete-time signal processing, communications, and feedback and control. We have also provided a list of other basic books on signals and systems as well as several texts on circuit theory. In addition, we have provided lists of references on several topics that rep- resent important subjects for more advanced study for those interested either in expanding their know ledge of the methods of signals and systems or in exploring applications that make use of these advanced techniques. In particular we include sections on state space models and methods, multidimensional signal and image processing, speech processing, multirate and multiresolution signal analysis, random signals and statistical signal pro- cessing, and nonlinear systems. Finally we have included a list of references dealing with a sampling of other applications and advanced topics. Together, the references collected in this bibliography should provide the reader with an appreciation for the breadth of topics and applications that comprise the field of signals and systems. B. 1 BACKGROUND AND BASIC MATHEMATICS B. 1 . 1 Calculus, Analysis, and Advanced Mathematics ARFKEN, G., and WEBER, H. J., Mathematical Methods for Physicists. 4th ed. Boston, MA: Academic Press, 1995. HILDEBRAND, F. B., Advanced Calculus for Applications. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1976. THoMAS, G. B., Jr., and FINNEY, R. L., Calculus and Analytic Geometry. 9th ed. Reading, MA: Addison-Wesley, 1996. B.1.2 Differential and Difference Equations BIRKHOFF, G., and RoTA, G.-C., Ordinary Differential Equations. 3rd ed. New York, NY: John Wiley, 1978. BoYcE, W. E., and DIPRIMA, R. C., Elementary Differential Equations. 3rd ed. New York, NY: John Wiley, 1977. 921 922 Bibliography HILDEBRAND, F. B., Finite Difference Equations and Simulations. Englewood Cliffs, NJ: Prentice Hall, 1968. LEvY, H., and LESSMAN, F., Finite Difference Equations. New York, NY: Macmillan, 1961. SIMMONS, G. F., Differential Equations: With Applications and Historical Notes. New York, NY: McGraw-Hill, 1972. B. 1 .3 Complex Variables CARRIER, G. F., KROOK, M., and PEARSON, C. E., Functions of a Complex Variable: Theory and Technique. Ithaca, NY: Hod Books, 1983. CHURCHILL, R. V., BROWN, J. W., and VERHEY, R. F., Complex Variables and Applications. 5th ed. New York, NY: McGraw-Hill, 1990. 8.2 SERIES EXPANSIONS AND TRANSFORMS 8.2.1 Fourier Series, Transforms, and Applications BRACEWELL, R.N., The Fourier Transform and Its Applications. 2nd ed. New York, NY: McGraw- Hill, 1986. CHuRCHILL, R. V., and BRowN, J. W., Fourier Series and Boundary Value Problems. 3rd ed. New York, NY: McGraw-Hill, 1978. DYM, H., and McKEAN, H. P., Fourier Series and Integrals. New York, NY: Academic Press, 1972. EDWARDS, R. E., Fourier Series: A Modern Introduction. 2nd ed. New York, NY: Springer-Verlag, 1979. GRAY, R. M., and GooDMAN, J. W., Fourier Transforms: An Introduction for Engineers. Boston, MA: Kluwer Academic Publishers, 1995. LIGHTHILL, M. J., Introduction to Fourier Analysis and Generalized Functions. New York, NY: Cam- bridge University Press, 1962. PAPOULIS, A., The Fourier Integral and Its Applications. New York, NY: McGraw-Hill, 1987. WALKER, P. L., The Theory of Fourier Series and Integrals. New York, NY: John Wiley, 1986. 8.2.2 Laplace Transforms DoETSCH, G., Introduction to the Theory and Applications of the Laplace Transformation with a Table of Laplace Transformations. New York, NY: Springer Verlag, 1974. LEPAGE, W. R., Complex Variables and the Laplace Transform for Engineers. New York, NY: McGraw-Hill, 1961. RAINVILLE, E. D., The Laplace Transform: An Introduction. New York, NY: Macmillan, 1963. 8.2.3 z-Transforms JuRY, E. 1., Theory and Application of the Z-Transform Method. Malabar, FL: R. E. Krieger, 1982. VIcH, R., Z Transform Theory and Applications. Boston, MA: D. Reidel, 1987. Bibliography 923 8.3 ADDITIONAL TOPICS IN MATHEMATICS 8.3.1 Generalized Functions ARsAc, J., Fourier Transforms and the Theory of Distributions. Translated by A. Nussbaum and G. C. Heim. Englewood Cliffs, NJ: Prentice Hall, 1966. GELFAND, I. M. et al., Generalized Functions. 5 vols. Translated by E. Saletan et al. New York, NY: Academic Press, 1964-68. HosKINS, R. F., Generalised Functions. New York, NY: Halsted Press, 1979. ZEMANIAN, A. H., Distribution Theory and Transform Analysis. New York, NY: McGraw-Hill, 1965. 8.3.2 Linear Algebra GoLUB, G. H., and VAN LoAN, C. F., Matrix Computations. 2nd ed. Baltimore: The Johns Hopkins University Press, 1989. HoRN, R. A., and JoHNSON, C. R., Matrix Analysis. New York, NY: Cambridge University Press, 1985. STRANG, G., Introduction to Linear Algebra. Wellesley, MA: Wellesley-Cambridge Press, 1993. 8.4 CIRCUIT THEORY BoBROW, L. S., Elementary Linear Circuit Analysis. New York, NY: Holt, Rinehart, and Winston, 1981. CHuA, L. 0., DESOER, C. A., and KuH, E. S., Basic Circuit Theory. New York: McGraw-Hill, 1987. IRviNE, R. G., Operational Amplifier Characteristics and Applications. Englewood Cliffs, NJ: Pren- tice Hall, 1994. RoBERGE, J. K., Operational Amplifiers: Theory and Practice. New York, NY: John Wiley, 1975. VANVALKENBURG, M. E., Network Analysis. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1974. 8.5 BASIC SIGNALS AND SYSTEMS CADzow, J. A., and VAN LANDINGHAM, H. F., Signals and Systems. Englewood Cliffs, NJ: Prentice Hall, 1985. CRuz, J. B., and VANVALKENBURG, M. E., Signals in Linear Circuits. Boston, MA: Houghton Mifflin, 1974. GABEL, R. A., and RoBERTS, R. A., Signals and Linear Systems. 3rd ed. New York, NY: John Wiley, 1987. GussoN, T. H., Introduction to System Analysis. New York, NY: McGraw-Hill, 1985. HouTs, R. C., Signal Analysis in Linear Systems. New York, NY: Saunders College, 1991. JACKSON, L. B., Signals, Systems, and Transforms. Reading, MA: Addison-Wesley, 1991. KAMEN, E., Introduction to Signals and Systems. New York, NY: Macmillan, 1987. LATHI, B. P., Linear Systems and Signals. Carmichael, CA: Berkeley-Cambridge Press, 1992. LIU, C. L., and LIU, J. W., Linear Systems Analysis. New York: McGraw-Hill, 1975. 924 Bibliography MAYHAN, R. J ., Discrete-time and Continuous-time Linear Systems. Reading, MA: Addison-Wesley, 1984. McGILLEM, C. D., and CooPER, G. R., Continuous and Discrete Signal and System Analysis. 3rd ed. New York, NY: Holt, Rinehart and Winston, 1991. NEFF, H. P., Continuous and Discrete Linear Systems. New York, NY: Harper and Row, 1984. PAPOULIS, A., Signal Analysis. New York, NY: McGraw-Hill, 1977. SIEBERT, W. M., Circuits, Signals, and Systems. Cambridge, MA: The MIT Press, 1986. SOLIMAN, S., and SRINATH, M., Continuous and Discrete Signals and Systems. New York, NY: Pren- tice Hall, 1990. TAYLOR, F. J., Principles of Signals and Systems. McGraw-Hill Series in Electrical and Computer Engineering. New York, NY: McGraw-Hill, 1994. ZIEMER, R. E., TRANTER, W. H., and FANNIN, D. R. Signals and Systems: Continuous and Discrete. 2nd ed. New York, NY: Macmillan, 1989. 8.6 DISCRETE-TIME SIGNAL PROCESSING BRIGHAM, 0. E., The Fast Fourier Transform and its Applications. Englewood Cliffs, NJ: Prentice Hall, 1988. BuRRus, C. S., McCLELLAN, J. H., OPPENHEIM, A. V., PARKS, T. W., ScHAFER, R. W., and ScHUESSLER, H. W. Computer-Based Exercises for Signal Processing Using MATIAB. Englewood Cliffs, NJ: Prentice Hall, Inc., 1994. GoLo, B., and RADER, C. M., Digital Processing of Signals. Lincoln Laboratory Publications. New York, NY: McGraw-Hill, 1969. OPPENHEIM, A. V., and ScHAFER, R. W., Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1975. . ' OPPENHEIM, A. V., and ScHAFER, R. W., Discrete-Time Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1989. · PELED, A., and Lm, B., Digital Signal Processing: Theory Design and Implementation. New York, NY: John Wiley, 1976. PROAKIS, J. G., and MANOLAKIS, D. G., Digital Signal Processing Principles, Algorithms, and Appli- cations. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1996. RABINER, L. R., and GoLD, B., Theory and Application of Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1975. RoBERTS, R. A., and MuLLIS, C. T., Digital Signal Processing. Reading, MA: Addison-Wesley, 1987. STRUM, R. D., and KIRK, D. E., First Principles of Discrete Systems and Digital Signal Processing. Addison-Wesley Series in Electrical Engineering. Reading, MA: Addison-Wesley, 1988. TRETTER, S. A., Introduction to Discrete-Time Signal Processing. New York, NY: John Wiley, 1976. B. 7 FILTER DESIGN ANTONIOU, A., Digital Filters, Analysis, Design, and Applications. 2nd ed. New York, NY: McGraw- Hill, 1993. CHRISTIAN, E., and EISENMANN, E., Filter Design Tables and Graphs. Knightdale, NC: Transmission Networks International, 1977. Bibliography 925 HAMMING, R. W., Digital Filters. 3rd ed. Englewood Cliffs, NJ: Prentice Hall, 1989. HUELSMAN, L. P., and ALLEN, P. E., Introduction to the Theory and Design of Active Filters. New York, NY: McGraw-Hill, 1980. PARKS, T. W., and BuRRUS, C. S., Digital Filter Design. New York, NY: John Wiley, 1987. VAN V ALKENBURG, M. E., Analog Filter Design. New York, NY: Holt, Rinehart and Winston, 1982. WEINBERG, L., Network Analysis and Synthesis. New York, NY: McGraw-Hill, 1962. ZvEREV, A. 1., Handbook of Filter Synthesis. New York, NY: John Wiley, 1967. 8.8 STATE-SPACE MODELS AND METHODS BROCKETT, R., Finite Dimensional Linear Systems. New York, NY: John Wiley, 1970. CHEN, C. T., Linear System Theory and Design. New York, NY: Holt, Rinehart, and Winston, 1984. CLosE, C. M., and FREDERICK, D.K. Modeling and Analysis of Dynamic Systems. Boston, MA: Houghton Mifflin, 1978 GuPTA, S.C., Transform and State Variable Methods in Linear Systems. New York, NY: John Wiley, 1966. KAILATH, T., Linear Systems. Englewood Cliffs, NJ: Prentice Hall, 1980. LJUNG, L., System Identification: Theory for the User. Englewood Cliffs, NJ: Prentice Hall, 1987. LuENBERGER, D. G., Introduction to Dynamic Systems: Theory, Models, and Applications. New York, NY: John Wiley, 1979. ZADEH, L.A., and DESOER, C. A., Linear System Theory: The State Space Approach. New York, NY: McGraw-Hill, 1963. 8.9 FEEDBACK AND CONTROL ANDERSON, B. D. 0., and MooRE, J. B., Optimal Control: Linear Quadratic Methods. Englewood Cliffs, NJ: Prentice Hall, 1990. D' AZZO, J. J., and HouPIS, C. H., Linear Control System Analysis and Design: Conventional and Modern. 4th ed. NY: McGraw-Hill, 1995. DoRF, R. C., and BisHoP, R. H., Modern Control Systems. 7th ed. Reading, MA: Addison-Wesley Publishing Company, 1995. DoYLE, J. C., FRANCIS, B. A., and TANNENBAUM, A. R., Feedback Control Theory. New York, NY: Macmillan Publishing Company, 1992. HosTETTER, G. H., SAVANT, Jr., C. J., and STEFANI, R. T., Design of Feedback Control Systems. 2nd ed. Saunders College Publishing, a Division of Holt, Reinhart and Winston, Inc., 1989. Kuo, B. C., Automatic Control Systems. 7th ed. Englewood Cliffs, NJ: Prentice Hall, 1995. OGATA, K., Modern Control Engineering. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1990. OGATA, K., Discrete-Time Control Systems. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1994. RAGAZZINI, J. R., and FRANKLIN, G. F., Sampled-Data Control Systems. New York, NY: McGraw- Hill, 1958. RoHRS, C. E., MELSA, J. L., and ScHULTZ, D. G., Linear Control Systems. New York, NY: McGraw- Hill, 1993. VACCARO, R. J., Digital Control: A State-Space Approach. New York, NY: McGraw Hill, 1995. 926 Bibliography B. 1 0 COMMUNICATIONS BENNETT, W. R., Introduction to Signal Transmission. New York, NY: McGraw-Hill, 1970. BLAHUT, R. E., Digital Transmission of Information. Reading, MA: Addison-Wesley Publishing Company, 1990. BLAHUT, R. E., Algebraic Methods for Signal Processing and Communications Coding. New York, NY: Springer-Verlag, 1992. CARLSON, A. B., Communication Systems: An Introduction to Signals and Noise in Electrical Com- munication. 3rd ed. New York, NY: McGraw-Hill, 1986. CoucH, II, L. W., Modern Communication Systems Principles and Applications. Upper Saddle River, NJ: Prentice Hall, Inc., 1995. CovER, T. M., and THOMAS, J. B., Elements of Information Theory. New York, NY: John Wiley and Sons, Inc., 1991. GALLAGER, R. M., Information Theory and Reliable Communication. New York, NY: John Wiley and Sons, Inc., 1968. HAYKIN, S., Digital Communications. New York, NY: John Wiley & Sons, 1988. JAYANT, N. S., and NoLL, P., Digital Coding of Waveforms: Principles and Applications to Speech and Video. Englewood Cliffs, NJ: Prentice Hall, Inc., 1984. LATHI, B. P., Modern Digital and Analog Communication Systems. 2nd ed. New York, NY: Holt, Rinehart and Winston, Inc., 1989. LEE, E. A., and MESSERSCHMITT, D. G., Digital Communication. 2nd ed. Boston, MA: Kluwer Aca- demic Publishers, 1994. PEEBLES, JR., P. Z., Communication System Principles. Reading, MA: Addison-Wesley Publishing Company, 1976. PROAKIS, J. G., Digital Communications. 3rd ed. New York, NY: McGraw-Hill, 1995. PROAKIS, J. G. and SALEHI, M., Communication Systems Engineering. Englewood Cliffs, NJ: Prentice Hall, 1994. RoDEN, M.S., Analog and Digital Communication Systems. 4th ed. Upper Saddle River, NJ: Pren- tice Hall, Inc., 1996. ScHWARTZ, M., Information Transmission, Modulation, and Noise. 4th ed. New York, NY: McGraw- Hill, 1990. SIMON, M. K., et al., eds., Spread Spectrum Communication Handbook. Rev. ed., New York, NY: McGraw-Hill, 1994. STREMLER, F. G., Introduction to Communication Systems. 3rd ed. Addison-Wesley Series in Elec- trical Engineering, Reading, MA: Addison-Wesley, 1990. TAUB, H., and ScHILLING, D. L., Principles of Communication Systems. 2nd ed. New York, NY: McGraw-Hill, 1986. VITERBI, A. J., and OMURA, J. K., Principles of Digital Communication and Coding. New York, NY: McGraw-Hill, 1979. ZIEMER, R. E. and TRANTER, W. H., Principles of Communications Systems, Modulation, and Noise. 4th ed. Boston, MA: Houghton Mifflin Co., 1995. B. 11 MULTI-DIMENSIONAL SIGNAL, IMAGE, AND VIDEO PROCESSING BRACEWELL, R.N., Two-Dimensional Imaging. Englewood Cliffs, NJ: Prentice Hall, Inc., 1995. CASTLEMAN, K. R., Digital Image Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1996. Bibliography 927 DuDGEON, D. E., MERSEREAU, R. M., Multidimensional Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1984. GoNZALEZ, R. C., and WooDs, R. E., Digital Image Processing. Reading, MA: Addison-Wesley, 1993. JAIN, A. K., Fundamentals of Digital Image Processing. Englewood Cliffs, NJ: Prentice Hall, 1989. LIM, J. S., Two-Dimensional Signal and Image Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1990. NETRAVALI, A. N., and HASKELL, B. G., Digital Pictures: Representation, Compression, and Stan- dards. 2nd ed. New York, NY: Plenum Press, 1995. PRATT, W. K., Digital Image Processing. 2nd ed. New York, NY: John Wiley and Sons, 1991. TEKALP, A.M., Digital Video Processing. Upper Saddle River, NJ: Prentice Hall, Inc., 1995. B. 1 2 SPEECH PROCESSING DELLER, J. R., PROAKIS, J. G., and HANSEN, J. H. L., Discrete-Time Processing of Speech Signals. Upper Saddle River, NJ: Prentice Hall, 1987. KLEIJN, W. B., and P., K. K., Speech Coding and Synthesis. Amsterdam: Elsevier, 1995. LIM, J. S., ed., Speech Enhancement. Englewood Cliffs, NJ: Prentice Hall, 1983. MARKEL, J.D., and GRAY, A. H., Linear Prediction of Speech. New York, NY: Springer-Verlag, 1976. RABINER, L. R., and JuANG, B.-H., Fundamentals of Speech Recognition. Englewood Cliffs, NJ: Prentice Hall, 1993. RABINER, L. R., and ScHAFER, R. W., Digital Processing of Speech Signals. Englewood Cliffs, NJ: Prentice Hall, 1978. B. 1 3 MULTI RATE AND MULTI RESOLUTION SIGNAL ANALYSIS AKANSU, A. N., and HADDAD, R. A., Multiresolution Signal Decomposition: Transforms, Subbands and Wavelets. San Diego, CA: Academic Press, Inc., 1992. CHui, C. K., An Introduction to Wavelets. San Diego, CA: Academic Press Inc., 1992. CROCHIERE, R. E., and RABINER, L. R., Multirate Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1983. DAUBECHIES, I., Ten Lectures on Wavelets. CBMS-NSF Series on Applied Mathematics, Philadel- phia: SIAM, 1992. MALVAR, H. S., Signal Processing with Lapped Transforms. Norwood, MA: Artech House, 1992. VAIDYANATHAN, P. P., Multirate Systems and Filter Banks. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. VETTERLI, M., and KovACEVIC, J., Wavelets and Subband Coding. Englewood Cliffs, NJ: Prentice Hall, Inc., 1995. WoRNELL, G. W., Signal Processing with Fractals: A Wavelet-Based Approach. Upper Saddle River, NJ: Prentice Hall, Inc., 1996. B. 14 RANDOM SIGNALS AND STATISTICAL SIGNAL PROCESSING B.14. 1 Basic Probability DRAKE, A. W., Fundamentals ofA pplied Probability Theory. New York, NY: McGraw Hill, 1967. Ross, S., Introduction to Probability Models. 5th ed. Boston, MA: Academic Press, 1993. 928 Bibliography B.14.2 Stochastic Processes, Detection and Estimation KAY, S.M., Fundamentals of Statistical Signal Processing: Estimation Theory. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. LEoN-GARCIA, A., Probability and Random Processes for Electrical Engineering. 2nd ed. Reading, MA: Addison-Wesley Publishing Co., 1994. PAPouus, A., Probability, Random Variables, and Stochastic Processes. 3rd ed. New York, NY: McGraw-Hill, 1991. PEEBLES, JR., P. Z., Probability, Random Variables, and Random Signal Principles. 3rd ed. New York, NY: McGraw-Hill, 1993. PoRAT, B., Digital Processing of Random Signals: Theory and Methods. Englewood Cliffs, NJ: Prentice Hall, Inc., 1994. THERRIEN, C. W., Discrete Random Signals and Statistical Signal Processing. Englewood Cliffs, NJ: Prentice Hall, Inc., 1992. VAN TREES, H. L., Detection, Estimation, and Modulation Theory: Part I. New York, NY: John Wiley and Sons, Inc., 1968. B. 1 5 NONLINEAR AND TIME-VARYING SYSTEMS CHuA, L. 0., Introduction to Nonlinear Network Theory. New York, NY: McGraw-Hill, 1969. D'ANGELO, H., Linear Time- Varying Systems: Analysis and Synthesis. Boston, MA: Allyn and Ba- con, 1970. GRAHAM, D., and McRuER, D., Analysis ofN onlinear Control Systems. New York, NY: Dover, 1971. HILLBORN, R. C., Chaos and Nonlinear Dynamics: An Introduction for Scientists and Engineers. New York, NY: Oxford University Press, 1994. KHALIL, H. K., Nonlinear Systems. New York, NY: Macmillan Publishing Company, 1992. LEFSCHETZ, S., Stability of Nonlinear Control Systems. Mathematics in Science and Engineering, no. 13. New York, NY: Academic Press, 1965. RICHARDS, J. A., Analysis of Periodically Time-Varying Systems. New York, NY: Springer-Verlag, 1983. STROGATZ, S. S., Nonlinear Dynamics and Chaos. Reading, MA: Addison-Wesley Publishing Com- pany, 1994. VIDYASAGER, M., Nonlinear Systems Analysis. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1993. B. 16 OTHER APPLICATIONS AND ADVANCED TOPICS Box, G. E. P., and JENKINS, G. M., Time Series Analysis: Forecasting and Control. Rev. ed. San Francisco, CA: Holden-Day, 1976. HAMILTON, J.D., Time Series Analysis. Princeton, NJ: Princeton University Press, 1994. HAYKIN, S., Adaptive Filter Theory. 2nd ed. Englewood Cliffs, NJ: Prentice Hall, 1991. HERMAN, G. T., Image Reconstruction from Projections. New York, NY: Academic Press, 1980. JoHNSON, D. H. and DuDGEON, D. E., Array Signal Processing: Concepts and Techniques. Englewood Cliffs, NJ: Prentice Hall, Inc., 1993. KAK, A. C., and SLANEY, M., Principles of Computerized Tomography. Englewood Cliffs, NJ: Pren- tice Hall, 1989. Bibliography 929 KAY, S.M., Modern Spectral Estimation: Theory and Application. Englewood Cliffs, NJ: Prentice Hall, 1988. MAcovsKI, A., Medical Imaging Systems. Englewood Cliffs, NJ: Prentice Hall, 1983. MARPLE, JR., S. L., Digital Spectra/Analysis with Applications. Englewood ~liffs, NJ: Prentice Hall, 1987. OPPENHEIM, A. V., ed., Applications of Digital Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1978. RoBINSON, E. A., et al., Geophysical Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1986. VAN TREES, H. L., Detection, Estimation, and Modulation Theory, Part II[: Radar-Sonar Signal Processing and Gaussian Signals in Noise. New York, NY: John Wiley, 1971. WmRow, B., and STEARNS, S.D., Adaptive Signal Processing. Englewood Cliffs, NJ: Prentice Hall, 1985. ANswERS 931 932 Answers Chapter 1 Answers 1.1. -0.5, -0.5, j, - j, j, 1 + j, 1 + j, 1 - j, 1 - j 1.2. 5eiO, 2eirr, 3e- jTTI2, e- jTTI3, J2eiTTI4, 2e- jTT12, J2eiTT14, eiTT12, e- jTT/12. 1.3. (a) Poo = 0, Eoo = ~ (b) Poo = 1, Eoo = 00 (c) Poo = ~, Eoo = oo (d) Poo = 0, Eoo = 1 (e) Poo = 1, Eoo = 00 (f) Poo = ~' Eoo = 00 1.4. (a) n < 1 and n > 7 (b) n < -6 and n > 0 (c) n < -4 and n > 2 (d) n < -2 and n > 4 (e) n < -6 and n > 0 1.5. (a) t > -2 (b) t > -1 (c) t > -2 (d) t < 1 (e) t < 9 1.6. (a) No (b) No (c) Yes 1.7. (a) lnl > 3 (b) all t (c) lnl < 3, lnl ~ 00 (d) ltl ~ 00 1.8. (a) A = 2, a = 0, w = 0, <f> = 7T (b) A = 1, a = 0, w = 3, <f> = 0 (c) A = 1, a = 1, w = 3, <f> = ¥ (d) A = 1, a = 2, w = 100, </> = ¥ 1.9. (a) T = ~ (b) Not periodic (c) N = 2 (d) N = 10 (e) Not periodic 1.10. 7T 1.11. 35 1.12. M = -1, no = -3 1.13. 4 1.14. At = 3, t1 = 0, A2 = -3, t2 = 1 1.15. (a) y[n] = 2x[n- 2] + 5x[n- 3] + 2x[n- 4] (b) No 1.16. (a) No (b) 0 (c) No 1.17. (a) No; e.g.,y( -7T) = x(O) (b) Yes 1.18. (a) Yes (b) Yes (c) C :5 (2n0 + 1)B 1.19. (a) Linear, not time invariant (b) Not linear, time invariant (c) Linear, tim~ invariant (d) Linear, not time invariant 1.20. (a) cos(3t) (b) cos(3t- 1) Chapter 2 Answers 2.1. (a) Yt [n] = 25[n + 1] + 45[n] + 25[n- 1] + 25[n- 2] - 25[n- 4] (b) Y2[n] = Yt[n + 2] (c) y3[n] = y2[n] 2.2. A = n - 9, B = n + 3 tn+l 2.3. 2[1 - 2 ]u[n] n - 6, 7 :5 n :5 11 2 4 [ ] l6, 12 :5 n :5 18 • • y n = 24 - n, 19 :5 n :5 23 0, otherwise Answers 933 2.5. N = 4 0 2.6. y[n] = { --~;, n < 2 , n 2: 0 2.7. (a) u[n- 2] - u[n- 6] (b) u[n- 4] - u[n- 8] (c) No (d) y[n] = 2u[n] - o[n] - o[n - 1] t + 3, -2 < t ::; -1 2.8. y(t) = l~ ~ ~t, -1<t:sO O<t:s1 0, elsewhere 2.9. A = t - 5, Bl t~ t - 4 O:st:sa 2.10. (a) y(t) = a:st:s1 a ' (b) a = 1 1 +a-t, I:st:s1+a 0, otherwise -00 < t ::; 3 2.11. (a) y(t) = { ~'-e;''--''' (l-e-6)e-3(r-5) , 5<t:Soo 3 (b) g(t) = e-3U-3)u(t- 3) - e-3(t-S)u(t- 5) (c) g(t) = dy(t) dt 2.12. A = 1 _~- 3 2.13. (a) A = ~ (b) g[n] = o[n] - ~o[n - 1] 2.14. hi (t), h2(t) 2.15. h2[n] 2.16. (a) True (b) False (c) True (d) True 2.17. (a) y(t) = 1 ~i[eC-I+ 3J)t- e-4t]u(t) (b) y(t) = i[e-t(cos3t+sin3t)-e-4t]u(t) 2.18. (114)n-l u[n- 1] 2.19. (a) a = ~. f3 = 1 (b) [2(~)n- (~)n]u[n] 2.20. (a) 1 (b) 0 (c) 0 Chapter 3 Answers 3.1. x(t) = 4 cos( it)+ 8 cose; t + 1) 3.2. x[n] = 1 + 2sin( 4 3 ; n + ;) + 4sin( 8 ; n + 5 ;) 3.3. wo = ~, ao = 2, a2 = a-2 = ~'as = a:__5 = -2} 0, k = 0 3.4. ak = { e-Jk1r/23siz~~)' k -::rf 0 934 Answers 3.5. w2 = w1, bk = e-Jkw 1 [a-k +ad 3.6. (a) X2(t), x3(t) (b) X2(t) j, k = 0 3.7. ak = { ~ k =.1= 0 Jo/-k' 3.8. x 1( t) = J2 sin( 7Tt), x2(t) = - J2 sin( 7Tt) 3.9. ao = 3, a1 = 1 - 2j, a2 = -1, a3 = 1 + 2j 3.10. ao = 0, a-1 = - j, a-2 = -2j, a-3 = -3j 3.11. A = 10, B = ~' C = 0 3.12. ck = 6 for all k 3.13. y(t) = 0 3.14. H(eirr/2) = H*(ei3rr12 ) = 2eirr14, H(ei0 ) = H(eirr) = 0 3.15. lkl > 8 3.16. (a) 0 (b) sine; n + *) (c) 0 3.17. S1 and S3 are not LTI. 3.18. S1 and S2 are not LTI. 3.19. (a) d~;t) + y(t) = x(t) (b) H(jw) = ( 1+1 Jw) (c) y(t) = h cos(t- *) 3.20. (a) dlr;t) + d~;t) + y(t) = x(t) (b) H(jw) = ( 1+ J~-w2 ) (c) -cost Chapter 4 Answers e- jw (b) 4e- jw 4.1. (a) 2+ jw 4+w2 4.2. (a) 2 cos w (b) -2j sin 2w 4.3. (a) ~ [eirr148(w - 27T) - e-Jrr148(w + 27T)] J (b) 27T8(w) + 1T[eirr!So(w - 61r) + e-Jrr188(w + 61r)] 4.4. (a) 1 + cos 41Tt (b) TTl 4 •5 • x(t) = - 2sin(3 3 rr(t-(3t/-2)/ 2)) ' t = k3rr + ~2 for nonzero integers k 4.6. (a) X1 (jw) = 2X(- jw) cos w (b) X2(jw) = ~e- i 2w X(/~) (c) X3(jw) = -w2e-Jw X(jw) 4.7. (a) neither, neither (b) imaginary, odd (c) imaginary, neither (d) real, even 4.8. (a) 2 si~(~/2) +1r8(w) (b) 2 sin(w/2) JW ~ 4.9. (a) si_n~ _ e~jw (b) sinw (c) s~nw _ co_sw }W JW W )W2 )W j 121T, -2 ::; w < 0 4.10. (a) X(jw) = - j/21T, 0 :s w < 2 (b) A = { 2~3 0, otherwise Answers 935 4.11. A = j, B = 3 4•1 2• (a ) _ 4jw (b)-'2 -lwl (l+w2)2 J 1TWe 4.13. (a) No (b) Yes (c) Yes 4.14. x(t) = Ji2[e-r - e- 2t]u(t) 4.15. x(t) = 2te-ltlu(t) 4.16. = £ 4o: lwl :=:; 1 (a) g(t) 1T o(t - k:) (b) X(jw) = { 1 < lwl :=:; 4 k = -oc 4.17. (a) False (b) True i· ltl < 1 4.18. h(t) = l- ~ + ~' 1 :=:; ltl :=:; 5 -w + ~· s < ltl < 7 0, otherwise 4.19. x(t) = e-4r u(t) 4.20. h(t) = )3e-t12 sin( Jf t)u(t) Chapter 5 Answers 5.1. (a) (b) 0.75e-;w 1.25-cosw 5.2. (a) 2 cos w (b) 2j sin(2w) 5.3. (a) ]{e.i7T14o(w - ~)- e- .i7T14o(w + ~)} (b) 47TB(w) + 1r{e.i7T18o(w- ~) + e-.i7T18o(w + ~)} 2 5.4. (a) XJ[n] = 1 +cos(-~n) (b) -4sin (~n) 2 7Tn sin[ !!.(n- ""- )] 5.5. x[n] = · ; 2 1 , and x[n] = 0 for n = :±:oo 7T n- 2) 5.6. (a) X1 (e.iw) = (2 cos w )X(e- jw) (b) X2(ejw) = CRe{X(e.iw)} (c) X3(e.iw) = - c;~2 X(e.iw)- 2j d:X(e.iw) + X(e.iw) 5.7. (a) imaginary, neither (b) real, odd (c) real, neither 1, n :=:; -2 5.8. x[n] = n + 3, -1 :=:; n :=:; 1 { 4, n 2:: 2 5.9. x[n] = -o[n + 2] + o[n + 1] + o[n] 5.10. A = 2 5.11. a = 1T 5.12. ~ :=:; lwei :=:; 1T 5.13. h2[n] = -2(~)''u[n] 5.14. h[n] = ~o[n] - f?o[n- 2] 936 Answers 5.15. We = 31T/4 5.16. (a) a = ± (b) N = 4 (c) No 5.17. bk = 4( -1)k 5.18. ak = ~(4)1kl 5.19. (a) H(efw) = (1-~rjw)l(l+~e-jw) (b) h[n] = ~(4 )nu[n] + ~(- ~ )nu[n] 4 -jw 5.20. (a) H(efw) = ~ 1- SC JW (b) y[n]- ~y[n- 1] = ~x[n- 1] Chapter 6 Answers 6.1. (a) A = IH(jwo)l (b) to = - <rH~~wo) 6.2. -tH(eiwo) = -n0(w0 ) + 21Tk for some integer k. 6.3. (a) A= 1 (b) T(w) > 0 forw > 0 6.4. (a) 2 cos(~n- 1r) (b) 2 sinC; n- 3 ;) 6.5. (a) g(t) = 2cos(2wct) (b) more concentrated 6.6. (a) g[n] = ( -l)n (b) more concentrated 6.7. (a) 1,000 Hz and 3,000 Hz (b) 800Hz and 3,200 Hz 6.8. 1T - w p ~ w ~ 1T 6.9. Final value = 2/5, to = 2/5 sec 20 w << 0.1 6.10. (a) 20 log 10 IH(jw )I = ~0 1dg10(w ), 0.1 << w << 40 { 32, w >> 40 20, w << 0.2 (b) 20log10 IH(jw)l = -20log10(w) + 6, 0.2 << w <<50 { -28, w >>50 20, w << 0.5 6.11. (a) 20 log 10 IH(jw )I = -20 log10(w) + 14, 0.5 << w << 50 { -40log10(w) + 48, w >>50 0, w << 1 (b) 20 log10 IH(jw )I = -40 log 10 w, 1 << w << 50 { -20 log10 w - 34, w >> 50 6 12 . ) _ O.Ol(Jw+40) • • H 2 ( JW - (jw+ l)(jw+8) 6.13. (a) not unique (b) unique 6.14. H1(jw) = 0.2 x 1o-4 Ciwc:~ol~~); 10) Answers 937 6.15. (a) critically damped (b) underdamped (c) overdamped (d) underdamped 6.16. y[n] + 4y[n- 1] = ~x[n] 6.17. (a) oscillatory (b) nonoscillatory 6.18. No 6.19. R~ 2ft 6.20. T(w) = 2 Chapter 7 Answers 7 .1. lw I > 5,0007T 7.2. (a) and (c) 7.3. (a) 8,0007T (b) 8,0007T (c) 16,0007T 7.4. (a) wo (b) wo (c) 2wo (d) 3wo 7 •5 • IH( J.W )I -_ { T, lwhl ::; W. e , w h ere w0 < We < T21 r _ w0 , xH( J·W ) 2 2 'j... -_ 0 0 , ot erw1se 7 .6. T max = _w__!!_w_+ 1 2 7.7. H(jw) = 2sin~~T/2) X ej(wT/2) 7.8. (a) Yes { O, k=O 4 (b) g(t) = .L akeik1rt, where ak = - j(4)k+l, k= -4 '(!)-k+ 1 J -4 ::; k ::; -1 2 ' 7.9. wo = 507T 7.10. (a) False (b) True (c) True 7.11. (a) Xe(jw) is real (b) Max{Xe(jw )} = 0.5 X 10-3 (c) Xe(jw) = 0 for lwl ~ 1,5007T (d) Xe(jw) = Xe(j(4J - 2,0007T)) for 0 ::; w ::; 2,0007T 7.12. lwl ~ 7507T 7.13. h[n] = o[n- 2] 714 h[] = _sin[TT(n-~)] · • n T1r(n- ~ )2 7.15. N = 2 7.16. x[n] = 4Cin(Trn/2) )2 1Tn 7 .17. ldeallowpass filter with cutoff frequency 7T/2 and passband gain of unity 7 .18. Ideallowpass filter with cutoff frequency 7T/4 and passband gain of 2. 7.19. (a) y[n] = sin(~:~n/3) (b) y[n] = ~o[n] 7.20. (a) Yes (b) No 938 Answers Chapter 8 Answers 8.1. m(t) = ~e- Jw,t 8.2. (a) No constraint necessary (b) lwei > 1,0007T 8.3. y(t) = 0 8.4. y(t) = sin 2007Tt 8.5. m = 2~ 8.6. A = 4 8.7. wo = 2wc. A = 2 8.8. (a) Yes (b) Yes, x(t) = {y(t)sinwcf}* 2 si;~,t 8.9. (a) lwl > 2wc (b) wo = We, A = 2 8.10. (a) X(jw) = 0 for lwl ~ 1,0007T (b) We = 1,0007T, A = 4 8.11. (a) u;' lwl 3 :S :S ~', Gain = 1 (b) A = 2latl. 4> = 1::at 8.12. 8 = 0.5 X 10-4 8.13. (a) p(O) = * (b) p(kT1 ) = 0 8.14. Y(jw) = 7TO(w -We) - ~1T o(w -We - Wm) - l2n~ o(w - We + Wm) -J J 8.15. wo = 0 and wo = 7T 8.16. 0 w 3 5 :S :S ; and ; :S w :S 7T 8.17. 0 :S lwl :S I 8.18. H(eiw) = { ~ . ~ ~ ~ :S ~ O ], 4 - w 8.19. N = 20 X 8.20. p[n] = L o[n - 2k] k=-X Chapter 9 Answers 9.1. (a) u > -5 (b) u < -5 (c) -oo :S u :S oo (d) no value of u (e) lui < 5 (0 u < 5 9.2. (a) e:~~s), CR~{s} > -5 (b) A = -1, to = -1, CR~{s} < -5 9.3. CR~{/3} = 3, dm{f3} arbitrary 9.4. 1 + 2j, 1 - 2j, CR~{s} < 1 9.5. (a) 1,1 (b) 0,1 (c) 1,0 9.6. (a) no (b) yes (c) no (d) yes 9.7. 4 9.8. two sided 9.9. x(t) = 4e-41 u(t) - 2e- 31 u(t) Answers 939 9.10. (a) lowpass (b) bandpass (c) highpass 9.11. IX(jw )I = 1 9.12. (a) not consistent (b) consistent (c) consistent 9.13. a = -1, {3 = 4 9.14. X(s) = 11[4(s2 - )2 + ~)(s2 + )2 + ~)], - 11 < CR£{s} < 11 9.15. X(s) = s2·:4 ; CR£{s} > 0, Y(s) = s2: 4 ; CR£{s} > 0 9.16. (a) 2 (b) a > 0 917 2 d y(t) + 10dy(t) + 16y(t) = 12x(t) + 3dx(t) • • dr2 dt dt 9.18. (a) H(s) = s2 +~\·+ 1 ,CR£{s} > -4 (b) Lowpass (c) 1 H(s) = s2 + 10_3s+ 1, CR£{s} > -0.0005 (d) Bandpass 9.19. (a) s~ 2 , CR£{s} > -2 (b) 1 + .:~~' CR£{s} > -2 (c) s~4 + s~2' CR£{s} > -2 9.20. (a) e-t u(t) - e-2t u(t) (b) e-tu(t) (c) 2e-t u(t) - e-2' u(t) Chapter 10 Answers 10.1. (a) lzl > 4 (b) lzl < 4 (c) lzl > 1 (d) 4 < lzl < 2 X(z) 1 7- 3 1 10.2. = m 1 _~!~-J; lzl > 5 5 {, 10.3. lal = 2, no arbitrary 10.4. poles at z = ~e±j7TI4 , ROC: lzl < ~ 10.5. (a) 1,1 (b) 2,0 (c) 1,2 10.6. (a) No (b) No (c) Yes (d) Yes 10.7. 3 10.8. two sided 10.9. x[n] = ~u[n] + ~( -2)nu[n] 10.10. (a) x[O] = 1, x[1] = ~' x[2] = - ~ (b) x[O] = 3, x[ -1] = -6, x[ -2] = 18 11 10.11. x[n] = { <4) 0 , ::; n.::; 9 0, otherwise 10.12. (a) highpass (b) lowpass (c) bandpass 10.13. (a) G(z) = 1 - z-6 ; lzl > 0 (b) X(z) = :=~=~; lzl > 0 7 10.14. (a) n0 = 2 (b) G(z) = ( z;~~~~ )2 940 Answers 10.15. ( ~ )nu[n] and (- ~ )nu[n] 10.16. (a) Not causal (b) Causal (c) Not causal 10.17. (a) Yes (b) Yes 10.18. (a) y[n] - ~y[n- 1] + ~y[n- 2] = x[n] - 6x[n- 1] + 8x[n- 2] (b) Yes 10.19. (a) XI (z) = _ L-~, lzl > ~ 1 4 (b) X2(Z) = 2, All z (c) X3(z) = I-L-~, lzl > ~ 2 10.20. (a) -(- ~)nu[n] (b) ~(-~)nu[n] + ~(~)nu[n] (c) - ~(- ~)nu[n] + ~(~)nu[n] Chapter 11 Answers 11.1. Ho(z) + 1 +~~~~~ (z) 112 H1(s)H2(s) • • 1+ Ht (s)Gt (s)+Ht (s)H2(s)G2(s) 11.3. b < -1 11.4. G(s) = ! s 11.5. - ~ < b < ~ 11.6. FIR 11.7. K > -6 11.8. -3 < k < 0 11.9. No, root locus stays on real axis 11.10. Double pole at s = -1, double zero at s = 1 11.11. 0 < k < ~ 11.12. Pole and zero positions alternate on the real axis 11.13. Unstable for all K 11.14. (a) 0 (b) 1 11.15. K > -1 11.16. K > -1 11.17. -1 < K < 4 11.18. -1 < K < 1 11.19. Unstable 11.20. Gain margin is infinite, phase margin is 2 tan- 1 j2 INDEX Absolutely summable impulse Analysis equation response, 113 continuous-time Fourier series, 191 Absolutely integrable impulse continuous-time Fourier response, 114 transform, 288 Accumulation property discrete-time Fourier series, 213 discrete-time Fourier series, 221 discrete-time Fourier transform, discrete-time Fourier transform, 361, 390 375-76 Angle criterion, 836-40 unilateral z transform, 793 Angle modulation, 611-13 Accumulator, 44 Angle (phase) of complex number, 71 Acoustic feedback, 830-32, 855 Anticausality, 695 Adders in block diagrams, 125, 126 Aperiodic convolution, 222 Additivity property, 53 Aperiodic signal, 12, 180 Aliasing, 527-34 continuous-time Fourier transform for, All-pass systems, 430, 498, 681-82 285-89 AM. See Amplitude modulation (AM) discrete-time Fourier transform for, Amplifier 359-62 chopper, 652 Associative property of LTI systems, operational, 821, 896-97 107-8 Amplitude modulation (AM), 236-37, Audio systems 322,324,583 feedback in, 830-32,855 pulse-train carrier, 601-4, 605 frequency-shaping filters in, 232 sinusoidal, 583-87 Autocorrelation functions, 65, 168, complex exponential carrier, 583-85 170-72, 738 demodulation for, 587-94 Automobile suspension system, analysis discrete-time, 619-23 of, 473-76 frequency-division multiplexing Average, weighted, 245 (FDM) using, 594-97 Averaging system, noncausal, 47 single-sideband, 597-601 sinusoidal carrier, 585-87 Band-limited input signals, 541 Amplitude-scaling factor, 483 Band-limited interpolation, 523-24 Analog-to-digital (A-to-D) Bandpass filters, 237-38, 326 converter, 535 Bandpass-sampling techniques, 564-65 941 942 Index Bandpass signal, 564-65 Closed-loop system function, 820 Bandwidth of an LTI system, 352-53 Coefficient multiplier, 125, 126 Bartlett (triangular) window, 421 Coefficients, Fourier series. See Fourier Bernoulli, D., 178 series coefficients Bilateral Laplace transform. See Laplace Communications systems, 582-653 transform amplitude modulation with pulse-train Bilinear transformation, 814-15 carrier, 601-4, 605 Bit, 610 discrete-time modulation, 619-23 Block diagram(s), 42 pulse-amplitude modulation, 604-10 cascade-form, 712, 713, 787-89 digital, 610 causal LTI systems, 708-13, intersymbol interference in, 607-10 784-89 sinusoidal amplitude modulation, direct-form, 712, 713, 787-89 583-87 first -order systems described by with complex exponential carrier, differential and difference 583-85 equations, 124-27 demodulation for, 587-94 parallel-form, 712, 713, 787-89 discrete-time, 619-23 Bode plots, 436-39 frequency-division multiplexing automobile suspension system, 475 (FDM) using, 594-97 rational frequency responses, single-sideband, 597-601 456-60 sinusoidal carrier, 585-87 Break frequency, 450 sinusoidal frequency modulation, 583, Butterworth filters, 446-47, 505, 703-6 611-19 narrowband, 613-15 Capacitor, 44 periodic square-wave modulating Carrier frequency, 584 signal, 617-19 Carrier signal, 583 wideband, 615-17 Cartesian (rectangular) form for complex Commutative property of LTI systems, number, 71 104 Cascade-form block diagrams, 712, 713, Compensation for nonideal elements, 787-89 821-22 Cascade (series) interconnection, 42 Complex conjugate, 72 Causal LTI systems, 46-48, 112-13, Complex exponential(s) 116-27 discrete-time, periodicity properties block diagram representations for, of, 25-30 708-13, 784-89 general, 20-21 first-order systems, 124-27 harmonically related, 19 Laplace transform for, 693-95, 697 linear combinations of harmonically z-transform for, 776-77 related. See Fourier series Channel equalization, 609-1 0 LTI system response to, 182-86, ""Chirp"" transform algorithm, 651 226-31 Chopper amplifier, 652 periodic, 186 Circle, unit, 743 sinusoidal amplitude modulation and, Circuit, quality of, 456 583-85 Closed-loop system, 818 Complex exponential signals, general, Closed-loop poles, 834-36 24-25 Index 943 Complex numbers, 71 distributive property of, 104-6 Conditionally stable systems, 892 operation of, 79, 85 Conjugate symmetry, 204-5, 206, 221, periodic, 222, 389-90 303-6, 375 Convolution integral, 90-102 Conjugation property evaluating, 97-102 continuous-time Fourier series, Convolution property 204-5, 206 continuous-time Fourier series, 206 continuous-time Fourier transform, continuous-time Fourier transform, 303-6 314-22 discrete-time Fourier series, 221 discrete-time Fourier series, discrete-time Fourier transform, 375 221, 222 Laplace transform, 687 discrete-time Fourier transform, unilateral, 71 7 382-88 z-transform, 770 Laplace transform, 687-88 unilateral, 793 unilateral, 717-18 Constants, time, 448 z-transform, 770-72 dominant, 500-501 unilateral, 793-94 Continuous-time signals Convolution sum, 75-90, 384-86 energy and power of, 5-7 evaluating, 81-84 examples and mathematical Correlation function, 65 representation of, 1-5 Critically damped systems, 453, 467 sampling of, 4 Cross-correlation functions, 65, Continuous-time systems, 448-60 168, 170 Bode plots for rational frequency Cutoff frequencies, 23 7 responses and, 456-60 examples of, 39-41 Damped sinusoids, 21 first-order, 448-51 Damping ratio, 452-53 interconnections of, 41-43 de offset, 207 second-order, 451-56 de sequence, 224 Continuous-to-discrete-time Deadbeat feedback, 907 conversion, 535 Decibels (dB), 232, 233, 437 Convergence. See also Region of Decimation, 549-55 convergence Degenerative (negative) feedback, continuous-time Fourier series, 822, 831-32 195-201 Delay, 44 continuous-time Fourier transform, group, 430-36 289-90 half-sample, 543-45 discrete-time Fourier series, 219-21 unit, 125 discrete-time Fourier transform, Delay time, 406 366-67 Demodulation, 585 Convolution defined, 583 aperiodic, 222 sinusoidal amplitude modulation, associative property of, 107-8 587-94 commutative property of, 104 asynchronous, 590-94 defining continuous-time unit impulse discrete-time, 622 function through, 131-32 synchronous, 587-90 944 Index Difference equations Discrete-time Fourier series pair, 213 discrete-time filters described by, Discrete-time LTI filters, 234-36 244-49 Discrete-time modulation, 619-23 first-order recursive, 244-45 Discrete-time nonrecursive filters, nonrecursive, 245-49 476-82 linear constant-coefficient. See Linear Discrete-time signals, 211 constant- coefficient difference energy and power of, 5-7 equations examples and mathematical Differencing property representation of, 1-5 discrete-time Fourier series, 222-23 sampling of, 545-55 discrete-time Fourier transform, decimation and interpolation, 375-76 549-55 z-transform, 775 impulse train, 545-49 unilateral, 793 Discrete-time systems, 461-72 Differential equations first -order, 461-65 continuous-time filters described by, second-order, 465-72 239-44 Discrete-time to continuous-time RC highpass filter, 241-44 conversion, 535 RC lowpass filter, 239-41 Dispersion, 433 linear constant-coefficient. See Linear Distortion constant- coefficient differential magnitude and phase, 428 equations quadrature, 636 Differentiating filters Distribution theory, 36, 136 continuous-time, 232-34, 235 Distributive property of LTI systems, discrete-time, 541-43 104-6 Differentiation Dominant time constant, 500-501 s domain, 688-90, 717 Double-sideband modulation (DSB), 598 time-domain, 688, 717 Downsampling, 551 z domain, 772, 793 Duality Differentiation property continuous-time Fourier transform, continuous-time Fourier series, 206 295, 309-11, 322 continuous-time Fourier transform, discrete-time Fourier transform, 390-96 306-8 between discrete-time Fourier discrete-time Fourier transform, 380 transform and continuous-time Differentiator Fourier series, 395-96 block diagram representation of, 126 digital, 541-43 Echo, 737 Digital-to-analog (D-to-A) converter, 535 Eigenfunctions, 183-84, 272 Direct Form I realization, 161-63 Eigenvalue, 183, 272 Direct Form II realization, 161-63 Elliptic filters, 446-47 Direct-form block diagrams, 712, 713, Encirclement property, 847-50 787-89 Energy-density spectrum, 312, 349, 381 Dirichlet, P.L., 180 Energy of signals, 5-7 Dirichlet conditions, 197-200, 290, 316 Envelope detector, 591, 592 Discontinuities, 198-200 Envelope function, 285 Discrete Fourier transform (DFT) for Equalization, 649 finite-duration signals, 417-18 channel, 609-10 Index 945 Equalizer, zero-force, 649 continuous-time differentiating, Equalizer circuits, 232, 233 232-34,235,541 Euler, L., 178 discrete-time, described by difference Euler's relation, 71 equations, 244-49 Even signals, 13-14 first-order recursive, 244-45 continuous-time Fourier series, 206 nonrecursive, 245-49 discrete-time Fourier series, 221 elliptic, 446-47 Exponentials. See Complex finite impulse response (FIR), 122 exponential(s) discrete-time, 476-82 Exponential signals, 14-30 linear-phase, causal, symmetric, continuous-time complex, 15-21 579-80 discrete-time complex, 21-30 frequency-selective, 231, 236-39, real, 22, 23 318, 423 Extension problems, 137 bandpass, 237-38, 326 highpass, 237-38, 241-44, Fast Fourier transform (FFT), 182, 418 374-75, 387 Feedback. See also Linear feedback ideal, 237-39, 439-44 systems lowpass, 237, 318, 321-22, 374-75, applications of, 820-32 384,440,442,443 angular position of telescope, time-domain properties of ideal, 816-18 439-44 audio, 830-32, 855 with variable center frequency, compensation for nonideal 325-27 elements, 821-22 frequency-shaping, 231, 232-36 inverse system design, 820-21 differentiating filters, 232-34, 235 inverted pendulum, 818-19 discrete-time LTI filters, 234-36 population dynamics, 824-26 infinite impulse response (IIR), sampled-data systems, 826-28 123,476 stabilization of unstable systems, matched, 170-72, 275 823-26 moving-average, 476-82 tracking systems, 828-30 nonideal, 444-47 closed-loop poles, 836-38 Filtering, defined, 231 deadbeat, 907 Final-value theorem, 690-91 degenerative (negative), 822, 831-32 Finite-duration signals, discrete Fourier destabilization caused by, 830-32 transform for, 417-18 positive (regenerative), 831-32 Finite impulse response (FIR) filters, 122 proportional, 823 discrete-time, 245-49, 476-82 proportional-plus-derivative, 824 linear-phase, causal, symmetric, Type/, 906 579-80 Feedback interconnection, 43 Finite sum formula, 73 Feedback path, system function of, 820 First difference, 111. See also Filter(s), 231-50 Differencing property Butterworth, 446-47, 505, 703-6 First harmonic components, 187 continuous-time, described by First-order continuous-time systems, differential equations, 239-44 448-51 RC highpass filter, 241-44 First-order discrete-time systems, RC lowpass filter, 239-41 461-65 946 Index First-order recursive discrete-time filters, Fourier series coefficients 244-45 continuous-time, 191, 286 Forced response, 118 convolution property of, 222 Forward path, system function of, 820 discrete-time, 212, 213 Fourier, Jean Baptiste Joseph, 179-81 real and imaginary parts of, 216, 217 Fourier series, 177-283. See also Fourier series pair, discrete-time, 213 Filter(s) Fourier transform, 180 continuous-time, 186-211 fast (FFT), 182, 418 analysis equation of, 191 geometric evaluation from pole-zero conjugation and conjugate plot, 674-82, 763-67 symmetry of, 204-5 all-pass systems, 681-82 convergence of, 195-201 first-order systems, 676-77, determination of, 190-95 763-65 Dirichlet conditions, 197-200 second-order systems, 677-81, duality between discrete-time 765-67 Fourier transform and, 395-96 inverse, 284, 288 examples, 205-11 magnitude-phase representation of, Gibbs phenomenon, 200-201, 219 423-27 linear combinations of harmonically Fourier transform, continuous-time, related complex exponentials, 284-357. See also Laplace 186-90 transform linearity property of, 202 analysis equations, 300 multiplication property of, 204 for aperiodic signal, 285-89 Parseval's relation for, 205, 206 convergence of, 289-90 square wave, 193-95, 200, 201, Dirichlet conditions, 290, 316 209, 218-19 even signals, 304 synthesis equation of, 191 examples of, 290-96 table of properties, 206 Gibbs phenomenon, 294 time reversal property of, 203 imaginary part of, 304 time scaling property of, 204 impulse train, 299-300 time shifting property of, 202-3 inverse, 284, 288 discrete-time, 211-26 odd signals, 304 analysis equation of, 213 periodic signals, 296-300 convergence of, 219-21 properties of, 300-330 determination of, 212-21 conjugation and conjugate first -difference property of, symmetry, 303-6 222-23 convolution, 314-22 linear combinations of harmonically differentiation and integration, related complex exponentials, 306-8 211-12 duality, 295, 309-11, 322 multiplication property of, 222 linearity, 301 Parse val's relation for, 223 multiplication, 322-27 square wave, 219-20, 224 Parse val's relation, 312-14 synthesis equation of, 213 tables of, 328-30 table of properties, 221 time and frequency scaling, 308-9 historical perspective, 178-82 time shifting, 301-3 LTI systems and, 226-31 real part of, 304 Index 947 rectangular pulse signal, 293-94 Nyquist, 519 sine functions, 295 passband, 237 symmetric periodic square wave, sampling, 516 297-98 stopband, 237 synthesis equation for, 288, 300, 314 Frequency-division multiplexing (FDM), two-dimensional, 356-57 594-97 of unit impulse, 292 Frequency-domain characterization. See Fourier transform, discrete-time, Time-domain and frequency- 358-422 domain characterization analysis equation, 361, 390 Frequency response, 227-28 convergence issues associated with, continuous-time delay, 543 366-67 continuous-time ideal band-limited development of, 359-62 differentiator, 541 examples of, 362-66 discrete-time delay, 543 finite-duration signals, 417-18 discrete-time filter, 542 impulse train, 371-72, 376 first-order system, 677 periodic signals, 367-72 highpass filter, 374-75 properties of, 372-90 lowpass filter, 374-75 conjugation and conjugation discrete-time ideal, 384 symmetry, 375 ideal, 318 convolution, 382-88 LTI systems, 427-39 differencing and accumulation, LTI systems analysis and, 316 375-76 open -loop, 821 differentiation in frequency, 380 raised cosine, 629 duality, 390-96 rational, Bode plots for, 456-60 linearity, 373 second-order systems, 677-78, 680 multiplication, 388-90 Frequency scaling of continuous-time Parseval's relation, 380--82 Fourier transform, 308-9 periodicity, 373 Frequency-selective filter, 231, 236-39, table of, 390, 391 318,423 time expansion, 377-80 bandpass, 237-38, 326 time reversal, 376-77 highpass, 237-38, 241-44, time shifting and frequency 374-75, 387 shifting, 373-75 ideal, 237-39 rectangular pulse, 365-66 time-domain properties of, synthesis equation, 361, 390 439-44 unit impulse, 367 lowpass, 237 z-transform and, 743 variable center frequency, 325-27 Fourier transform pairs, 288 Frequency-shaping filters, 231, continuous-time, 329 232-36 discrete-time, 361, 392 differentiating filters, 232-34, 235 Frequency(ies) discrete-time LTI filters, 234-36 carrier, 584 Frequency shifting property cutoff, 237 continuous-time Fourier series, 206 differentiation in, 380 continuous-time Fourier transform, fundamental, 17-18 311, 328 instantaneous, 613 discrete-time Fourier series, 221 948 Index discrete-time Fourier transfortn, Fourier series coefficients, 216, 217 373-75 Impulse response Frequency shift keying (FSK), 646 absolutely integrable, 114 Fundamental components, 187 absolutely summable, 113 Fundamental frequency, 17-18 associated with group delay, 435-36 Fundamental period, 12 causal LTI system, 112 continuous-time periodic signal, 186 continuous-time ideal lowpass discrete-time periodic signal, 211 filter, 442 discrete-time ideallowpass filter, 442 Gain, 428 first-order discrete-time system, linear feedback systems, 835, 858-66 461-62 tracking system, 829-30 ideallowpass filter, 321-22, 384 General complex exponentials, 20-21, second-order systems, 677-78 24-25 discrete-time systems, 466-68 Generalized functions, 36, 136 Impulse train Gibbs phenomenon, 200-201, 219, 294 continuous-time Fourier series of, Group delay, 430-36 208-10 continuous-time Fourier transform of, Half-sample delay, 543-45 299-300 Hanning window, 422 discrete-time Fourier transform of, Harmonically related complex 371-72, 376 exponentials, 19 Impulse-train sampling, 516-20, 545-49 Harmonic analyzer, 200 Incrementally linear systems, 56 Harmonic components, 187 Independent variable, 3-4 Heat propagation and diffusion, 180 transformation of, 7-14 Higher order holds, 526-27 Infinite impulse response (IIR) filters, High pass filter, 23 7-3 8 123,476 frequency response of, 374-75 Infinite sum formula, 73, 89 ideal bandstop characteristic, 387 Infinite Taylor series, 277 RC, 241-44 Initial-value theorem Highpass-to-lowpass Laplace transform, 690-91 transformations, 498 z-transform, 773-74 Hilbert transform, 351 Instantaneous frequency, 613 Hold(s) Integral, convolution, 90-102 higher order, 526-27 evaluating, 97-102 zero-order, 520-22, 523-26 Integration, in time-domain, 690, 717 Homogeneity (scaling) property, 53 Integration property continuous-time Fourier series, 206 Ideal frequency-selective filter, 237-39 continuous-time Fourier transform, time-domain properties of, 439-44 306-8 Idealization, 67 Integrator, 126-27 Identity system, 44 Interconnection Image processing convolution property and analysis of, differentiating filters for, 232-34, 235 386-87 phase representation and, 425-27 distributive property of convolution Imaginary part and, 105 complex number, 71 feedback, 43 Index 949 LTI systems differentiation in s domain, 688-90 Laplace transform for, 707-8 differentiation in time-domain, 688 system functions for, 707-8 initial- and final-value theorems, z-transform for, 784 690-91 parallel, 42-43 integration in time-domain, 690 Interference, intersymbol, 607-10 linearity, 683-84 Intermediate-frequency (IF) stage, 596 s-domain shifting, 685 Interpolation, 549-55 table of, 691-92 band-limited, 523-24 time scaling, 685-87 linear, 522-23, 526 time shifting, 684 reconstruction using, 522-27 region of convergence for, 657-58, Intersymbol interference (lSI), 607-10 662-70 Inverse Fourier transform, 284, 288 left-sided signal, 666, 669 Inverse Laplace transform, 670-73 rational transform, 669 Inverse LTI system, 734 right-sided signal, 665-66, 669 Inverse system design, 820-21 two-sided signal, 666-68 Inverse systems, 45-46 s-plane representation of, 660, Inverse z-transform, 757-63 663-64 examples of, 758-763 as system function, 693, 701-13 Inverted pendulum, 818-19 block diagram representations, Invertibility of LTI systems, 109-11 708-13 Invertible systems, 45-46 Butterworth filters, 703-6 interconnections of LTI systems, Lacroix, S .F., 180 707-8 Lag,phase,491-92 transform pairs, 692-93 Lag network, 890 unilateral, 714-20 Lagrange, J.L., 178-79, 180 examples of, 714-16 Laguerre polynomials, 738 properties of, 716-19 Laplace, P.S. de, 180 solving differential equations using, Laplace transform, 654-740 719-20 bilateral, 655, 714 zeros of, 660 geometric evaluation of, 674-82 Lead,phase,491-92 all-pass systems, 681-82 Lead network, 890 first-order systems, 676-77 Left-half plane, 666 second-order systems, 677-81 Left- sided signal, 666 inverse, 670-73 Linear, time-invariant (LTI) systems, LTI system analysis and 41, 56, 74-176. See also Linear characterization using, 693-706 constant -coefficient difference causality, 693-95, 697 equations; Linear constant- linearity constant -coefficient coefficient differential equations differential equations, 698-700 bandwidth of, 352-53 stability, 695-98 causal, 46-48, 112-13, 116-27 poles of, 660 block diagram representations for, pole-zero plot of, 660-62, 674-82 124-27, 708-13, 784-89 properties of, 682-92 described by linear constant- conjugation, 687 coefficient difference equations, convolution, 687-88 116, 121-24, 396-99 950 Index described by linear constant- unit impulse response of cascade of coefficient differential equations, two, 107 116, 117-21 unit step response of, 115-16 real-part sufficiency property windowing in design of, 420-21 of, 417 z-transform to analyze and continuous-time, 90-102 characterize, 774-83 convolution-integral representation causality, 776-77 of, 94-102 stability, 777-79 described by linear constant- Linear constant -coefficient difference coefficient differential equations, equations, 116, 121-24, 396-99, 330-33 779-81 representation in terms of impulses, finite impulse response (FIR) 90-94 system, 122 convolution property and, 319-21 infinite impulse response (IIR) discrete-time, 75-90 system, 123 convolution -sum representation of, natural responses as solutions to, 122 77-90 nonrecursive, 122 representation in terms of impulses, recursive, 122, 123 75-77 unilateral z-transform to solve, feedback. See Linear feedback 795-96 systems Linear constant-coefficient differential filtering with, 231, 234-36 equations, 116, 117-21, 330-33, Fourier series and, 226-31 698-700 frequency response of, 316 block diagram representation of, magnitude-phase representation of, 124-27 427-39 initial rest condition, 119-20 interconnection of natural responses as solutions to, Laplace transform for, 707-8 119, 121 z-transform for, 784 particular and homogeneous solutions inverse, 109-11, 734 to, 118-19 Laplace transform to analyze and unilateral Laplace transform to solve, characterize, 693-706 719-20 causality, 693-95, 697 Linear feedback systems, 816-908. linearity constant -coefficient See also Feedback differential equations, closed-loop, 818 698-700 gain and phase margin, 835, stability, 695-98 858-66 with and without memory, 108-9 Nyquist stability criterion, properties of, 103-8 846-58 associative, 107-8 continuous-time, 850-55 commutative, 104 discrete-time, 856-58 distributive, 104-6 encirclement property, 847-50 response to complex exponentials, open-loop, 818 182-86,226-31 properties and uses of, 819-20 stability for, 113-15 root-locus analysis of, 832-46 system functions for interconnections angle criterion, 836-40 of, 707-8 end points, 836 Index 951 equation for closed-loop poles, Matched filter, 170-72, 275 834-36 Measuring devices, imperfect, 355-56 properties of, 841-46 Memory, LTI systems with and without, sampled-data, 826-28 108-9 Linear interpolation, 522-23, 526 Memoryless systems, 44 Linearity, 53-56 Michelson, Albert, 200 Linearity property Modulating signal, 583 continuous-time Fourier series, Modulation. See also Sinusoidal 202, 206 amplitude modulation continuous-time Fourier angle, 611-13 transform, 301 defined, 582 discrete-time Fourier series, 221 discrete-time, 619-23 discrete-time Fourier transform, 373 double-sideband (DSB), 598 Laplace transform, 683-84 percent, 591 unilateral, 71 7 phase, 611-13 z-transform, 767 pulse-amplitude, 604-10 unilateral, 793 digital, 610 Log magnitude-phase diagram, 861-62, intersymbol interference in, 607-10 864-66 pulse-code, 61 0 Log-magnitude plots, 436-39 sinusoidal frequency, 583, 611-19 Lossless coding, 46 narrowband,613-15 Lowpass filters, 237 periodic square-wave modulating frequency response of, 374-75 signal, 617-19 ideal wideband, 615-17 continuous-time, 440, 442, 443 Modulation index, 591, 614 discrete-time, 384, 442, 443 Modulation property, 323. See also frequency response of, 318 Multiplication property impulse responses of, 321-22, Monge, G., 180 384,442 Moving-average filters, 245-47, 476-82 step response of, 443 Multiplexing RC, 239-41 defined, 583 Lowpass-to-highpass transformations, frequency-division (FDM), 594-97 498 quadrature, 646 LTI. See Linear, time-invariant (LTI) time-division (TDM), 604, 605 systems Multiplication property continuous-time Fourier series, Magnitude of complex number, 71 204, 206 Magnitude-phase representation continuous-time Fourier transform, Fourier transform, 423-27 322-27 frequency response of LTI systems, discrete-time Fourier series, 221, 222 427-39 discrete-time Fourier transform, group delay, 430-36 388-90 linear and nonlinear phase, 428-30 Multiplicities, 155 log-magnitude and Bode plots, 436-39 Narrowband frequency modulation, Mass-spring -dashpot mechanical 613-15 system, 824 Natural frequency, undamped, 452-53 952 Index Natural responses, 119, 121, 122 continuous-time Fourier series, Negative (degenerative) feedback, 822, 205, 206 831-32 discrete-time Fourier transform, Networks, lag and lead, 890 380-82 Nonanticipative system, 46-47 discrete-time Fourier series, 223 Noncausal averaging system, 47 Partial-fraction expansion, 909-20 Nonideal filters, time-domain continuous-time signals and systems and frequency-domain and, 910-16 characterization of, 444-47 discrete-time signals and systems and, Nonrecursive filters. See Finite impulse 916-20 response (FIR) filters Passband edge, 445 N onrecursi ve linear constant -coefficient Passband frequency, 237 difference equations, 122 Passband ripple, 445 Normalized functions, 273 Pendulum, inverted, 818-19 Nyquist frequency, 519 Percent modulation, 591 Nyquist plots, 853-58 Period, sampling, 516 Nyquist rate, 519, 556 Periodic complex exponentials, Nyquist stability criterion, 846-58 16-20, 186 continuous-time, 850-55 Periodic convolution, 222, 389-90 discrete-time, 856-58 continuous-time Fourier series, 206 encirclement property, 847-50 discrete-time Fourier series, 221 Periodicity property of discrete-time Odd harmonic continuous-time periodic Fourier transform, 373 signal, 262 Periodic signals, 11-13. See also Odd signals, 13-14 Fourier transform; Fourier series; continuous-time Fourier series, Periodic complex exponentials; 206 Sinusoidal signals discrete-time Fourier series, 221 continuous-time, 12 Open-loop system, 818 discrete-time, 12 Open-loop frequency response, 821 power of, 312 Operational amplifiers, 821, 896-97 Periodic square wave Orthogonal functions, 273-75 continuous-time, 193-95, 200-201, Orthogonal signals, 280-81 209, 218-19, 285-86, 297-98 Orthonormal functions, 273-75 discrete-time, 219-20, 224 Orthonormal signals, 280-81 Periodic train of impulses Oscilloscope, sampling, 534 continuous-time Fourier series of, Overdamped system, 453 208-10 Oversampling, 529 continuous-time Fourier transform of, Overshoot, 454 299-300 discrete-time Fourier transform of, Parallel-form block diagrams, 712, 713, 371-72, 376 787-89 Periodic signal, odd-harmonic, 262 Parallel interconnection, 42-43 Phase (angle) of complex number, 71 Parity check, 610 Phase lag, 491-92 Parseval's relation Phase lead, 491-92 continuous-time Fourier transform, Phase margin in linear feedback systems, 312-14 858-66 Index 953 Phase modulation, 611-13 Real part Phase reversal, 532 complex number, 7,1 Phase shift, 428, 636 Fourier series coefficients, 216, 217 Plant, 828 Real-part sufficiency, 350, 417 Polar form for complex number, 71 Rectangular (Cartesian) form for Poles complex number, 71 closed-loop, 834-36 Rectangular pulse Laplace transform, 660 continuous-time, 293-94 Pole-zero plot(s) discrete-time, 365-66 all-pass systems, 682 Rectangular window, 420 first-order systems, 676-77, 763-65 Recursive (infinite impulse response) Laplace transforms, 660-62, 674-82 filters, 123, 476 second-order systems, 678-80, Recursive linear constant-coefficient 765-67 difference equations, 122, 123 z-transform, 763-67 Regenerative (positive) feedback, Polynomials, Laguerre, 738 831-32 Population dynamics, feedback model of, Region of convergence 824-26 Laplace transform~ 657-58, 662-70 Positive (regenerative) feedback, left-sided signal, 666, 669 831-32 rational transform, 669 Power of signals, 5-7, 312 right -sided signal, 665-66, 669 Power-series expansion method, 761-63 two-sided signal, 666-68 Principal-phase function, 433, 434 z-transforih, 743-44, 748-57 Proportional feedback system, 823-24 bounded by poles or infinity, 756 Proportional (P) control, 905 centered about origin, 74 8-49 Proportional-plus-derivative finite-duration sequence, 74 9-50 feedback, 824 left-sided sequence, 751, 752 Proportional-plus-integral (PI) right-sided sequence, 750-51' 752 control, 905 two-sided sequence, 7 51-53 Proportional-plus-integral-plus- Right-half plane, 666 differential (PID) control, 905 Right-sided signal, 665-66 Pulse-amplitude modulation, 604-10 Ringing, 443, 454 digital, 610 Rise time, 352-53, 444 intersymbol interference in, 607-10 Root-locus analysis, 832-46 Pulse-code modulation, 610 angle cri~erion, 836-40 Pulse-train carrier, 601-4, 605 end points, 836 equation for closed-loop poles, 834-36 Quadrature distortion, 636 properties of, 841-46 Quadrature multiplexing, 646 Running sum of discrete-time Fourier Quality of circuit, 456 series, 221 Raised cosine frequency response, 629 Sampled-data feedback systems, Rational frequency responses, Bode plots 826-28 for, 456-60 Sampling, 29, 39, 514-81 RC highpass filter, 241-44 aliasing, 527-34 RC lowpass filter, 239-41 bandpass signal, 564-65 Real exponential signals, 15, 22, 23 continuous-time signals, 4 954 Index discrete-time processing of transformation of independent continuous-time signals, variable, 7-14 534-45 examples of, 8-11 digital differentiator, 541-43 unit impulse and unit step functions, half-sample delay, 543-45 30-38 discrete-time signals, 545-55 continuous-time, 32-38 decimation and interpolation, discrete-time, 30-32 549-55 Sine functions, 295 impulse train, 545-49 Single-sideband sinusoidal amplitude impulse-train, 516-20, 545-49 modulation, 597-601 reconstruction using interpolation, Singularity functions, 36, 67, 127-36. 522-27 See also Unit impulse with zero-order hold, 520-22, 523-26 Sinusoidal amplitude modulation, Sampling frequency, 516 583-87 Sampling function, 516 complex exponential carrier, Sampling oscilloscope, 534 583-85 Sampling period, 516 demodulation for, 587-94 Sampling property of continuous-time asynchronous, 590-94 impulse, 35 synchronous, 587-90 Sampling theorem, 514-15, 518 discrete-time, 619-23 Scaling (homogeneity) property, 53 frequency-division multiplexing Scaling in z domain, 768-69, 793 (FDM) using, 594-97 Scrambler, speech, 633-34 single-sideband, 597-601 Second harmonic components, 187 with sinusoidal carrier, 585-87 Second-order continuous-time systems, Sinusoidal frequency modulation, 583, 451-56 611-19 Second-order discrete-time systems, narrowband,613-15 465-72 periodic square-wave modulating Series (cascade) interconnection, 42 signal, 617-19 Shifting property wideband, 615-17 s-domain, 717 Sinusoidal signals, 14-30, 186 unilateral z transform, 794-95 continuous-time complex, 15-21 Sifting property discrete-time, 24 continuous-time impulse, 90-92 discrete-time complex, 21-30 discrete-time unit impulse, 77 real, 22, 23 Signal(s), 1-38. See also Periodic signals Sinusoids, damped, 21 continuous-time and discrete-time, Sliding, 85 1-7 Spectral coefficient. See Fourier series energy and power of, 5-7 coefficients examples and mathematical Spectrum, 288, 361. See also Fourier representation of, 1-5 transform sampling of, 4, 545-55 energy-density, 312, 349, 381 exponential and sinusoidal, Speech scrambler, 633-34 14-30, 186 Square wave, periodic continuous-time complex, 15-21 continuous-time, 193-95, 200-201, discrete-time complex, 21-30 209, 218-19, 285-86, 297-98 real, 22, 23 discrete-time, 219-20, 224 Index 955 Stability, 48-50 z-transform as, 781-89 feedback systems, 858-66 block diagram representations, LTI systems, 113-15, 695-98, 784-89 777-79 interconnection of LTI systems, 784 Stabilization with feedback, 823-26 Systems, 38-56 Step-invariant transformation, 828, 908 continuous-time and discrete-time, Step response, 115-16 38-43 automotive suspension system, 476 examples of, 39-41 continuous-time ideal lowpass first-order, 448-51, 461-65 filter, 443 interconnections of, 41-43 discrete-time ideal lowpass filter, 443 second-order, 451-56, 465-72 first-order discrete-time system, properties of, 44-56 461,463 causality, 46-48 second-order discrete-time systems, invertible and inverse, 45-46 466,468 linearity, 53-56 Stopband edge, 445 memory and memoryless, 44-45 Stopband frequency, 237 stability, 48-50 Stopband ripple, 445 time-invariance, 50-53 Stroboscopic effect, 533-34 Sufficiency, real-part, 350, 417 Taylor series, infinite, 277 Summer, 44 Telescope, angular position of, 816-18 Superposition integral. See Convolution Time, 3-4 integral Time advance property of unilateral Superposition property, 53 z-transform, 793 Superposition sum. See Convolution sum Time constants, 448 Suspension system, analysis of, 473-76 dominant, 500-501 Symmetry, conjugate, 204-5, 206, 221, Time delay property of unilateral 303-6, 375 z-transform, 793 Synthesis equation Time-division multiplexing (TDM), continuous-time Fourier series, 191 604, 605 continuous-time Fourier transform, Time-domain and frequency-domain 288, 300, 314 characterization, 423-513 discrete-time Fourier series, 213 continuous-time systems, 448-60 discrete-time Fourier transform, Bode plots for rational frequency 361, 390 responses and, 456-60 System function(s), 227 first -order, 448-51 closed-loop, 820 second-order, 451-56 feedback path, 820 discrete-time systems, 461-72 forward path, 820 first -order, 461-65 interconnections of LTI systems, second-order, 465-72 707-8 examples of, 4 72-82 Laplace transform as, 693, 701-13 automobile suspension system, block diagram representations, 473-76 708-13 discrete-time nonrecursive filter, Butterworth filters, 703-6 476-82 interconnections of LTI systems, magnitude-phase representation 707-8 Fourier transform, 423-27 956 Index frequency response of LTI systems, Transformation 427-39 bilinear, 814-15 nonideal filters, 444-47 highpass-to-lowpass, 498 Time expansion property of independent variable, 7-14 discrete-time Fourier series, 221 lowpass-to-highpass, 498 discrete-time Fourier transform, step-invariant, 828, 908 377-80 Transition band, 445 z-transform, 769-70 Transmodulation (t ransmultiplexing), unilateral, 793 discrete-time, 623, 624 Time invariance, 50-53, 77 Triangular (Bartlett) window, 421 Time reversal property, 8 Trigonometric series, 178-79 continuous-time Fourier series, Two-sided signal, 666-68 203, 206 Type l feedback system, 906 continuous-time Fourier transform, 309 Undamped natural frequency, 452-53 discrete-time Fourier series, 221 Undamped system, 454 discrete-time Fourier transform, Underdamped systems, 467 376-77 Undersampling, 527-34 Laplace transform, 687 Unilateral Laplace transform, 714-20 z-transform, 769 examples of, 714-16 Time scaling property, 8, 9, 10 properties of, 716-19 continuous-time Fourier series, solving differential equations using, 204, 206 719-20 continuous-time Fourier transform, Unilateral z-transform, 742, 789-96 308-9 examples of, 779-92 discrete-time Fourier series, 221 properties of, 792-95 discrete-time Fourier transform, solving difference equations using, 377-80 795-96 Laplace transform, 685-87 Unit circle, 743 unilateral, 717 Unit delay, 125 z-transform, 769-70 Unit doublets, 132-36, 172 unilateral, 793 Unit impulse, 30-38 Time shifting property, 8-9, 10 cascade of two LTI systems and, 107 continuous-time Fourier series, continuous-time, 32-38, 127-36, 292 202-3, 206 defining through convolution, continuous-time Fourier transform, 131-32 301-3 as idealized short pulse, 128-31 discrete-time Fourier series, 221 sifting property of, 90-92 discrete-time Fourier transform, discrete-time, 30-32, 367 373-75 sifting property of, 77 Laplac~ transform, 684 representation of continuous-time LTI z-transform, 767-68 systems in terms of, 90-94 Time window, 420 representation of discrete-time LTI Tracking systems, 828-30 systems in terms of, 75 -77 Transfer function. See System Unit impulse response, discrete-time, function(s) 77-90. See also Impulse response Index 957 Unit ramp function, 135 LTI systems analysis and Unit step functions, 30-38 characterization using, 774-83 continuous-time, 32-38 causality, 776-77 discrete-time, 30-32 linear constant -coefficient Unit step response of LTI systems, difference equations, 779-81 115-16 stability, 777-79 Unstable systems, feedback to stabilize, pole-zero plot, 763-67 823-26 properties of, 767-74 Unwrapped phase, 433, 434 conjugation, 770 Upsampling, 552, 553 convolution, 770-72 differentiation in z-domain, 772 Variable, independent, 3-4 initial-value theorem, 773-74 transformation of, 7-14 linearity, 767 scaling in z-domain, 768-69 Walsh functions, 170, 276 table of, 77 5 Wave, square. See Periodic square wave time expansion, 769-70 Wavelengths, 181 time reversal, 7 69 Weighted average, 245 time shifting, 767-68 Wideband frequency modulation, region of convergence for, 74 3-44, 615-17 748-57 Windowing, 420-22 bounded by poles or infinity, 756 centered about origin, 748-49 Zero-force equalizer, 649 finite-duration sequence, 749-50 Zero-input response, 55-56, 720 left-sided sequence, 751, 752 Zero-order hold, 520-22, 523-26 right-sided sequence, 750-51, 752 Zeros of Laplace transform, 660 two-sided sequence, 751-53 Zero-state response, 720 system functions, 781-89 z-transform, 741-815 block diagram representations, bilateral, 742, 789 784-89 defined, 742 for interconnection of LTI discrete-time Fourier transform systems, 784 and, 743 unilateral, 742, 789-96 geometric evaluation of, 763-67 examples of, 789-796 first-order systems, 763-65 properties of, 792-95 second-order systems, 765-67 solving difference equations using, inverse, 757-63 795-96 examples of, 79-92 z-transform pairs, 774, 776"
